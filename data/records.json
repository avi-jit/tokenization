{"papers":[{"url":"https://www.semanticscholar.org/paper/ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning","venue":"Workshop on Representation Learning for NLP","year":2022,"referenceCount":23,"citationCount":2,"influentialCitationCount":0,"publicationDate":"22/04/2022","authors":"Md. Mofijul Islam,Gustavo Aguilar,Pragaash Ponnusamy,Clint Solomon Mathialagan,Chengyuan Ma,Chenlei Guo","citations":[{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"}],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"8ae8e5e840c5f43d4d72cbc4595690fba01aa799","title":"LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation"},{"paperId":"c9b56cb026a38e39bb0228faac57accd6f65e6f7","title":"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"7f9ca11d122957dc59088543f6cd9f907c00d0d3","title":"Neural Models of Text Normalization for Speech Applications"},{"paperId":"e07f2c383a34217a9403a4d58a4e7d61e57d9163","title":"Character Eyes: Seeing Language through Character-Level Taggers"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"5b1516c87818084dc5d195cc274e1ee8923210d2","title":"Neural Cross-Lingual Named Entity Recognition with Minimal Resources"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":null,"title":"Xnli: Evaluating crosslingual sentence representations"},{"paperId":"664ec878de4b7170712baae4a7821fc2602bba25","title":"Learning to Generate Reviews and Discovering Sentiment"},{"paperId":null,"title":"Characterlevel language modeling with hierarchical recurrent neural networks"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"}],"id":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","summary":"This work proposes a vocabulary-free neural tokenizer by distilling segmentation information from heuristic-based subword tokenization, which allows end-to-end task learning, resulting in optimal task-specific tokenization."},{"url":"https://www.semanticscholar.org/paper/9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":79,"citationCount":3,"influentialCitationCount":0,"publicationDate":"06/06/2022","authors":"Ayush Kaushal,Kyle Mahowald","citations":[{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"}],"references":[{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"3dcfa05a1c162e6cab927c5b08d0444f7b6691f4","title":"Probing Classifiers: Promises, Shortcomings, and Advances"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"911b7539e964782670e555930b291de16fa971c5","title":"Flexible Generation of Natural Language Deductions"},{"paperId":"8b723be33e62bf5bd9278769244f1c13a9510898","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP"},{"paperId":"538f8e8a36e70ca408f2c5fb6f10f303c52fc317","title":"Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"1d7f3297924a9dd90cfc0df522ebe9138c28b46f","title":"Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"cf592385909a1e3e9a428d8d6d8f427ab70b60a9","title":"Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"1b94ebedacda0c21a4b8a40a5a40afcea4cc719a","title":"When Combating Hype, Proceed with Caution"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":null,"title":"Gpt-j-6b: A 6 billion parameter autoregressive language model"},{"paperId":null,"title":"Why don’t people use characterlevel machine translation? arXiv preprint arXiv:2110.08191"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"738c6d664aa6c3854e1aa894957bd595f621fc42","title":"Information-Theoretic Probing for Linguistic Structure"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"5034cf3e2483b9e80fa43c6d75ed309a3e028980","title":"Ré-entraîner ou entraîner soi-même ? Stratégies de pré-entraînement de BERT en domaine médical (Re-train or train from scratch ? Pre-training strategies for BERT in the medical domain )"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"9eb4cd1a4b4717c97c47e3dc4563a75779ae9390","title":"BERT is Not an Interlingua and the Bias of Tokenization"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"199ff73d2f728e997f860b62a2322823d3e3d9e8","title":"Designing and Interpreting Probes with Control Tasks"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"73cf24d4c8ca809727553925a31253a649702582","title":"Meaning to Form: Measuring Systematicity as Information"},{"paperId":"98050eeda3b79a18f555480881e1f3bc7d447882","title":"What Kind of Language Is Hard to Language-Model?"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"70f7542579aeec99d684754b646e245481a71bda","title":"On the Complexity and Typology of Inflectional Morphological Systems"},{"paperId":"9852ae077c7da6a9d178acaa2b44a335289507a6","title":"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"2019. Bert is not an interlingua and"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"84aa9e1685779019fc162a697d0cdb98066f808c","title":"Subword-level Composition Functions for Learning Word Embeddings"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"97856a4c31fec7b189446a130aab4cbfa8d6a3e8","title":"Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure"},{"paperId":"31f2c1c42cb179820565839868b369c4752e735d","title":"Words cluster phonetically beyond phonotactic regularities"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":null,"title":"spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing"},{"paperId":"645283253f7e79077879ae6b7382661411e8d004","title":"Sound–meaning association biases evidenced across thousands of languages"},{"paperId":"39f22dd35bd8c00095355eab614f7d2ef9034bba","title":"What do you know about an alligator when you know the company it keeps"},{"paperId":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"7b269488183363d49e44d8b63d26d2f1be86bca3","title":"Learning novel phonological neighbors: Syntactic category matters"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"d208fa50239973289e7fab6c5af5d956c3cef3ad","title":"How arbitrary is language?"},{"paperId":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging"},{"paperId":"db734a0e1dc65fe3fe2eef474aefba6d083f54dd","title":"A New Algorithm For Data Compression"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"6ae9ec8288df868505b07bb46911b3e8619055f4","title":"Adding part-of-speech information to the SUBTLEX-US word frequencies"},{"paperId":"bbec2094640e47652234dd8fcfe218fd04886ab5","title":"The indeterminacy of word segmentation and the nature of morphology and syntax"},{"paperId":"cfdd423c8672a7b178ea85d56079328df4eea647","title":"Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit"},{"paperId":"f3054a53bb066ab229cc6f50dfdcddc5d3174060","title":"On the Origin of Language"},{"paperId":null,"title":"Probing for Character Information We use off-the-shelf APIs for lemmatization and WordNet from NLTK (Apache License 2.0"},{"paperId":"2b591d66ffa5f2e017be3c440b492ab3439ca7c5","title":"Exploring systematicity between phonological and context-cooccurrence representations of the mental lexicon"},{"paperId":"1d51de5328d6dd8336033a5b5c8e334e14000880","title":"Probabilistic Modeling in Psycholinguistics: Linguistic Comprehension and Production"},{"paperId":"978aebcea5ffb37f916fe9f1a9eeea8618b82117","title":"The differential role of phonological and distributional cues in grammatical categorisation"},{"paperId":"af25dc4cd32ef790690b10878c10d109d52d2021","title":"The Psychological Reality of Phonaesthemes"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"63470730dea262829c56722739bc53bbee99c6b8","title":"Using sound to solve syntactic problems: the role of phonology in grammatical category assignments."},{"paperId":"2461c19e8859ad54107b0bb4e11ca885cab4096f","title":"On Psychological Reality"},{"paperId":"43cd44b0c9b8bdd5d790fec110d6c0b6eab036b1","title":"Phonetic symbolism in English word-formation"},{"paperId":"1dbcdb06369d365dba0e909811825daf7dca6061","title":"Course in General Linguistics"},{"paperId":null,"title":"Fuzzy match and misspellings Fuzzy match and misspellings Fuzzy match and misspellings \"S1GNATURE"},{"paperId":null,"title":"Some examples of variations in tokenization for two example words"},{"paperId":null,"title":"Exact match and whitespaces Exact match and whitespaces \" signature"}],"id":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","summary":"The mechanisms through which PLMs acquire English-language character information during training are investigated and it is argued that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings."},{"url":"https://www.semanticscholar.org/paper/188cd686fb2200f237f688dbda7f64ffc75e67ac","title":"Subword Pooling Makes a Difference","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":17,"citationCount":13,"influentialCitationCount":1,"publicationDate":"22/02/2021","authors":"Judit Ács,'Akos K'ad'ar,András Kornai","citations":[{"paperId":"7abfcf54eeac3a5c298717a4f0fe8a5daceeaa42","title":"Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling"},{"paperId":"9518b6f3389bb85346eb7240c1b6e8ab1170e1b3","title":"AdaSL: An Unsupervised Domain Adaptation framework for Arabic multi-dialectal Sequence Labeling"},{"paperId":"62f488d3178ce448c9916caf131b83d22ff5d5ff","title":"Comparison of text preprocessing methods"},{"paperId":"00298eef5c6fcd87b572a876e1e13df9bca85bce","title":"UM6P-CS at SemEval-2022 Task 11: Enhancing Multilingual and Code-Mixed Complex Named Entity Recognition via Pseudo Labels using Multilingual Transformer"},{"paperId":"95773a105d1c14f4864c5da1023659783afe914e","title":"FiNER: Financial Numeric Entity Recognition for XBRL Tagging"},{"paperId":"90c6a48dd32f91b08fdf5a0cf9a37a54799229b1","title":"A Latent-Variable Model for Intrinsic Probing"},{"paperId":"e9cb8609d27c5e52ca7e9dfee4086c223116bb32","title":"Extended Overview of HIPE-2022: Named Entity Recognition and Linking in Multilingual Historical Documents"},{"paperId":"a42965768043c57610142d700666f59daf42d18d","title":"An ELECTRA Model for Latin Token Tagging Tasks"},{"paperId":"6da8debf6b4236454662dc3c2542ee8e6fc1e168","title":"ID10M: Idiom Identification in 10 Languages"},{"paperId":"4a3a92c876e984b564b21fdf18fd2a98bd15837d","title":"Evaluating Transferability of BERT Models on Uralic Languages"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"2ed528f2c1cb8185cd048dc37103515cb2245d44","title":"IWCLUL 2021 The Seventh International Workshop on Computational Linguistics of Uralic Languages"},{"paperId":"f7c14e79d3eb1d24b4184d106244be1672113ce2","title":"WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER"}],"references":[{"paperId":"421beeabd2b981754c2871055a8d48e08cd2a5ad","title":"Attentive Pooling with Learnable Norms for Text Representation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"31c872514c28a172f7f0221c8596aa5bfcdb9e98","title":"75 Languages, 1 Model: Parsing Universal Dependencies Universally"},{"paperId":"526cae4863eb15b5bc39112449c2d5fdf1db85b2","title":"Multilingual Constituency Parsing with Self-Attention and Pre-Training"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"697e110df76fe33e232f019d7e44097af3572abd","title":"Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms"},{"paperId":null,"title":"Universal Dependencies 2.3. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (ÚFAL)"},{"paperId":null,"title":"Baseline needs more love: On simple wordembedding-based models and associated pooling"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"adb00206a14768c73e2255ab1a5123f17c9260cf","title":"Improving the Arabic Pronunciation Dictionary for Phone and Word Recognition with Linguistically-Based Pronunciation Rules"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"d9c71db75046473f0e3d3229950d7c84c09afd5e","title":"Text Chunking using Transformation-Based Learning"},{"paperId":null,"title":"We extract 2000 train, 200 dev and 200 test sentences for each task. We keep UD's original splits, in other words, all of our train sentences come from"}],"id":"188cd686fb2200f237f688dbda7f64ffc75e67ac","summary":"This paper investigates how the choice of subword pooling affects the downstream performance on three tasks: morphological probing, POS tagging and NER, in 9 typologically diverse languages and shows that mBERT is better than XLM-RoBERTa in all 9 languages."},{"url":"https://www.semanticscholar.org/paper/7771aa7badc3375a31bfac8dc47755ff5d5c7780","title":"From characters to words: the turning point of BPE merges","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":31,"citationCount":4,"influentialCitationCount":0,"publicationDate":2021,"authors":"Ximena Gutierrez-Vasques,C. Bentz,O. Sozinova,T. Samardžić","citations":[{"paperId":"befaa6e28da4eaaa42de5f5c1a31516de7fb26da","title":"Towards robust complexity indices in linguistic typology"},{"paperId":"8e23530814b61a0cc42233f0248a9d4e106a8ce4","title":"TeDDi Sample: Text Data Diversity Sample for Language Comparison and Multilingual NLP"},{"paperId":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}],"references":[{"paperId":"0b407aaf4a2ffff17d46d24c7b87c3dbaadc38aa","title":"Investigating the effects of i-complexity and e-complexity on the learnability of morphological systems"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"52b3692a99269307144b16db112af8490d32de07","title":"Productivity and Predictability for Measuring Morphological Complexity"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"96c75eaa732028f92065c290723de5ae4c347ab8","title":"Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche"},{"paperId":"98050eeda3b79a18f555480881e1f3bc7d447882","title":"What Kind of Language Is Hard to Language-Model?"},{"paperId":"d323d011a3214116a18d623501bca9a31d33cf4c","title":"Are All Languages Equally Hard to Language-Model?"},{"paperId":"375f5318839cdabb664f4cc00b0d6eb75a973319","title":"Word and Paradigm Morphology"},{"paperId":"236f10fa4a054e67f7a7cab334f31a80a36781a2","title":"The Entropy of Words - Learnability and Expressivity across More than 1000 Languages"},{"paperId":"a921e014249395496c22783fe58d293746af852b","title":"From Characters to Words to in Between: Do We Capture Morphology?"},{"paperId":"5cedb6606803fd849a73cd1a33160a926e9878cf","title":"A Rich Morphological Tagger for English: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax"},{"paperId":"05b359264e6fed937cb76e9e434935944c5a5b0f","title":"The statistical trade-off between word order and word structure – Large-scale evidence for the principle of least effort"},{"paperId":"339b602bd0c82c3ff6074ec2bc61221faef05ae0","title":"A Comparison Between Morphological Complexity Measures: Typological Data vs. Language Corpora"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"c4c9d7dce2d1733f98ae268ca80eb35a3cc6e71b","title":"An information-theoretic approach to assess linguistic complexity"},{"paperId":"6140f733fad17f9f12bb10e072e8babe723c1e9c","title":"Word Order Typology through Multilingual Word Alignment"},{"paperId":"dbf336a8cb911bdda6ef06bc584015a8ed2b1565","title":"Creating a massively parallel Bible corpus"},{"paperId":"a3cec34e50c0e267514b9b92f40b84b886ca570e","title":"From the extraction of continuous features in parallel texts to visual analytics of heterogeneous areal-typological datasets"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"896265da201dcc75d94faf17c995f59762e38bb6","title":"Morphological Organization: The Low Conditional Entropy Conjecture"},{"paperId":"cea92778250fd37c74d2d407f5e244ce64f6a38a","title":"Lexical typology through similarity semantics: Toward a semantic map of motion verbs"},{"paperId":"01d743a38a9759a18c220c598c42c96e6a87b4ca","title":"The type-token relationship in Slavic parallel texts"},{"paperId":"5e56afcd36262b17a36d5e3f26368ef4f9b5bce1","title":"Parallel texts: using translational equivalents in linguistic typology"},{"paperId":"a4f74ef44f176b0a2d2bdf8576388087ca4086d8","title":"Word-based morphology"},{"paperId":"6d12a1d23b21a9b170118a56386552bc5d4727de","title":"A Mathematical Theory of Communication"},{"paperId":"f193a31921ffd25b0a514dcfe71f61b11a53572d","title":"Entropy Measures, Maximum Entropy Principle and Emerging Applications"},{"paperId":"7ce5098ff4e490614157d36f3be865b90ce61bd2","title":"Cognition, quantitative linguistics, and systemic typology"},{"paperId":"384dafe7a6a3a576fd34127e0c27b00477728b6d","title":"Measuring Linguistic Complexity: The Morphological Tier"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"6713c11cd2363c24059c4dad65c68cb3e3e2b479","title":"Information Theory and Redundancy"},{"paperId":"c2dfe0f672e80b1ad1fc0fcae9345ea28e5deae2","title":"A Quantitative Approach to the Morphological Typology of Language"}],"id":"7771aa7badc3375a31bfac8dc47755ff5d5c7780","summary":"It is shown that text entropy values tend to converge at specific subword levels: relatively few BPE merges lead to the most similar distributions across languages."},{"url":"https://www.semanticscholar.org/paper/17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":2,"influentialCitationCount":1,"publicationDate":"08/04/2022","authors":"Edward Gow-Smith,Harish Tayyar Madabushi,Carolina Scarton,A. Villavicencio","citations":[{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"}],"references":[{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224","title":"Superbizarre Is Not Superb: Derivational Morphology Improves BERT’s Interpretation of Complex Words"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"7fbf9d5e1bbac56efba21021d4577620ba3b3ee5","title":"MorphyNet: a Large Multilingual Database of Derivational and Inflectional Morphology"},{"paperId":"3f02219184319aa8ca1ef182e6b091b6a7539a04","title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"44189a062182cab428e6d7ea010819f2fe978533","title":"DagoBERT: Generating Derivational Morphology with a Pretrained Language Model"},{"paperId":"8515c83e6562415021b4c536c28ed198f05a8691","title":"Emerging trends: Subwords, seriously?"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"2e808fa44bd52e7234f04f9fb8819ff840608586","title":"Morfessor EM+Prune: Improved Subword Segmentation with Expectation Maximization and Pruning"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"d56c1fc337fb07ec004dc846f80582c327af717c","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},{"paperId":"a9c3a009d754a110379574b069b48c0b4c75db40","title":"Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"},{"paperId":"2d505eb3d7cba1b2513e2879ac8f025fcb481b77","title":"UniMorph 3.0: Universal Morphology"},{"paperId":null,"title":"BART: Denoising sequence-to-sequence pretraining for natural language"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"6c1beae31b92c70b42ebeb99e5598d73bff6eea5","title":"NEZHA: Neural Contextualized Representation for Chinese Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"7da138d704ef0fc055825fa132f5c452ed3fb52a","title":"LADEC: The Large Database of English Compounds"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"3cea3a3eb5b83612a7f8da49fde0d7244058ee06","title":"MorphoLex: A derivational morphological database for 70,000 English words"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"5c71a835e13a834cbe8f1241159a7237b820dc92","title":"Morfessor 2.0: Python Implementation and Extensions for Morfessor Baseline"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":null,"title":"Japanese 693 and korean voice search Attention is all 707 you need"},{"paperId":"56010a55d49ac1f42355538f494427fd22402be1","title":"Exploring the Limits"},{"paperId":"aaa8f3b7504476727e72806e6f9ded01e13cd494","title":"The English Lexicon Project"},{"paperId":"19b8d1def48288a22cc4256a9a620cdf7f294d2f","title":"Morpheme Segmentation Gold Standards for Finnish and English"},{"paperId":null,"title":"On the role of derivational affixes in recognizing complex words"},{"paperId":"0c5043108eda7d2fa467fe91e3c47d4ba08e0b48","title":"Unsupervised Discovery of Morphemes"}],"id":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","summary":"An alterna- 013 tive tokenisation approach where spaces are treated as individual tokens are experiments, which show that the modi- 022 ﬁed algorithms give improved performance on downstream NLP tasks that involve handling 024 complex words, whilst having no detrimental effect on performance in general natural lan- 026 guage understanding tasks."},{"url":"https://www.semanticscholar.org/paper/373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation","venue":"AMTA","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Salvador Carrión-Ponz,F. Casacuberta","citations":[],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":null,"title":"Autonmt: A framework to streamline the research of seq2seq models"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed","title":"Sparse is Enough in Scaling Transformers"},{"paperId":"9436806f98d0c71245135d5d45025427bb36cd33","title":"Optimizing Transformer for Low-Resource Neural Machine Translation"},{"paperId":"3fd45fc420a882ab2fba3166ef08f376cc758ad0","title":"On Long-Tailed Phenomena in Neural Machine Translation"},{"paperId":"0156fb2096c39839384c0cae0194e123e01cc575","title":"Character-Level Transformer-Based Neural Machine Translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"295065d942abca0711300b2b4c39829551060578","title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"6dc710b46bc510a42af487a3ac220e4fabf4d518","title":"VOLT: Improving Vocabularization via Optimal Transport for Machine Translation"},{"paperId":"10b9eee99b2632359d4d26f991e765bff8d91dee","title":"Revisiting Low-Resource Neural Machine Translation: A Case Study"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"7e77d37e7fbfe90f58e4e96e8198903f364d6402","title":"FearNet: Brain-Inspired Model for Incremental Learning"},{"paperId":"8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a","title":"Learning without Forgetting"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"5151d6cb3a4eaec14a56944d58338251fca344ab","title":"Overcoming catastrophic forgetting in neural networks"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"22bcad7bed8b82f98d5144b7b57fbfc01d46f64a","title":"Neurogenesis Deep Learning"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"53c9443e4e667170acc60ca1b31a0ec7151fe753","title":"Progressive Neural Networks"},{"paperId":"84ca430856a92000e90cd728445ca2241c10ddc3","title":"Very Deep Convolutional Networks for Natural Language Processing"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"1eb09fecd75eb27825dce4f964b97f4f5cc399d7","title":"On the Properties of Neural Machine Translation: Encoder–Decoder Approaches"},{"paperId":"105146a7872835a52c8c5c55a3aae62c5d8852a1","title":"Substring-based machine translation"},{"paperId":"e9ad27106dd487893bcc1cc12bbf645168c60f87","title":"Can We Translate Letters?"},{"paperId":null,"title":"Bleu : A method for automatic eval - 662 uation of machine translation A call for clarity in reporting BLEU 666 scores"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"}],"id":"373588ff1fb9f7590db000a04de8d838b1516e5a","summary":"This work suggests that quasi-character-level models have practically the same generalization capabilities as character-based models but at lower computational costs and appear to help achieve greater consistency between domains than standard subword- level models, although the catastrophic forgetting problem is not mitigated."},{"url":"https://www.semanticscholar.org/paper/d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP","venue":"ArXiv","year":2021,"referenceCount":186,"citationCount":20,"influentialCitationCount":1,"publicationDate":"20/12/2021","authors":"Sabrina J. Mielke,Zaid Alyafeai,Elizabeth Salesky,Colin Raffel,Manan Dey,Matthias Gallé,Arun Raja,Chenglei Si,Wilson Y. Lee,Benoît Sagot,Samson Tan","citations":[{"paperId":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models"},{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"092312cb2a59502a2db7ffea1e31c19ad2eecc7c","title":"H2-Golden-Retriever: Methodology and Tool for an Evidence-Based Hydrogen Research Grantsmanship"},{"paperId":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"58b322dbfd76a50d501415d9441053e726e6b8fa","title":"Prototyping Deep Learning Applications with Non-Experts: An Assistant Proposition"},{"paperId":"8a37503b60898afee407162f27c8376d3137a2fd","title":"Evolving Label Usage within Generation Z when Self-Describing Sexual Orientation"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"fe35bf433202b4e130709d042aae09dbf4d76232","title":"Text Generation with Text-Editing Models"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"cd48780a4bb7513e6b81cf3bb993b4f4fb962e13","title":"Word-order Typology in Multilingual BERT: A Case Study in Subordinate-Clause Detection"},{"paperId":"61d9162eea5aceb5b78c2d1230f18d8dfe10a208","title":"The Risks of Machine Learning Systems"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","title":"Beyond Characters: Subword-level Morpheme Segmentation"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE"}],"references":[{"paperId":"13b4d9a1096113db63158590e9d2e58744766e3b","title":"Unsupervised Word Segmentation with Bi-directional Neural Language Model"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"d7aa3c65eae71776ccc3cddafb1d4b288deae2cb","title":"A Masked Segmental Language Model for Unsupervised Natural Language Segmentation"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","title":"How BPE Affects Memorization in Transformers"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"58e13e1fed9b28e50efcaff611772801d7980c80","title":"Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation"},{"paperId":"fd400801136261ca329aa84b11307be0e28571f5","title":"Recurrent Neural Networks with Mixed Hierarchical Structures for Natural Language Processing"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"38e99b3b56d03ad77cc058381bcc90f6cae5cb75","title":"The Effectiveness of Morphology-aware Segmentation in Low-Resource Neural Machine Translation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"5e8da0a332906a2cc5262483ddc27eb9b792dbaf","title":"Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine Translation: The Case of Fon Language"},{"paperId":"b73b3b086465c629170fd7be5a25164652655f26","title":"One Size Does Not Fit All: Finding the Optimal N-gram Sizes for FastText Models across Languages"},{"paperId":"fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224","title":"Superbizarre Is Not Superb: Derivational Morphology Improves BERT’s Interpretation of Complex Words"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"72eed6570807c1d3c9a60289c7fc6813277e1e98","title":"Fast WordPiece Tokenization"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"1c59de25af45cef20d846ec7454251e8237d45d1","title":"A Statistical Extension of Byte-Pair Encoding"},{"paperId":"7771aa7badc3375a31bfac8dc47755ff5d5c7780","title":"From characters to words: the turning point of BPE merges"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"d7a1c8da1b8163f5bbd4d12e58cad5002947105e","title":"Word Shape Matters: Robust Machine Translation with Visual Embedding"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"110c13fbf4ff87b52ee1fd9eb2d3616c839ceb41","title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank"},{"paperId":"9ae116139ae77ca6d78e162e9639681adef5761c","title":"The Unstoppable Rise of Computational Linguistics in Deep Learning"},{"paperId":"82665af3fa153ef6def31cc42a1c0aac39afa61b","title":"Neural Polysynthetic Language Modelling"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"9d92905edc1a5d027b0827d1309bc6ac03dd94a0","title":"Character-Level Translation with Self-attention"},{"paperId":"651c01b897f18e31bab3ad482154a980ac92c638","title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"2e808fa44bd52e7234f04f9fb8819ff840608586","title":"Morfessor EM+Prune: Improved Subword Segmentation with Expectation Maximization and Pruning"},{"paperId":"76a211ec3b4b96f37038c3993c81597cb1ea7a4a","title":"Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation"},{"paperId":"3b2538f84812f434c740115c185be3e5e216c526","title":"Cross-Lingual Ability of Multilingual BERT: An Empirical Study"},{"paperId":"229aff74294275d31ded5c8e6e80f6aa6a88fcfd","title":"A Latent Morphology Model for Open-Vocabulary Neural Machine Translation"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":null,"title":"Unsupervised cross-lingual representation"},{"paperId":null,"title":"Linguist vs"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"7c848b37605cdfe6d2d4778b06e66c0710c3ec34","title":"On the Importance of Word Boundaries in Character-level Neural Machine Translation"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"3504dd566b25f24f44cf647731370760a537b483","title":"Stochastic Tokenization with a Language Model for Neural Text Classification"},{"paperId":"8a7e9f91d949764d6159c114d4feb961ec07b32d","title":"Training Hybrid Language Models by Marginalizing over Segmentations"},{"paperId":"98050eeda3b79a18f555480881e1f3bc7d447882","title":"What Kind of Language Is Hard to Language-Model?"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"2fa3f7ce620a1c7155daef6620dd6bb0e01934f3","title":"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"},{"paperId":"b401aca53c04cec865e1a733090a2c60ca7ebe97","title":"Glyce: Glyph-vectors for Chinese Character Representations"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"20ff6bd5fe67a2d73fb3423216ec2b7369d5a091","title":"Comparing neural‐ and N‐gram‐based language models for word segmentation"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"9852ae077c7da6a9d178acaa2b44a335289507a6","title":"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Characterbased nmt with transformer"},{"paperId":null,"title":"Exploring BERT's vocabulary"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"a98c340f316fc041dd071efcec1fbddf176282d9","title":"Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"64b8361a133ad545cbf9c035e109ef8461b31f67","title":"Super Characters: A Conversion from Sentiment Classification to Image Classification"},{"paperId":"1e6d69f8cfd698b8139cde557252753b22c7d712","title":"BPE and CharCNNs for Translation of Morphology: A Cross-Lingual Comparison and Analysis"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"589e237c9b15c49f15b4989a4684792d9e705cce","title":"Revisiting the Hierarchical Multiscale LSTM"},{"paperId":"2e638ac785703b7332393e5f2d8064b03a3fccf8","title":"Universal Word Segmentation: Implementation and Interpretation"},{"paperId":"19885d5b288e6043eb989d296cc12bc8dbead8e3","title":"Morphological and Language-Agnostic Word Segmentation for NMT"},{"paperId":"d323d011a3214116a18d623501bca9a31d33cf4c","title":"Are All Languages Equally Hard to Language-Model?"},{"paperId":"771c1cb5fb161231e9aaa0a189caba672256a880","title":"Using Morphological Knowledge in Open-Vocabulary Neural Language Models"},{"paperId":"c79ce664a42d7867846ee468180bf9ad8c9e17b2","title":"CoNLL-UL: Universal Morphological Lattices for Universal Dependency Parsing"},{"paperId":"57faf160097049d14399ac6d317bbe4d1b8aa2de","title":"Compositional Representation of Morphologically-Rich Input for Neural Machine Translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"287efcd4f62c84c45818a71cf5bdd70364ff10eb","title":"An Evaluation of Two Vocabulary Reduction Methods for Neural Machine Translation"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"5616064812996ab1fae525f9679f300c7c307895","title":"Towards Neural Phrase-based Machine Translation"},{"paperId":"834724753187ba86709f9d859740f23861d42191","title":"Meaningless yet meaningful: Morphology grounded subword-level NMT"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"8e39bc4e1711e941881f64935b203a93259acb50","title":"Glyph-aware Embedding of Chinese Characters"},{"paperId":"a3a39cebbe65cc21009182808300f8f83af2213c","title":"A Dataset for Sanskrit Word Segmentation"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"45e3f381d16e6d6db5449b7a44b7bdf294a8a822","title":"Multiscale sequence modeling with a learned dictionary"},{"paperId":"ed5003e6a2b97dd4b65c5190ccb88b9c45b032b8","title":"Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"},{"paperId":"db253b17043b6a86e02173b6aa597664b0c7f256","title":"Learning Character-level Compositionality with Visual Features"},{"paperId":"8da6b21fb29fcbe3d6596a8b87242ccb3b06a894","title":"From Segmentation to Analyses: a Probabilistic Model for Unsupervised Morphology Induction"},{"paperId":"5f09d0cbd59e7c84b912692b6ded42107e8f9733","title":"Chinese–Spanish neural machine translation enhanced with character and word bitmap fonts"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"a486e2839291111bb44fa1f07731ada123539f75","title":"Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"},{"paperId":"2d7782c225e0fc123d6e227f2cb253e58279ac73","title":"Improving Neural Language Models with a Continuous Cache"},{"paperId":"105788dd22393d5a4333c167814ec3d38c7d6612","title":"Latent Sequence Decompositions"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"f797fd44b9ddd5845611eb7a705ca9464a8819d1","title":"Very Deep Convolutional Networks for Text Classification"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":"acd87e4f672f0b92ea4164414c213560c23bee52","title":"Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"6b904d6e84c98c6ce22ce6923224b205a2a24ee1","title":"Segmental Recurrent Neural Networks"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"c2e1e362e27ff5ced52bfc9bde3ca165e7eafe76","title":"Neural machine translation using bitmap fonts"},{"paperId":"6dab1c6491929d396e9e5463bc2e87af88602aa2","title":"Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"},{"paperId":"f22e78a284df04b57d8fc8e622acf1942f288c61","title":"An Unsupervised Method for Uncovering Morphological Chains"},{"paperId":"746bd8b886be53b4f3bdd27d64de5e6181ba8195","title":"Morfessor FlatCat: An HMM-Based Method for Unsupervised and Semi-Supervised Learning of Morphology"},{"paperId":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"5522764282c85aea422f1c4dc92ff7e0ca6987bc","title":"A Clockwork RNN"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"1e6c802a7663e309d47ee45487c03333fd388014","title":"A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability"},{"paperId":"876e51a39ae8bf9b1d48f4e219bbc4e0705ee1cd","title":"Text segmentation with character-level text embeddings"},{"paperId":"62c76ca0b2790c34e85ba1cce09d47be317c7235","title":"Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"ae9768ea90a257f710756fcf19a852d3545fb5c1","title":"Language-independent compound splitting with morphological operations"},{"paperId":"ea9a3a0aaee44cf95d61cd7134c26b8d5e5f4b0e","title":"The sequence memoizer"},{"paperId":"aa5b0f501670a76eac7deaf3027ded218971b633","title":"Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models"},{"paperId":"3df68bed55021596d09b44769d4d74f7495379d2","title":"Empirical Comparison of Evaluation Methods for Unsupervised Learning of Morphology"},{"paperId":"1fd7fc06653723b05abe5f3d1de393ddcf6bdddb","title":"SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"},{"paperId":"dd79821ce068f263ab7d6c2037a83c20e562dff3","title":"Morpho Challenge 2005-2010: Evaluations and Results"},{"paperId":"9e022fa8effeaaff77d86be0a3d1bf50d899b5b8","title":"Painless Unsupervised Learning with Features"},{"paperId":"0dda3342416a221eb840f1499d01a47b05d7c7f4","title":"Learning from Unseen Data"},{"paperId":"51c88cf10b75ed1a5445660cc64cee3d1c6fd8c5","title":"Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling"},{"paperId":"46f31f9069bb934498a288126053bcab01ff34aa","title":"A Bayesian framework for word segmentation: Exploring the effects of context"},{"paperId":"41fbb2cbace32055d26616fa8c00da5e4782c776","title":"Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars"},{"paperId":"07943f471d65614d3f545f1d37be4ec25739d218","title":"Probabilistic ParaMor"},{"paperId":"57458bc1cffe5caa45a885af986d70f723f406b4","title":"A unified architecture for natural language processing: deep neural networks with multitask learning"},{"paperId":"b773bc1bed0976c0e3f8ebe4ad883bf582e5902c","title":"SxPipe 2: architecture pour le traitement pré-syntaxique de corpus bruts"},{"paperId":"1375dca2b57bdadbfc263e641dd95d7826a06073","title":"ParaMor: Minimally Supervised Induction of Paradigm Structure and Morphological Analysis"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"02711b7e8c73779db802cb00b6cbf407d0872b66","title":"Unsupervised models for morpheme segmentation and morphology learning"},{"paperId":"e04fa3599e22b92336a06eca79417f4d6af040f3","title":"Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models"},{"paperId":"a99d6ebe6e583b752d1639a9002dd60581983dc1","title":"Contextual Dependencies in Unsupervised Word Segmentation"},{"paperId":"6bf6c77b895069239ef7a180aee5332ed7b40c79","title":"A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes"},{"paperId":"24b20f7b118588055346f4ac5cdb1fe22e886dda","title":"Interpolating between types and tokens by estimating power-law generators"},{"paperId":"fcc5e6ab7e7d88835732e58661d264d0b6e9381c","title":"MAF: a Morphosyntactic Annotation Framework"},{"paperId":"a218eb044d91fce34126a2e672ac0a0935a8a7f1","title":"INDUCING THE MORPHOLOGICAL LEXICON OF A NATURAL LANGUAGE FROM UNANNOTATED TEXT"},{"paperId":"cdaae7a8f0db8b280266606004f1c6f164a13f6d","title":"Empirical Methods for Compound Splitting"},{"paperId":"af4c762a7a4f4803dfca6223955aba43895aa606","title":"Lexicon-directed segmentation and tagging in Sanskrit"},{"paperId":null,"title":"Finite-state morphology: Xerox tools and techniques"},{"paperId":"0c5043108eda7d2fa467fe91e3c47d4ba08e0b48","title":"Unsupervised Discovery of Morphemes"},{"paperId":"0649db23e4d672d3122435e48030880ad6b0b0db","title":"A Probabilistic Model for Learning Concatenative Morphology"},{"paperId":"f7a8bd1a62d22de3a3db5edfcc364b462cd4558e","title":"A Bayesian Model for Morpheme and Paradigm Identification"},{"paperId":"30545fe538a773b57e06b4217cd495ef84230bc8","title":"Knowledge-Free Induction of Inflectional Morphologies"},{"paperId":"9f834ee11902ada79b874e7fe5072159d72a0f9f","title":"Unsupervised Learning of the Morphology of a Natural Language"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","title":"A Neural Probabilistic Language Model"},{"paperId":"4d1dac08bb3d960baa88e4ff3477ec834446d056","title":"Minimally Supervised Morphological Analysis by Multimodal Alignment"},{"paperId":"efada5827fd7eedecb4a7dc101caa4509a9f770f","title":"Empirical Estimates of Adaptation: The chance of Two Noriegas is closer to p/2 than p2"},{"paperId":"63d165e916b851a0aeaafaa528f6aad99a78d04f","title":"Distributional cues in morpheme discovery: A computational model and empirical evidence"},{"paperId":"72661eb1c6b3c94128c6edc394e22d2c9ffde8d0","title":"Offline dictionary-based compression"},{"paperId":"d4e8bed3b50a035e1eabad614fe4218a34b3b178","title":"An empirical study of smoothing techniques for language modeling"},{"paperId":"7ec7bc1d7f35558640ac05e94f9b854ec17d7bc0","title":"Linguistic Structure as Composition and Perturbation"},{"paperId":"b13813b49f160e1a2010c44bd4fb3d09a28446e3","title":"Hierarchical Recurrent Neural Networks for Long-Term Dependencies"},{"paperId":"01fa57bd91f731522c861404d29e4604ba6ac6d3","title":"A hierarchical Dirichlet language model"},{"paperId":null,"title":"CELEX2"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"0b44fcbeea9415d400c5f5789d6b892b6f98daff","title":"Building a Large Annotated Corpus of English: The Penn Treebank"},{"paperId":"db6ae486a695efc02910b1dc08eeba13b50d5ca8","title":"Tokenization as the Initial Phase in NLP"},{"paperId":"50c770b425a5bb25c77387f687a9910a9d130722","title":"Learning Complex, Extended Sequences Using the Principle of History Compression"},{"paperId":"c69201d091dd92699fd90a17b9e3407319726791","title":"Neural Sequence Chunkers"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","title":"Finding Structure in Time"},{"paperId":"66e5551231b018f32004b53b559f8144f38e3c4e","title":"Stochastic Complexity in Statistical Inquiry Theory"},{"paperId":"72d5278244aa183a580fb4df47f87949de7f1184","title":"On the syntactic structure of protein sequences and the concept of grammar complexity"},{"paperId":"3d951ad1164c17217a24e46b57f9b31cea1b2b96","title":"Data compression via textual substitution"},{"paperId":"a651bb7cc7fc68ece0cc66ab921486d163373385","title":"An algorithm for suffix stripping"},{"paperId":"5e37f888248c44a8d8098ccd07e91cc06e8b4d42","title":"AN ALGORITHM FOR THE SEGMENTATION OF AN ARTIFICIAL LANGUAGE ANALOGUE"},{"paperId":null,"title":"Robbie Jimmerson, Vasilisa Andriyanets"}],"id":"d617f51833860dc50d202af7f80be71304b2e994","summary":"This survey connects several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subwordbased approaches based on learned segmentation have been proposed and evaluated."},{"url":"https://www.semanticscholar.org/paper/15db055ac7cb49bac96cc1be1270d9bf2b0b1770","title":"Beyond Characters: Subword-level Morpheme Segmentation","venue":"SIGMORPHON","year":2022,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Ben Peters,André F. T. Martins","citations":[{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"73cb4755f15502b9cb34797d487b6fb77c7ba4b4","title":"Word-level Morpheme segmentation using Transformer neural network"}],"references":[{"paperId":"b522b0ece0cc38d7a3a5b25c5c19ce626556a897","title":"CLUZH at SIGMORPHON 2022 Shared Tasks on Morpheme Segmentation and Inflection Generation"},{"paperId":"238d45d79209f9f28f01a02dc337888942499e2f","title":"Morfessor-enriched features and multilingual training for canonical morphological segmentation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"1e6171d9937a36b2dba6b622bd5857f2407d1b2e","title":"Smoothing and Shrinking the Sparse Seq2Seq Search Space"},{"paperId":"28d7562a716099a5147509bfd84f76de08b1192c","title":"Searching for Search Errors in Neural Morphological Inflection"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"05cfd96eed27ceb2aa35285991a745a5cd119abc","title":"If Beam Search Is the Answer, What Was the Question?"},{"paperId":"8c122ff82dedf72fa7ef5342622667bf5848916e","title":"One-Size-Fits-All Multilingual Models"},{"paperId":"1e0a14db59c5a1a18c83dfbeb17eb5be9e67623d","title":"Modeling Word Formation in English–German Neural Machine Translation"},{"paperId":"82665af3fa153ef6def31cc42a1c0aac39afa61b","title":"Neural Polysynthetic Language Modelling"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"d3231772937a2182b2377d028417245c49868dd1","title":"On NMT Search Errors and Model Errors: Cat Got Your Tongue?"},{"paperId":"3cee801d10f410f0feb1a2390776a01ba2765001","title":"Sparse Sequence-to-Sequence Models"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"e6272f72f780fa4a14cc83569abb7389bcf592d1","title":"IT–IST at the SIGMORPHON 2019 Shared Task: Sparse Two-headed Models for Inflection"},{"paperId":null,"title":"Exploring bert’s vocabulary"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"834724753187ba86709f9d859740f23861d42191","title":"Meaningless yet meaningful: Morphology grounded subword-level NMT"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"69ffe3277011087b55afaeaef0a3933160beee9a","title":"Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":"c75ecddc5eda6b71b54bf8ef40f6fa9da1d8f108","title":"Neural Morphological Analysis: Encoding-Decoding Canonical Segments"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"1f462943c8d0af69c12a09058251848324135e5a","title":"Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"},{"paperId":"b20c0758a38bd5a4083f64eff53af924499a8e29","title":"Possible generalization of Boltzmann-Gibbs statistics"},{"paperId":"145c0b53514b02bdc3dadfb2e1cea124f2abd99b","title":"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"},{"paperId":null,"title":"Mathias Creutz, and Mikko Kurimo. 2022. Morfessorenriched features and multilingual training for canonical morphological segmentation"}],"id":"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","summary":"This paper presents DeepSPIN’s submissions to the SIGMORPHON 2022 Shared Task on Morpheme Segmentation, and challenges the assumption that models for morphological tasks should be trained at the character level by building a transformer that generates morphemes as sequences of unigram language model-induced subwords."},{"url":"https://www.semanticscholar.org/paper/f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models","venue":"","year":2021,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/06/2021","authors":"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Yasheng Wang,Qun Liu,Maosong Sun","citations":[{"paperId":"c991f7f99dd35eadb66b383b4d711f9cc25af7e3","title":"Pronunciation-aware unique character encoding for RNN Transducer-based Mandarin speech recognition"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"59ca487aca988c50dac2f1605756cc04c36603b4","title":"The effect of second-language orthographic input on the phonological encoding of Mandarin words"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"cc50f846ed7222698d130cddbc58ed4d547914ed","title":"CPM: A Large-scale Generative Chinese Pre-trained Language Model"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training With Whole Word Masking for Chinese BERT"},{"paperId":"582649c87e590101617a21524f0ec203ea935937","title":"MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"6dd1a0f657ff547b1736c2d8b30451f49a9a48f5","title":"OCNLI: Original Chinese Natural Language Inference"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"6afe0fb12ceacadbbfed7202d430770a3f344731","title":"Revisiting Pre-Trained Models for Chinese Natural Language Processing"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"e0697594749cca8e1a272033d35e6102bfd39c00","title":"CLUENER2020: Fine-grained Name Entity Recognition for Chinese"},{"paperId":"6007bd2a34385132a7885b934d90b519a1f65bba","title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"1b07a24b81834116f6ad1d0232485ba81b9445f3","title":"Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"},{"paperId":"6c1beae31b92c70b42ebeb99e5598d73bff6eea5","title":"NEZHA: Neural Contextualized Representation for Chinese Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e278e072774f23675266881750e20bca74804cb9","title":"ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"},{"paperId":"ae43556f2cc1cecb8b48762b4e09df319fbaa4d9","title":"Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"b401aca53c04cec865e1a733090a2c60ca7ebe97","title":"Glyce: Glyph-vectors for Chinese Character Representations"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":null,"title":"The #BenderRule: On Naming the Languages We Study and Why It Matters. The Gradient"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"7afb83134d5b7914131e10b229d30dc2593266f6","title":"The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification"},{"paperId":"57b57e88edcc9a20c78388e847b42e088b451c55","title":"cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":null,"title":"THULAC: An Efficient Lexical Analyzer for Chinese"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"7f147cd5d509e4feb628972922297f48b86ebd3e","title":"Punctuation as Implicit Annotations for Chinese Word Segmentation"},{"paperId":"5b2beddeb063bd5988da70ae58f3e0a6564e647a","title":"Optimizing Chinese Word Segmentation for Machine Translation Performance"},{"paperId":"885476e9d1b2ea67405f7f9b46c2c8536657a204","title":"Scalable Term Selection for Text Categorization"},{"paperId":"a6c2773efba0c69e4d0ca5043cbafef9b5fc3d26","title":"Writing Systems of the World"}],"id":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","summary":"Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency, and 2) Pronunciation-based SubChartokenization can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to all homophone typos."},{"url":"https://www.semanticscholar.org/paper/d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation","venue":"ArXiv","year":2021,"referenceCount":96,"citationCount":4,"influentialCitationCount":0,"publicationDate":"10/09/2021","authors":"Yuval Pinter","citations":[{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"d7a793e7ce1e42e1feaadb026633e481131a8692","title":"Char2Subword: Extending the Subword Embedding Space from Pre-trained Models Using Robust Character Compositionality"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"03826718c8c47d8f1f25e437fd6ff15165162e8c","title":"Will it Unblend?"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"b08545e1281c1eb748e4474687eb61fd3b25d1a6","title":"NYTWIT: A Dataset of Novel Words in the New York Times"},{"paperId":null,"title":"Will it unblend? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1525–1535, Online"},{"paperId":"2af39b0698b2872c13c2d207e7937e4f1e317cae","title":"Attending Form and Context to Generate Specialized Out-of-VocabularyWords Representations"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"fddd3dab90c243ab7fc038bc6449ef62c0e06037","title":"Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"5fc7b4dbc154bbbf26d8cee2f18f31ecbf286bcf","title":"Generalizing Word Embeddings using Bag of Subwords"},{"paperId":"fac1c95993e86f92c7adeec7f72e06503e4190d5","title":"Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"4aa508296cf1addc47db2fc23820b20eb5c714ed","title":"Predicting Semantic Relations using Global Graph Properties"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"ce2d5b5856bb6c9ab5c2390eb8b180c75a162055","title":"Recent Trends in Deep Learning Based Natural Language Processing [Review Article]"},{"paperId":"9697d32ed0a16da167f2bdba05ef96d0da066eb5","title":"Convolutional 2D Knowledge Graph Embeddings"},{"paperId":"4190a06408f029ae46d8a9656df20f9e17ce7e03","title":"How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?"},{"paperId":"cd8a9914d50b0ac63315872530274d158d6aff09","title":"Modeling Relational Data with Graph Convolutional Networks"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"Recent trends in deep learning based natural language processing. ieee Computational intelligenCe magazine"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"d77312a62ac174b93e9ff980876369ca841b40e5","title":"Context encoders as a simple but powerful extension of word2vec"},{"paperId":"1590bd1bca945fc6ff50b8cdf2da14ea2061c79a","title":"Poincaré Embeddings for Learning Hierarchical Representations"},{"paperId":"90d32d6ecfc83f88d7685508993dca537ce3b91c","title":"Sound Symbolism in English: Weighing the Evidence"},{"paperId":"89f985dda7bbbae161d19e318257dbe13fd92d21","title":"The Interplay of Semantics and Morphology in Word Embeddings"},{"paperId":"495e83b32d2306c683ecf0f3a2f8eda1c30af3ed","title":"Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection"},{"paperId":"e3274206b36a603abc4a335af91273ecba5e73cc","title":"ProjE: Embedding Projection for Knowledge Graph Completion"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"87f714f3534c7a3ca2bf41ce5825139ddc8247bf","title":"What to do about non-standard (or non-canonical) language in NLP"},{"paperId":"f690c45d7cb80ce46a438c4dd1f9b50e1a45a84e","title":"Morphological Priors for Probabilistic Neural Word Embeddings"},{"paperId":"59761abc736397539bdd01ad7f9d91c8607c0457","title":"context2vec: Learning Generic Context Embedding with Bidirectional LSTM"},{"paperId":"36ee2c8bd605afd48035d15fdc6b8c8842363376","title":"node2vec: Scalable Feature Learning for Networks"},{"paperId":"12e9d005c77f76e344361f79c4b008034ae547eb","title":"Charagram: Embedding Words and Sentences via Character n-grams"},{"paperId":"acd87e4f672f0b92ea4164414c213560c23bee52","title":"Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss"},{"paperId":"24158c9fc293c8a998ac552b1188404a877da292","title":"Neural Architectures for Named Entity Recognition"},{"paperId":"45c84b1d25cfdfad6fcbebec230539aef308926e","title":"Mapping Unseen Words to Task-Trained Embedding Spaces"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"6dab1c6491929d396e9e5463bc2e87af88602aa2","title":"Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"},{"paperId":"b5c29457a90ee9af7c3b2985e9f665ce4b5b97d6","title":"Observed versus latent features for knowledge base and text inference"},{"paperId":"18bd7cd489874ed9976b4f87a6a558f9533316e0","title":"Knowledge Graph Embedding via Dynamic Mapping Matrix"},{"paperId":"e745b0506f4133263633eb05e5006a8cff4129f0","title":"Traversing Knowledge Graphs in Vector Space"},{"paperId":"86412306b777ee35aba71d4795b02915cb8a04c3","title":"Embedding Entities and Relations for Learning and Inference in Knowledge Bases"},{"paperId":"7a96765c147c9c814803c8c9de28a1dd069271da","title":"SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation"},{"paperId":"7e928ef936c2815d7522c5176163d6ab7309a8b7","title":"Representing Text for Joint Embedding of Text and Knowledge Bases"},{"paperId":"8c68094a59dd2f24415082c53464abf45387f0bb","title":"Compositional Vector Space Models for Knowledge Base Inference"},{"paperId":null,"title":"Retrofitting word vectors to semantic lexicons"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"dab7e605237ad4f4fe56dcba2861b8f0a57112be","title":"Wikidata: a free collaborative knowledgebase"},{"paperId":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging"},{"paperId":"46f418bf6fab132f193661226c5c27d67f870ea5","title":"Compositional Morphology for Word Representations and Language Modelling"},{"paperId":"2582ab7c70c9e7fcb84545944eba8f3a7f253248","title":"Translating Embeddings for Modeling Multi-relational Data"},{"paperId":"50d53cc562225549457cbc782546bfbe1ac6f0cf","title":"Reasoning With Neural Tensor Networks for Knowledge Base Completion"},{"paperId":"fdb813d8b927bdd21ae1858cafa6c34b66a36268","title":"Learning deep structured semantic models for web search using clickthrough data"},{"paperId":"53ab89807caead278d3deb7b6a4180b277d3cb77","title":"Better Word Representations with Recursive Neural Networks for Morphology"},{"paperId":"c4fd9c86b2b41df51a6fe212406dda81b1997fd4","title":"Linguistic Regularities in Continuous Space Word Representations"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"5fac0ca1b3ea3b6f234dd0821e1f3678f0b6096d","title":"Relation Extraction with Matrix Factorization and Universal Schemas"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"f6764d853a14b0c34df1d2283e76277aead40fde","title":"A Three-Way Model for Collective Learning on Multi-Relational Data"},{"paperId":"bc1022b031dc6c7019696492e8116598097a8c12","title":"Natural Language Processing (Almost) from Scratch"},{"paperId":"31d33747d8fff0b7a0c40dcf9944015af9a15b1a","title":"BabelNet: Building a Very Large Multilingual Semantic Network"},{"paperId":"569b09c261c232d38be9a5beabf950aa680bba1e","title":"English Word Formation Processes"},{"paperId":"57458bc1cffe5caa45a885af986d70f723f406b4","title":"A unified architecture for natural language processing: deep neural networks with multitask learning"},{"paperId":"1976c9eeccc7115d18a04f1e7fb5145db6b96002","title":"Freebase: a collaboratively created graph database for structuring human knowledge"},{"paperId":"02711b7e8c73779db802cb00b6cbf407d0872b66","title":"Unsupervised models for morpheme segmentation and morphology learning"},{"paperId":"54bf9a296ccdf7344d58993bc33c69d51026e320","title":"Word-Formation in English"},{"paperId":null,"title":"A neural probabilistic language model. The journal of machine learning research"},{"paperId":"0c5043108eda7d2fa467fe91e3c47d4ba08e0b48","title":"Unsupervised Discovery of Morphemes"},{"paperId":"be26417ca5af69535be71fb0aeed1f6c38d0ba2e","title":"Global organization of the Wordnet lexicon"},{"paperId":"e0c01df98a6b633b25c96c1a99b713ac96f1c5be","title":"Placing search in context: the concept revisited"},{"paperId":null,"title":"How many words are there? Glottometrics"},{"paperId":null,"title":"Word: a typological framework"},{"paperId":null,"title":"How many words are there? Glottometrics, 4:61–86"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","title":"A Neural Probabilistic Language Model"},{"paperId":"d87ceda3042f781c341ac17109d1e94a717f5f60","title":"WordNet : an electronic lexical database"},{"paperId":"d560a8d279075a529e9cadb0d664b27957aac5a2","title":"TnT - A Statistical Part-of-Speech Tagger"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"2b2eb4a9bb146e3ffaa0b025fba0ed14240c683f","title":"Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging"},{"paperId":"3de5d40b60742e3dfa86b19e7f660962298492af","title":"Class-Based n-gram Models of Natural Language"},{"paperId":"20a80a7356859daa4170fb4da6b87b84adbb547f","title":"Indexing by Latent Semantic Analysis"},{"paperId":"48658035de019d1dc062810a9c4db0ef7cb728ee","title":"Sound Advice on Brand Names"},{"paperId":"7ef3ac14cdb484aaa2b039850093febd5cf73a21","title":"Contextual correlates of synonymy"},{"paperId":"decd9bc0385612bdf936928206d83730718e737e","title":"Distributional Structure"},{"paperId":"ca12a908e86a87db152c0991ae9c5a40f1a5d2a3","title":"The nature and measurement of meaning."},{"paperId":null,"title":"Gestalt psychology, 2nd edn new york"},{"paperId":null,"title":"Cours de linguistique générale (roy harris, trans.)"}],"id":"d87647784c12517d31964cc508d5b8423cc24f50","summary":"A survey of the distributional, compositional, and relational approaches to addressing the problem of representing the atomic elements of language in modern neural learning systems is presented, with special emphasis on the word level and the out-of-vocabulary phenomenon."},{"url":"https://www.semanticscholar.org/paper/dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?","venue":"WNUT","year":2021,"referenceCount":59,"citationCount":7,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Arij Riabi,Benoît Sagot,Djamé Seddah","citations":[{"paperId":"b16a7adc26d791d83971708aad17e8855b95db4d","title":"MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning"},{"paperId":"e92b881ec7b762fee1c2e6df47693aecfc3aafb1","title":"Part-of-Speech and Morphological Tagging of Algerian Judeo-Arabic"},{"paperId":"b162638cd42b65e6add199b0b34f1b375070fc7c","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"c2d2d98a1dd0ba591224b729a37673b899146708","title":"Text normalization for endangered languages: the case of Ligurian"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"3979141ef17f9bd2b8a5c290e33775f90a14154c","title":"DziriBERT: a Pre-trained Language Model for the Algerian Dialect"}],"references":[{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"fff1aaa53a249da73ab5f71a023427c23ea7d006","title":"Treebanking User-Generated Content: a UD Based Overview of Guidelines, Corpora and Unified Recommendations"},{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"d33d05a558cea37cfe06be6163754f974f784aaf","title":"DziriBERT: a Pre-trained Language Model for the Algerian Dialect"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"1cfd0f08bb7ab449bbccc3d69a62ffbf43a7eb7f","title":"First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT"},{"paperId":"29599829d26b4acaaf0ad88698ed4362d33b2ae7","title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models"},{"paperId":"1b86df622f1cc3e2e666318f427ffb7a03aa34e6","title":"Analyse en dépendances du français avec des plongements contextualisés (French dependency parsing with contextualized embeddings)"},{"paperId":null,"title":"Byt5: Towards a token-free"},{"paperId":"b29ebfe83ccd5aae614ca9986fb847d4c739aff4","title":"Arabizi Language Models for Sentiment Analysis"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"5259de991f875b2c614c4bd7bf7bda97b81b9264","title":"On the Importance of Pre-training Data Volume for Compact Language Models"},{"paperId":"bc3c662d2b6f5159202d703d201a9283687d43b9","title":"Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell"},{"paperId":"14489ec7893e373a0dcc9555c52b99b2b3a429f6","title":"Are All Languages Created Equal in Multilingual BERT?"},{"paperId":"370648eabfcde7e0d6ef2c9e2a332672b556e8a6","title":"Can Multilingual Language Models Transfer to an Unseen Dialect? A Case Study on North African Arabizi"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"b805693c17961af2cc7f859c1a54320b26036f46","title":"Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"4fa37d012ad0014552a6a5a03624b29f95558bf7","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"a9c3a009d754a110379574b069b48c0b4c75db40","title":"Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"},{"paperId":"95236a88cc959656958d7a49422f4004015d594d","title":"Comparison between NMT and PBSMT Performance for Translating Noisy User-Generated Content"},{"paperId":"3f9df96b26c42dea6dd6cad64557a3b7d698ea90","title":"MultiFiT: Efficient Multi-lingual Language Model Fine-tuning"},{"paperId":"55ace73cb8f7c3c4bd6d8ead45c0ba6193d1afda","title":"A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"92343cecdc990380de362b969eec60081959f507","title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Linguistic variation [online]. 2019"},{"paperId":null,"title":"How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996– 5001, Florence, Italy"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"c935bcc748c4e9b87420deaf23074535592950d0","title":"Addressing Code-Switching in French/Algerian Arabic Speech"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":"60e328b861d43ae94cf00bd2e8e8c82e22f17921","title":"Massively Multilingual Word Embeddings"},{"paperId":null,"title":"Fasttext"},{"paperId":"e0945081b5b87187a53d4329cf77cd8bff635795","title":"Highway Networks"},{"paperId":"a72325ced3a56f00114c4947cb0e702a95df074b","title":"Rhapsodie: a Prosodic-Syntactic Treebank for Spoken French"},{"paperId":"6fc879a6f264ecb791ffcfac686562230c428b89","title":"An Algerian Arabic-French Code-Switched Corpus"},{"paperId":"e95ad36302f92f45abe169fbc5185e55407bcc34","title":"Universal Dependency Annotation for Multilingual Parsing"},{"paperId":"c928e453122fa7c0658e02a7aa07a623d4e5b679","title":"What to do about bad language on the internet"},{"paperId":"2a04cb0103c3e94bb98a341bca195f821d58d0c1","title":"The French Social Media Bank: a Treebank of Noisy User Generated Content"},{"paperId":"4af5ce5ca404054aada0ff421f50c541078a2c3d","title":"Le corpus Sequoia : annotation syntaxique et exploitation pour l’adaptation d’analyseur par pont lexical (The Sequoia Corpus : Syntactic Annotation and Use for a Parser Lexical Domain Adaptation Method) [in French]"},{"paperId":"99fd177dfddeb77de290e337e690552bc4c8be7d","title":"Introduction to Arabic Natural Language Processing"},{"paperId":"ca9c140636a7e9545e0071d0af5337fee8a349f6","title":"“cba to check the spelling”: Investigating Parser Performance on Discussion Forum Posts"},{"paperId":"b741a9b06c4253c8fc89ce6196d58fbf544a9baa","title":"Statistical French Dependency Parsing: Treebank Conversion and First Results"},{"paperId":"fa5ea3e99d5846ba05dbdc35f67e9f95244d432c","title":"Linguistic Variation"},{"paperId":null,"title":"Djamé Seddah, and Amir Zeldes. 2020. Treebanking user-generated content: a UD based overview of guidelines"}],"id":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","summary":"This work shows that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models."},{"url":"https://www.semanticscholar.org/paper/62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Makesh Narsimhan Sreedhar,Xiangpeng Wan,Yu-Jie Cheng,Junjie Hu","citations":[],"references":[{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"51d62830c1112ea7443398990b850a988ed7c86c","title":"Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":"0bfa9275561f45bc772c4d544fd75f5d65143c5e","title":"When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"651c01b897f18e31bab3ad482154a980ac92c638","title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"95856e0789481eedc2cedc413581a0a819ef8fc8","title":"Unsupervised Domain Clusters in Pretrained Language Models"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"1c84438eff3d10cfb97fb64fc9f35d734209a465","title":"Character-based NMT with Transformer"},{"paperId":"9237d6efc603465765e80eb5ca1268c2bd7b5c24","title":"compare-mt: A Tool for Holistic Comparison of Language Generation Systems"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4","title":"Rapid Adaptation of Neural Machine Translation to New Languages"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"f0b6c1ffed9984317050d0c1dfb005cb65582f13","title":"On the State of the Art of Evaluation in Neural Language Models"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"7dba53e72c182e25e98e8f73a99d75ff69dda0c2","title":"Recurrent Highway Networks"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"81aace0e90c6a962059b117c24db0d856f340f41","title":"Report on the 11th IWSLT evaluation campaign"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"25ca4a36df2955b345634b5f8a6b6bb66a774b3c","title":"Parallel Data, Tools and Interfaces in OPUS"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"}],"id":"62ece609555bf833a2afd25ef796d72b5f59e767","summary":"This paper proposes a Lo cal B yt e F usion (LOBEF) method for byte-based machine translation—utilizing byte n -gram and word boundaries—to aggregate local semantic information and indicates that the byte- based models are parameter-efﬁcient and can be trained faster than subword models while showcasing competitive performance."},{"url":"https://www.semanticscholar.org/paper/12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Hui-li Xue,Nikolaos Aletras","citations":[],"references":[{"paperId":"5ca0a54fa0f76ae0e1881899c61b36d35d3bd166","title":"How does the pre-training objective affect what large language models learn about linguistic properties?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"cddf40e579a596d0110b260313adf43470617c4c","title":"Datasets: A Community Library for Natural Language Processing"},{"paperId":"acac699d02a972f58b091bfbef7518f0e61c8225","title":"Frustratingly Simple Pretraining Alternatives to Masked Language Modeling"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"1531ffc206bdb310ae08adc02e3df44d1d125d99","title":"On-Device Text Representations Robust To Misspellings via Projections"},{"paperId":"05a6fa3bb47bd283c95b3d86ba4cbbc471cbd6ac","title":"ProFormer: Towards On-Device LSH Projection Based Transformers"},{"paperId":"738215a396f6eee1709c6b521a6199769f0ce674","title":"Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"},{"paperId":"090be9851f8ceea2acaebce4763c780287c85693","title":"Extremely Small BERT Models from Mixed-Vocabulary Training"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"82ba6cc2fd424b7374cb8c2a056496925b4c2948","title":"Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"fb73b93de3734a996829caf31e4310e0054e9c6b","title":"Green AI"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"e26c03768664a1b2a2821659b1e234f59c738c43","title":"Efficient On-Device Models using Neural Projections"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"4bf6ce4a9366cdba069a45651606538f2febd8e6","title":"Compressing Word Embeddings via Deep Compositional Code Learning"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"8215fb083cb4b0ed2b6858b81dcc30fbd0afb6e1","title":"Mining of Massive Datasets"},{"paperId":"511b0bb924d109767feb11855acd059125ff0164","title":"Hash Kernels"},{"paperId":"3fb1c64b763d27fba2a18c923637c5ea0e048e3b","title":"Small Statistical Models by Random Feature Mixing"},{"paperId":null,"title":"The MD5 messagedigest algorithm"},{"paperId":null,"title":"Albert Villanova del Moral"}],"id":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","summary":"It is empirically demonstrated that H ASH F ORMERS are more memory efﬁcient compared to standard pre-trained transformers while achieving comparable predictive performance when ﬁne-tuned on multiple text classiﬂcation tasks."},{"url":"https://www.semanticscholar.org/paper/023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Shaked Yehezkel,Yuval Pinter","citations":[{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"}],"references":[{"paperId":"8c82d3d758897ef9f166924683831ecf6085f21a","title":"MaxMatch-Dropout: Subword Regularization for WordPiece"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"a0ea1ee10daeb27d421a66de0075323e038b78ac","title":"Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"7694aae9766d5f1fe74d900cd82aee898cb6e8e9","title":"How to Train BERT with an Academic Budget"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"0438f16147dc44669863722f81efaa13420885f4","title":"Subword Sampling for Low Resource Word Alignment"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"9127c3873cac9979433a208e69d426782747910b","title":"Evaluating Sub-word Embeddings in Cross-lingual Models"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"997855e1f17d34dd3922d953a587742d198844e6","title":"CrossWeigh: Training Named Entity Tagger from Imperfect Annotations"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"c39e409d6b7744200c4fd12a6b81e51f6145cfae","title":"Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"834724753187ba86709f9d859740f23861d42191","title":"Meaningless yet meaningful: Morphology grounded subword-level NMT"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"531bc31aa70981cc989180f6e3f7d3c6442e7d14","title":"POLYGLOT-NER: Massive Multilingual Named Entity Recognition"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":null,"title":"Inflectional synthesis of the verb"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":null,"title":"Gensim–python framework for vector space modelling"},{"paperId":"57458bc1cffe5caa45a885af986d70f723f406b4","title":"A unified architecture for natural language processing: deep neural networks with multitask learning"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":null,"title":"ponence of selected inflectional formatives"}],"id":"023fd42f0867a88a2206f906c7f127701058feb6","summary":"SAGE is presented, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase, and does a better job than current widespread tokenizers in keeping token contexts cohesive, while not incurring a large price in terms of encoding efficiency or domain robustness."},{"url":"https://www.semanticscholar.org/paper/2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers","venue":"ACL","year":2022,"referenceCount":54,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Valentin Hofmann,Hinrich Schütze,J. Pierrehumbert","citations":[{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"1606793daaa20d4a4a78e859c2fd6b4f7535680c","title":"Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment"}],"references":[{"paperId":"6cd9c0630c422f70ceced4dc400fcbc6001d15b5","title":"ABC: Attention with Bounded-memory Control"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"03aa38229a6c853d96430edfc8f1542598f2116e","title":"AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain"},{"paperId":"3c477f0e43660ce1ba39c111df312929da37f81f","title":"Efficient Domain Adaptation of Language Models via Adaptive Tokenization"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"3376118362db3751cfbd88acd0c090b8a3897733","title":"Superbizarre Is Not Superb: Improving BERT's Interpretations of Complex Words with Derivational Morphology"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"bb0b0c9853f658db9d6a7f4aa90c24906ba4b1dc","title":"A Pointer Network Architecture for Joint Morphological Segmentation and Tagging"},{"paperId":"fd2dc47c38945c0364641ac1085239b076454d40","title":"Tackling the Low-resource Challenge for Canonical Segmentation"},{"paperId":"110c13fbf4ff87b52ee1fd9eb2d3616c839ceb41","title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank"},{"paperId":"dcd72571e380eacf3ddccf225f3324eb8c51ece0","title":"A Graph Auto-encoder Model of Derivational Morphology"},{"paperId":"df8511a57b5d72e3f7e7f3b83bb271d58ef66a39","title":"Predicting the Growth of Morphological Families from Social and Linguistic Factors"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"44189a062182cab428e6d7ea010819f2fe978533","title":"DagoBERT: Generating Derivational Morphology with a Pretrained Language Model"},{"paperId":"8515c83e6562415021b4c536c28ed198f05a8691","title":"Emerging trends: Subwords, seriously?"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"fb73b93de3734a996829caf31e4310e0054e9c6b","title":"Green AI"},{"paperId":"21079df28af037f93620a0bd63f04fe49cf5df7d","title":"On the Importance of Tokenization in Arabic Embedding Models"},{"paperId":"7da138d704ef0fc055825fa132f5c452ed3fb52a","title":"LADEC: The Large Database of English Compounds"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"a6cd2c524740aff36d79468ae99f1f484de01e1b","title":"Correcting Whitespace Errors in Digitized Historical Texts"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"5839b0a5f7fce57941620e4e9e2e7168a335a43c","title":"Subword-Level Language Identification for Intra-Word Code-Switching"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"25687348fd01908c56664b40e249a611fb4b6dd8","title":"Gendered associations of English morphology"},{"paperId":"c7ec1b98fce650503168a51131a889942c6f0aa8","title":"A Distributional and Orthographic Aggregation Model for English Derivational Morphology"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"38afef35de217bdce44e2d02f366ca42bedfb60e","title":"Joint Semantic Synthesis and Morphological Analysis of the Derived Word"},{"paperId":"44a590466bf95545eb47daf8ce2a468fa8818751","title":"On Hapax Legomena and Morphological Productivity"},{"paperId":null,"title":"Journal of the Association for Laboratory Phonology"},{"paperId":"90cbdddcf9e8490b42b08ee17602d20bc214354a","title":"Paradigm Completion for Derivational Morphology"},{"paperId":"5bcc16402c812c236adee3449a5e9ba5659b5796","title":"Neural Sequence-to-sequence Learning of Internal Word Structure"},{"paperId":"3cda98ecef94392d8ce4492ee3515b2ebe75899b","title":"Context-Aware Prediction of Derivational Word-forms"},{"paperId":"c75ecddc5eda6b71b54bf8ef40f6fa9da1d8f108","title":"Neural Morphological Analysis: Encoding-Decoding Canonical Segments"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"0a275c52c61ecf2429189387426d6173e40d695a","title":"A Joint Model of Orthography and Morphological Segmentation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"9c73d98785cc143db64da987659dd5656c08db23","title":"Singulars and plurals in Dutch: Evidence for a parallel dual-route model"},{"paperId":"66e73e696414e9fde9201b176c447bf1ae91b00b","title":"The Cambridge encyclopedia of the english language"},{"paperId":"60258897d250a41f11cfee27de828a0130110b5e","title":"The CELEX Lexical Database (CD-ROM)"},{"paperId":"061e742d1d63631ed28d5dcc8b8e3c884519e924","title":"Distributional properties of derivational affixes: Implications for processing"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"9ba4b5ac12496b14e9d2e571515fad89904d19fa","title":"Lexical access and inflectional morphology"},{"paperId":null,"title":". Needle and Janet B . Pierrehumbert . 2018 . Gendered associations of english morphology"}],"id":"2f07f97563a73d9b691ec6144e4bba25a347ab87","summary":"FLOTA (Few Longest Token Approximation) leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise."},{"url":"https://www.semanticscholar.org/paper/6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"C HARFORMER : F AST C HARACTER T RANSFORMERS VIA G RADIENT - BASED S UBWORD T OKENIZATION","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","citations":[],"references":[{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"2365410a710b421b2295cdca0074946cb50bb1d4","title":"Are Pre-trained Convolutions Better than Pre-trained Transformers?"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"7e9ff94476f41041c75e253e84f487db00e9c861","title":"Long Range Arena: A Benchmark for Efficient Transformers"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"26b277f2d7cd86b67bc3572257557edc4640b8e9","title":"Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","title":"GLU Variants Improve Transformer"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"b611a8095630557229dc5fb6b07c272f1cd614da","title":"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"2270b8628fd8ca67ae39d277f45bc3c38ac63d5f","title":"Mesh-TensorFlow: Deep Learning for Supercomputers"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"1db9bd18681b96473f3c82b21edc9240b44dc329","title":"Image Transformer"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":null,"title":"FRAGE: FrequencyAgnostic Word Representation"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"deff3b0635e141297238797d924d7a9aba3a132a","title":"Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"6f35b070c250507dbec8a7365cf01b25eb66d792","title":"Ex Machina: Personal Attacks Seen at Scale"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"649d03490ef72c5274e3bccd03d7a299d2f8da91","title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"3bf2e6941dbb87ac0d2c771c159e1e27366a26e3","title":"Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics"},{"paperId":"0c09f282d78f42f77d73a52b08cb425e1cabb9b2","title":"Are Vowels and Consonants Processed Differently? Event-related Potential Evidence with a Delayed Letter Paradigm"},{"paperId":"475354f10798f110d34792b6d88f31d6d5cb099e","title":"Automatically Constructing a Corpus of Sentential Paraphrases"},{"paperId":"084c55d6432265785e3ff86a2e900a49d501c00a","title":"Foundations of statistical natural language processing"},{"paperId":"98b405fd0153f24e8359448d79acc49c3ffc8f4c","title":"The relative contribution of consonants and vowels to word identification during reading"},{"paperId":null,"title":"Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee"}],"id":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","summary":"This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion that paves the way for highly performant token-free models that are trained completely end-to-end."},{"url":"https://www.semanticscholar.org/paper/b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA","venue":"","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Peng Chen","citations":[],"references":[{"paperId":"ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51","title":"Efficiently Modeling Long Sequences with Structured State Spaces"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"7e9ff94476f41041c75e253e84f487db00e9c861","title":"Long Range Arena: A Benchmark for Efficient Transformers"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"0964490205fdc38c2f0980c9d778069089ca92e3","title":"HiPPO: Recurrent Memory with Optimal Polynomial Projections"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"8c1b00128e74f1cd92aede3959690615695d5101","title":"QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"3a7b63b50c64f4ec3358477790e84cbd6be2a0b4","title":"Bidirectional Attention Flow for Machine Comprehension"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"698d675ba7134ac701de810c9ca4a6de72cb414b","title":"Character-Level Question Answering with Attention"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":null,"title":"Longformer: The long-document transformer. CoRR, abs"}],"id":"b639124771f9c62cd656a24e8e685a456918e0ff","summary":"A question answering model that encodes text inputs at character level to utilize subword structures and mitigate the out-of-vocabulary problem is proposed, and both S4 and character-level encoding improve the model performance on the question answering task."},{"url":"https://www.semanticscholar.org/paper/f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?","venue":"Findings","year":2021,"referenceCount":78,"citationCount":4,"influentialCitationCount":0,"publicationDate":"15/10/2021","authors":"Jindřich Libovický,Helmut Schmid,Alexander Fraser","citations":[{"paperId":"11ddb0953eae196dab339bfdc117221594cf945e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}],"references":[{"paperId":"00191ddacdbef6f5819e8682ddfe184d5b415d27","title":"What Do You Get When You Cross Beam Search with Nucleus Sampling?"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"af59aeeb46fb5d8412f550f6dd5c5dc99afc9c1a","title":"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation"},{"paperId":"37c4ec8d4fcaaf7f2c3f01a2fb30e9e41d8865a2","title":"TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":"0bfa9275561f45bc772c4d544fd75f5d65143c5e","title":"When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation"},{"paperId":null,"title":"What do you get when you cross beam search with nucleus sampling? CoRR, abs/2107.09729"},{"paperId":null,"title":"What do you get when you cross beam search with nucleus sampling? CoRR"},{"paperId":"f3aaec8d24e07d6370b78aefc757e5d5fe642f60","title":"The University of Edinburgh’s English-German and English-Hausa Submissions to the WMT21 News Translation Task"},{"paperId":null,"title":"CUNI system for WMT 17 automatic post - editing task Attention is all you need"},{"paperId":null,"title":"mT5: A massively"},{"paperId":null,"title":"CUNI system for WMT 17 automatic post - editing task Attention is all you need"},{"paperId":"648bea1c1e11936ecae023593a43d848e3942c0e","title":"Why Neural Machine Translation Prefers Empty Outputs"},{"paperId":"c204d40384d39c59cd7249bde4cd8615972acaac","title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness"},{"paperId":"f85040f8aecee4cdb63137bbad5bbab3ba838c33","title":"Gender Coreference and Bias Evaluation at WMT 2020"},{"paperId":"05cfd96eed27ceb2aa35285991a745a5cd119abc","title":"If Beam Search Is the Answer, What Was the Question?"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"dd7fdaa997a074dbbc4849d0330a42985e7b3c3a","title":"Announcing CzEng 2.0 Parallel Corpus with over 2 Gigawords"},{"paperId":"0156fb2096c39839384c0cae0194e123e01cc575","title":"Character-Level Transformer-Based Neural Machine Translation"},{"paperId":"8b6c9adf85a9d6391e3ccd503c2e5af929a36735","title":"Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation"},{"paperId":"9d92905edc1a5d027b0827d1309bc6ac03dd94a0","title":"Character-Level Translation with Self-attention"},{"paperId":"651c01b897f18e31bab3ad482154a980ac92c638","title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems"},{"paperId":"027946f80f3cb276ea38bc2cf19903052f59cd0e","title":"How Decoding Strategies Affect the Verifiability of Generated Text"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"671cd646d61c670fffc2fea4cf7b7a1f6f80dcf7","title":"NRC Systems for the 2020 Inuktitut-English News Translation Task"},{"paperId":"e8f297e161f57e461ede2d4e0c26573981cad077","title":"Findings of the 2020 Conference on Machine Translation (WMT20)"},{"paperId":"1c84438eff3d10cfb97fb64fc9f35d734209a465","title":"Character-based NMT with Transformer"},{"paperId":"7c848b37605cdfe6d2d4778b06e66c0710c3ec34","title":"On the Importance of Word Boundaries in Character-level Neural Machine Translation"},{"paperId":"d3231772937a2182b2377d028417245c49868dd1","title":"On NMT Search Errors and Model Errors: Cat Got Your Tongue?"},{"paperId":"b33a9df8e39966217786855ef8fcf87d11294774","title":"The University of Helsinki Submissions to the WMT19 Similar Language Translation Task"},{"paperId":"a5690b0a514a7cbc913871e41e54c9ad4f6362db","title":"Findings of the First Shared Task on Machine Translation Robustness"},{"paperId":"6e722ac4d386489aa47703887881835ec0e1331d","title":"Tagged Back-Translation"},{"paperId":"1670a07b70f90cc4ddba71343e6a7ee4b5198595","title":"Evaluating Gender Bias in Machine Translation"},{"paperId":"10b9eee99b2632359d4d26f991e765bff8d91dee","title":"Revisiting Low-Resource Neural Machine Translation: A Case Study"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"90e531d15be3c19f3d83085b906d5c9b5aae540d","title":"JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English"},{"paperId":"48fbdf1be70221ac8a6b22079245030ab6158760","title":"Findings of the 2018 Conference on Machine Translation (WMT18)"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"4236cf737c8825e751db16689b6025d44a75ef97","title":"The WMT’18 Morpheval test suites for English-Czech, English-German, English-Finnish and Turkish-English"},{"paperId":"b699fce01693dcec868a6905488fcd3a652dd412","title":"Neural Machine Translation of Logographic Language Using Sub-character Level Information"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"8fec5d6ac57e90f459e7330775165f2671abc445","title":"Training Deeper Neural Machine Translation Models with Transparent Attention"},{"paperId":"622d324604a2e219dddc1d474ba9c38d9e05ac6c","title":"Character-level Chinese-English Translation through ASCII Encoding"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"d3707cf521e3596313af1f53acba6413d0d528a6","title":"Training Tips for the Transformer Model"},{"paperId":"aeeb2e5ecb5b926bd5673d1fef3e3e61589fabb2","title":"Findings of the WMT 2017 Biomedical Translation Shared Task"},{"paperId":"51b8a5d5264b0e891595a1e111b863efb86c8c72","title":"Evaluating the morphological competence of Machine Translation Systems"},{"paperId":"346e012f79ee607f1b563bc66fd8243f1b1ae4e9","title":"Extending hybrid word-character neural machine translation with multi-task learning of morphological analysis"},{"paperId":"89cb259f39eb8350dd337fc1d02e2f4128c8398c","title":"Byte-based Neural Machine Translation"},{"paperId":"d4cc6fb1b81dd6cffde542f60b001129e3175b93","title":"The Helsinki Neural Machine Translation System"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"63c4114bd373dd0fcfe0d25a605b353c62be2995","title":"How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"d2b2d748208ff79fdaa7064cf9e7bf84a408f2ac","title":"University of Rochester WMT 2017 NMT System Submission"},{"paperId":"cb0ab255c4079e2082ba6e3a807529527d96687c","title":"Overview of the IWSLT 2017 Evaluation Campaign"},{"paperId":"6dc33afdf5a90e44e3fa20086fc270ddc88681a2","title":"CUNI System for WMT17 Automatic Post-Editing Task"},{"paperId":"b561b860e6931436c66f378386a4aec5c778f9d4","title":"The TALP-UPC Neural Machine Translation System for German/Finnish-English Using the Inverse Direction Model in Rescoring"},{"paperId":"ea40fba2dc52bde32dedd45b05f83d27dc2ea6c7","title":"Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe"},{"paperId":null,"title":"The TALP-UPC neural machine"},{"paperId":"1a327709cc53ff9e52454e50a643abf4a0ac92af","title":"Findings of the 2016 Conference on Machine Translation"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"e0945081b5b87187a53d4329cf77cd8bff635795","title":"Highway Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"0f8992ee6418d367d8e50ecbb59b08ea15e8431f","title":"Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems"},{"paperId":"cb826a3899752b796f14df1c50378c64954a6b0a","title":"Statistical Significance Tests for Machine Translation Evaluation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"cd5a169879504ea91660a443b9151753cc29c42f","title":"Minimum Bayes-risk automatic speech recognition"}],"id":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","summary":"It is shown that even with recent modeling innovations in character-level natural language processing, character- level MT systems still struggle to match their subword-based counterparts, and show neither better domain robustness, nor better morphological generalization."},{"url":"https://www.semanticscholar.org/paper/2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":2,"influentialCitationCount":1,"publicationDate":"19/04/2022","authors":"Cagri Toraman,E. Yilmaz,Furkan Sahinuc,Oguzhan Ozcelik","citations":[{"paperId":"7aef8e81883e9ded80076b8d2002e29ec5555564","title":"Bioberturk: Exploring Turkish Biomedical Language Model Development Strategies in Low Resource Setting"},{"paperId":"a611ecc0b48b1302abbd33712b23f025f35be0b4","title":"ARC-NLP at CheckThat!-2022: Contradiction for Harmful Tweet Detection"}],"references":[{"paperId":"bef55aedf15ebcf9b21b0e3f5df4dde8def512bd","title":"Large-Scale Hate Speech Detection with Cross-Domain Transfer"},{"paperId":"72d8a122c62abdc223280dfd56b34f7634e27e92","title":"Ethical Challenges in AI"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":null,"title":"Climate Impact by Area"},{"paperId":"c7d50a6b33d1fc9b460e5bb0c141a00203c82645","title":"Towards Tokenization and Part-of-Speech Tagging for Khmer: Data and Discussion"},{"paperId":"019e6093a5439468abc82038dc9e6d72c49e23b7","title":"The social cost of carbon dioxide under climate-economy feedbacks and temperature variability"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5e8da0a332906a2cc5262483ddc27eb9b792dbaf","title":"Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine Translation: The Case of Fon Language"},{"paperId":"1c15d9f928d2ff13ba857e3eafc1e784a0100838","title":"Finding Better Subwords for Tibetan Neural Machine Translation"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"86b369759bff13ec23b45ca23cc5461292a75415","title":"WangchanBERTa: Pretraining transformer-based Thai Language Models"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"2c953a3c378b40dadf2e3fb486713c8608b8e282","title":"Pretrained Transformers for Text Ranking: BERT and Beyond"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"7b3f9d6343aeb5659b2d6c39e8a4004b00b0695e","title":"Semantic Similarity Based Evaluation for Abstractive News Summarization"},{"paperId":"3b6bab26e32a297b15999160c5cf5727e2635210","title":"Should we find another model?: Improving Neural Machine Translation Performance with ONE-Piece Tokenization Method without Model Modification"},{"paperId":"e53c82abd656858f6c4715e545737aa3c9889f36","title":"Pre-Training on Mixed Data for Low-Resource Neural Machine Translation"},{"paperId":null,"title":"Oscar Dataset Huggingface"},{"paperId":"4d1367141eb8210a981428f3a7fdbdbc9db3e248","title":"A Tokenization System for the Kurdish Language"},{"paperId":"3f02219184319aa8ca1ef182e6b091b6a7539a04","title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"2f3f885a52b92b2ab6bddcc3ce54b7eff541bd02","title":"Enhancing Tokenization by Embedding Romanian Language Specific Morphology"},{"paperId":"1cbcc8a05fcd76254211b0d499063b268f6a1a9a","title":"Data and Representation for Turkish Natural Language Inference"},{"paperId":"41a7fece6e8c47d5bff75f7701a702f351b110d6","title":"BERTurk - BERT models for Turkish"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"13f25c69973373e616c48688d06a6b6ae2736ef0","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":null,"title":"Primary Energy and GHG Emissions Coefficients of Electricity"},{"paperId":null,"title":"Turkish Language Models"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"92343cecdc990380de362b969eec60081959f507","title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"690edf44e8739fd80bdfb76f40c9a4a222f3bba8","title":"BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"},{"paperId":"3d4e09b30ede9ad6790fe830d39e1333fb6e5ee4","title":"NOVA: A Feasible and Flexible Annotation System for Joint Tokenization and Part-of-Speech Tagging"},{"paperId":"7365f887c938ca21a6adbef08b5a520ebbd4638f","title":"Model Cards for Model Reporting"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"2019. BERT: Pre-training"},{"paperId":"a9a5d671271fff45429084e184a788f611b6f194","title":"FRAGE: Frequency-Agnostic Word Representation"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"052418ebf497a3a4eaa76cd350987932253dee2b","title":"Morphological Disambiguation for Turkish"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"66182e1ba6dca20f315cd952866a94f54d8ac820","title":"Cross-lingual polarity detection with machine translation"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"b8417c267678a66919e3a85f7ffb05de8e71e4ff","title":"Information extraction from web services: a comparison of Tokenisation algorithms"},{"paperId":"61031f5b863de2d2b235c964d28de468d02bd2d5","title":"Developing a text categorization template for Turkish news portals"},{"paperId":"22dc2148ab8bd58c6b4491fe7b59257f5739d2e7","title":"Exploiting Separation of Closed-Class Categories for Arabic Tokenization and Part-of-Speech Tagging"},{"paperId":null,"title":"Language Detection Library for Java"},{"paperId":null,"title":"Zemberek, An Open Source NLP Framework for Turkic Languages"},{"paperId":"28c80bce6fe1b21dc52ad165c3a329eeddab8c3d","title":"A statistical information extraction system for Turkish"},{"paperId":"d9c71db75046473f0e3d3229950d7c84c09afd5e","title":"Text Chunking using Transformation-Based Learning"}],"id":"2ffacbeeebd3d9e7467610057b4308635a165b6b","summary":"Morphological and word-level tokenizers outperform de-facto tok- 017 enizers in particular cases and mini models can be competitive to larger state-of-the-art models, such that a 14-times smaller model can recover 94% of the performance of a larger model."},{"url":"https://www.semanticscholar.org/paper/17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?","venue":"Conference of the Association for Machine Translation in the Americas","year":2022,"referenceCount":37,"citationCount":2,"influentialCitationCount":0,"publicationDate":"29/04/2022","authors":"Shiyue Zhang,Vishrav Chaudhary,Naman Goyal,James Cross,Guillaume Wenzek,Mohit Bansal,Francisco Guzmán","citations":[{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"c464352b7ab5f74bc201b5cf94bb4b9a14f5f487","title":"Masked Part-Of-Speech Model: Does Modeling Long Context Help Unsupervised POS-tagging?"}],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"4d8a5338042da99819746ff835b6f299135e2023","title":"Statistical Power Analysis for the Behavioral Sciences"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c17985a669522e7e85ae3d34754c7df49c7187d1","title":"Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Exploring bert’s vocabulary"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"5308b9d6c001304a882a50891ccce9f7ccb1c3ec","title":"On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627","title":"Six Challenges for Neural Machine Translation"},{"paperId":"a486e2839291111bb44fa1f07731ada123539f75","title":"Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"9ed9bff37ec952134564b3b2a022b7aba9479ff2","title":"Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"17e2977b907aad2532c45185947539e83ac639cd","summary":"This work analyzes how translation performance changes as the data ratios among languages vary in the tokenizer training corpus and finds that while relatively better performance is often observed when languages are more equally sampled, the downstream performance is more robust to language imbalance than the authors usually expected."},{"url":"https://www.semanticscholar.org/paper/062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/08/2022","authors":"Borun Chen,Hongyin Tang,Jingang Wang,Qifan Wang,Haitao Zheng,Wei Yu Wu,Liqian Yu","citations":[],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","title":"LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization"},{"paperId":"8b954e1654c6b759a957fd11e66c111e6105fb3f","title":"CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding"},{"paperId":"077c713bccd9d2c7fde68d4cbde06ab0f07a6855","title":"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"},{"paperId":"19537be34dbadbcaa4fffcf028a8ada5095b1b5c","title":"COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"32d281a1e7a0a2d4e2b3f34e0f71780c987e1374","title":"DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training With Whole Word Masking for Chinese BERT"},{"paperId":"b9478e237b58160c65acd2c41894493d27e2c277","title":"WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"},{"paperId":"83ed88e4f745cc9aecd1fbd479612b11beddcb86","title":"CLEAR: Contrastive Learning for Sentence Representation"},{"paperId":"6dd1a0f657ff547b1736c2d8b30451f49a9a48f5","title":"OCNLI: Original Chinese Natural Language Inference"},{"paperId":"574c7c7260a795d2906f03b5229ace3f54ee0ab3","title":"An Unsupervised Sentence Embedding Method by Mutual Information Maximization"},{"paperId":"38643c2926b10f6f74f122a7037e2cd20d77c0f1","title":"Supervised Contrastive Learning"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"17d5884215b5afa53545cd7cb6135de5478da4ec","title":"CERT: Contrastive Self-supervised Learning for Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"ae43556f2cc1cecb8b48762b4e09df319fbaa4d9","title":"Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"549c1a581b61f9ea47afc6f6871845392eaebbc4","title":"LCQMC:A Large-scale Chinese Question Matching Corpus"},{"paperId":"c997d481606f0346164511cabe74c6d1ef3f6be5","title":"DRCD: a Chinese Machine Reading Comprehension Dataset"},{"paperId":"2e10560579f2bdeae0143141f26bd9f0a195b4b7","title":"Mixed Precision Training"},{"paperId":"7afb83134d5b7914131e10b229d30dc2593266f6","title":"The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"346d900f2f73b706ec539caecb4800d131fc9f6a","title":"An empirical study of sentiment analysis for chinese documents"},{"paperId":"885476e9d1b2ea67405f7f9b46c2c8536657a204","title":"Scalable Term Selection for Text Categorization"}],"id":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","summary":"This work proposes a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations, and implicitly encodes the coarse-grained information into the fine-Grained representations through contrastive learning on multi-grains information."},{"url":"https://www.semanticscholar.org/paper/035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences","venue":"EMNLP","year":2019,"referenceCount":37,"citationCount":13,"influentialCitationCount":0,"publicationDate":"01/11/2019","authors":"Matthias Gallé","citations":[{"paperId":"2e53fe6ff7d6d74da962957a5c0b282eaabd4edf","title":"How Effective is Byte Pair Encoding for Out-Of-Vocabulary Words in Neural Machine Translation?"},{"paperId":"1c98fcd62a5889c70ea9da4cd168b36253c7101c","title":"No Features Needed: Using BPE Sequence Embeddings for Web Log Anomaly Detection"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey"},{"paperId":"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","title":"Beyond Characters: Subword-level Morpheme Segmentation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"617c0f6fc2d6807a9cfa72f487b0542b25342fd0","title":"Optimizing Word Alignments with Better Subword Tokenization"},{"paperId":"d36d6abf8f9f1e80124d8a72dc5203802a6fdb26","title":"IIITT at CASE 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection"},{"paperId":"308e3090cc3c77712733a5552d10d25f210cd905","title":"Practical Random Access to SLP-Compressed Texts"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"25fc1d08e98172afe9ac2e4a64fb8fe359045547","title":"Morphological segmentation method for Turkic language neural machine translation"},{"paperId":"ec69e191e25ff4ab9b59178fdc5bd171f9f147b6","title":"Using Context to Help Predict Speaker's Emotions in Social Dialogue"},{"paperId":"d458218398f84b6ce259f3ef684582eea9e9158e","title":"String Processing and Information Retrieval: 27th International Symposium, SPIRE 2020, Orlando, FL, USA, October 13–15, 2020, Proceedings"}],"references":[{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"4ab8662b0004305470c234acebe9b0381234107d","title":"Rpair: Rescaling RePair with Rsync"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features"},{"paperId":"6e205e973af09673028918658b99196f31fb8684","title":"Neural Machine Translation for Morphologically Rich Languages with Improved Sub-word Units and Synthetic Data"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6cddfbed35c46937588bd9d6b846ca2855953cea","title":"Neural Lattice-to-Sequence Models for Uncertain Inputs"},{"paperId":"f958d4921951e394057a1c4ec33bad9a34e5dad1","title":"A Convolutional Encoder Model for Neural Machine Translation"},{"paperId":"e43f713e0d2d438a4c0b03eacab58c334e869e6a","title":"Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":"5a14f3fe1b555afe30a62e780965bff414e1444c","title":"Lexis: An Optimization Framework for Discovering the Hierarchical Structure of Sequential Data"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"9aebe7899e9de235254dd78ec0bdc4cdef234f72","title":"An effective heuristic for the smallest grammar problem"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"611a498be099f8bd97b0104ea28f79bcac895221","title":"Searching for smallest grammars on large sequences and application to DNA"},{"paperId":"021002a852fdc590fbfbad6027cf8342ff2ea975","title":"The Smallest Grammar Problem as Constituents Choice and Minimal Grammar Parsing"},{"paperId":null,"title":"The Kyoto free translation task"},{"paperId":"fda0ba4dc485df29f1bcb11fc6badff356594489","title":"Linear-Time Text Compression by Longest-First Substitution"},{"paperId":"5aada25e3d3122f02a84d5d393c2ef2a78bf7e8b","title":"Text Compression"},{"paperId":"35dc54a582a06007c53e00943e51f2ecfa28966d","title":"Re-pair Achieves High-Order Entropy"},{"paperId":"54d7a8d91c2a2f09ea12d3a789bcb5bb33c3fe51","title":"The smallest grammar problem"},{"paperId":"32bdcb13b099314dcab80fea0e7416ea8a97d41b","title":"Off-line dictionary-based compression"},{"paperId":"e0f0f1f1872264e7769ccbd9edaaf435bb400b81","title":"Data compression using long common strings"},{"paperId":"7c72b917a38b09e6d3ab19d28a4344ba54edb6ae","title":"Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology"},{"paperId":"062f5fd99da136c01d6dff356073c54a91838412","title":"Compression and Explanation Using Hierarchical Grammars"},{"paperId":"7ec7bc1d7f35558640ac05e94f9b854ec17d7bc0","title":"Linguistic Structure as Composition and Perturbation"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"1de77281d812c67b605e160d2db841415ec19c17","title":"An analysis of the longest match and the greedy heuristics in text encoding"},{"paperId":"39cc07b9bd6ac368c46544d0c2805b78f68bb923","title":"On the syntactic structure of protein sequences and the concept of grammar complexity"},{"paperId":"3d951ad1164c17217a24e46b57f9b31cea1b2b96","title":"Data compression via textual substitution"},{"paperId":"d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5","title":"Modeling By Shortest Data Description*"},{"paperId":"5e37f888248c44a8d8098ccd07e91cc06e8b4d42","title":"AN ALGORITHM FOR THE SEGMENTATION OF AN ARTIFICIAL LANGUAGE ANALOGUE"},{"paperId":"91e56dee713c103aa8a86ab12383391d292cca45","title":"A comparison of algorithms for data base compression by use of fragments as language elements"},{"paperId":"d435f1a0d3c4325d02332f5b39b581b3f0c34488","title":"Common phrases and minimum-space text storage"}],"id":"035df9ecf84da7ae475175f326095ab16b97dd47","summary":"The experiments show that - given a fixed vocabulary size budget - the fewer tokens an algorithm needs to cover the test set, the better the translation (as measured by BLEU)."},{"url":"https://www.semanticscholar.org/paper/5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation","venue":"Findings","year":2020,"referenceCount":36,"citationCount":32,"influentialCitationCount":1,"publicationDate":"05/04/2020","authors":"Thamme Gowda,Jonathan May","citations":[{"paperId":"caec2c201aa002ee23d0c7fea7dff6e7ffa7b267","title":"Towards a general purpose machine translation system for Sranantongo"},{"paperId":"9d83941a205e31d394f614424cdc553cea9e35f9","title":"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"a30318cc5281187bd315915f0d0cccd7007f75b9","title":"The VolcTrans System for WMT22 Multilingual Machine Translation Task"},{"paperId":"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","title":"Checks and Strategies for Enabling Code-Switched Machine Translation"},{"paperId":"bb459f27d173b3ffa293a0213ec29e31e44c404d","title":"Reinforcement Learning with Large Action Spaces for Neural Machine Translation"},{"paperId":"3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class"},{"paperId":"4f68042a0aa40f34027a49ceec64ad2bbe2211aa","title":"When does Parameter-Efficient Transfer Learning Work for Machine Translation?"},{"paperId":"88aeac6f5efe99bf24aae982a9c2d00d02685bf7","title":"IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"a821453809f36a23402ce93b6042c76d85a720a6","title":"Frequency-Aware Contrastive Learning for Neural Machine Translation"},{"paperId":"48d5e267ab336a0b4caf2e1909d7860b02c6a736","title":"Direct Speech-to-Speech Translation With Discrete Units"},{"paperId":"3442f4bd36ee6b23196b8db40a0b8f84c55a960d","title":"SeqTrans: Automatic Vulnerability Fix via Sequence to Sequence Learning"},{"paperId":"4ddd463c22fab24e6d89105b6aa887e149832dac","title":"HFT: High Frequency Tokens for Low-Resource NMT"},{"paperId":"9e3f3789ef3bfbf8f2ab3174c4d9a49b92085787","title":"Limitations and Challenges of Unsupervised Cross-lingual Pre-training"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"b9bba726584babe18ed249c3ff55e9b851a2876c","title":"Automatic Gloss-level Data Augmentation for Sign Language Translation"},{"paperId":"52d4e170bc9a34797c521fa7c35c650538b5f909","title":"IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations"},{"paperId":"674265c672777b6d10d5455adc58a6cacb0d0cfe","title":"BPE beyond Word Boundary: How NOT to use Multi Word Expressions in Neural Machine Translation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"318853f0448a9b95235dbb99338af0b19d138eaa","title":"Quasi Character-Level Transformers to Improve Neural Machine Translation on Small Datasets"},{"paperId":"2607dce6dcb9043ca9cae67e25e6a24411f08c0b","title":"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation"},{"paperId":"94772377a9c08ad81e506240f844534b6669b8e9","title":"Diversifying Dialog Generation via Adaptive Label Smoothing"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"49a5a1a550d0ce757befbad7715f373d8a5b3b87","title":"On the Strengths of Cross-Attention in Pretrained Transformers for Machine Translation"},{"paperId":"3e32139deb17761a25075f8839daa61ad5992fc9","title":"Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation"},{"paperId":"3b34e79610e5acaba352def4323a59f6d531fac7","title":"Macro-Average: Rare Types Are Important Too"},{"paperId":"a49b0997efaec2db61109a6deed1512672c3cd0c","title":"Many-to-English Machine Translation Tools, Data, and Pretrained Models"},{"paperId":"7cb4b8406255d0f8115afa23d8efea1bb780cfb8","title":"On the Compatibility of Tokenizations Across Languages"},{"paperId":"7d4e38cc1c26ef513b3e9764d1778917e5420187","title":"Transformers for Low-Resource Languages: Is Féidir Linn!"},{"paperId":"f742e7a7582d647ffe9b8037ba286c4932eb84dd","title":"Boosting Neural Machine Translation from Finnish to Northern Sámi with Rule-Based Backtranslation"},{"paperId":"7771aa7badc3375a31bfac8dc47755ff5d5c7780","title":"From characters to words: the turning point of BPE merges"}],"references":[{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"ea3e18c7b10a137d495054682c055a80b5be768c","title":"Findings of the 2019 Conference on Machine Translation (WMT19)"},{"paperId":"f9163156eeba67762a7441db48fe6720106137cd","title":"Survey on deep learning with class imbalance"},{"paperId":"5dc1a37bf05fddcdb77efafec3bc50c8ded7cef0","title":"End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features"},{"paperId":"d3707cf521e3596313af1f53acba6413d0d528a6","title":"Training Tips for the Transformer Model"},{"paperId":"15e81c8d1c21f9e928c72721ac46d458f3341454","title":"Non-Autoregressive Neural Machine Translation"},{"paperId":"9757ef31095227cb289af22b0a4010eda754d100","title":"A systematic study of the class imbalance problem in convolutional neural networks"},{"paperId":"25c0e5ad89b77ae9c7ff91765ebd4e1e21abdcb3","title":"The IIT Bombay English-Hindi Parallel Corpus"},{"paperId":null,"title":"Nonautoregressive neural machine translation"},{"paperId":"2d8edc4e38bf9907170238726ec902cb3739393b","title":"Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627","title":"Six Challenges for Neural Machine Translation"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"63e39cdf1ad884da6bc69096bb3413b5b1100559","title":"Using the Output Embedding to Improve Language Models"},{"paperId":null,"title":"Tjong Kim Sang and Fien De Meulder . 2003 . Introduction to the CoNLL - 2003 shared task : Language - independent named entity recognition"},{"paperId":"9fbeebb98f479405dadd912e95796ba0256b74ac","title":"CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies"},{"paperId":null,"title":"Mark Steedman . 2008 . On becoming a discipline"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":null,"title":", Matt Post , and Marcos Zampieri . 2019 . Findings of the 2019 conference on machine translation ( WMT 19 )"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"1eb09fecd75eb27825dce4f964b97f4f5cc399d7","title":"On the Properties of Neural Machine Translation: Encoder–Decoder Approaches"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"649d03490ef72c5274e3bccd03d7a299d2f8da91","title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"fa0324d3fddbbac02d24887b8a644e4db111d574","title":"Training neural network classifiers for medical decision making: The effects of imbalanced datasets on classification performance"},{"paperId":"5e379191996d63afed4daee23b0a4568d1715b90","title":"Last Words: On Becoming a Discipline"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746","title":"The class imbalance problem: A systematic study"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"8dcaf96f66340c453e775ab217a1b1bd9ba63449","title":"Time series analysis, forecasting and control"}],"id":"5e788c833321b12671206b96a438c0e5b1202027","summary":"This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and reveals an explanation for why certain vocabulary sizes are better than others."},{"url":"https://www.semanticscholar.org/paper/5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":52,"citationCount":13,"influentialCitationCount":1,"publicationDate":"16/04/2021","authors":"Elizabeth Salesky,David Etter,Matt Post","citations":[{"paperId":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only"},{"paperId":"a94e2ce7f94d189e5f788cfa431c504b3fb49402","title":"A Major Obstacle for NLP Research: Let's Talk about Time Allocation!"},{"paperId":"c9b98d7dd15acfddf8de8448263bfff0feb6c382","title":"Logographic Information Aids Learning Better Representations for Natural Language Inference"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"4743ee49af83d3010549ee105e0c193b36fa239a","title":"Machine Translation Robustness to Natural Asemantic Variation"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e","title":"vTTS: visual-text to speech"},{"paperId":"96fd89de07a69dd2dc94d71f884e64174c5974e2","title":"Interpreting the Robustness of Neural NLP Models to Textual Perturbations"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"a76c98c6814ce8de07707b81c18520af508b7184","title":"BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks"},{"paperId":"38375a9961778355f073bf974e643e4e00d6c10e","title":"Visual Cues and Error Correction for Translation Robustness"}],"references":[{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":null,"title":"Costa ­ jussà , David Aldón , and José A . R . Fonollosa"},{"paperId":"c204d40384d39c59cd7249bde4cd8615972acaac","title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness"},{"paperId":"da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71","title":"OCR Post-Correction for Endangered Language Texts"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"d7a1c8da1b8163f5bbd4d12e58cad5002947105e","title":"Word Shape Matters: Robust Machine Translation with Visual Embedding"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"5dd34d2781ca702d0e3cd1224517ff60d6c3e2ee","title":"Phonetic and Visual Priors for Decipherment of Informal Romanization"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"e8f297e161f57e461ede2d4e0c26573981cad077","title":"Findings of the 2020 Conference on Machine Translation (WMT20)"},{"paperId":"a5690b0a514a7cbc913871e41e54c9ad4f6362db","title":"Findings of the First Shared Task on Machine Translation Robustness"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"4dda68faa3ea2c888711ce5ced009afcb612e05b","title":"Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems"},{"paperId":"bdbf635476477eec5be5a292b494e20b8902cc35","title":"Improving Robustness of Machine Translation with Synthetic Noise"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":", andM . Carreiras . 2008 . R 34 d 1 ng w 0 rd 5 w 1 th numb 3 r 5"},{"paperId":null,"title":"Glyce: Glyph­vectors for chinese"},{"paperId":"64b8361a133ad545cbf9c035e109ef8461b31f67","title":"Super Characters: A Conversion from Sentiment Classification to Image Classification"},{"paperId":"1e6d69f8cfd698b8139cde557252753b22c7d712","title":"BPE and CharCNNs for Translation of Morphology: A Cross-Lingual Comparison and Analysis"},{"paperId":"ce89ee7aaeeea2c9d474707690f3ea9d948776a3","title":"MTNT: A Testbed for Machine Translation of Noisy Text"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"d09e0187879c6dbaacb16c23a2dddb31d74b8b0b","title":"On the Impact of Various Types of Noise on Neural Machine Translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"6ae164b36a8b712ca5b51659e7d77aa124151b4e","title":"COPPA V2. 0: Corpus Of Parallel Patent Applications Building Large Parallel Corpora with GNU Make"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":null,"title":"The multitarget TED talks task"},{"paperId":"4b5d9e8225d9caa2643217ad033b32ba3fe97b05","title":"Combining Convolutional Neural Networks and LSTMs for Segmentation-Free OCR"},{"paperId":"8e39bc4e1711e941881f64935b203a93259acb50","title":"Glyph-aware Embedding of Chinese Characters"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"db253b17043b6a86e02173b6aa597664b0c7f256","title":"Learning Character-level Compositionality with Visual Features"},{"paperId":"5f09d0cbd59e7c84b912692b6ded42107e8f9733","title":"Chinese–Spanish neural machine translation enhanced with character and word bitmap fonts"},{"paperId":"711e6e9e2fb1f4ab7238cdd556002ae36deeece7","title":"Robsut Wrod Reocginiton via Semi-Character Recurrent Neural Network"},{"paperId":"892e53fe5cd39f037cb2a961499f42f3002595dd","title":"Bag of Tricks for Efficient Text Classification"},{"paperId":"8e9149ab00236d04db23394774e716c4f1d89231","title":"An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"c2e1e362e27ff5ced52bfc9bde3ca165e7eafe76","title":"Neural machine translation using bitmap fonts"},{"paperId":null,"title":"2016a. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651"},{"paperId":null,"title":"Robsut wrod reocgini ­ ton via semi ­ character recurrent neural network Optimizing segmentation granularity for neural machine trans ­ lation"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","title":"Show and tell: A neural image caption generator"},{"paperId":"fba7c0a51a6301ca4086a5ce59b1f13af9acad7f","title":"Pointwise Prediction for Robust, Adaptable Japanese Morphological Analysis"},{"paperId":"a7bfdac75a0f0dfbd5faa375d90132c3115f9725","title":"R34D1NG W0RD5 W1TH NUMB3R5."},{"paperId":null,"title":"Journal of experi mental psychology. Human perception and perfor mance"},{"paperId":"3169b65a8769302ccb3ef41d98617bd2d9826293","title":"Raeding Wrods With Jubmled Lettres"},{"paperId":"cf61a21a58d3c762e0dc868ce09114e18304ed9d","title":"Word recognition inside out and outside in"}],"id":"5c3005e22e6fb218aa76fea49971f3f991993b32","summary":"This work proposes the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows, and demonstrates significant robustness to varied types of noise."},{"url":"https://www.semanticscholar.org/paper/379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"F. Meyer,Jan Buys","citations":[],"references":[{"paperId":"13b4d9a1096113db63158590e9d2e58744766e3b","title":"Unsupervised Word Segmentation with Bi-directional Neural Language Model"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"d7aa3c65eae71776ccc3cddafb1d4b288deae2cb","title":"A Masked Segmental Language Model for Unsupervised Natural Language Segmentation"},{"paperId":"5a50d90c7ad715c57b5f0cd9d8473b3dff705d40","title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"7b8d0413ee38d09421e0243dbb63c1e51fd1b571","title":"Low-Resource Language Modelling of South African Languages"},{"paperId":"bf607962d2edc1f52f06622853c93ec65efc6d03","title":"Canonical and Surface Morphological Segmentation for Nguni Languages"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"b2474a00d7de3373bab934c09acef1994fa82207","title":"Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"550871a80614dd481d153a43f94ef880cc4c461b","title":"Adaptation of Deep Bidirectional Transformers for Afrikaans Language"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"b079db3fe7cc3037fc15a5a1071254a0046774ac","title":"On the Importance of Subword Information for Morphological Tasks in Truly Low-Resource Languages"},{"paperId":"40c531fb0267437b0f76fd8f0080fb4de9ffe146","title":"A Systematic Study of Leveraging Subword Information for Learning Word Representations"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"7e55cb75b085ea144597710e9fb75be7479b2a64","title":"Towards an unsupervised morphological segmenter for isiXhosa"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"c46fd08184f91144dbc56bb2e41dd800669ac5f8","title":"Evaluation of combined bi-directional branching entropy language models for morphological segmentation of isiXhosa"},{"paperId":"e6ffe957179541a4cad7b6eba888c79c4aad8f91","title":"Unsupervised Morphological Segmentation for Low-Resource Polysynthetic Languages"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Exploring BERT's vocabulary"},{"paperId":"a98c340f316fc041dd071efcec1fbddf176282d9","title":"Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"fce6bacacd59de6ad7b486261d7f955374164c7f","title":"Unsupervised Morphological Segmentation Using Neural Word Embeddings"},{"paperId":"0a275c52c61ecf2429189387426d6173e40d695a","title":"A Joint Model of Orthography and Morphological Segmentation"},{"paperId":"6b904d6e84c98c6ce22ce6923224b205a2a24ee1","title":"Segmental Recurrent Neural Networks"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"1a7715e8d41a228f8fc35f7e3f2580988eea9b24","title":"Developing Text Resources for Ten South African Languages"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"5f4df9ebb4d4dddc4e84bcf1fb53d053f69105c3","title":"Unsupervised Morphological Segmentation with Log-Linear Models"},{"paperId":"02711b7e8c73779db802cb00b6cbf407d0872b66","title":"Unsupervised models for morpheme segmentation and morphology learning"},{"paperId":"fb3670b1b33adc6d90b6cf2326b48e9869c68081","title":"A Comparison of Approaches to Word Class Tagging: Disjunctively vs. Conjunctively Written Bantu Languages"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","title":"Finding Structure in Time"}],"id":"379722c04fb3b54215f82512eb86398cb02d42dd","summary":"A subword segmental language model (SSLM) that learns how to segment words while being trained for autoregressive language modelling, enabling the model to discover morpheme-like subwords that improve its LM capabilities."},{"url":"https://www.semanticscholar.org/paper/70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Jimin Sun,Patrick Fernandes,Xinyi Wang,Graham Neubig","citations":[],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"58e13e1fed9b28e50efcaff611772801d7980c80","title":"Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"4ceff7472c04ee6d76bce89d61ba4b445d8dbf74","title":"InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training"},{"paperId":"8741a0768d643c1593f8fea75dfdd0e5e90e1147","title":"Vocabulary Adaptation for Domain Adaptation in Neural Machine Translation"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"e99e2bd4812b30e104db0feddb681f32acd88758","title":"Massively Multilingual Transfer for NER"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"}],"id":"70dd68c07b322b68836eded1fb4f78c0efcad685","summary":"Surprisingly, it is found that subword-based models might still be the most practical choice in many settings, achieving better performance for lower inference latency and memory usage."},{"url":"https://www.semanticscholar.org/paper/e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE","venue":"","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"N. Chirkova,Sergey Troshin","citations":[],"references":[{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"31bfcbb1cbd74e8f38d97911f987108bd9517c6b","title":"Efficient Inference for Multilingual Neural Machine Translation"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c2b98ea55333895f736b9267414b4c9b63b9d04b","title":"AVATAR: A Parallel Corpus for Java-Python Program Translation"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1c84438eff3d10cfb97fb64fc9f35d734209a465","title":"Character-based NMT with Transformer"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"735ce0447e459e13f89ae751b19323d76a2af786","title":"Program Synthesis and Semantic Parsing with Learned Code Idioms"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"6faf62ac9e69e66cb92c923b749fa071554d23ca","title":"Learning Programmatic Idioms for Scalable Semantic Parsing"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"Tensor2Tensor for neural machine translation. In Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)"},{"paperId":null,"title":"URL https://d4mucfpksywv"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"3a29aa4eff48624752c07059a44d3288a678c8ab","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"de7293a0137fefe92412d61d3db93e22c0988136","title":"Evaluating clone detection tools with BigCloneBench"},{"paperId":null,"title":"[ From : Machine translation : from real users to research : 6 th conference of the Association for Machine Translation in the Americas"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"Graphcode { bert } : Pre - training code representations with data flow Character - based nmt with transformer"},{"paperId":null,"title":"Unsupervised translation of programming languages Dobf : A deob - fuscation pre - training objective for programming languages"}],"id":"e6b73466bab5e52ce0db19dd06d9353c26557dae","summary":"This work proposes subtokenziation that reduces average length by 17–40% without downstream performance drop, and shows that a carefully chosen subtokenization may significantly improve quality by 0.5-2%, possibly with some length increase."},{"url":"https://www.semanticscholar.org/paper/4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification","venue":"Neural Processing Letters","year":2021,"referenceCount":52,"citationCount":6,"influentialCitationCount":1,"publicationDate":"14/06/2021","authors":"Zaid Alyafeai,Maged S. Al-shaibani,Mustafa Ghaleb,Irfan Ahmad","citations":[{"paperId":"29c09defb807a9e16b4ea7cbf91587c9f4361121","title":"A Review Study on Arabic Text Classification"},{"paperId":"8c8aa03f8940b95336d0d70a9c37e024eff9e7bf","title":"Review on Recent Arabic Information Retrieval Techniques"},{"paperId":"a6fad9b1b32ace586b735e1764d26f1d67f2f7c7","title":"Arabic Document Classification: Performance Investigation of Preprocessing and Representation Techniques"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"d1d3dde91e3e73ccfbeb176f3af565a4507be077","title":"AI-Based Misogyny Detection from Arabic Levantine Twitter Tweets"},{"paperId":"b2ed3ed8d8b5717644be0ae705c13e2c2121a058","title":"Cloud computing architecture for Tagging Arabic Text Using Hybrid Model"}],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"2e4089ca4e1c4bda989e9e73fc92223e0ec96c99","title":"Classifying and diacritizing Arabic poems using deep recurrent neural networks"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"f3460673df92ec46de780f8e7813e1c35c3bb41f","title":"A comparative study of effective approaches for Arabic sentiment analysis"},{"paperId":"1e4cda8be54999ced1324777fa462a85e2c9746c","title":"ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic"},{"paperId":"45aabb7249efb8f6deed0e14aa635bb03a9e530f","title":"The Korean Morphologically Tight-Fitting Tokenizer for Noisy User-Generated Texts"},{"paperId":"22e7c2cac5f6d810f3777a0d966f79caabdd3910","title":"MetRec: A dataset for meter classification of arabic poetry"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"22b9b4ca72a7dff11a60f5f43d96b0555014ad76","title":"Meter classification of Arabic poems using deep bidirectional recurrent neural networks"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"4fa37d012ad0014552a6a5a03624b29f95558bf7","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"21079df28af037f93620a0bd63f04fe49cf5df7d","title":"On the Importance of Tokenization in Arabic Embedding Models"},{"paperId":null,"title":"Arabic optical characters recognition by neural network based arabic unicode"},{"paperId":null,"title":"Arabic optical characters recognition by neural network based arabic unicode"},{"paperId":"d6271bbe0b76e5e248767f09ae669177f554ba78","title":"Arabic sentiment analysis: studies, resources, and tools"},{"paperId":"91663fbb9257b1ee5d22d7b9e796e03f374a7a6a","title":"hULMonA: The Universal Language Model in Arabic"},{"paperId":"f89df2ef9cd3a4e5f7becaf8123c4d66fcbddd35","title":"The Impact of Preprocessing on Arabic-English Statistical and Neural Machine Translation"},{"paperId":"ad3ce37737fdbab5e4834a161d755249b0c2ee24","title":"A Survey of Opinion Mining in Arabic"},{"paperId":"60d00c7c09135d3643fd3efbd59d922bb60a5904","title":"Learning meters of Arabic and English poems with Recurrent Neural Networks: a step forward for language understanding and synthesis"},{"paperId":"a1f1dd2e73f6ce765a19787849b6e5b23735ab9c","title":"A comprehensive survey of arabic sentiment analysis"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"1e6d69f8cfd698b8139cde557252753b22c7d712","title":"BPE and CharCNNs for Translation of Morphology: A Cross-Lingual Comparison and Analysis"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"20e5dd2fe186cae77e9cca9be5dc66e18c596ffb","title":"Morphological Word Embeddings for Arabic Neural Machine Translation in Low-Resource Settings"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"4f94c40285c52128329d29b053976734349a0371","title":"DataSet for Arabic Classification"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"f94f20ee56489dff949d368e0a17d02dee7bad83","title":"Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging"},{"paperId":"2ebc9c842376e2f8cd56686e50d1daf19cbf080d","title":"Arabic Online Handwriting Recognition (AOHR)"},{"paperId":"39c3073181f2330804696a489f32d9c189febec1","title":"Arabic Tweets Sentimental Analysis Using Machine Learning"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"3924aa213ff891812c66a6909ab902684d3eb107","title":"AraVec: A set of Arabic Word Embedding Models for use in Arabic NLP"},{"paperId":null,"title":"Arabic online handwriting recognition (aohr) a survey"},{"paperId":"f3eeef4afb81223df96575adadf808fe7fe440b4","title":"1.5 billion words Arabic Corpus"},{"paperId":"8da1c2acdb8174f16566606d8d8b7bf3870d649a","title":"Orthographic Syllable as basic unit for SMT between Related Languages"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"a1aaa7c75464e7ebe41cfe5c5258241ca34c6414","title":"Farasa: A Fast and Furious Segmenter for Arabic"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"93a9694b6a4149e815c30a360347593b75860761","title":"Variable-Length Word Encodings for Neural Translation Models"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"f5886c433084d1baaa24152cd3e4c555c79bfe4f","title":"MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"a9c3a94b3e620e479ef52e2dc312202ec22af6bf","title":"LABR: A Large Scale Arabic Book Reviews Dataset"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"bc5097c5821d7fefd776d93e238247e76dba7554","title":"A comparative study between methods of Arabic baseline detection"},{"paperId":null,"title":"A comparative study between methods of arabic baseline detection Dataset for arabic classification"}],"id":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","summary":"This paper introduces three new tokenization algorithms for Arabic and compares them to three other baselines using unsupervised evaluations and shows that the performance of suchtokenization algorithms depends on the size of the dataset, type of the task, and the amount of morphology that exists in the dataset."},{"url":"https://www.semanticscholar.org/paper/bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation","venue":"ArXiv","year":2022,"referenceCount":13,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/05/2022","authors":"Lukas Edman,Antonio Toral,Gertjan van Noord","citations":[{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":null,"title":"Why don’t people use characterlevel machine translation? arXiv preprint arXiv:2110.08191"},{"paperId":null,"title":"Why don't people use characterlevel machine translation?"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","summary":"The GBST method from Charformer groups (aka downsamples) characters is used, thereby enabling character grouping in the decoder, and promising performance on English– Turkish translation indicate the potential of character-level models for morphologically rich languages."},{"url":"https://www.semanticscholar.org/paper/1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","title":"Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings","venue":"EACL","year":2021,"referenceCount":18,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"O-Yeon Kwon,Dohyun Kim,Soobee Lee,Junyoung Choi,SangKeun Lee","citations":[{"paperId":"f22ba8b56b89b2f19e84fbe3439bf78e8fc926b2","title":"KLASIFIKASI EMOSI PADA CUITAN DI TWITTER DENGAN PRINCIPAL COMPONENT ANALYSIS DAN SUPPORT VECTOR MACHINE"}],"references":[{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"8fa7d1f3f82526935ba122c20c8d0648506301b3","title":"Misspelling Oblivious Word Embeddings"},{"paperId":"9348186c18bbd35d77a9011474fdd76ef98c86c5","title":"Robust Word Vectors: Context-Informed Embeddings for Noisy Texts"},{"paperId":"6c145caf5da0e2f078c34d0b65b399d7124e2fd8","title":"Learning to Generate Word Representations using Subword Information"},{"paperId":"1dff26dc9be229daae61231a340f57efa8126e0d","title":"Subword-level Word Vector Representations for Korean"},{"paperId":"c0fbe0378150bb35e94bdd800bf02a4e57d9f2c2","title":"Morpheme-based Efficient Korean Word Embedding"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"},{"paperId":"6fe682de00baba4d50bb855ae243f55a4a2a4e14","title":"A Word Embedding and a Josa Vector for Korean Unsupervised Semantic Role Induction"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":null,"title":"Analyzing of hangul search query spelling error patterns and developing query spelling correction system based on user logs"},{"paperId":"b92a80c848d51bb009758c117ab4f05a99913106","title":"THE KOREAN LANGUAGE: Structure, use and context"},{"paperId":"b0130277677e5b915d5cd86b3afafd77fd08eb2e","title":"Estimation of probabilities from sparse data for the language model component of a speech recognizer"}],"id":"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","summary":"A robust Hangeul word embedding model against typos is proposed that utilizes a Convolutional Neural Network architecture with a channel attention mechanism that learns to infer the original word embeddings."},{"url":"https://www.semanticscholar.org/paper/9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information","venue":"ArXiv","year":2021,"referenceCount":32,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/08/2021","authors":"Yuval Pinter,A. Stent,Mark Dredze,Jacob Eisenstein","citations":[{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"188cd686fb2200f237f688dbda7f64ffc75e67ac","title":"Subword Pooling Makes a Difference"},{"paperId":"e7a2ffd26cd76e5b662ecb8624ecb3e177dbb8da","title":"Pitfalls of Static Language Modelling"},{"paperId":null,"title":"Recent Advances in Language Model Fine-tuning"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"641250e235d81e5b5c0023c32e71731aa0b0027c","title":"Multi-resolution Annotations for Emoji Prediction"},{"paperId":"d7a793e7ce1e42e1feaadb026633e481131a8692","title":"Char2Subword: Extending the Subword Embedding Space from Pre-trained Models Using Robust Character Compositionality"},{"paperId":"9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2","title":"TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"b08545e1281c1eb748e4474687eb61fd3b25d1a6","title":"NYTWIT: A Dataset of Novel Words in the New York Times"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a022bda79947d1f656a1164003c1b3ae9a843df9","title":"How to Fine-Tune BERT for Text Classification?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3e9674a344db5e4e7aa02222d659e58e4307b084","title":"SemEval 2018 Task 2: Multilingual Emoji Prediction"},{"paperId":"a235999a41cfcc5868ddacb94b47cda933f00a8c","title":"Peperomia at SemEval-2018 Task 2: Vector Similarity Based Approach for Emoji Prediction"},{"paperId":"8245fc60669776991dcb8ecd53a107f37bec29d2","title":"Hatching Chick at SemEval-2018 Task 2: Multilingual Emoji Prediction"},{"paperId":"321f91528af535cefa1b6971df31c609673f463f","title":"Backpropagating through Structured Argmax using a SPIGOT"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"72e87913632d0f1295fbcfa46795c5bb26d0a422","title":"Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"28ac5c728449c19be229e8839f5b5d6acd896f15","title":"Results of the WNUT16 Named Entity Recognition Shared Task"},{"paperId":"a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee","title":"MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"d895647b4a80861703851ef55930a2627fe19492","title":"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"}],"id":"9c2e4e5ee224c20a45c37244924138b50f3fe603","summary":"It is shown that incorporating XRAYEMB’s learned vectors into sequences of pre- trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on nonstandard English text."},{"url":"https://www.semanticscholar.org/paper/937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":16,"citationCount":7,"influentialCitationCount":1,"publicationDate":"25/08/2021","authors":"I. Itzhak,Omer Levy","citations":[{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"b162638cd42b65e6add199b0b34f1b375070fc7c","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"8b723be33e62bf5bd9278769244f1c13a9510898","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP"}],"references":[{"paperId":"7694aae9766d5f1fe74d900cd82aee898cb6e8e9","title":"How to Train BERT with an Academic Budget"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"e9d59dd33698adcdafabeb0f9ea873b2ed36f20b","title":"Farasa: A New Fast and Accurate Arabic Word Segmenter"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"01a660ec8aa995a88a00bfb41cb86c022047a9db","title":"NLTK: The Natural Language Toolkit"},{"paperId":"b2f8876482c97e804bb50a5e2433881ae31d0cdd","title":"Binary codes capable of correcting deletions, insertions, and reversals"}],"id":"937b177d2ed7cee27ee45300c690f2f60c81bae5","summary":"The results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell doesn’t appear to enhance its performance on such tasks."},{"url":"https://www.semanticscholar.org/paper/3b34e79610e5acaba352def4323a59f6d531fac7","title":"Macro-Average: Rare Types Are Important Too","venue":"NAACL","year":2021,"referenceCount":42,"citationCount":4,"influentialCitationCount":1,"publicationDate":"12/04/2021","authors":"Thamme Gowda,Weiqiu You,Constantine Lignos,Jonathan May","citations":[{"paperId":"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","title":"Checks and Strategies for Enabling Code-Switched Machine Translation"},{"paperId":"cb48e3bcc185ae94899007e1ad3cdb49ff39428b","title":"Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video"},{"paperId":"0a82ffe5c889d4defbd6300fffab4f3fa3dade99","title":"Deep convolution neural network with weighted loss to detect rice seeds vigor based on hyperspectral imaging under the sample-imbalanced condition"},{"paperId":"04cd4c224f61e8f25a405103d4210f161d091d1c","title":"Look It Up: Bilingual Dictionaries Improve Neural Machine Translation"}],"references":[{"paperId":"868207797e69df5055f5c3fd4aa78a33e5a7ca45","title":"Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics"},{"paperId":"de02a7e9c5e0eeb2cab4d7dd810e5e573327a2f5","title":"Corpora for Cross-Language Information Retrieval in Six Less-Resourced Languages"},{"paperId":"4ae52766028e69186052ea8f33a137fbbbdb986a","title":"BLEURT: Learning Robust Metrics for Text Generation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":null,"title":"XLM-UNMT-Models"},{"paperId":null,"title":"Multilingual denoising"},{"paperId":"dce91eb862d19a646b8f5171ec66e61a987f3b3c","title":"Putting Evaluation in Context: Contextual Embeddings Improve Machine Translation Evaluation"},{"paperId":"145b8b5d99a2beba6029418ca043585b90138d12","title":"MASS: Masked Sequence to Sequence Pre-training for Language Generation"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"9237d6efc603465765e80eb5ca1268c2bd7b5c24","title":"compare-mt: A Tool for Holistic Comparison of Language Generation Systems"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"c590d2c8c2fb6ce5d32ee9165ab24171165f2b70","title":"Assessing gender bias in machine translation: a case study with Google Translate"},{"paperId":"03853267d98af5ec5fdc3dc5cb85cc0681e435e4","title":"Human vs Automatic Metrics: on the Importance of Correlation Design"},{"paperId":"c2a7afbb5609a723f8eea91bfde4b02579b048d6","title":"Unsupervised Neural Machine Translation"},{"paperId":null,"title":"2020), which is fine-tuned on WMT Metrics ratings data from 2015-2018. The BLEURT model is retrieved from https://storage.googleapis.com/ bleurt-oss/bleurt-base-128.zip"},{"paperId":"dc4b3112000e151583da2532b63e0b1e59cbff8a","title":"Results of the WMT17 Metrics Shared Task"},{"paperId":"19a632b17b2ee5f64df41bdd23755316a02fb939","title":"Creating Training Corpora for NLG Micro-Planners"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"1a327709cc53ff9e52454e50a643abf4a0ac92af","title":"Findings of the 2016 Conference on Machine Translation"},{"paperId":"6c35070b6824ae5a2af4c667860e87f2c7ef6a9a","title":"Complementarity, F-score, and NLP Evaluation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"41ae4078cc698edd8dbb510dbfbb0f14b998d132","title":"BEER: BEtter Evaluation as Ranking"},{"paperId":null,"title":"Findings of the 2014 workshop"},{"paperId":"6836eadca4e506b580fc3f7c3374bff363fe0664","title":"The TAO of ATWV: Probing the mysteries of keyword search performance"},{"paperId":"0f8992ee6418d367d8e50ecbb59b08ea15e8431f","title":"Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems"},{"paperId":"b82756edb00e92f7314f1a3e036bf292664ad3a5","title":"A similarity measure for indefinite rankings"},{"paperId":"7bb21d4d4505db402d70c383962d19c42d9a7cbe","title":"Influence functions of the Spearman and Kendall correlation measures"},{"paperId":"20c11546a035d2fa2fa1121a7b31e890d20d6b6b","title":"(Meta-) Evaluation of Machine Translation"},{"paperId":"51951073580f6995e55be873db9a7f6a9736ca86","title":"A Study of Translation Edit Rate with Targeted Human Annotation"},{"paperId":"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7","title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":null,"title":"Europarl: A parallel corpus"},{"paperId":"4f09e6ec1b7d4390d23881852fd7240994abeb58","title":"A statistical interpretation of term specificity and its application in retrieval"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f","title":"Automatic evaluation of machine translation quality using n-gram co-occurrence statistics"},{"paperId":null,"title":"Les irréductibles du M23, soit quelques centaines de combattants, étaient retranchés à près de 2000 mètres d’altitude"},{"paperId":"1f7e50d220f41f4fac985a991c8d5187323aab4c","title":"Applications and Explanations of Zipf’s Law"},{"paperId":"3dcda1bec36586b46b1dc67a477beca2c5a105be","title":"MUC-4 evaluation metrics"},{"paperId":null,"title":"Information Retrieval, 2nd edition"},{"paperId":"2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac","title":"Human behavior and the principle of least effort"},{"paperId":null,"title":"2-3 octombrie), Iasi (Palas, 6-7 octombrie), Brasov (Casa Armatei, 14-15 octombrie), Cluj Global (Sala Polivalenta, 20-21 octombrie), Cluj IT (Sala Polivalenta, 22-23 octombrie)"}],"id":"3b34e79610e5acaba352def4323a59f6d531fac7","summary":"It is found that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance, and can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods’ outputs."},{"url":"https://www.semanticscholar.org/paper/b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels","venue":"ArXiv","year":2022,"referenceCount":136,"citationCount":5,"influentialCitationCount":1,"publicationDate":"14/07/2022","authors":"Phillip Rust,Jonas F. Lotz,Emanuele Bugliarello,Elizabeth Salesky,Miryam de Lhoneux,Desmond Elliott","citations":[{"paperId":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"fb41147776fd3ef8c3370ce2574efc15486c9a0f","title":"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding"},{"paperId":"527603f0d3c120bbe56753ce4cd7e5a41e0d5e6a","title":"Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems"},{"paperId":"134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?"}],"references":[{"paperId":"33ebe01c8705a19def511040686d198871145409","title":"Story Beyond the Eye: Glyph Positions Break PDF Text Redaction"},{"paperId":"18c92da1b7f7f8ea6877e5b3a0d5a6df14a09e00","title":"SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners"},{"paperId":"8f26262437bde0ff8fe5e14d5a6cb4cc05a495ef","title":"Multimodal Masked Autoencoders Learn Transferable Representations"},{"paperId":"0d273fe1504a6b4251a773796853c75c2514df03","title":"Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models"},{"paperId":"88b15679a7e60a99d5da1f1fde8aa332b368e1b8","title":"Masked Autoencoders As Spatiotemporal Learners"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"b05f5006a879fab3668365918a15dd35acd77204","title":"Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Models"},{"paperId":"ac6e85cc3f105567e20e019f5752ed38a0a34a25","title":"Breaking Character: Are Subwords Good Enough for MRLs After All?"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"3682a8716817743c6341003b2a121fbd183d6bbc","title":"MAE-AST: Masked Autoencoding Audio Spectrogram Transformer"},{"paperId":"a41196670d894ac8304d06f9bfeb0dd84fa5cb5d","title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"},{"paperId":"a8fffb507d1790851550af5e4ebdd06e5bae1cee","title":"Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"b28e23afe988ad83ab5bd9ce7f826c140c533be8","title":"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages"},{"paperId":"2d439ec2c301d058bd4a8b4743328e3d9939625e","title":"Should You Mask 15% in Masked Language Modeling?"},{"paperId":"2ec3fb471f059fe401c7e1d7d7a199ba4feae814","title":"JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension"},{"paperId":"44f75282915d468413e38ac0e1f59b3ee6860485","title":"Does Transliteration Help Multilingual Language Modeling?"},{"paperId":"d0336e5be72e97e0493b1ba77ef8ec3c349d496a","title":"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"5b540745f4b51f95bf90fb3420e51edb037fc51a","title":"The MultiBERTs: BERT Reproductions for Robustness Analysis"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"dab46dd3985d1de5cd6549319797ab3705b6a801","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"7a670e9c4cf6655cfbcacf169565d4d645c0d475","title":"Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":null,"title":"Story beyond the eye"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"7f631586a368f1762866b01ff9f43c265851d52e","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"03aa38229a6c853d96430edfc8f1542598f2116e","title":"AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain"},{"paperId":"dbd9fab59832369040661bb050daef6de405230c","title":"Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"59c0c6b62e33850cda08663d4c9ecabcf5d21596","title":"IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization"},{"paperId":"8dad3a12d2a652122458dd377f62025354b17a2a","title":"Subword Mapping and Anchoring across Languages"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"964f079b80e6f3ce3164dd883dd1836299f0dba3","title":"GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph"},{"paperId":"06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer"},{"paperId":"a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"a76c98c6814ce8de07707b81c18520af508b7184","title":"BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"13bcfb944779165983aaef22cec8a3bbd3e98e62","title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"29599829d26b4acaaf0ad88698ed4362d33b2ae7","title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"575ac3f36e9fddeb258e2f639e26a6a7ec35160a","title":"Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"3ba96d6f6f4a828b6b32812d7867d86897b88df1","title":"Morphologically-Guided Segmentation For Translation of Agglutinative Low-Resource Languages"},{"paperId":"b62a56f629f77acce9ea69456d67dd77de1c7dfb","title":"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"f5dfed82b0c8747e41a1206f52a6d0ea3dce4a5c","title":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"85dc7829455819283270eb643817bcf97133464d","title":"From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"335a05c16fcbc7a2c2bf5221299de608b08030f0","title":"From Hero to Zéroe: A Benchmark of Low-Level Adversarial Attacks"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"e4ad5c24985127e015be86a53d57e1bb43876b4c","title":"On Romanization for Model Transfer Between Scripts in Neural Machine Translation"},{"paperId":"df29486c04eafd004f2f0816e84c798783802cdf","title":"Transliteration for Cross-Lingual Morphological Inflection"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"14489ec7893e373a0dcc9555c52b99b2b3a429f6","title":"Are All Languages Created Equal in Multilingual BERT?"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"b805693c17961af2cc7f859c1a54320b26036f46","title":"Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"0495d9df8eb84dcdab4e5536179823cd26279949","title":"Big Transfer (BiT): General Visual Representation Learning"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"21079df28af037f93620a0bd63f04fe49cf5df7d","title":"On the Importance of Tokenization in Arabic Embedding Models"},{"paperId":null,"title":"BERT: Pre-training of deep"},{"paperId":"e4f19271b9491b0297a03318a96b8a5157873acd","title":"Writing Across the World's Languages: Deep Internationalization for Gboard, the Google Keyboard"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"6ffb1cc32ddde6ae01e2fc0286eafa116ade0ffb","title":"KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"c0aaee2337e5af680e5dca1bfc349a737dfec573","title":"Fixing the train-test resolution discrepancy"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"e547f4c0728552c1c41df44b7c6469198cf44924","title":"MorphoBERT: a Persian NER System with BERT and Morphological Analysis"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Exploring BERT’s Vocabulary"},{"paperId":null,"title":"Exploring BERT's Vocabulary. Blog Post"},{"paperId":"4fcec7ea62825953ac8d483cfa5c748b4daa4e7e","title":"The Coptic Universal Dependency Treebank"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e74ad670a6cbd693e225b0150f3a33fc1931faf7","title":"Universal Dependencies Version 2 for Japanese"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"2c4574c7f42ac3e887670876a37ea6708a77966e","title":"AROMA: A Recursive Deep Learning Model for Opinion Mining in Arabic as a Low Resource Language"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"b022f2a277a4bf5f42382e86e4380b96340b9e86","title":"SGDR: Stochastic Gradient Descent with Warm Restarts"},{"paperId":"3a29aa4eff48624752c07059a44d3288a678c8ab","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"4361e64f2d12d63476fdc88faf72a0f70d9a2ffb","title":"Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":null,"title":"Ud_chinese-gsd. GitHub repository, 2016"},{"paperId":null,"title":"Ud_chinese-gsd. GitHub repository"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"753e30826f1908a62a8d251fc6b1b598f86d2bb2","title":"Shared Tasks of the 2015 Workshop on Noisy User-generated Text: Twitter Lexical Normalization and Named Entity Recognition"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"82cf69e48ede65b9d1f419da786c0349342d449d","title":"A Gold Standard Dependency Corpus for English"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"77ef6c449d3b7f5f5c55a06564b79eb4438c85b9","title":"Prague Dependency Style Treebank for Tamil"},{"paperId":"2fa38d8b67c7f59ac2a2bf1b53173a973422d8e2","title":"Prague Arabic Dependency Treebank 1.0"},{"paperId":"a7d1c7979b2d8205e9382e9f4fc39eb999fcd955","title":"Building a Large Syntactically-Annotated Corpus of Vietnamese"},{"paperId":"6f59b8056fd7367f5b5088d9c566d6849ea8a663","title":"Hindi Syntax: Annotating Dependency, Lexical Predicate-Argument Structure, and Phrase Structure"},{"paperId":null,"title":"Hassanová. Prague arabic dependency treebank"},{"paperId":"bb7d7dfad94e6a94bf0926fd28e6a0dd7d13b278","title":"Advances on natural language processing"},{"paperId":"944e7729088c5f36cbe0b0801ca0fea432dc5326","title":"Pango, an open-source Unicode text layout engine"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"for an overview of the rendering pipeline"},{"paperId":null,"title":"Transformers: State-of-the-art natural Preprint. Work in progress. language processing"},{"paperId":null,"title":"European Language Resources Association (ELRA"}],"id":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","summary":"PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels, and is more robust to noisy text inputs than BERT, further confirming the benefits of modelling language with pixels."},{"url":"https://www.semanticscholar.org/paper/59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization","venue":"Findings","year":2020,"referenceCount":46,"citationCount":26,"influentialCitationCount":7,"publicationDate":"27/08/2020","authors":"Xinsong Zhang,Hang Li","citations":[{"paperId":"701a9882884a473faa92324ea6c1ff6c9dacc3ce","title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks"},{"paperId":"1d08b41de815c79d3fb874ddb159fbe21828c024","title":"VSCA: A Sentence Matching Model Incorporating Visual Perception"},{"paperId":"beb40cc99a6ab931aa8ea1758c1a0397ecd32847","title":"Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling"},{"paperId":"31d6e59dc275f33da71a72b6b86b38daeed9ffaa","title":"mmLayout: Multi-grained MultiModal Transformer for Document Understanding"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"f8a2428bee464ae69cdb25ce5c2259e88ce576b4","title":"Clickbait Detection of Indonesian News Headlines using Fine-Tune Bidirectional Encoder Representations from Transformers (BERT)"},{"paperId":"05ea28584e5db18c0c31d1aac40e9c1905327557","title":"Effectiveness of Fine-tuned BERT Model in Classification of Helpful and Unhelpful Online Customer Reviews"},{"paperId":"2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"8e1227b0d58b769a0919f8debdfd093ff6f6a65a","title":"MarkBERT: Marking Word Boundaries Improves Chinese BERT"},{"paperId":"18dc24da603896db2264e9174116407a95c593d5","title":"“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"35eeeefcbea6fa8edae4c310ee5ee717861c7e60","title":"Improving Constituent Representation with Hypertree Neural Networks"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"976f47bade21fd787f029142b39631cb17f16ec2","title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation"},{"paperId":"5722d101859846a6a023b8fa00830742203cd0c1","title":"Span Fine-tuning for Pre-trained Language Models"},{"paperId":"27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","title":"LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization"},{"paperId":"4690eb050572a279f94560b6bbdccaae577b45f5","title":"MVP-BERT: Multi-Vocab Pre-training for Chinese BERT"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"964da4ed9ac61b12bc2a7adc1e94e3964cc63861","title":"Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question Answering Data"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"518cb6d4247bdebf21e2811f296b0c7372602a0a","title":"PMI-Masking: Principled masking of correlated spans"},{"paperId":"923d2376103dbc9e9b2af4518b56299d7630b46b","title":"J URASSIC -1: T ECHNICAL D ETAILS AND E VALUATION"}],"references":[{"paperId":"7c5c149699a0ba54b52cd5b9e291077f4a1f9d13","title":"Synthesizer: Rethinking Self-Attention in Transformer Models"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training With Whole Word Masking for Chinese BERT"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"913e9384a1440dbf860f6d8597953187237fa1b4","title":"Disagreement-Regularized Imitation Learning"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"6007bd2a34385132a7885b934d90b519a1f65bba","title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"d56c1fc337fb07ec004dc846f80582c327af717c","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"1b07a24b81834116f6ad1d0232485ba81b9445f3","title":"Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"},{"paperId":null,"title":"2e-5 1.0 C DATA AUGMENTATION To enhance the performance, we conduct data augmentation for the three Chinese classification tasks of TNEWS, CSL, and CLUEWSC2020"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"0de0a44b859a3719d11834479112314b4caba669","title":"A Multiscale Visualization of Attention in the Transformer Model"},{"paperId":"e278e072774f23675266881750e20bca74804cb9","title":"ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"ae43556f2cc1cecb8b48762b4e09df319fbaa4d9","title":"Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"c34b7796a666f5e205693f6bd1e25e993db19075","title":"Probing Prior Knowledge Needed in Challenging Chinese Machine Reading Comprehension"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","title":"Neural Network Acceptability Judgments"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Openwebtext corpus"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"2019) is a large-scale Chinese IDiom cloze test. C3 (Sun et al., 2019a) is a free-form multiple-choice machine reading comprehension for Chinese"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"883d1d06d857a85a0e64bb19f0b17d56f2cc9d7b","title":"KenLM: Faster and Smaller Language Model Queries"},{"paperId":"0f8468de03ee9f12d693237bec87916311bf1c24","title":"The Seventh PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"db8885a0037fe47d973ade79d696586453710233","title":"The Sixth PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"475354f10798f110d34792b6d88f31d6d5cb099e","title":"Automatically Constructing a Corpus of Sentential Paraphrases"},{"paperId":null,"title":"We adopt the standard hyper-parameters of BERT in pre-training of the models. Table 8 shows the hyper-parameters in our"}],"id":"59c0076b3d814588e320820b95563965733d1875","summary":"This paper proposes a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine- grained and coarse-grains tokenizations, which outperforms the existing best performing models in almost all cases."},{"url":"https://www.semanticscholar.org/paper/969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":82,"citationCount":69,"influentialCitationCount":14,"publicationDate":"11/03/2021","authors":"J. Clark,Dan Garrette,Iulia Turc,J. Wieting","citations":[{"paperId":"6f17ca0bf326da71614c528179c2a56b1ceba02e","title":"Curriculum Script Distillation for Multilingual Visual Question Answering"},{"paperId":"0351167c875b8931366e85eb5e517819d7db80cc","title":"What Makes for Good Tokenizers in Vision Transformer?"},{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"11ddb0953eae196dab339bfdc117221594cf945e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy"},{"paperId":"9f9196beee29c02b8c5837c6edbc69967189b735","title":"Efficient Transformers with Dynamic Token Pooling"},{"paperId":"9769992ca9ff1c3894d19f4bd2b6dac27ed1befd","title":"A review on abusive content automatic detection: approaches, challenges and opportunities"},{"paperId":"fc78d652c4d395ba2737ab0406bc53fd025d4aad","title":"Detecting Languages Unintelligible to Multilingual Models through Local Structure Probes"},{"paperId":"9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages"},{"paperId":"4061a9941fa0ff106e884272d9ed753650417ec4","title":"Collateral facilitation in humans and language models"},{"paperId":"dd568e6838903ad7c381f13c1268c94c5db08b02","title":"Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing"},{"paperId":"10672baf790962195677c7581a2fe984032e7f98","title":"Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages"},{"paperId":"265ebfc1074c73917233dbecad802c0c64921a1c","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"695ea88d22e7d39fa3d0b9f53e75d41e82be81d8","title":"Transformers with Learnable Activation Functions"},{"paperId":"ba9a592438447c2a35afc203be40f7f0a2d3fb5a","title":"A Compact Pretraining Approach for Neural Language Models"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"0376c01a9320027694f2dca57236c27f0c5b36eb","title":"Multilingual Abusiveness Identification on Code-Mixed Social Media Text"},{"paperId":"7ed63a4f928fc126f6780d27e75fa6447a00a8c4","title":"Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference"},{"paperId":"12809bcb734beafeb47876f42e7b438e27fe99fe","title":"General-purpose, long-context autoregressive modeling with Perceiver AR"},{"paperId":"7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models"},{"paperId":"2ec281d654f622da7267d7da8145e96bb77a9ede","title":"An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"CHARFORMER: FAST CHARACTER TRANSFORMERS"},{"paperId":"610088e1d7a8e44f921b2c4894fe1a7b204ce5a9","title":"Vicomtech at LivingNER2022"},{"paperId":"e11d8663ccf2cc412f408853fa5f19ebac75df54","title":"How to encode arbitrarily complex morphology in word embeddings, no corpus needed"},{"paperId":"b1be10b76314ea8569249f2310f1e6eead8a5adc","title":"Building Domain-specific Corpora from the Web: the Case of European Digital Service Infrastructures"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"12fec03c538c7aaeca1a4e1ab8d66aed5f793fc0","title":"reamtchka at SemEval-2022 Task 6: Investigating the effect of different loss functions for Sarcasm detection for unbalanced datasets"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"e43a360728e34e43934707665db0f7be02c8e5a7","title":"Interpreting BERT-based stance classification: a case study about the Brazilian COVID vaccination"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5","title":"Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"dca4128a33ca22c02031b5c0c28548a0df022d80","title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"b62a56f629f77acce9ea69456d67dd77de1c7dfb","title":"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"},{"paperId":"58403c6bc061670f7ce6267a3a0cd01ba1bb8e50","title":"Intérêt des modèles de caractères pour la détection d’événements (The interest of character-level models for event detection)"}],"references":[{"paperId":"b57da3ccf214e8dad49116c8db9590c2c89629f5","title":"MasakhaNER: Named Entity Recognition for African Languages"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"9ed25f101f19ea735ca300848948ed64064b97ca","title":"Random Feature Attention"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"88340786475bf1649b83b7ac0ad7f57e60a20b52","title":"Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"2a218786f4615b82389f78472e7ff22e6ce57490","title":"ConvBERT: Improving BERT with Span-based Dynamic Convolution"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b1a71677a13299755a12375f0c982484088aa9ef","title":"English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"56676aef356ebb13cba77fc9e4d70760fbc151f5","title":"ETC: Encoding Long and Structured Inputs in Transformers"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b","title":"RobBERT: a Dutch RoBERTa-based Language Model"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"bc789aef715498e79a74f857fa090ece9e383bf1","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"},{"paperId":"d619570b03e629653d69b5157764be183e8521bf","title":"UnihanLM: Coarse-to-Fine Chinese-Japanese Language Model Pretraining with the Unihan Database"},{"paperId":"8912eaad7d9c779bd698c73409e410fdf8c5e3c2","title":"PRADO: Projection Attention Networks for Document Classification On-Device"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"8a7e9f91d949764d6159c114d4feb961ec07b32d","title":"Training Hybrid Language Models by Marginalizing over Segmentations"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"1d51b59fcf0297e8df931c8b614bd55165b24172","title":"Better Character Language Modeling through Morphology"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"e07f2c383a34217a9403a4d58a4e7d61e57d9163","title":"Character Eyes: Seeing Language through Character-Level Taggers"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"9852ae077c7da6a9d178acaa2b44a335289507a6","title":"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"bae055edb1d552c24bbea56556bafdde9ef61c50","title":"On the Strength of Character Language Models for Multilingual Named Entity Recognition"},{"paperId":"b23300ff8ba9fcd82e349da03a6f13386bff0077","title":"What do character-level models learn about morphology? The case of dependency parsing"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"5b1516c87818084dc5d195cc274e1ee8923210d2","title":"Neural Cross-Lingual Named Entity Recognition with Minimal Resources"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"4dd9d67a0259eef54ca32770db24fab5e42d362a","title":"Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction"},{"paperId":"771c1cb5fb161231e9aaa0a189caba672256a880","title":"Using Morphological Knowledge in Open-Vocabulary Neural Language Models"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"fc7e6502dace26305e3e062e426034f61a18095e","title":"Byte-Level Machine Reading Across Morphologically Varied Languages"},{"paperId":"0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b","title":"Neural Lattice Language Models"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"b959e905c093e1aa4a27eda0c0f1b4f727b93e63","title":"Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"07466fb914982f55051cc0b236fd524bdcd8bdc7","title":"Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"45e3f381d16e6d6db5449b7a44b7bdf294a8a822","title":"Multiscale sequence modeling with a learned dictionary"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a921e014249395496c22783fe58d293746af852b","title":"From Characters to Words to in Between: Do We Capture Morphology?"},{"paperId":"ed5003e6a2b97dd4b65c5190ccb88b9c45b032b8","title":"Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"},{"paperId":"664ec878de4b7170712baae4a7821fc2602bba25","title":"Learning to Generate Reviews and Discovering Sentiment"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"12e9d005c77f76e344361f79c4b008034ae547eb","title":"Charagram: Embedding Words and Sentences via Character n-grams"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"4dabd6182ce2681c758f654561d351739e8df7bf","title":"Multilingual Language Processing From Bytes"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"fdb813d8b927bdd21ae1858cafa6c34b66a36268","title":"Learning deep structured semantic models for web search using clickthrough data"},{"paperId":"31100d7f8fdd3d263238624a888dab0f7aba3d5a","title":"Adaptor Grammars for Learning Non-Concatenative Morphology"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"727209a57c643472c8fc166ba3cc373936acc8d0","title":"Learning a Part-of-Speech Tagger from Two Hours of Annotation"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"9fae0f18e23db076e27b23f17a417f3149f63e2e","title":"TweetMotif: Exploratory Search and Topic Summarization for Twitter"},{"paperId":"e9d100404934e025a2df61882faf37ae2031af03","title":"Bloom Maps"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"502858989368adae91e0f4a643ebc2aa6efd5738","title":"Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":null,"title":"Language-wise breakdown for TYDI QA primary tasks"}],"id":"969287b8a96e242793b11f0dbb99ec341228106f","summary":"Canine is presented, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias."},{"url":"https://www.semanticscholar.org/paper/485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation","venue":"Machine Translation","year":2018,"referenceCount":26,"citationCount":24,"influentialCitationCount":1,"publicationDate":"19/10/2018","authors":"Elizabeth Salesky,Andrew Runge,Alex Coda,J. Niehues,Graham Neubig","citations":[{"paperId":"9d83941a205e31d394f614424cdc553cea9e35f9","title":"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages"},{"paperId":"3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class"},{"paperId":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"24fd022750ea88e44e6790c7c7e1923635885c71","title":"Translation Mechanism of Neural Machine Algorithm for Online English Resources"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"6dc710b46bc510a42af487a3ac220e4fabf4d518","title":"VOLT: Improving Vocabularization via Optimal Transport for Machine Translation"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"38b40ae531ddca434de07015637b78413370d15a","title":"The Roles of Language Models and Hierarchical Models in Neural Sequence-to-Sequence Prediction"},{"paperId":"405cd5cd6e056675d4545eba12742174cc75d7e7","title":"Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes"},{"paperId":"8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","title":"Adversarial Subword Regularization for Robust Neural Machine Translation"},{"paperId":"4d08dcd2cc1e9691defe664a10f021424a896a1e","title":"Neural Machine Translation: A Review"},{"paperId":"201fae97e51fb6aea7ed8120147e806e43834de6","title":"Neural Machine Translation: A Review and Survey"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation"},{"paperId":"95236a88cc959656958d7a49422f4004015d594d","title":"Comparison between NMT and PBSMT Performance for Translating Noisy User-Generated Content"},{"paperId":"fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a","title":"Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation"},{"paperId":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019"}],"references":[{"paperId":"ab77f32382d63bceaa3ff9b8267e4e086945fa82","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"491d29cb0018578b47e9abe19b93d5b65dc59113","title":"Improving Character-Based Decoding Using Target-Side Morphological Information for Neural Machine Translation"},{"paperId":"ac7c04a668bdb0e3eff168e65cb85689b4f7ef57","title":"Lifelong Learning with Dynamically Expandable Networks"},{"paperId":"b36a5bb1707bb9c70025294b3a310138aae8327a","title":"Automatic differentiation in PyTorch"},{"paperId":"69a037af886a6235d9af3cdeef4c7233d34c86ce","title":"Growing a Brain: Fine-Tuning by Increasing Model Capacity"},{"paperId":"1f080959faf9bf2a282c0aadbd8584d8b32f6e24","title":"Modeling Target-Side Inflection in Neural Machine Translation"},{"paperId":"69ffe3277011087b55afaeaef0a3933160beee9a","title":"Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English"},{"paperId":"100104b980d56a40cadfbd7034fa7807ce49b3fb","title":"Nematus: a Toolkit for Neural Machine Translation"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":null,"title":"Autodiff Workshop: the future of gradient-based machine learning software and techniques"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"16cb6876666f3a7b56a636c1d85ad00bd0d98bf3","title":"Net2Net: Accelerating Learning via Knowledge Transfer"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"6dab1c6491929d396e9e5463bc2e87af88602aa2","title":"Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"88b66f705a329da8292e7b8aa4bfe26de4759cfa","title":"Machine Translation without Words through Substring Alignment"},{"paperId":"791c0df5557890e7a4d8c81b12cd966531e7b42c","title":"Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability"},{"paperId":"8f4987d648d0daf3e36172e1abcda796fe8cf865","title":"An exponential translation model for target language morphology"},{"paperId":"42fc4c6580bfa54d57b5d6c56b5dfde58c6f6abb","title":"English-to-Czech Factored Machine Translation"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"319af0958e268b3243975b8628262ebc1980ce40","title":"of the European Chapter of the Association for Computational Linguistics"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":null,"title":"Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations"}],"id":"485b3f77b9913e151e7ca897d99497e70e7f30d1","summary":"This paper incrementally introduces new BPE vocabulary online based on the held-out validation loss, and matches the results found with grid search, optimizing segmentation granularity while significantly reducing overall training time."},{"url":"https://www.semanticscholar.org/paper/ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features","venue":"International Conference on Computational Linguistics","year":2018,"referenceCount":17,"citationCount":16,"influentialCitationCount":0,"publicationDate":"12/04/2018","authors":"Makoto Morishita,Jun Suzuki,Masaaki Nagata","citations":[{"paperId":"2dcdc4ef997507630f2c9b1b6682646ae31bdb7f","title":"SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models"},{"paperId":"2e6028f8b156c8b344e1a68d15d88403a978c71d","title":"Learning Multiscale Transformer Models for Sequence Generation"},{"paperId":"c68959f4da94fd35da5e8649465177b24d0dd351","title":"Byte-based Multilingual NMT for Endangered Languages"},{"paperId":"bfbfeb0613beae296d76077b2adc9e38a6b678a4","title":"Sub-Subword N-Gram Features for Subword-Level Neural Machine Translation"},{"paperId":"188f913be48a218b44b0bd964662b4926fd0b0b8","title":"Neural Machine Translation: A Review of Methods, Resources, and Tools"},{"paperId":"ca83469977ab49baae02ef1e12f419120927ecdc","title":"Mixed-Level Neural Machine Translation"},{"paperId":"fac7f9b04fba4889b445e309d384644d15ee7e88","title":"Trends and Advances in Neural Machine Translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation"},{"paperId":"9f3f6deeb1f03ebd52f9ab44275ad382fe60d073","title":"Latent Part-of-Speech Sequences for Neural Machine Translation"},{"paperId":"0aef56962035a79101821480f51897fdc4443945","title":"Character n-gram Embeddings to Improve RNN Language Models"},{"paperId":"0ab0fda8774c303be8f8f8c8f684a890dcf5d455","title":"Lattice-Based Transformer Encoder for Neural Machine Translation"},{"paperId":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019"},{"paperId":"247328a082d86199ed5a98e1d726aa205c1da9df","title":"Neural Machine Translation"}],"references":[{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"56cf69bbe6598471d1655a4d0ccd4a61e946a532","title":"NTT Neural Machine Translation Systems at WAT 2017"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627","title":"Six Challenges for Neural Machine Translation"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"503981515e1a209f57f8119f616cc0f0c6bf168d","title":"Kyoto University Participation to WAT 2017"},{"paperId":"3bc6bcb60c7c00efcb471191fb62fd3ebd231d66","title":"Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions"},{"paperId":"46d0aa6b357c5427f46c7f8ff7053617c4309649","title":"Linguistic Input Features Improve Neural Machine Translation"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"09cd7876b72d6105c83db59052572433a0a2b36c","title":"WIT3: Web Inventory of Transcribed and Translated Talks"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"ab139e341c005929848a7326f3d44f8a6aa9863c","summary":"It is confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets and the assumption that in the NMT model, the appropriate subword units for the following three modules can differ is confirmed."},{"url":"https://www.semanticscholar.org/paper/205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training","venue":"","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Jing Huang,Zhengxuan Wu,Kyle Mahowald,Christopher Potts","citations":[],"references":[{"paperId":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"e638b9369e9b588b1c4fcfeee6409e51b4625f9b","title":"Causal Proxy Models for Concept-Based Model Explanations"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"32afdb07021fda775ceaedd231c58bfed0aa980a","title":"Automated Crossword Solving"},{"paperId":"be9bb35bc3dc5630ff09895129d4d515aa94ed97","title":"AmbiPun: Generating Humorous Puns with Ambiguous Context"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"075054b8038dd34089b54e20f2aa4c402d0bd29c","title":"Causal Distillation for Language Models"},{"paperId":"ccd04c27bf1237368b35eb456b3dd1c18ef9a9b9","title":"Inducing Causal Structure for Interpretable Neural Networks"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"5951ede73a1dbb9c5ea8b4d95f31c5ed646aacbd","title":"Causal Abstractions of Neural Networks"},{"paperId":"8b723be33e62bf5bd9278769244f1c13a9510898","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP"},{"paperId":"538f8e8a36e70ca408f2c5fb6f10f303c52fc317","title":"Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":null,"title":"Why don’t people use characterlevel machine translation? arXiv preprint arXiv:2110.08191"},{"paperId":"2d9fa173baa6e07112eaa0ae89b85f37cc2c000a","title":"Homophonic Pun Generation with Lexically Constrained Rewriting"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"03826718c8c47d8f1f25e437fd6ff15165162e8c","title":"Will it Unblend?"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"3cfd09d6d14fb9a34ee72a487cd610d168a60530","title":"Attentive Mimicking: Better Word Embeddings by Attending to Informative Contexts"},{"paperId":"b959805c9af504d961f15905e581c317d75a0a46","title":"Abstracting Causal Models"},{"paperId":"648f2e457fb287cc2b5661d784aa372d8ad403fb","title":"Approximate Causal Abstractions"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"68c03788224000794d5491ab459be0b2a2c38677","title":"WordNet: A Lexical Database for English"},{"paperId":null,"title":"2022. Faithful, interpretable model explanations via causal abstraction"}],"id":"205cc15fca6963b355e4c071071368e874ee103e","summary":"While simple character-level tokenization approaches still perform best on purely form-based tasks like string reversal, this method is superior for more complex tasks that blend form, meaning, and context, such as spelling correction in context and word search games."},{"url":"https://www.semanticscholar.org/paper/353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering","venue":"","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Rosanne Liu,Daniel H Garrette,Chitwan Saharia,William Chan,Adam Roberts,Sharan Narang,Irina Blok,R. Mical,Mohammad Norouzi,Noah Constant","citations":[],"references":[{"paperId":"1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe","title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"38b0567e83386ddc294d6c81b541deacbd8e3c2a","title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"231af7dc01a166cac3b5b01ca05778238f796e41","title":"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":null,"title":"Studio shot of words \"I like coffee because it gives me the illusion that I might be awake"},{"paperId":null,"title":"a picture of a powerful-looking vehicle that looks like it was designed to go off-road, with a text saying \"i'm a truck"},{"paperId":null,"title":"Quails of North America"},{"paperId":null,"title":"Studio shot of words \"the food is terrible and the portions are too small\" made from hotdogs, museum quality, framed photo"},{"paperId":null,"title":"with a fork and knife sticking out of the oyster, with a caption that says \"oysters for lunch"},{"paperId":null,"title":"Time is temporary, everything is temporary"},{"paperId":null,"title":"dslr portrait of a robot is holding a sign with text \"i am not a robot"},{"paperId":null,"title":"A hastily handwritten note that says \"I'll be back at"},{"paperId":null,"title":"A storefront with \"The world's best deli\" written on it"},{"paperId":null,"title":"Grape vines in the shape of text 'open your mind' sprouting out of a head with flowers and butterflies. DSLR photo"}],"id":"353d6a18a222aec0be66de0b8be111fbbe67012d","summary":"This paper trains a suite of image generation models, and shows that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (the authors' DrawText benchmark), and sets a much higher state-of-the-art on visual spelling."},{"url":"https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing","venue":"ArXiv","year":2021,"referenceCount":301,"citationCount":50,"influentialCitationCount":6,"publicationDate":"12/08/2021","authors":"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha","citations":[{"paperId":"99c951469b1f79ffd8f44e0da5a3301d6ec10455","title":"Identifying COVID-19 english informative tweets using limited labelled data"},{"paperId":"a3129f5e4d6505376f8f2661db137853a582a819","title":"Understanding Postpartum Parents' Experiences via Two Digital Platforms"},{"paperId":"49ab76e76b0943862d6f3c6b99d78cc1296f91d2","title":"HeartBEiT: Vision Transformer for Electrocardiogram Data Improves Diagnostic Performance at Low Sample Sizes"},{"paperId":"3c861f509f9da5133648a3e8f2b891b7f7638491","title":"MaNLP@SMM4H’22: BERT for Classification of Twitter Posts"},{"paperId":"3f27b81416206977a22270fa9eaf520a185d1c42","title":"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers"},{"paperId":"f1f78bbb3e146874501e3c52b56cff6abf731842","title":"Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges"},{"paperId":"2fb65b092ea0df14a8f54645329ed811fb10e4b1","title":"PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations"},{"paperId":"13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","title":"MarianCG: a code generation transformer model inspired by machine translation"},{"paperId":"8458052b99daeb81d95716100e916bf785313b0b","title":"Research on the Classification of Policy Instruments Based on BERT Model"},{"paperId":"119e72daf28e3ac9e0950cfa827ad2b1e81857b6","title":"CPSAA: Accelerating Sparse Attention using Crossbar-based Processing-In-Memory Architecture"},{"paperId":"8971d2a43a11b4beca31afadf88d5ed8cda75297","title":"Block Format Error Bounds and Optimal Block Size Selection"},{"paperId":"f5669510806e6a671cece0920b7593adafdae7d8","title":"JoeyS2T: Minimalistic Speech-to-Text Modeling with JoeyNMT"},{"paperId":"0f3d7b2bb0b3bd3bad87a39c545d93ddfd383362","title":"Selective Token Generation for Few-shot Natural Language Generation"},{"paperId":"17f2a787db8cf5104ffa71ca619d8f8092b05ca5","title":"A Survey on Generative Diffusion Model"},{"paperId":"2227980ce08aebbf18a42d4abea42381062c4bd5","title":"Method of Transformation of Image Classification Labels into Segmentation Masks"},{"paperId":"2a672342035defd8d75b54e08597ef124c6a0172","title":"Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models"},{"paperId":"0ab111fce85d37389b93ea5ca8caca14c755c3cf","title":"Attention-based Dependability Prediction for Industrial Wireless Communication Systems"},{"paperId":"74671b4f898daf2b8234d5a4cdf2f25b81f1e4fd","title":"TIMS: A Novel Approach for Incrementally Few-Shot Text Instance Selection via Model Similarity"},{"paperId":"5b71f03167bea77cde1ad15cf95979f2d08f9e45","title":"TransDBC: Transformer for Multivariate Time-Series based Driver Behavior Classification"},{"paperId":"c27c68597065dad70cb0b4f882cd861826bf28fa","title":"Automated Construction Contract Summarization Using Natural Language Processing and Deep Learning"},{"paperId":"1429d4b6f6c19d6447d60d8bed05841c5de72b38","title":"FluSa-Tweet: A Benchmark Dataset for Influenza Detection in Saudi Arabia"},{"paperId":"0fb235cc59cd7198b5a1494157b0250cfca04386","title":"Detecting Harmful Online Conversational Content towards LGBTQIA+ Individuals"},{"paperId":"e86869d44e78d4cffd1bf1b62f2f8e56a519e23c","title":"E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation"},{"paperId":"277dd73bfeb5c46513ce305136b0e71fcd2a311c","title":"Recipe for a General, Powerful, Scalable Graph Transformer"},{"paperId":"52dccf13f442e7451f4b81db203b24e056142557","title":"TransGrasp: A Multi-Scale Hierarchical Point Transformer for 7-DoF Grasp Detection"},{"paperId":"2002883eecd8f8e0c094c357defa5dcc40b081d9","title":"UMass PCL at SemEval-2022 Task 4: Pre-trained Language Model Ensembles for Detecting Patronizing and Condescending Language"},{"paperId":"77c1eed5c13928d39c5b62ba6063b47b10ab6636","title":"Spatial Transformer Network on Skeleton-based Gait Recognition"},{"paperId":"94d05045e9bf69e015f5398086ac5c27a70d13e6","title":"A Comparative Evaluation Of Transformer Models For De-Identification Of Clinical Text Data"},{"paperId":"0ec91fd9c5c3ff9758d9cfddc2e5046eaa3c1ab0","title":"Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking"},{"paperId":"627be995900638aef82279a22013a7b03b5d732d","title":"A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification"},{"paperId":"b6adc972f4bdbbb4dc7143713d16a3408a71ef7e","title":"Automated Customer Complaint Processing for Water Utilities Based on Natural Language Processing—Case Study of a Dutch Water Utility"},{"paperId":"7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"},{"paperId":"da1ee6de15da1c28d0069e3276f447f2371d281c","title":"Video Transformers: A Survey"},{"paperId":"489af1ae6db21c198b6efeffc3ba68313f9bd4b3","title":"Neuromorphic Camera Denoising using Graph Neural Network-driven Transformers"},{"paperId":"420c897bc67e6f438db522d919d925df1a10aa8c","title":"AMMU - A Survey of Transformer-based Biomedical Pretrained Language Models"},{"paperId":"b42e3a759348f27cca2f918a6bd0b139a5312e44","title":"A Survey of Pretrained Language Models Based Text Generation"},{"paperId":"a40ca8eba2e45b453ee98b846edab928f89b5342","title":"PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations"},{"paperId":"e85c5fdc4c44d3e17c45c239adfad8d0942e2805","title":"Text-to-Text Extraction and Verbalization of Biomedical Event Graphs"},{"paperId":"cbf284fe85795eaeb94bfb3dc9e98276dcd33788","title":"Neural Architecture Search for Transformers: A Survey"},{"paperId":"f3c6ffbb9ed061ebb17623c066b84c596f55b796","title":"міток маски"},{"paperId":"4e8e404a6fff91ac55f9c7476a041e2ba31c0454","title":"Know Better – A Clickbait Resolving Challenge"},{"paperId":"bdc2819cbb6e950d4d96bd492e7484717177831d","title":"A COMPARATIVE EVALUATION OF TRANSFORMER MODELS"},{"paperId":"551dc8a18770a5807c2ea0724701c3f946bc7c0c","title":"Hindi/Bengali Sentiment Analysis Using Transfer Learning and Joint Dual Input Learning with Self Attention"},{"paperId":"6fb5dc674bf0013ad5e269b3905c0f4253c3890b","title":"Investigating Cross-Linguistic Gender Bias in Hindi-English Across Domains"},{"paperId":"1b6798695de27880009346c6c2023665139b0014","title":"Ontology-Based Question Answering over Corporate Structured Data"},{"paperId":"7c59a4e11eed57aea54d27bbb28a3e4dbc26b0f3","title":"Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling"},{"paperId":"d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","title":"A Primer on Pretrained Multilingual Language Models"},{"paperId":"d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5","title":"Pretrained Language Models for Text Generation: A Survey"},{"paperId":"ca6d0b5ea60687b707fb5ed35f7433f939537e2a","title":"Sentiment Analysis on Dravidian Code-Mixed YouTube Comments using Paraphrase XLM-RoBERTa Model"},{"paperId":"00f899ec60300ca972962e6014d63b4ea835ef38","title":"CMS Optimisation with Deep Learning Techniques"}],"references":[{"paperId":"a93632237958800217341d7bad847200afdd60e3","title":"Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better"},{"paperId":"370b680057a6e324e67576a6bf1bf580af9fdd74","title":"Self-Supervised Learning: Generative or Contrastive"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"4566c0d22ebf3c31180066ab23b6c445aeec78d5","title":"Deduplicating Training Data Makes Language Models Better"},{"paperId":"94b38a1cf2905a9f8dd90f5e22f904a07a59b6bc","title":"XLM-E: Cross-lingual Language Model Pre-training via ELECTRA"},{"paperId":"966a38882be844dbf7e8b15478e1bdf3c75ef8a6","title":"A Closer Look at How Fine-tuning Changes BERT"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"d8d2e574965fe733eb1416e03df2b5c2914fc530","title":"A Survey of Transformers"},{"paperId":"981995fd64611f475179b280f4e9c241051ac185","title":"Knowledge Inheritance for Pre-trained Language Models"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"d4f5f1a196e203226e4a69d52a04d46823f32fb3","title":"Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"8994bce4b85a8b4087584661c49f8776f868f7dd","title":"Interpretable Bias Mitigation for Textual Data: Reducing Genderization in Patient Notes While Maintaining Classification Performance"},{"paperId":"3a906b77fa218adc171fecb28bb81c24c14dcc7b","title":"Transformers in Vision: A Survey"},{"paperId":"a79b520571f7373cbeb8c6ffc02f6a719b3bce38","title":"CODER: Knowledge infused cross-lingual medical term embedding for term normalization"},{"paperId":"9baab08fbe37369856688b2abe5b3c90cce1682c","title":"Compression of Deep Learning Models for Text: A Survey"},{"paperId":"54523ff961a1ac57a86696ef9a53b3a630b482c0","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"ff2f48fe6438adcaf860aac0f41c584568beafb5","title":"CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision"},{"paperId":"4237cbebe788a97174f48dc398082739bbffe95b","title":"FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark"},{"paperId":"2ee03e28208a9310a9be4032c2b04ebdddb83cc7","title":"FLEX: Unifying Evaluation for Few-Shot NLP"},{"paperId":"8b954e1654c6b759a957fd11e66c111e6105fb3f","title":"CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding"},{"paperId":"cf5e670a79847d9be0eb185fb372d99d30d4d98f","title":"Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains"},{"paperId":"114aa720872462b0ca1b97bfdec0ebd56c36fd0a","title":"Towards Understanding and Mitigating Social Biases in Language Models"},{"paperId":"a6a7724763d8adba466519489b0b9d209e7f2d15","title":"BARTScore: Evaluating Generated Text as Text Generation"},{"paperId":"00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d","title":"CPM-2: Large-scale Cost-effective Pre-trained Language Models"},{"paperId":"c6fd846b9b8f9eb0a492d6d6242fffce987c4580","title":"Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning"},{"paperId":"2384c92bbde47f5dbc8d8f175aa67e0f95c413d4","title":"FastSeq: Make Sequence Generation Faster"},{"paperId":"7509c66a666e2e3f14bc8676b969b945ee6e136f","title":"CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"},{"paperId":"077c713bccd9d2c7fde68d4cbde06ab0f07a6855","title":"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"},{"paperId":"0934952763f1549e75b142261e73f2b27a2f495b","title":"RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model"},{"paperId":"28459083ba624020c8f1c1ed7c3a075f48b4e709","title":"KLUE: Korean Language Understanding Evaluation"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"c553280c1fc1d0bc7b94683bb75910e309b0d579","title":"Larger-Scale Transformers for Multilingual Masked Language Modeling"},{"paperId":"6563251e69e4378c189d0a0c94d8d19508d552c8","title":"MathBERT: A Pre-Trained Model for Mathematical Formula Understanding"},{"paperId":"78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f","title":"PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"},{"paperId":"b4f30496a8fa212a40461ca1bdef32169e998902","title":"Efficient pre-training objectives for Transformers"},{"paperId":"279a19b9eba7afd513394c7a733834b0f41f97fb","title":"mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs"},{"paperId":"c5dd0c12353d683179fae1df079d5c5ae0e2cd23","title":"Dual-View Distilled BERT for Sentence Embedding"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"0e027f8206a4d04f0b50b88dfe31e7f2f46e2d60","title":"IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation"},{"paperId":"7b99c51d562e33309a46601c846abbe72a65c6a4","title":"What to Pre-Train on? Efficient Intermediate Task Selection"},{"paperId":"2435c04832d486975304a094e55ecbab8acf8a5f","title":"Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"b6647280615f667dd7418bfb9b13d828a22c1cfe","title":"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning"},{"paperId":"1cf2e9e198feef3893da2800a7949f6880ddc084","title":"ExplainaBoard: An Explainable Leaderboard for NLP"},{"paperId":"6fd41ee12ae828bb7de965bf7e06bb1bd5259fe7","title":"IndT5: A Text-to-Text Transformer for 10 Indigenous Languages"},{"paperId":"96afd5a4bcf814fc9db32b1fd0324c3dbb63ed61","title":"MuRIL: Multilingual Representations for Indian Languages"},{"paperId":"128917425601a541c93c600a2f67d654512928bb","title":"GPT Understands, Too"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"57e4877fc700c075a3ed332b3695551e8b7e8b94","title":"Bidirectional Representation Learning From Transformers Using Multimodal Electronic Health Record Data to Predict Depression"},{"paperId":"50068fbea4d1cafcf4c99873ab272c701c08dfcb","title":"OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language Models"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"cbd4f06a08eb4223b97c9079007a87dda4339afe","title":"Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models"},{"paperId":"824cd8db8a68732db04f4d8b7139eb4475e59ff2","title":"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"},{"paperId":"fcfc9648561a221750b8085790ad9ba1bebb1800","title":"Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models"},{"paperId":"86b369759bff13ec23b45ca23cc5461292a75415","title":"WangchanBERTa: Pretraining transformer-based Thai Language Models"},{"paperId":"7e6983efd9eaad24e7ac520e4fa437a0ac945e9f","title":"GENIE: Toward Reproducible and Standardized Human Evaluation for Text Generation"},{"paperId":"0e8afb694ad0a84c5deafd4eedfb75e953776e65","title":"Cloud-based intelligent self-diagnosis and department recommendation service using Chinese medical BERT"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"9f9fe98b60d75c6407726efff8193e8bee3ee13b","title":"KoreALBERT: Pretraining a Lite BERT Model for Korean Language Understanding"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"c342798bafc1eaaa60c652fc90fd738941542133","title":"AraGPT2: Pre-Trained Transformer for Arabic Language Generation"},{"paperId":"aea9f18d70a78d39ca927f2baa143e084c486086","title":"AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"1e4cda8be54999ced1324777fa462a85e2c9746c","title":"ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"21d347db8adc949b0ad08e9c42b66a63739c866f","title":"ParsiNLU: A Suite of Language Understanding Challenges for Persian"},{"paperId":"5fe78eb0f142902237df11cb67c455787a759172","title":"GLGE: A New General Language Generation Evaluation Benchmark"},{"paperId":"35dffa79dc5c511d7cda7b6c25c0562eba3a5e0c","title":"Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning"},{"paperId":"9927a15ddf5313d97c98f0111fd191caf507ce72","title":"HateBERT: Retraining BERT for Abusive Language Detection in English"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"615204452304331004532c5800399ef55d58b4c7","title":"Self-Alignment Pretraining for Biomedical Entity Representations"},{"paperId":"03935e520c612ac9f137d9e9ef388e0c08568b60","title":"UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus"},{"paperId":"0c775d7ed34fb4690b4291490778649ae75c48d2","title":"TurboTransformers: an efficient GPU serving system for transformer models"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"a6a83754a0d1e9a8e41c1e9bbdbca32d3b9d1fd3","title":"It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"},{"paperId":"4ceff7472c04ee6d76bce89d61ba4b445d8dbf74","title":"InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training"},{"paperId":"1882f194cb43828852cc052887671e55a80f945a","title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"},{"paperId":"8b9d77d5e52a70af37451d3db3d32781b83ea054","title":"On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"be158d5ab493b2f2dae736a2ca92afcd66ed5be4","title":"ParsBERT: Transformer-based Model for Persian Language Understanding"},{"paperId":"5d4de0fa45aeddc31142e6a24666d06ed7923f1e","title":"Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction"},{"paperId":"64b4d2c2181e3fb4ecacf797bf5f35db203c1437","title":"MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask Learning"},{"paperId":"0171ad4cc87cc7db25b4ec3169e293eed9a13b39","title":"Training with Quantization Noise for Extreme Model Compression"},{"paperId":"738215a396f6eee1709c6b521a6199769f0ce674","title":"Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"},{"paperId":"f48f90464d9694e2ea18767f14842c64c9a1e8fb","title":"WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia"},{"paperId":"a9693b7b57f203940889de6d3f979c70c09202ed","title":"Shuffled-token Detection for Refining Pre-trained RoBERTa"},{"paperId":"2ada5bea5e567313fdac9541725fac0cdc49bc36","title":"BanglaBERT: Combating Embedding Barrier for Low-Resource Language Understanding"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"76db3624f1278a4f4f1e69b343bfff7bba306d47","title":"XLM-T: A Multilingual Language Model Toolkit for Twitter"},{"paperId":"b9478e237b58160c65acd2c41894493d27e2c277","title":"WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"},{"paperId":"cbd78779af4e83fe101ba3f7ba4d4786388d12d8","title":"Semantic Re-tuning with Contrastive Tension"},{"paperId":"1554887c6bd76c443a477b27dbcab35877787b27","title":"LightSeq: A High Performance Inference Library for Transformers"},{"paperId":"5322e5936e4a46195b1a92001467a2350fe72782","title":"KART: Privacy Leakage Framework of Language Models Pre-trained with Clinical Records"},{"paperId":"092442a694b811dff5b7715fba9e363e0ce4108c","title":"SentiX: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis"},{"paperId":"5eea6a9de39c41715e105f5943ac0fcb98fa245c","title":"Enhancing Clinical BERT Embedding using a Biomedical Knowledge Base"},{"paperId":"aa8f5faf8890b6846d4da9cb7e60a8df6e96ba4a","title":"KD-Lib: A PyTorch library for Knowledge Distillation, Pruning and Quantization"},{"paperId":"70b0c85638d195dbde56cbedc94ae4363b272b58","title":"A Pre-Training Technique to Localize Medical BERT and to Enhance Biomedical BERT"},{"paperId":"b70ee890b5e0ec7d6db04cfd5471b3bbbd0320fc","title":"Self-Supervised Learning from Contrastive Mixtures for Personalized Speech Enhancement"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"1109d62ebd2b29a7dc148bc30dd6cfc803a63dec","title":"IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP"},{"paperId":"d7b93f247f9e46115d6b78a67a458c44de0f9039","title":"iNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages"},{"paperId":"28ba41305e8268592ec829600f5a3b54cd10fbca","title":"Learning from Unlabelled Data for Clinical Semantic Textual Similarity"},{"paperId":"7c462df4adefcf90af3c27ce7f7a8d83efbff2b0","title":"Measurement of Semantic Textual Similarity in Clinical Texts: Comparison of Transformer-Based Models"},{"paperId":"c81f36f343b583a1d53e98a181cefb277d22fe17","title":"Identification of Semantically Similar Sentences in Clinical Notes: Iterative Intermediate Training Using Multi-Task Learning"},{"paperId":"70efbd71c840e78d0ecee101d389db0aaea93652","title":"exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources"},{"paperId":"dc6d598b44f82b905079c5947bd0a3bcb30605d2","title":"RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark"},{"paperId":"b68b2e81ae2de647394ec05ee62ecf108bf2b50a","title":"Eliciting Knowledge from Language Models Using Automatically Generated Prompts"},{"paperId":"9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2","title":"TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"},{"paperId":"5ad57623099f1fb6045a67fee313fee2573ef5ec","title":"A Benchmark for Lease Contract Review"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"1d95011355628f7aef068ab1914198e43258c530","title":"BERTimbau: Pretrained BERT Models for Brazilian Portuguese"},{"paperId":"708dcd8456426cd609c89a86344e0007c04c80bf","title":"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models"},{"paperId":"6d6595766a35f12a6ad671d05634b5e2159d4f3e","title":"Bio-Megatron: Larger Biomedical Domain Language Model"},{"paperId":"f6245b3e6270e4dc2e279c4b728030523dffcff4","title":"LEGAL-BERT: The Muppets straight out of Law School"},{"paperId":"1c6f94fb3d888167355afb580f04d55cd517ebc6","title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"},{"paperId":"2274b14cd3f513bee527a92f5859d14aea093aaa","title":"DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue"},{"paperId":"097210dc65924f8ce59523faf444e635523dc714","title":"TernaryBERT: Distillation-aware Ultra-low Bit BERT"},{"paperId":"574c7c7260a795d2906f03b5229ace3f54ee0ab3","title":"An Unsupervised Sentence Embedding Method by Mutual Information Maximization"},{"paperId":"8b0a0f6d1cd6f3aa9b54be45d5127bb016a98171","title":"The birth of Romanian BERT"},{"paperId":"03f22e693a0c00bae8a66a64a2fecb0f11a4b034","title":"IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding"},{"paperId":"6eadb6b8061327a79faa469fdc2de5a33a418947","title":"Conceptualized Representation Learning for Chinese Biomedical Text Mining"},{"paperId":"725264948d7b6946259af5b8d966e996b9570f99","title":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"},{"paperId":"5b2ec7a534cab750c6b00fe491500681ae3b1527","title":"PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data"},{"paperId":"2a218786f4615b82389f78472e7ff22e6ce57490","title":"ConvBERT: Improving BERT with Span-based Dynamic Convolution"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"09bfe057c9285577242636950c6835b8731a07fb","title":"Multi-task learning for natural language processing in the 2020s: where are we going?"},{"paperId":"214a0eede75f546b631d6d28871bd9028a66fc46","title":"Playing with Words at the National Library of Sweden - Making a Swedish BERT"},{"paperId":"9e594ae4ae9c38b6495810a8872f513ae19be29c","title":"Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?"},{"paperId":"c0091585ae4f9cccd4c1dba5aa7409c0886553fa","title":"Transferability of Natural Language Inference to Biomedical Question Answering"},{"paperId":"49a049dc85e2380dde80501a984878341dd8efdf","title":"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"},{"paperId":"3578a7792904e6af3db8ffefdff86ab6a387c7c3","title":"FinBERT: A Pretrained Language Model for Financial Communications"},{"paperId":"82453548b97f78ab2cdb9a8626ff858db9ce5a82","title":"Pre-training Polish Transformer-based Language Models at Scale"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"72cdd6ebe0221fb568ef20534f44ba5b35190a56","title":"BERTweet: A pre-trained language model for English Tweets"},{"paperId":"126fb7df6bcab2b70000dfe5b940ada63ae1ba6a","title":"COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter"},{"paperId":"1b0c8b26affd13e10ace5770e85478d60dcc368e","title":"GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"},{"paperId":"02809fc23aecf33e3ed95b83d1d03b54fb5c3d0a","title":"An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining"},{"paperId":"f527bcef68aeda601aae314fe5c75185c716e579","title":"KLEJ: Comprehensive Benchmark for Polish Language Understanding"},{"paperId":"c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c","title":"Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset"},{"paperId":"8ae8e5e840c5f43d4d72cbc4595690fba01aa799","title":"LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation"},{"paperId":"00cd2650a89734105fa0c0aba3bf07935b318290","title":"GLUECoS: An Evaluation Benchmark for Code-Switched NLP"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"56676aef356ebb13cba77fc9e4d70760fbc151f5","title":"ETC: Encoding Long and Structured Inputs in Transformers"},{"paperId":"1c8aea2bfb61f4661b6907018a5a8bca390900dd","title":"PALM: Pre-training an Autoencoding&autoregressive Language Model for Context-conditioned Generation"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"2b9955bc08fc5f4ddba73082ddabcfaabdbb4416","title":"Poor Man's BERT: Smaller and Faster Transformer Models"},{"paperId":"3b504f939e55d567652737ef093c1087cd40689b","title":"Analyzing Redundancy in Pretrained Transformer Models"},{"paperId":"25a49187e0d1e3ebebda71c7e77f31bc49358044","title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA"},{"paperId":"2573af4e13d9a5dddb257d22cd38a600528d9a8b","title":"MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"},{"paperId":"10467a1466aeec246ac0a577bfc311ec4de110de","title":"Alternating Language Modeling for Cross-Lingual Pre-Training"},{"paperId":"297ad41c0e7264e67ae078921e2a57436293ce72","title":"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation"},{"paperId":"c8a3c4e1773a4fb3e465bb8842dcd50a57081742","title":"Modified Bidirectional Encoder Representations From Transformers Extractive Summarization Model for Hospital Information Systems Based on Character-Level Tokens (AlphaBERT): Development and Performance Evaluation"},{"paperId":"b2fd96a52ded7a64f60c1e54f5bb488c787629c0","title":"What Happens To BERT Embeddings During Fine-tuning?"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","title":"Pre-trained models for natural language processing: A survey"},{"paperId":"e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286","title":"A Survey on Contextual Embeddings"},{"paperId":"0dde065405210ebc399c58ab6b7e843a18caad51","title":"CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model"},{"paperId":"a622332550eaf535cf0f0f6c3a3f3ba197c39cac","title":"PhoBERT: Pre-trained language models for Vietnamese"},{"paperId":"501a8b86428563539667e8117cd8409674ef97c3","title":"TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"60a4a3a886338d0c8e3579d392cb32f493430255","title":"MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"},{"paperId":"d9b824dbecbe3a1f0b1489f9e4521a532a63818d","title":"Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"baf60d13c98916b77b09bc525ede1cd610ed1db5","title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"81ed0e757ae2d66a43d73407ad6f7e0359adf6d7","title":"PMIndia - A Collection of Parallel Corpora of Languages of India"},{"paperId":"dc5da5ac3aff86e4b0156c52d9641d05dc1eeace","title":"MT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b","title":"RobBERT: a Dutch RoBERTa-based Language Model"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"069e0d896da7c79faeee4cf057548d5da7ce885e","title":"FlauBERT: Unsupervised Language Model Pre-training for French"},{"paperId":"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","title":"How Can We Know What Language Models Know?"},{"paperId":"4fa37d012ad0014552a6a5a03624b29f95558bf7","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"46b8201f1b84950f141cbbb5eeccaa1437159ff4","title":"A Massive Collection of Cross-Lingual Web-Document Pairs"},{"paperId":"2bd5b4aed18400bf1a1cc866d9b8d931aa047290","title":"E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT"},{"paperId":"68c1bf884f0fc0e86641466a1f1fa67e79f16a17","title":"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"},{"paperId":"348be8e64565a3df80d743f0580b63d3fbb49f35","title":"SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"4099c4d272c12081b562392606e6d567e4ae7031","title":"Masked Language Model Scoring"},{"paperId":"07588dd5d0252c7abc99b3834a81bf23741ead4b","title":"LIMIT-BERT : Linguistics Informed Multi-Task BERT"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"327d7e55d64cb34d55bd3a3fe58233c238a312cd","title":"exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1","title":"Reducing Transformer Depth on Demand with Structured Dropout"},{"paperId":"222b9a7b8038120671a1610e857d3edbc7ac5550","title":"Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models"},{"paperId":"aa2e8b6eecaed5c4af018c03abe5eb8adcabaf47","title":"Cross-Lingual Natural Language Generation via Pre-Training"},{"paperId":"0cbf97173391b0430140117027edcaf1a37968c7","title":"TinyBERT: Distilling BERT for Natural Language Understanding"},{"paperId":"4fb8fd55b476909a26a8dc594e0ae98d4923ad4d","title":"Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"},{"paperId":"3af5a41e43158a75bf7a8bb3f9517edc4163b3ca","title":"Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity"},{"paperId":"772717eb2e369cd68c11b7da7aa779450dced9d0","title":"SenseBERT: Driving Some Sense into BERT"},{"paperId":"d56c1fc337fb07ec004dc846f80582c327af717c","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"57633ff5c6f0708be25e651f51eef29d2fbfe48b","title":"BEHRT: Transformer for Electronic Health Records"},{"paperId":"a9c3a009d754a110379574b069b48c0b4c75db40","title":"Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"},{"paperId":"e61a3a5ba2b93458774f2ccbe480f3cf6cd74fa1","title":"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?"},{"paperId":"bc789aef715498e79a74f857fa090ece9e383bf1","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"2af4e8d14d03cd9031ae4a6b1ef39fce2ab3f504","title":"NetBERT: A Pre-trained Language Representation Model for Computer Networking"},{"paperId":"49e17ad5bf10eb17f4c35a93a1588a6f0f8760db","title":"A Survey on Visual Transformer"},{"paperId":"888c3a3788c52d6637d45dc4238691083884589d","title":"Investigating Learning Dynamics of BERT Fine-Tuning"},{"paperId":"17d5884215b5afa53545cd7cb6135de5478da4ec","title":"CERT: Contrastive Self-supervised Learning for Language Understanding"},{"paperId":"5c5751d45e298cea054f32b392c12c61027d2fe7","title":"S2ORC: The Semantic Scholar Open Research Corpus"},{"paperId":"a4d5e425cac0bf84c86c0c9f720b6339d6288ffa","title":"BERTje: A Dutch BERT Model"},{"paperId":"477d66dcd2c08243dcc69822d6da7ec06393773a","title":"Multilingual is not enough: BERT for Finnish"},{"paperId":"04690b6e72d1d236d91196b93fe64a48f4666a52","title":"Automated Brain Image Classification Based on VGG-16 and Transfer Learning"},{"paperId":"41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76","title":"Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"},{"paperId":"ce106590145e89ea4b621c99665862967ccf5dac","title":"Q8BERT: Quantized 8Bit BERT"},{"paperId":"12dc43176fd607557d6cc8d46af8b8d77c121b47","title":"Domain-Relevant Embeddings for Medical Question Similarity"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"1a00229c25dcc740fd0388ac1e98c42eaa52912e","title":"Pre-trained Language Model for Biomedical Question Answering"},{"paperId":"bfeb827d06c1a3583b5cc6d25241203a81f6af09","title":"Knowledge Enhanced Contextual Word Representations"},{"paperId":"65f788fb964901e3f1149a0a53317535ca85ed7d","title":"Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"7102bb3fe73bd057ff161d9db5214a267c1ef312","title":"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"},{"paperId":"80cf2a6af4200ecfca1c18fc89de16148f1cd4bf","title":"Patient Knowledge Distillation for BERT Model Compression"},{"paperId":"d78aed1dac6656affa4a04cbf225ced11a83d103","title":"Revealing the Dark Secrets of BERT"},{"paperId":"18a1c21f35153c45d0ef30c564bffb7d70a13ccc","title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"1cd8167b2a6be4fdc0f2bfeec4e6f23a9bbb7090","title":"Tuning Multilingual Transformers for Language-Specific Named Entity Recognition"},{"paperId":"8aa1d9145640b4a63258b82bc8180c3683d072b5","title":"KU_ai at MEDIQA 2019: Domain-specific Pre-training and Transfer Learning for Medical NLI"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"92343cecdc990380de362b969eec60081959f507","title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"347bac45298f37cd83c3e79d99b826dc65a70c46","title":"Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"3b3f47170ec5c4fabac510585b33aeb87b384396","title":"Variational Pretraining for Semi-supervised Text Classification"},{"paperId":"ad7129af0644dbcafa9aa2f111cb76526ea444a1","title":"Defending Against Neural Fake News"},{"paperId":"4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9","title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"},{"paperId":"07a64686ce8e43ac475a8d820a8a9f1d87989583","title":"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"},{"paperId":"cc94d15ba408c260c8fe4fa4f1cb6797a996dd21","title":"Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"145b8b5d99a2beba6029418ca043585b90138d12","title":"MASS: Masked Sequence to Sequence Pre-training for Language Generation"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"b03c7ff961822183bab66b2e594415e585d3fd09","title":"Are Sixteen Heads Really Better than One?"},{"paperId":"2a567ebd78939d0861d788f0fedff8d40ae62bf2","title":"Publicly Available Clinical BERT Embeddings"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"660d3472d9c3733dedcf911187b234f2b65561b5","title":"BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"658721bc13b0fa97366d38c05a96bf0a9f4bb0ac","title":"Multi-Task Deep Neural Networks for Natural Language Understanding"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"06a1bf4a7333bbc78dbd7470666b33bd9e26882b","title":"Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"2a0870bc2ecfd17dfb1b96cc34613bb73bb4506a","title":"BLACK BOX ATTACKS ON TRANSFORMER LANGUAGE MODELS"},{"paperId":"e1e43d6bdb1419e08af833cf4899a460f70da26c","title":"AlBERTo: Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets"},{"paperId":null,"title":"Bertviz: A tool for visualizing multihead self-attention in the bert model"},{"paperId":"b47381e04739ea3f392ba6c8faaf64105493c196","title":"Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25","title":"T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"},{"paperId":"8c207ece66e0a63627869c49fb37c6811072539b","title":"The brWaC Corpus: A New Open Resource for Brazilian Portuguese"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"25c0e5ad89b77ae9c7ff91765ebd4e1e21abdcb3","title":"The IIT Bombay English-Hindi Parallel Corpus"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"786f95cada23d4639aa1a8b922cdb9fb9a9c03fa","title":"Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b","title":"Recurrent Neural Network for Text Classification with Multi-Task Learning"},{"paperId":"95cd83603a0d2b6918a8e34a5637a8f382da96f5","title":"MIMIC-III, a freely accessible critical care database"},{"paperId":"800366078f063a637e6a4880c0c49c217c7905ea","title":"The United Nations Parallel Corpus v1.0"},{"paperId":"36f652172792f8aab1cf3c4441a72a1bf79d17c8","title":"Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"},{"paperId":"9a501e501a60b431b6031f81dc2c19b390b0aff3","title":"Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","title":"Distilling the Knowledge in a Neural Network"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"27725a2d2a8cee9bf9fffc6c2167017103aba0fa","title":"A Convolutional Neural Network for Modelling Sentences"},{"paperId":"d770060812fb646b3846a7d398a3066145b5e3c8","title":"Do Deep Nets Really Need to be Deep?"},{"paperId":"032d67d27ecacbf6c5b82eb67e5d02d81fb43a7a","title":"A Survey on Multi-view Learning"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"1b97b4623cf2f183340e548e0aa53abf0f2963d8","title":"Representing General Relational Knowledge in ConceptNet 5"},{"paperId":"a25fbcbbae1e8f79c4360d26aa11a3abf1a11972","title":"A Survey on Transfer Learning"},{"paperId":"0d2336389dff3031910bd21dd1c44d1b4cd51725","title":"Why Does Unsupervised Pre-training Help Deep Learning?"},{"paperId":null,"title":"Wordnet"},{"paperId":"30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9","title":"Model compression"},{"paperId":"0adeb54d32925d200bb313a8e0f06116e49c67fb","title":"The Unified Medical Language System (UMLS): integrating biomedical terminology"},{"paperId":"45a23651bcc5a6cc993d722e71b0d301a6dc9dee","title":"Open Mind Common Sense: Knowledge Acquisition from the General Public"},{"paperId":null,"title":"Low resource multitask sequence tagging – revisiting dynamic conditional random fields"}],"id":"6c761cfdb031701072582e434d8f64d436255da6","summary":"This comprehensive survey paper explains various core concepts like pretraining, Pretraining methods, pretraining tasks, embeddings and downstream adaptation methods, presents a new taxonomy of T-PTLMs and gives brief overview of various benchmarks including both intrinsic and extrinsic."},{"url":"https://www.semanticscholar.org/paper/ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation","venue":"ArXiv","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Lukas Edman,Antonio Toral,Gertjan van Noord","citations":[],"references":[{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"99b12d0df2b93e800207a5e4618a353912f3dff8","title":"Multilingual Unsupervised Neural Machine Translation with Denoising Adapters"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":null,"title":"Why don’t people use characterlevel machine translation? arXiv preprint arXiv:2110.08191"},{"paperId":"f81b036fb6c28deb50f572b26b80297becac2a18","title":"HW-TSC’s Participation in the WMT 2021 News Translation Shared Task"},{"paperId":null,"title":"Translation To analyze our models on lower-resource translation, we chose to train and evaluate on Xhosa–Zulu, specifically the data provided by the WMT2021"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"3b233bdb697cc43effa1eb6d2868ff14efbbab7a","title":"UDapter: Language Adaptation for Truly Universal Dependency Parsing"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"48530f3d6425f2f150f07ccdd61ba951951a0a7d","title":"Simple, Scalable Adaptation for Neural Machine Translation"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"bf8fe437f779f2098f9af82b534aa51dc9edb06f","title":"Scaling Neural Machine Translation"},{"paperId":null,"title":"Translation We also run experiments in the higher resource setting of the WMT14 German→English data. Following the principles of Ott et al. (2018), we use larger batch sizes of 50k and 240k tokens"},{"paperId":"69ffe3277011087b55afaeaef0a3933160beee9a","title":"Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"68c03788224000794d5491ab459be0b2a2c38677","title":"WordNet: A Lexical Database for English"}],"id":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","summary":"This newdownsampling method not only outperforms existing downsampling methods, showing that downsamplings characters can be done without sacriﬁcing quality, but also leads to promising performance compared to subword models for translation."},{"url":"https://www.semanticscholar.org/paper/9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Chul Lee,Qipeng Guo,Xipeng Qiu","citations":[],"references":[{"paperId":"bb15f3727f827a3cb88b5d3ca48415c09b40a88f","title":"What Language Model to Train if You Have One Million GPU Hours?"},{"paperId":"82819510c79101b889f62fb6938a8902be40674e","title":"Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words"},{"paperId":"3425495ee3b6ead009f35aeb70edeac4e6eb2d10","title":"Patches Are All You Need?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"7694aae9766d5f1fe74d900cd82aee898cb6e8e9","title":"How to Train BERT with an Academic Budget"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"921c1216edbf6b2931b15874f24847ff1007ad8c","title":"EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":null,"title":"Asher Trockman and J . Zico Kolter . 2022 . Patches are all you need ? Ashish Vaswani"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"512b8ef0002e0bfd0ecb5ab17d533c1762eb9786","title":"Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"28fcaaa1855d0ed995f649e22c0da2ad415001b4","title":"UniMorph 2.0: Universal Morphology"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":null,"title":"Xnli: Evaluating crosslingual sentence representations"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":null,"title":"Hinton"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"}],"id":"9f4351600c72d5dac0251cd49984f691dca2fcd1","summary":"This work overhauls the existing character-aware method that takes character-level inputs but makes word-level sequence modeling and prediction a token free model with slim input embeddings for downstream tasks by introducing a cross-attention network and a sub-word level prediction based on word- level hidden states."},{"url":"https://www.semanticscholar.org/paper/11ddb0953eae196dab339bfdc117221594cf945e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models","venue":"","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Jonas Belouadi,Steffen Eger","citations":[],"references":[{"paperId":"77a94f6c91ee1590dd2c6fd80b4a6d8bffdb91ac","title":"Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing"},{"paperId":"6ab78343ab82fa9d7baa68027f9f7e8cd9863737","title":"Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks"},{"paperId":"4bc51cb3ba793de7c06bb77770f2f9a91ff809f7","title":"MVP: Multi-task Supervised Pre-training for Natural Language Generation"},{"paperId":"4cde74e8da60a35e36f19ca8a62fc431fff02c9c","title":"Why is constrained neural language generation particularly challenging?"},{"paperId":"8cc1d839276a8d97d922f57e36f44ecf3f31cbf2","title":"PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation"},{"paperId":"5ae837949dca9fedcc72704746a1ba6c83868f97","title":"BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog"},{"paperId":"1ce77b594dac02d3d114fe2dba895852216825db","title":"Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features"},{"paperId":"880c8973ea1376c6bff23226b3f64792657aa666","title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion"},{"paperId":"bb1c9cb431e771660cffdda1d80a7f15ff40c764","title":"Measuring the Mixing of Contextual Information in the Transformer"},{"paperId":"adea6aa83b847752940129185428ea61935a0027","title":"Quantifying Memorization Across Neural Language Models"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"622f5ee4d704e21d8b47c7a47dc05e3d8f0a49bc","title":"One Line at a Time - Generation and Internal Evaluation of Interactive Poetry"},{"paperId":"73eeeb36cee123b940c4e94277800c339324eb7b","title":"Constrained Language Models for Interactive Poem Generation"},{"paperId":null,"title":"ChatGPT: Optimizing language models for dialogue"},{"paperId":null,"title":"Eighteenth-century poetry archive"},{"paperId":"a7dd208b248ead76b8f15562b39c2d0a3bdb7fa0","title":"Transformers analyzing poetry: multilingual metrical pattern prediction with transfomer-based language models"},{"paperId":"042c6d99bdf381b55048b8bb48c8479dbcfbcd5a","title":"Incorporating Residual and Normalization Layers into Analysis of Masked Language Models"},{"paperId":"19a9937a1029ee5b1f6da8a945ed9a5c5a029c57","title":"DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling"},{"paperId":"d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5","title":"Pretrained Language Models for Text Generation: A Survey"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"9dc624d7258d1a56117ca720aea953ce46b66b21","title":"Efficient Attentions for Long Document Summarization"},{"paperId":"aa28873534c24e4a8c5deb7bff723cd5fc69a6f0","title":"QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization"},{"paperId":"9623e9e461647a10a8f14419a9abe40482e9eb47","title":"MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization"},{"paperId":"39dbf18f4149ea890a73da36a472b7b713dfeea4","title":"Metrical Tagging in the Wild: Building and Annotating Poetry Corpora with Rhythmic Features"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"63169665bd592fb818678c47644b29302877d50e","title":"UBAR: Towards Fully End-to-End Task-Oriented Dialog Systems with GPT-2"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"f1b618f60e0bd64293a67dc9b3f3b9519694b906","title":"Syllable Neural Language Models for English Poem Generation"},{"paperId":"504a9112f52c8d845f5117879ad7afc7c06aa4cf","title":"FFCD: A Fast-and-Frugal Coherence Detection Method"},{"paperId":"f31a11257aad37847893b5495024865ca5f41ef9","title":"End-to-end style-conditioned poetry generation: What does it take to learn from examples alone?"},{"paperId":"f4b33eda909c873c519fce390c3fd66a568c2e27","title":"Automatic Poetry Generation from Prosaic Text"},{"paperId":"9ce3d29c3b4bfe677414a00ecff5dc141475feb5","title":"Cross-lingual Retrieval for Iterative Self-Supervised Training"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"132593995892a5918090320a86c120c7db8c3f0a","title":"PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"eb606d9ce65139754232cee62f6ab77f3e0c665f","title":"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"},{"paperId":"cfc54398ff62e3a6cb9759457f87f65d99364765","title":"Findings of the WMT 2020 Shared Task on Chat Translation"},{"paperId":"5641584e314da699a9fb1473403aabd8440bfe99","title":"A Three Sample Hypothesis Test for Evaluating Generative Models"},{"paperId":null,"title":"GerPT2: German large and small versions of GPT2"},{"paperId":"e17337f1812d883450119c24cba5ad6e4adee05f","title":"Next Sentence Prediction helps Implicit Discourse Relation Classification within and across Domains"},{"paperId":"9d9c8202a1efaaeccc2eec1e9cf966d119611bbe","title":"Learning Rhyming Constraints using Structured Adversaries"},{"paperId":"33fbaf34fa0119e5249dff3267795e13fa0eaa37","title":"Neural data-to-text generation: A comparison between pipeline and end-to-end architectures"},{"paperId":"c737a760ae035a9fb9aaa66c7fe079683e885208","title":"Neural Poetry: Learning to Generate Poems using Syllables"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b497160adf0800ae3fd8f156de0ad06100773896","title":"Supervised Rhyme Detection with Siamese Recurrent Networks"},{"paperId":"05c7f8778e7a7b054e475288758f0d8ebc02931a","title":"Deep-speare: A joint neural model of poetic language, meter and rhyme"},{"paperId":"524fd6d2bd91bc9fc6ff7f4ee19e52f652718644","title":"Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation"},{"paperId":"f902f0a8c1aac417d67de5861bb8af449bc8561a","title":"Automatically Generating Rhythmic Verse with Neural Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"332b05ff46f779754a8fefbfe3695f98aed9ed81","title":"Capturing Reliable Fine-Grained Sentiment Associations by Crowdsourcing and Best–Worst Scaling"},{"paperId":"5f4398f0df93ddd548f244b75a49b97f51abd161","title":"Best-Worst Scaling: Theory, Methods and Applications"},{"paperId":"07cec27d3ee3c96c490d93de5dd06800c31ee829","title":"'The Sounds of the Psalter: Computational Analysis of Soundplay'"},{"paperId":"cb4917b5f430e4f796aa3013c472927f0169b98a","title":"Poetic Data and the News from Poems: A For Better for Verse Memoir"},{"paperId":"3e7841061391a4ea6b8f325d78c905b0a770cd00","title":"MaxDiff Analysis : Simple Counting , Individual-Level Logit , and HB"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":null,"title":"Pattern matching: The gestalt approach"},{"paperId":null,"title":"A mathematical model for alliteration"},{"paperId":"caf121025ea1044aee26b10a2d2e7837d66b7026","title":"On the measurement of alliteration in poetry"},{"paperId":"b7b846a80baced27ab16acf51570628540b0bf3d","title":"A Handbook to Literature"},{"paperId":"b2d5cb9e3cb452619fad9fb068f93fbb1b4fad00","title":"The alliteration in Shakespeare's sonnets: a study in literary behavior."},{"paperId":null,"title":"Maler Nolten"}],"id":"11ddb0953eae196dab339bfdc117221594cf945e","summary":"This work successfully pre-train and release ByGPT5, a new token-free decoder-only language model, and successfully tunes it on a large custom corpus of English and German quatrains annotated with the authors' styles, demonstrating its runtime performance and introspect the model’s understanding of style conditions."},{"url":"https://www.semanticscholar.org/paper/660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only","venue":"","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"M. Tschannen,Basil Mustafa,N. Houlsby","citations":[],"references":[{"paperId":"fb41147776fd3ef8c3370ce2574efc15486c9a0f","title":"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding"},{"paperId":"aa509ec67f311cd09d109356f7fa37a40072aabb","title":"Phenaki: Variable Length Video Generation From Open Domain Textual Description"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"cd724569488970381ecfea0976891cd0c19403dc","title":"TVLT: Textless Vision-Language Transformer"},{"paperId":"9fc5878d49c41beb12b62032b0afe9a8501fe9db","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"ace4d199aa72ab0808af0f30a61fc16727c95dec","title":"AudioLM: a Language Modeling Approach to Audio Generation"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"b2e1935b4d79e8d9f3c3e0a6712736358ccba236","title":"Masked Autoencoders that Listen"},{"paperId":"1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe","title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"},{"paperId":"6a87f773d62a509135bf78faa4c35a3497eb2c10","title":"OmniMAE: Single Model Masked Pretraining on Images and Videos"},{"paperId":"499d3bb3acbc10730dd6582bd9b8f646bf22ccd5","title":"Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"a8260077135246476a0b0601495ef08e56c21a50","title":"Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"055cd2faeebc7a9df43923d554a61ae924a4af6b","title":"UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes"},{"paperId":"a26a7a74f1e5fd562be95c3611a0680759fbdf84","title":"CoCa: Contrastive Captioners are Image-Text Foundation Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"e606725d8947b49667857900418e9cc316702372","title":"Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation"},{"paperId":"c689d1f3ae2447fd5b2f108b5b4436276e4d3761","title":"LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"32e89d4d1225f3e02ac38ec93efc39fb61ccdde8","title":"Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning"},{"paperId":"c3d086d0f50ff9efa28d56616ed127547d836e55","title":"Omnivore: A Single Model for Many Visual Modalities"},{"paperId":"2f33b928d76504714e17cd948695d170329843fc","title":"OCR-Free Document Understanding Transformer"},{"paperId":"197d5867a45a2988f4dd159063cdfbfe90164962","title":"LiT: Zero-Shot Transfer with Locked-image text Tuning"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"8f167ec1149921fac63b1ea855443de109bb013a","title":"How Much Can CLIP Benefit Vision-and-Language Tasks?"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"01730636fe12bd3c15597e9439aba9b0b27ac150","title":"A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned & Perspectives"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"c8f92e2033630bec76d4e3d3c02b11088e30dda9","title":"Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text"},{"paperId":"00d7dfde1cd69d2247e8c36d10807b0dee9656d7","title":"PolyViT: Co-training Vision Transformers on Images, Videos and Audio"},{"paperId":"da2bb505ba1fcfa8877c008309bbed49be1332a5","title":"Combined Scaling for Open-Vocabulary Image Classification"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"dc32a984b651256a8ec282be52310e6bd33d9815","title":"Highly accurate protein structure prediction with AlphaFold"},{"paperId":"e6cbb6aa65d7761fabb6c5eb363f66524abe0a4c","title":"DocFormer: End-to-End Transformer for Document Understanding"},{"paperId":"c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500","title":"Decision Transformer: Reinforcement Learning via Sequence Modeling"},{"paperId":"f0524b3005720bcff886bcb0227f7f0dd924ff07","title":"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"0e2d8b8d81092037f9866c1ceddcebb87318e38b","title":"AST: Audio Spectrogram Transformer"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"32d281a1e7a0a2d4e2b3f34e0f71780c987e1374","title":"DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations"},{"paperId":"83ed88e4f745cc9aecd1fbd479612b11beddcb86","title":"CLEAR: Contrastive Learning for Sentence Representation"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"598a2ee223e2949c3b28389e922c1892b4717d2a","title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"8a1744da011375d711ed75fc2d160c6fdca2cf89","title":"Deep Modular Co-Attention Networks for Visual Question Answering"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"512b8ef0002e0bfd0ecb5ab17d533c1762eb9786","title":"Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"36c3972569a6949ecca90bfa6f8e99883e092845","title":"Pythia v0.1: the Winning Entry to the VQA Challenge 2018"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"54a13bcc9613dcaa76fb25fbe96572f376cfcca9","title":"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"},{"paperId":"bc1d609520290e0460c49b685675eb5a57fa5935","title":"An efficient framework for learning sentence representations"},{"paperId":"372f9e5b4d3ce4663dc336b763b20746a3dbfb78","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"44040913380206991b1991daf1192942e038fe31","title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":null,"title":"Towards endto-end in-image neural machine translation. CoRR, abs"},{"paperId":null,"title":"Shared Task: Machine Translation of News"}],"id":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","summary":"This work explores an additional uniﬁcation: the use of a pure pixel-based model to perform image, text, and multimodal tasks and exploits the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multi-modal retrieval without modi ﬁcations."},{"url":"https://www.semanticscholar.org/paper/0351167c875b8931366e85eb5e517819d7db80cc","title":"What Makes for Good Tokenizers in Vision Transformer?","venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence","year":2022,"referenceCount":91,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Shengju Qian,Yi Zhu,Wenbo Li,Mu Li,Jiaya Jia","citations":[],"references":[{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"a5c41f188b0eb0acb444cb4899bf6af378ee9ede","title":"CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"},{"paperId":"90f9b1a029dc9aede3909a10c9833eba4d3bcd3d","title":"PVTv2: Improved Baselines with Pyramid Vision Transformer"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"2805917375bf84ab06ad658af4ceaec85d4a5906","title":"On Efficient Transformer and Image Pre-training for Low-level Vision"},{"paperId":"5e708602a7cb97a092653f1be0300aa7774391df","title":"Blending Anti-Aliasing into Vision Transformer"},{"paperId":"2e644c67a697073d561da4f4dad35e5ad5316cfd","title":"SOFT: Softmax-free Transformer with Linear Complexity"},{"paperId":"0a485fd94b2cb554e281d0f8d7e9f71db4891ce0","title":"Token Pooling in Vision Transformers"},{"paperId":"ee35122f0be32652dc1239c6601ff31732b7026d","title":"Leveraging Batch Normalization for Vision Transformers"},{"paperId":"f7e449d7695fbbf43081cc820a81fe0ccb11c3db","title":"PnP-DETR: Towards Efficient Visual Analysis with Transformers"},{"paperId":"c945efdeefaacb8ca679298720f4b0b054dc84bd","title":"Vision Transformer with Progressive Sampling"},{"paperId":"57e81d6545dc0279af6d63018bf82a6b9e363fec","title":"DPT: Deformable Patch-based Transformer for Visual Recognition"},{"paperId":"48418b285a92376a38daafa664a2dd07d42e3fe3","title":"Focal Self-attention for Local-Global Interactions in Vision Transformers"},{"paperId":"7b664a306b7d2f68dd816ea1d6586cf3472d75c1","title":"Early Convolutions Help Transformers See Better"},{"paperId":"e2f2662f0734e2edc2b4b36a734de111c7f8d54d","title":"IA-RED2: Interpretability-Aware Redundancy Reduction for Vision Transformers"},{"paperId":"48bcfea07f343b29128c71bb2cce5f3ab62f6d85","title":"TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?"},{"paperId":"9f4b69762ffb1ba42b573fd4ced996f3153e21c0","title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes"},{"paperId":"dbdcabd0444ad50b68ee09e30f39b66e9068f5d2","title":"DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"},{"paperId":"e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60","title":"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"},{"paperId":"6709d5583f658f589ae6a2184805933aceb18849","title":"Twins: Revisiting the Design of Spatial Attention in Vision Transformers"},{"paperId":"8d3ddc27dce9c6c0fe110e4f9cb45d3b59feb04b","title":"Visformer: The Vision-friendly Transformer"},{"paperId":"8754533bead3996f20440e4a1d0220d4971d00d7","title":"VidTr: Video Transformer Without Convolutions"},{"paperId":"18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6","title":"Multiscale Vision Transformers"},{"paperId":"7093016e02e8f4580ffea18396f95d129d37858d","title":"All Tokens Matter: Token Labeling for Training Better Vision Transformers"},{"paperId":"5b68522f58b61e7235b852677337ef3725075fd9","title":"Co-Scale Conv-Attentional Image Transformers"},{"paperId":"739ceacfafb1c4eaa17509351b647c773270b3ae","title":"An Empirical Study of Training Self-Supervised Vision Transformers"},{"paperId":"b364cdb02d18b9d9a3c097f5ea446f7e9ab10325","title":"Going deeper with Image Transformers"},{"paperId":"e775e649d815a02373eac840cf5e33a04ff85c95","title":"CvT: Introducing Convolutions to Vision Transformers"},{"paperId":"0eff37167876356da2163b2e396df2719adf7de9","title":"CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"},{"paperId":"91e8117e7ebc966bc76de2cb52ec717d2acdb1a4","title":"Scaling Local Self-Attention for Parameter Efficient Visual Backbones"},{"paperId":"0ae67202f0584afccefa770865d14a46655d2975","title":"Transformer in Transformer"},{"paperId":"3e398bad2d8636491a1034cc938a5e024c7aa881","title":"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"},{"paperId":"dbe077f8521ecbe0a1477d6148c726d4f053d9c9","title":"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"},{"paperId":"32a81aef8063274b1d8cc770a7f6dcfd8efe5336","title":"Revisiting Locally Supervised Learning: an Alternative to End-to-end Training"},{"paperId":"d29430adccb805ab57b349afa8553954347b3197","title":"Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"},{"paperId":"ad7ddcc14984caae308c397f1a589aae75d4ab71","title":"Training data-efficient image transformers & distillation through attention"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"39ca8f8ff28cc640e3b41a6bd7814ab85c586504","title":"Deformable DETR: Deformable Transformers for End-to-End Object Detection"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"b50bd1f96e20952d7ad1bf774ea6199ce12f2fa6","title":"Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length"},{"paperId":null,"title":"T2t vit"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"9257a6b9b072b3c1b82bcc76db26406fd0ce24e0","title":"Attentive Normalization for Conditional Image Generation"},{"paperId":"4076e421d1758fdb68411242044cd45747b7e35b","title":"PowerNorm: Rethinking Batch Normalization in Transformers"},{"paperId":"748629cb0b8e5a5708e1c6605f71b36eb525a3ce","title":"On Layer Normalization in the Transformer Architecture"},{"paperId":"a573c125e85d1230626c8f3cf6193354f753958d","title":"Temporal Interlacing Network"},{"paperId":"b04889922aae7f799affb2ae6508bc5f5c989567","title":"A Mutual Information Maximization Perspective of Language Representation Learning"},{"paperId":"6bbd51697c25493ea15f4cac830e28eeac143898","title":"Randaugment: Practical automated data augmentation with a reduced search space"},{"paperId":"a88c914f5a738d38f02790bb5de41453bf17bde1","title":"Object-Contextual Representations for Semantic Segmentation"},{"paperId":"72564a69bf339ff1d16a639c86a764db2321caab","title":"Focal Loss for Dense Object Detection"},{"paperId":"022dd244f2e25525eb37e9dda51abb9cd8ca8c30","title":"Mask R-CNN"},{"paperId":"112fd54ee193237b24f2ce7fce79e399609a29c5","title":"The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives"},{"paperId":"8835a2a56c04d8a1dc47798ceb567748bb32fac0","title":"Make a Face: Towards Arbitrary High Fidelity Face Manipulation"},{"paperId":"97de5731334de4a183d2f2ff8d4c8d37d7a79247","title":"Aggregation via Separation: Boosting Facial Landmark Detector With Semi-Supervised Style Translation"},{"paperId":"c2c083df88e88223e1a411e61040b94c233b1b63","title":"MMDetection: Open MMLab Detection Toolbox and Benchmark"},{"paperId":"a1a19aaddf57c0546357d890d9269092ba0afb26","title":"Semantic Image Synthesis With Spatially-Adaptive Normalization"},{"paperId":"cf4d64bc1f84be8cf646c579e997d15aafe480c1","title":"Greedy Layerwise Learning Can Scale to ImageNet"},{"paperId":"eae7d5b15423a148e6bb32d24bbabedfacd0e2df","title":"Learning deep representations by mutual information estimation and maximization"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Pytorch image models"},{"paperId":null,"title":"Mocov3"},{"paperId":"a9a5d671271fff45429084e184a788f611b6f194","title":"FRAGE: Frequency-Agnostic Word Representation"},{"paperId":"aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1","title":"Unified Perceptual Parsing for Scene Understanding"},{"paperId":"d39a5ea57b1c242a0450385523fd3471b172458c","title":"Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net"},{"paperId":"1d033b30f38642e4b6dd146bb8b464bfb58aad96","title":"Deep Clustering for Unsupervised Learning of Visual Features"},{"paperId":"f723eb3e7159f07b97464c8d947d15e78612abe4","title":"AutoAugment: Learning Augmentation Policies from Data"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"030ff7012b92b805a60976f8dbd6a08c1cecebe6","title":"DCAN: Dual Channel-wise Alignment Networks for Unsupervised Scene Adaptation"},{"paperId":"d6e8e560100c43ea22070535b2d0e31f518ffb72","title":"The Contextual Loss for Image Transformation with Non-Aligned Data"},{"paperId":"0a255e716a89b787336ab956f0aa74424629c950","title":"On the information bottleneck theory of deep learning"},{"paperId":"ab559473a01836e72b9fb9393d6e07c5745528f3","title":"cGANs with Projection Discriminator"},{"paperId":"c3955d74f2a084a8ddcbd7e73952c326e81804b2","title":"Mutual Information Neural Estimation"},{"paperId":"04957e40d47ca89d38653e97f728883c0ad26e5d","title":"Cascade R-CNN: Delving Into High Quality Object Detection"},{"paperId":"88512be44744615f4baa8e14f600f036db4c2433","title":"Semantic Understanding of Scenes Through the ADE20K Dataset"},{"paperId":"dd6d044696df5e4353ff7c92b8009e1201c85129","title":"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"},{"paperId":"45dfef0cc1ed96558c1c650432ce39d6a1050b6a","title":"Fixing Weight Decay Regularization in Adam"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"be0ef77fb0345c5851bb5d297f3ed84ae3c581ee","title":"Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization"},{"paperId":"24da6180db314619060d7b8fc798390f0c7a139a","title":"Revisiting Batch Normalization For Practical Domain Adaptation"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"63de0ad39d807f0c256f851428f211e8d5fcd3bb","title":"Instance Normalization: The Missing Ingredient for Fast Stylization"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"9fa3720371e78d04973ce9752781bc337480b68f","title":"Perceptual Losses for Real-Time Style Transfer and Super-Resolution"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"5f5dc5b9a2ba710937e2c413b37b053cd673df02","title":"Auto-Encoding Variational Bayes"},{"paperId":"23a99224c7b7d148675c1798c796ec1b0904620a","title":"Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics"},{"paperId":"e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","title":"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"}],"id":"0351167c875b8931366e85eb5e517819d7db80cc","summary":"Through extensive experiments on various transformer architectures, this work observes both improved performance and intriguing properties of these two plug-and-play designs with negligible computational overhead, indicating the importance of the commonly-omitted designs of tokenizers in vision transformer."},{"url":"https://www.semanticscholar.org/paper/9f9196beee29c02b8c5837c6edbc69967189b735","title":"Efficient Transformers with Dynamic Token Pooling","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/11/2022","authors":"P. Nawrot,J. Chorowski,Adrian La'ncucki,E. Ponti","citations":[],"references":[{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"6bda0a1f4ab54e0b169f896de998186970e3138d","title":"Variable-rate hierarchical CPC leads to acoustic unit discovery in speech"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"0e802c0739771acf70e60d59c2df51cd7e8c50c0","title":"Memorizing Transformers"},{"paperId":"6281c40c66febca1d8003bcc6fdfd2189b30c38f","title":"SCROLLS: Standardized CompaRison Over Long Language Sequences"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"1f133158a8973fb33fea188f20517cd7e69bfe7f","title":"FNet: Mixing Tokens with Fourier Transforms"},{"paperId":"5a50d90c7ad715c57b5f0cd9d8473b3dff705d40","title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"680e61a17e27a1e8e121276c7ec53fc4fd40babb","title":"Revisiting the Uniform Information Density Hypothesis"},{"paperId":"5d032bd2632b6f5847767f39ce247098c6bbc563","title":"Combiner: Full Attention Transformer with Sparse Computation Cost"},{"paperId":"642dab29e680f516eb25949d616a24e0ad147a19","title":"Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation"},{"paperId":"320efa53dea3e8f836790682fbd4196132c49749","title":"Segatron: Segment-Aware Transformer for Language Modeling and Understanding"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"657329c633709dd1ac34a30d57341b186b1a47c2","title":"Efficient Content-Based Sparse Attention with Routing Transformers"},{"paperId":"d52df686da98c4b5fc35705529253f708c7ed1d0","title":"Self-Supervised Contrastive Learning for Unsupervised Phoneme Segmentation"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"01203341a8b5b7df21dec5359afe8cc388786ebf","title":"Wiki-40B: Multilingual Language Model Dataset"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"cf3c81c2e2200163c0b12fd02caedb94904f7789","title":"Preserving activations in recurrent neural networks based on surprisal"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"1e4cfedd79a108d0d04cc498bb146e4dcd4b5f0a","title":"Neural Speed Reading via Skim-RNN"},{"paperId":"ae8d5be3caea59a21221f02ef04d49a86cb80191","title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"87a913817503379547bec61a5f010abac5b0f76b","title":"Fast-Slow Recurrent Neural Networks"},{"paperId":"29e944711a354c396fad71936f536e83025b6ce0","title":"Categorical Reparameterization with Gumbel-Softmax"},{"paperId":"515a21e90117941150923e559729c59f5fdade1c","title":"The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"9f0687bcd0a7d7fc91b8c5d36c003a38b8853105","title":"Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"},{"paperId":"5b4cef5ce21753a38234740b3d5bf5ff0f7d9b3a","title":"Surprisal-Driven Zoneout"},{"paperId":"15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7","title":"Gaussian Error Linear Units (GELUs)"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"9e1e23463e5e8a40aac955700c331f9fc8e2b2ec","title":"Zipf's law of abbreviation as a language universal"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"438bb3d46e72b177ed1c9b7cd2c11a045644a1f4","title":"Gradient Estimation Using Stochastic Computation Graphs"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"5522764282c85aea422f1c4dc92ff7e0ca6987bc","title":"A Clockwork RNN"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":null,"title":"Large text compression benchmark"},{"paperId":"7bea855e19fd13461590e4f2d44bbf7b807ce3e3","title":"A bitter lesson."},{"paperId":"b268600834c639e4b0240404685bdf4ee2799d59","title":"Finding Structure via Compression"},{"paperId":"b13813b49f160e1a2010c44bd4fb3d09a28446e3","title":"Hierarchical Recurrent Neural Networks for Long-Term Dependencies"},{"paperId":"710b5c54c011377130f436d2531e6c1f89dd884f","title":"Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology"}],"id":"9f9196beee29c02b8c5837c6edbc69967189b735","summary":"Dynamic pooling, which jointly segments and models language, is of-ten both faster and more accurate than vanilla Transformers and ﬁxed-length pooling within the same computational budget."},{"url":"https://www.semanticscholar.org/paper/e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model","venue":"Findings","year":2021,"referenceCount":40,"citationCount":11,"influentialCitationCount":0,"publicationDate":"26/05/2021","authors":"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki","citations":[{"paperId":"75a05bffa2cc35a876ce04edb0b57f9592716d3b","title":"Extending the Subwording Model of Multilingual Pretrained Models for New Languages"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"8c82d3d758897ef9f166924683831ecf6085f21a","title":"MaxMatch-Dropout: Subword Regularization for WordPiece"},{"paperId":"cd2cd8cb1aa27e7f2404fe335dd5832e14ebb5f8","title":"Composing Word Embeddings for Compound Words Using Linguistic Knowledge"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a","title":"Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation"},{"paperId":"3227844af4d38bfa702781e321cf6712bf537e2c","title":"Word-level Perturbation Considering Word Length and Compositional Subwords"},{"paperId":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"CHARFORMER: FAST CHARACTER TRANSFORMERS"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"}],"references":[{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"3504dd566b25f24f44cf647731370760a537b483","title":"Stochastic Tokenization with a Language Model for Neural Text Classification"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"90fe32dd2fccd4ca14713d8b0252aa34a6b910e8","title":"Subword Encoding in Lattice LSTM for Chinese Word Segmentation"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"4c8f666fc2221bfd569f4e277e8242e7e47611dc","title":"Juman++: A Morphological Analysis Toolkit for Scriptio Continua"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"80e7eb77c5ab8ffb8d870db0d6fa34a40cc792a4","title":"Sudachi: a Japanese Tokenizer for Business"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"34b4c0ae0160d5414729522612df43d72983d686","title":"Neural Word Segmentation with Rich Pretraining"},{"paperId":"1e0b165d10704d0d9be4aa55ae89ab566f919758","title":"Fast and Accurate Neural Word Segmentation for Chinese"},{"paperId":null,"title":"Attention is all you need. Advances in neural information processing systems"},{"paperId":"36f652172792f8aab1cf3c4441a72a1bf79d17c8","title":"Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"bae2cbe50c62798305acbaf9eaeebbba27afdf81","title":"Daily-Aware Personalized Recommendation based on Feature-Level Time Series Analysis"},{"paperId":"99cfd7ae6383acd5023e8e74ae02b31ebe7e95d8","title":"Morphological Analysis for Unsegmented Languages using Recurrent Neural Network Language Model"},{"paperId":null,"title":"Rakuten dataset. Informatics Research Data Repository, National Institute of informatics"},{"paperId":null,"title":"Rakuten dataset"},{"paperId":null,"title":"Rakuten dataset"},{"paperId":"a384185d8588c6bd148ef624730242b29efdd2ba","title":"Nonparametric Word Segmentation for Machine Translation"},{"paperId":"a575a9f508240a17ecf4e253984ab3ad5af5b8d1","title":"Unsupervised Tokenization for Machine Translation"},{"paperId":"51c88cf10b75ed1a5445660cc64cee3d1c6fd8c5","title":"Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling"},{"paperId":"46f31f9069bb934498a288126053bcab01ff34aa","title":"A Bayesian framework for word segmentation: Exploring the effects of context"},{"paperId":"977a2e71c0889315a302fe69e3ec2c47496ea4cb","title":"Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation"},{"paperId":"5b2beddeb063bd5988da70ae58f3e0a6564e647a","title":"Optimizing Chinese Word Segmentation for Machine Translation Performance"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"a99d6ebe6e583b752d1639a9002dd60581983dc1","title":"Contextual Dependencies in Unsupervised Word Segmentation"},{"paperId":"7241a4fc1489d34a77b4f4b7b81ebe12e83435c9","title":"Improving Statistical MT through Morphological Analysis"},{"paperId":"70b849773678010942a0975f2887e527c17cda76","title":"MeCab : Yet Another Part-of-Speech and Morphological Analyzer"},{"paperId":"da336ed15bd24c268cb2a09efe2aa4f298cda3ba","title":"Statistical Machine Translation with Scarce Resources Using Morpho-syntactic Information"},{"paperId":"1f701007f789890d1066172094bbf158f002f673","title":"Bayesian Methods for Hidden Markov Models"},{"paperId":"f30a129113961242c6279436d60df17e9043ad08","title":"A Stochastic Japanese Morphological Analyzer Using a Forward-DP Backward-A* N-Best Search Algorithm"},{"paperId":"145c0b53514b02bdc3dadfb2e1cea124f2abd99b","title":"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"}],"id":"e6b252ad22486c10b1b288e0a5e1ad468690be70","summary":"Experimental results show that the proposed method improves the performance by determining appropriate tokenizations and can be used to explore the appropriate tokenization for an already trained model as post-processing."},{"url":"https://www.semanticscholar.org/paper/00675f1591392622b0db2d9cd37a8a1f32e37aa8","title":"Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks","venue":"","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Kaiser Sun,Peng Qi,Yuhao Zhang,Lan Liu,William Yang Wang,Zhiheng Huang","citations":[],"references":[{"paperId":"190b831643573cd73d543f620c50051078d8bce9","title":"CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing"},{"paperId":"8f90451a627c0b5d0cdff5f6e11b4cdf2fe008db","title":"Improving the Numerical Reasoning Skills of Pretrained Language Models"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":null,"title":"ByT5: Towards a token-free"},{"paperId":"996f0d401acd11e95ce5586010e7e4e18f5c3bb9","title":"How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation"},{"paperId":"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"1c59de25af45cef20d846ec7454251e8237d45d1","title":"A Statistical Extension of Byte-Pair Encoding"},{"paperId":"0bfa9275561f45bc772c4d544fd75f5d65143c5e","title":"When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1be21e96eaac56f626e7b41c1f332b6b46131608","title":"MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"a7bbb084f5de4f318c811776afeba2b05439c234","title":"DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension"},{"paperId":"c071a1ad68310fed7f0876b6f01cb7b135043bc3","title":"Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"3adff57fd09965224506a1bacc0579d9d3c8c11e","title":"SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine"},{"paperId":"3eda43078ae1f4741f09be08c4ecab6229046a5c","title":"NewsQA: A Machine Comprehension Dataset"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"c4dd9a19d822c965ce8cde55ab23b8a0b628278a","title":"An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"}],"id":"00675f1591392622b0db2d9cd37a8a1f32e37aa8","summary":"It is shown that, with consistent tokenization, the model performs better in both in-domain and out-of-domain datasets, with a notable average of +1.7 F 1 gain when a BART model is trained on SQuAD and evaluated on 8 QA datasets."},{"url":"https://www.semanticscholar.org/paper/bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/12/2022","authors":"Nathan Godey,Roman Castagn'e,Eric Villemonte de la Clergerie,Benoît Sagot","citations":[],"references":[{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"2b66a0d8a00b0ba7b585e11ab34879420ad351cb","title":"Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"f2588de5173fb047192dbb93d62ce6636bdf46bd","title":"Lessons from Natural Language Inference in the Clinical Domain"},{"paperId":"e9b711ec25c8d919ab60dfefe362251f60309214","title":"A simple and fast method for computing the Poisson binomial distribution function"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5c2658fedd4bff3c8aea27cefe72cf72a36d949d","title":"An Algorithm for Computing the Distribution Function of the Generalized Poisson-Binomial Distribution"},{"paperId":"6f35b070c250507dbec8a7365cf01b25eb66d792","title":"Ex Machina: Personal Attacks Seen at Scale"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"}],"id":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","summary":"MANTa is a differentiable tokenizer trained end-to-end with the language model that improves robustness to character perturbations and out-of-domain data and is considerably faster than strictly byte-level models."},{"url":"https://www.semanticscholar.org/paper/3227844af4d38bfa702781e321cf6712bf537e2c","title":"Word-level Perturbation Considering Word Length and Compositional Subwords","venue":"Findings","year":2022,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki","citations":[{"paperId":"75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a","title":"Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation"}],"references":[{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"4f90ef30a35db06b4e953cfce8b8fc4179d4bff7","title":"Rethinking Perturbations in Encoder-Decoders for Fast Training"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"969d5b8cd07803eabea766e13a26dc31d577ee1e","title":"Token Drop mechanism for Neural Machine Translation"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"3504dd566b25f24f44cf647731370760a537b483","title":"Stochastic Tokenization with a Language Model for Neural Text Classification"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"78a5efc6d0ef9a1e4cfd375e189474305d0b099f","title":"Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"b1b9b73967a40830d6924f9a823c4f910735376d","title":"Word Embedding Perturbation for Sentence Classification"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"786f95cada23d4639aa1a8b922cdb9fb9a9c03fa","title":"Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling"},{"paperId":"36f652172792f8aab1cf3c4441a72a1bf79d17c8","title":"Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"},{"paperId":"0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652","title":"A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"df137487e20ba7c6e1e2b9a1e749f2a578b5ad99","title":"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks"},{"paperId":"bae2cbe50c62798305acbaf9eaeebbba27afdf81","title":"Daily-Aware Personalized Recommendation based on Feature-Level Time Series Analysis"},{"paperId":"f1b0df6b2977d28e55df82576b108e4f5d87e044","title":"Text Understanding from Scratch"},{"paperId":null,"title":"Rakuten dataset. Informatics Research Data Repository, National Institute of informatics"},{"paperId":null,"title":"Rakuten dataset"},{"paperId":null,"title":"jieba"},{"paperId":"51c88cf10b75ed1a5445660cc64cee3d1c6fd8c5","title":"Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling"},{"paperId":"70b849773678010942a0975f2887e527c17cda76","title":"MeCab : Yet Another Part-of-Speech and Morphological Analyzer"},{"paperId":"cb826a3899752b796f14df1c50378c64954a6b0a","title":"Statistical Significance Tests for Machine Translation Evaluation"},{"paperId":"eef6a78dcd4dd887cba26a35f83ab9e8e696ffde","title":"Automatic Extraction of New Words from Japanese Texts using Generalized Forward-Backward Search"},{"paperId":null,"title":"data/ Data_5d832973308d57446583ed9f mum length of reviews to 100 characters, and the total number of reviews was 525,000. We created datasets for Rating(Ja) and Genre(Ja) from the same reviews"}],"id":"3227844af4d38bfa702781e321cf6712bf537e2c","summary":""},{"url":"https://www.semanticscholar.org/paper/8c82d3d758897ef9f166924683831ecf6085f21a","title":"MaxMatch-Dropout: Subword Regularization for WordPiece","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/09/2022","authors":"Tatsuya Hiraoka","citations":[{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"}],"references":[{"paperId":"75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a","title":"Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation"},{"paperId":"70d73c5c519a7259a52da232a8392616ec283c52","title":"WRIME: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"28459083ba624020c8f1c1ed7c3a075f48b4e709","title":"KLUE: Korean Language Understanding Evaluation"},{"paperId":"72eed6570807c1d3c9a60289c7fc6813277e1e98","title":"Fast WordPiece Tokenization"},{"paperId":"8724dbc032a936efd7d2761be56be66069fd1cb6","title":"Evaluating Robustness to Input Perturbations for Neural Machine Translation"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":null,"title":"Pretrained language models for korean"},{"paperId":"3504dd566b25f24f44cf647731370760a537b483","title":"Stochastic Tokenization with a Language Model for Neural Text Classification"},{"paperId":"2a66d626f575fb418dd7b82eb156b721142f7420","title":"Filtering Method for Twitter Streaming Data Using Human-in-the-Loop Machine Learning"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8ff46c88964a36985f2b45933a3d47b81bd87bd0","title":"Quora Question Pairs"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"36f652172792f8aab1cf3c4441a72a1bf79d17c8","title":"Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"db8885a0037fe47d973ade79d696586453710233","title":"The Sixth PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"685d42a668413422615519a52ac75d66fded4611","title":"Framewise phoneme classification with bidirectional LSTM networks"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"}],"id":"8c82d3d758897ef9f166924683831ecf6085f21a","summary":"The proposed method, MaxMatch-Dropout, randomly drops words in a search using the maximum matching algorithm for tokenization to realize finetuning with subword regularization for popular pretrained language models such as BERT-base."},{"url":"https://www.semanticscholar.org/paper/75a05bffa2cc35a876ce04edb0b57f9592716d3b","title":"Extending the Subwording Model of Multilingual Pretrained Models for New Languages","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"K. Imamura,E. Sumita","citations":[],"references":[{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"13bcfb944779165983aaef22cec8a3bbd3e98e62","title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts"},{"paperId":"29599829d26b4acaaf0ad88698ed4362d33b2ae7","title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"ec7566ec272814560c45f9b97412395eff8f8460","title":"The University of Edinburgh’s English-Tamil and English-Inuktitut Submissions to the WMT20 News Translation Task"},{"paperId":"0701aad2252c619096083b86e5151d6b35d78a59","title":"Facebook AI’s WMT20 News Translation Task Submission"},{"paperId":"1e169d99f966c3f377c8f2e9f666ff47193b09e8","title":"CUNI Submission for the Inuktitut Language in WMT News 2020"},{"paperId":"1f58c42f44113f1c3c8a97c538e78f37f839f4b8","title":"Multilingual Translation with Extensible Multilingual Pretraining and Finetuning"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"bcec2d28d1eb2d60d4efcf3e2714a92d03df6e52","title":"Machine Translation for English–Inuktitut with Segmentation, Data Acquisition and Pre-Training"},{"paperId":"671cd646d61c670fffc2fea4cf7b7a1f6f80dcf7","title":"NRC Systems for the 2020 Inuktitut-English News Translation Task"},{"paperId":null,"title":"The NiuTransmachine translation systems for WMT 20"},{"paperId":null,"title":"André Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, and Matteo Negri, editors"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"f16410d241616553ae6e63fcf4ba8959eda58aea","title":"Applying Conditional Random Fields to Japanese Morphological Analysis"},{"paperId":null,"title":"Foundations of Statistial Natural Language Processing"}],"id":"75a05bffa2cc35a876ce04edb0b57f9592716d3b","summary":"This paper adds new subwords to the SentencePiece tokenizer to apply a multilingual pretrained model to new languages (Inuk-titut in this paper) and applies the mBART-50 pret trained model to English-Inuktituts translation."},{"url":"https://www.semanticscholar.org/paper/0ab0fda8774c303be8f8f8c8f684a890dcf5d455","title":"Lattice-Based Transformer Encoder for Neural Machine Translation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2019,"referenceCount":49,"citationCount":37,"influentialCitationCount":5,"publicationDate":"04/06/2019","authors":"Fengshun Xiao,Jiangtong Li,Zhao Hai,Rui Wang,Kehai Chen","citations":[{"paperId":"3390f8fc6e0f81e791baea2b82ff1b6f0bb74cc4","title":"Lattention: Lattice-Attention in ASR Rescoring"},{"paperId":"44930df2a3186edb58c4d6f6e5ed828c5d6a0089","title":"Attention, please! A survey of neural attention models in deep learning"},{"paperId":"a81ed90d219196b50e27795530b0eec51843df67","title":"Confusion2Vec 2.0: Enriching ambiguous spoken language representations with subwords"},{"paperId":"ddfc4e1508c2d723b88bcbe0220ccffffb412d37","title":"Transformer versus LSTM Language Models trained on Uncertain ASR Hypotheses in Limited Data Scenarios"},{"paperId":"6b2a3003b27525f51c4f84f65bc65a98cbd9a731","title":"PCBERT: Parent and Child BERT for Chinese Few-shot NER"},{"paperId":"ca1d71b52b3b81a7e1cca297f19d47f353cf3340","title":"Relation-Aware Graph Transformer for SQL-to-Text Generation"},{"paperId":"0b18f7f0e06b6b048e79bed6437b27ac3d496f64","title":"Enhanced encoder for non-autoregressive machine translation"},{"paperId":"08ffdec40291a2ccb5f8a6cc048b01247fb34b96","title":"Relative Positional Encoding for Transformers with Linear Complexity"},{"paperId":"7da93508244f150d518ad96122d215e6f8c8262f","title":"Optimizing Word Segmentation for Downstream Tasks by Weighting Text Vector"},{"paperId":"5a84bf724ee79c507e66e5dc564c98c76d553a53","title":"Porous Lattice Transformer Encoder for Chinese NER"},{"paperId":"88ef2416a4f337b72a0b7ade0e1fea5afcec0512","title":"Constituency Lattice Encoding for Aspect Term Extraction"},{"paperId":"e9fdb52f861d0e13ebf98780587dfc66b151a766","title":"Lexicon-constrained Copying Network for Chinese Abstractive Summarization"},{"paperId":"de17ba267fbf02e9dff443745230eefac5f7d581","title":"Incorporating Named Entity Information into Neural Machine Translation"},{"paperId":"067906c924810e8ffc595ff8c9c4b0b2906cca85","title":"SJTU-NICT’s Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task"},{"paperId":"479c9cefe82c774471ed96800835db34bf04e3d3","title":"High-order Semantic Role Labeling"},{"paperId":"b3eaf6ac2bc38d42585fcb20cfcb40ccf3ac4629","title":"Learning Spoken Language Representations with Neural Lattice Language Modeling"},{"paperId":"93d22f0c38e1c465d75ce1e4ca870ed95a3ef0b8","title":"Bipartite Flat-Graph Network for Nested Named Entity Recognition"},{"paperId":"876be9b226601821eeade310013506a03f023824","title":"Capsule-Transformer for Neural Machine Translation"},{"paperId":"7066df8fd89cca546d1ef3d66679cb15eba48d50","title":"FLAT: Chinese NER Using Flat-Lattice Transformer"},{"paperId":"3fa8d2a9e9a9cf3ee9626424a157888580dcfaba","title":"A Survey of Deep Learning Techniques for Neural Machine Translation"},{"paperId":"66c81a4cd0ba6f2cfce8e1e76ec1d2dd0e389add","title":"A Hierarchical Clustering Approach to Fuzzy Semantic Representation of Rare Words in Neural Machine Translation"},{"paperId":"011be9f75e5c009eeb4ff8f220a73d716befac76","title":"English to Urdu: Optimizing Sequence Learning in Neural Machine Translation"},{"paperId":"18621213820fb05948b326e6d52dfff2deae3ea5","title":"Hierarchical Contextualized Representation for Named Entity Recognition"},{"paperId":"47a60929d2e3a54511b89d28f8b4f1f43f764808","title":"Named Entity Recognition Only from Word Embeddings"},{"paperId":"8c473a8adca5635c3cde5af793ed7b68afec9d77","title":"Dual Co-Matching Network for Multi-choice Reading Comprehension"},{"paperId":"875fe31a7adaf7462ae5b91231572e1ac24f0145","title":"SWITCHING-ALIGNED-WORDS DATA AUGMENTATION"},{"paperId":"4452fbcd04370f2cfbc46066bef3b749a7b3b5a4","title":"Korean-to-Chinese Machine Translation using Chinese Character as Pivot Clue"},{"paperId":"bafc0d4f9c6f4d05b89e5e12cd9e88b93c70f408","title":"Porous Lattice-based Transformer Encoder for Chinese NER"},{"paperId":"3e2a688a97f5158d441fce9387592de1e3ab1264","title":"Porous Lattice-based Transformer Encoder for Chinese NER"},{"paperId":"b134ce39a3a195f5b0c277a1d2c0d776e5ce6a52","title":"Syntax-aware Transformer Encoder for Neural Machine Translation"},{"paperId":"4d68c1b4167f858979c6a8e8b9ad0b484cd48c63","title":"Cross Aggregation of Multi-head Attention for Neural Machine Translation"},{"paperId":"72c0e5b7365ae4981db13cfa15ca808a8eb3a8a1","title":"Head-Driven Phrase Structure Grammar Parsing on Penn Treebank"},{"paperId":"dc477a26c162ddfa8ab36d4f7975f62f5ad04ab5","title":"Open Vocabulary Learning for Neural Chinese Pinyin IME"},{"paperId":"be4e226afde6879620d4563f3c39cf3443e22d1f","title":"SJTU-NICT at MRP 2019: Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing"},{"paperId":"c8e707a0ea3a1f3bc104f191e72c4444b28be834","title":"SJTU at MRP 2019: A Transition-Based Multi-Task Parser for Cross-Framework Meaning Representation Parsing"},{"paperId":"c70df347923d90e05ff19ebd724ffc2fd6232594","title":"Neural-based Pinyin-to-Character Conversion with Adaptive Vocabulary"},{"paperId":"5bfb0b5494885d35bc15952c025fa2d8fbbd8c98","title":"Explicit Contextual Semantics for Text Comprehension"}],"references":[{"paperId":"72c0e5b7365ae4981db13cfa15ca808a8eb3a8a1","title":"Head-Driven Phrase Structure Grammar Parsing on Penn Treebank"},{"paperId":"f3abc20cf0632b667d6f4d9c99f962b243d3beae","title":"Chinese Word Segmentation: Another Decade Review (2007-2017)"},{"paperId":"ca13f33970bc70d44a6159db5e14f65841abcf5a","title":"Dependency or Span, End-to-End Uniform Semantic Role Labeling"},{"paperId":"90fe32dd2fccd4ca14713d8b0252aa34a6b910e8","title":"Subword Encoding in Lattice LSTM for Chinese Word Segmentation"},{"paperId":"ea49cef629b6e5a87a596944ad4ea9144651bfdd","title":"Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models"},{"paperId":"c70df347923d90e05ff19ebd724ffc2fd6232594","title":"Neural-based Pinyin-to-Character Conversion with Adaptive Vocabulary"},{"paperId":"226ceb666cdb2090fc3ab786129e83f3ced56e05","title":"Sentence Selection and Weighting for Neural Machine Translation Domain Adaptation"},{"paperId":"8924c57128e878ec5af751ad826768c43f40f0e3","title":"NICT’s Neural and Statistical Machine Translation Systems for the WMT18 News Translation Task"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"5f0c801a72675327c80a9c2beaa971564f578818","title":"Exploring Recombination for Efficient Decoding of Neural Machine Translation"},{"paperId":"aaf17a1615db72b956e345ad4104bad08166faa9","title":"Seq2seq Dependency Parsing"},{"paperId":"b343458f1dcf23439a33313c39fb0275fcde8afa","title":"Finding Better Subword Segmentation for Neural Machine Translation"},{"paperId":"74b3f93ee47fe36ff1862ec7d52745f30ec7be49","title":"Syntax for Semantic Role Labeling, To Be, Or Not To Be"},{"paperId":"8fb5a6fe93a3d19ffee50677c0ae563e3377d2d4","title":"Chinese NER Using Lattice LSTM"},{"paperId":"65cf18f190c191ab77b43841dec97b448ac90737","title":"Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"4ddd5d6c973632f977ff3a92c3233e41f097b096","title":"Syntax-Directed Attention for Neural Machine Translation"},{"paperId":"33998aff64ce51df8dee45989cdca4b6b1329ec4","title":"Graph Attention Networks"},{"paperId":"c80725ad0c0cd06416f3c01a78b7c419359d3fe2","title":"Instance Weighting for Neural Machine Translation Domain Adaptation"},{"paperId":"d145755c038599f340f8958e4a27e93ce522b5f9","title":"Neural Machine Translation with Source Dependency Representation"},{"paperId":"78e592524e336afdf586c51ac228488a71e31340","title":"Sentence Embedding for Neural Machine Translation Domain Adaptation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1e0b165d10704d0d9be4aa55ae89ab566f919758","title":"Fast and Accurate Neural Word Segmentation for Chinese"},{"paperId":"6cddfbed35c46937588bd9d6b846ca2855953cea","title":"Neural Lattice-to-Sequence Models for Uncertain Inputs"},{"paperId":"aab5002a22b9b4244a8329b140bd0a86021aa2d1","title":"OpenNMT: Open-Source Toolkit for Neural Machine Translation"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"e43f713e0d2d438a4c0b03eacab58c334e869e6a","title":"Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation"},{"paperId":"6f84694963842a27ab3932473c66a0845c8b0cd7","title":"A Character-Aware Encoder for Neural Machine Translation"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"0a3489330820856f82e100de239aac82a3459c4a","title":"Neural Word Segmentation Learning for Chinese"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"a4ebc46452c9386a0e439e3e2e693cf0dacf7406","title":"A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing"},{"paperId":"2ac2db458c8fe8ffee9f214521bd757efbbdd05d","title":"An Empirical Study on Word Segmentation for Chinese Machine Translation"},{"paperId":"4a85665fede60ecb5ef3eb2a9352f3133152e266","title":"Integrating unsupervised and supervised word segmentation: The role of goodness measures"},{"paperId":"a0fb0816ddf94e57d12b51971f53a7286cfe06b3","title":"Word Lattice Reranking for Chinese Word Segmentation and Part-of-Speech Tagging"},{"paperId":"860c1880c5a98cc6a5aeb331113eeda0d84e725c","title":"Generalizing Word Lattice Translation"},{"paperId":"f5b1146b7ca79322aab124fd63825b9c175c02cf","title":"Clause Restructuring for Statistical Machine Translation"},{"paperId":"2c72257ae7a4a32dc60569f4e1fe4504b2678112","title":"The Penn Chinese TreeBank : Phrase structure annotation of a large corpus"},{"paperId":"66e5300af0b4cef6c236f5b220157622882a751a","title":"Integrated Chinese Word Segmentation in Statistical Machine Translation"},{"paperId":"65e90d9f6754d32db464f635e7fdec672fad9ccf","title":"The Second International Chinese Word Segmentation Bakeoff"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"0ab0fda8774c303be8f8f8c8f684a890dcf5d455","summary":"This work proposes lattice-based encoders to explore effective word or subword representation in an automatic way during training and proposes two methods: 1) lattice positional encoding and 2) lattICE-aware self-attention to further improve translation performance."},{"url":"https://www.semanticscholar.org/paper/8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","title":"Adversarial Subword Regularization for Robust Neural Machine Translation","venue":"Findings","year":2020,"referenceCount":47,"citationCount":3,"influentialCitationCount":0,"publicationDate":"01/04/2020","authors":"Jungsoo Park,Mujeen Sung,Jinhyuk Lee,Jaewoo Kang","citations":[{"paperId":"457e73be2f876e0b838f0f8d7aa921993b7f607d","title":"Consistency Training with Virtual Adversarial Discrete Perturbation"},{"paperId":"710033d861e50f80b28c1fa68316c9ea8ab3c42c","title":"Prediction Difference Regularization against Perturbation for Neural Machine Translation"},{"paperId":"055367775f92ab03d53c470f42bc7284d4a256d8","title":"KOAS: Korean Text Offensiveness Analysis System"}],"references":[{"paperId":"32e7c6f91ab45e8db1ce6735e0d54a0e60f0d098","title":"Domain Robustness in Neural Machine Translation"},{"paperId":"229aff74294275d31ded5c8e6e80f6aa6a88fcfd","title":"A Latent Morphology Model for Open-Vocabulary Neural Machine Translation"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"b1e7ffa599c0075ab03ce7a949376ae4ccce9a05","title":"Subword Language Model for Query Auto-Completion"},{"paperId":"18a1c21f35153c45d0ef30c564bffb7d70a13ccc","title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"2ecac1099a4a49a504d1d7919c4a69897e173cdc","title":"Naver Labs Europe’s Systems for the WMT19 Machine Translation Robustness Task"},{"paperId":"4ddb924d2018f006324d930034a031b41af8c763","title":"Effective Adversarial Regularization for Neural Machine Translation"},{"paperId":"a5690b0a514a7cbc913871e41e54c9ad4f6362db","title":"Findings of the First Shared Task on Machine Translation Robustness"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"e364a32064235297e6bcf7b86aeb73679527222c","title":"Robust Neural Machine Translation with Doubly Adversarial Inputs"},{"paperId":"e84d754564c9e2ce993596370e0a1493c9c6e4b1","title":"Improving Neural Language Modeling via Adversarial Training"},{"paperId":"09342a388050fa7138116acea2a1271cf42fa95e","title":"Subword Regularization and Beam Search Decoding for End-to-end Automatic Speech Recognition"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"712cd873d7370db280f4ceaaf000dc49f76b59fe","title":"On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models"},{"paperId":"be312e930f6739a709e60547aa0dfb9c3dc44497","title":"Multilingual Neural Machine Translation With Soft Decoupled Encoding"},{"paperId":"0b9ac1035918823ffca1c6f55ec316b42d4e033f","title":"Training on Synthetic Noise Improves Robustness to Natural Noise in Machine Translation"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"ce89ee7aaeeea2c9d474707690f3ea9d948776a3","title":"MTNT: A Testbed for Machine Translation of Noisy Text"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"514e7fb769950dbe96eb519c88ca17e04dc829f6","title":"HotFlip: White-Box Adversarial Examples for Text Classification"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"ffb949d3493c3b2f3c9acf9c75cb03938933ddf0","title":"Adversarial Examples for Evaluating Reading Comprehension Systems"},{"paperId":"f6b7583dffc04b51a2d262a0d5b209984c062f53","title":"Towards Crafting Text Adversarial Samples"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"2cd55ded95d5d13430edfa223ba591b514ebe8a5","title":"Adversarial Training Methods for Semi-Supervised Text Classification"},{"paperId":"88edf12127a628bed608cae0bdf3700d00824df4","title":"Toward Robust Neural Machine Translation for Noisy Input Sequences"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"8c3cf30db12d17638b01e0e464e09d6b58a88187","title":"Variable length word encodings for neural translation models"},{"paperId":"bee044c8e8903fb67523c1f8c105ab4718600cdb","title":"Explaining and Harnessing Adversarial Examples"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"226966243877f186d346be01047cf71cee1b5ec4","title":"Online EM for Unsupervised Models"},{"paperId":"96912cf06fe63e7047cbd6df3063a08f8827d398","title":"The Infinite PCFG Using Hierarchical Dirichlet Processes"},{"paperId":"2d6a97f83bb8207ea9d88118618ed3ab52054a88","title":"Unsupervised Morpheme Segmentation and Morphology Induction from Text Corpora Using Morfessor 1.0"},{"paperId":"cb826a3899752b796f14df1c50378c64954a6b0a","title":"Statistical Significance Tests for Machine Translation Evaluation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"d36efb9ad91e00faa334b549ce989bfae7e2907a","title":"Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"},{"paperId":"145c0b53514b02bdc3dadfb2e1cea124f2abd99b","title":"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"}],"id":"8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","summary":"This paper presents adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for exposing diverse subword segmentations and experimentally shows that the model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the performance of N MT models in low-resource and out-domain datasets."},{"url":"https://www.semanticscholar.org/paper/0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":102,"citationCount":76,"influentialCitationCount":11,"publicationDate":"31/12/2020","authors":"Phillip Rust,Jonas Pfeiffer,Ivan Vulic,Sebastian Ruder,Iryna Gurevych","citations":[{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"bfff952fb890f3eb4ba22718f1df70a030741b74","title":"Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?"},{"paperId":"44e67f6bdb132fadff230b961e61304804f32657","title":"Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training"},{"paperId":"6cc1d05250511e596e3607827fda4614abb6ed74","title":"Neural Transfer Learning with Transformers for Social Science Text Analysis"},{"paperId":"15a802ca1e77e678db79f61e36cf9eaf6b273c24","title":"MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource Code Completion"},{"paperId":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"296c9f9ba0139a3d7c7a1197960a1c9bce5141e7","title":"Punctuation Restoration for Singaporean Spoken Languages: English, Malay, and Mandarin"},{"paperId":"d3a01c0aeddb98b749bb48c97a48310e91e66d36","title":"Multilingual Multimodality: A Taxonomical Survey of Datasets, Techniques, Challenges and Opportunities"},{"paperId":"22fbef2bfef213a7619ee4f307e8f42d1888e638","title":"LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation"},{"paperId":"e874510a64c90ec41659338b1f1f6baee1736a15","title":"You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"265ebfc1074c73917233dbecad802c0c64921a1c","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","title":"MonoByte: A Pool of Monolingual Byte-level Language Models"},{"paperId":"52b65b29a3d9a7d9e8c016a156454ef8ad80858d","title":"BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification"},{"paperId":"089fd688de463519a68bd25b11ae1c3eb57b207d","title":"Compositional Evaluation on Japanese Textual Entailment and Similarity"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"6fd36dd51da2a18b53fc7bdb9797f279ceb80462","title":"Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"20b0452359524d15a6182269be51302df850f2a7","title":"hmBERT: Historical Multilingual Language Models for Named Entity Recognition"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"d69ec0bbc9fc4fe898ac8cb73f629d253358bf66","title":"Stop Filtering: Multi-View Attribute-Enhanced Dialogue Learning"},{"paperId":"99be6dc17a7fd399f4af80c4c1cd7ee5247591a1","title":"Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese"},{"paperId":"210980149a6b41d3e8d95c12daa41d6aa391681f","title":"Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models"},{"paperId":"04207549bf872158d117600029dbe1f1cf8e5b59","title":"Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"597cad6c7b9de94eecc153c7cdcaf824905fe915","title":"You Are What You Write: Preserving Privacy in the Era of Large Language Models"},{"paperId":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation"},{"paperId":"d97867578a50f824a19168c024e138e3ca482746","title":"Deep Lexical Hypothesis: Identifying personality structure in natural language"},{"paperId":"b28e23afe988ad83ab5bd9ce7f826c140c533be8","title":"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages"},{"paperId":"63bafbf3e7cfdb576407870137b5751cbb579864","title":"Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies"},{"paperId":"44f75282915d468413e38ac0e1f59b3ee6860485","title":"Does Transliteration Help Multilingual Language Modeling?"},{"paperId":"698a183048f9a81f5c37facfd5fd8454aa813f4c","title":"To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource NLP"},{"paperId":"eb0024439858af7cc951ce2efa5a6533c3781799","title":"WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models"},{"paperId":"dec42306af017bc778bbf1496776f3cd4d5bd42e","title":"Pre-trained transformer-based language models for Sundanese"},{"paperId":"ed4705fc97d35f8b02a4c11633cb6dd6bd316cc7","title":"Cross-lingual Transfer of Monolingual Models"},{"paperId":"071440ccd1084d20d345fafd0bcaa5993f71fb04","title":"xGQA: Cross-Lingual Visual Question Answering"},{"paperId":"21230f72afae4f19f59c4ce9c099075e7ecfe77c","title":"gaBERT — an Irish Language Model"},{"paperId":"9076b287843441fb749d37b69317162c1fa272e3","title":"XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond"},{"paperId":"3885bdef44dbbcaae85a6b4ccaf279593daadb80","title":"Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems"},{"paperId":"ea786f376c51dba5c3be79eed6e4814e8afc9290","title":"Faster and Cheaper Energy Demand Forecasting at Scale"},{"paperId":"f1edc02e4096acb8ea491b4b47c2cbdf0ff3ec91","title":"Writing System and Speaker Metadata for 2,800+ Language Varieties"},{"paperId":"a4fc3d86b84f351e87bcdbe1376eae64750629ac","title":"Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations"},{"paperId":"d3f72c17404ba7f8e56d6d4d7fdf9d808f559570","title":"Fine-tuning de modèles de langues pour la veille épidémiologique multilingue avec peu de ressources (Fine-tuning Language Models for Low-resource Multilingual Epidemic Surveillance)"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE"},{"paperId":"bd0e39d075553a68e7af6fdf437c2a890ad59a58","title":"Scene-Text Aware Image and Text Retrieval with Dual-Encoder"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"070759c65bd32df9f1f57ef52a4c49a77d3057d1","title":"To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource NLP"},{"paperId":"6b2062812d2e353ea884a0cc077e9f6c73351423","title":"On Generalist and Domain-Specific Music Classification Models and Their Impacts on Brazilian Music Genre Recognition"},{"paperId":"633780929ae262e461ea35c20b36a5d7042350e7","title":"MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"8dad3a12d2a652122458dd377f62025354b17a2a","title":"Subword Mapping and Anchoring across Languages"},{"paperId":"10a863a33c4dda778cd6a552ecbb02ac42510445","title":"On the ability of monolingual models to learn language-agnostic representations"},{"paperId":"657a8a8c83339dff13892b26bb989f97b2acb182","title":"Code-switched inspired losses for generic spoken dialog representations"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"9e8bad21221b88b516edd28fe902e591ac08efa5","title":"Modelling Latent Translations for Cross-Lingual Transfer"},{"paperId":"d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","title":"A Primer on Pretrained Multilingual Language Models"},{"paperId":"a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study"},{"paperId":"92f2000f29f1aed7f2cc4e3cb5f783e079ef553f","title":"MergeDistill: Merging Pre-trained Language Models using Distillation"},{"paperId":"28459083ba624020c8f1c1ed7c3a075f48b4e709","title":"KLUE: Korean Language Understanding Evaluation"},{"paperId":"76db3624f1278a4f4f1e69b343bfff7bba306d47","title":"XLM-T: A Multilingual Language Model Toolkit for Twitter"},{"paperId":"529edafa160a77901bec123cf8858e6c08f6cd06","title":"When does pretraining help?: assessing self-supervised learning for law and the CaseHOLD dataset of 53,000+ legal holdings"},{"paperId":"7b99c51d562e33309a46601c846abbe72a65c6a4","title":"What to Pre-Train on? Efficient Intermediate Task Selection"},{"paperId":"a4ab1bd1501668e932c986725b33d065e4f0a233","title":"The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models"},{"paperId":"64cc584aa9bafebbab0bc012e225563cf2873ecc","title":"Introduction to Neural Transfer Learning with Transformers for Social Science Text Analysis"},{"paperId":"dca4128a33ca22c02031b5c0c28548a0df022d80","title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding"},{"paperId":"7cb4b8406255d0f8115afa23d8efea1bb780cfb8","title":"On the Compatibility of Tokenizations Across Languages"},{"paperId":"22c8444eb4da5ae8d43829b127d5e7ee9950b151","title":"Crossing the Conversational Chasm: A Primer on Multilingual Task-Oriented Dialogue Systems"},{"paperId":"6adc9c231d874ea358554b8680a6aaba4bd6c963","title":"MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer"},{"paperId":"892731fd37a1d38901b0c5bc68168d96f43bd5d0","title":"Monolingual Pre-trained Language Models for Tigrinya"},{"paperId":"7e5133e41d5a06b8ad4bad7f2046ac73edf43e38","title":"TUDa at WMT21: Sentence-Level Direct Assessment with Adapters"},{"paperId":"8484b4da1aff6460edbd2df78bf2f5e801569248","title":"GSI-UPM at IberLEF2021: Emotion Analysis of Spanish Tweets by Fine-tuning the XLM-RoBERTa Language Model"},{"paperId":"91ea7176cc41f7b97867d483075df7886aa3dc33","title":"Practical Transformer-based Multilingual Text Classification"}],"references":[{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"13bcfb944779165983aaef22cec8a3bbd3e98e62","title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts"},{"paperId":"31392ad8722d9c66181b621936e2013199e02edc","title":"When Do You Need Billions of Words of Pretraining Data?"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"bdeec55f95fd6b73e3e4635459b14c7248543efb","title":"AdapterDrop: On the Efficiency of Adapters in Transformers"},{"paperId":"575ac3f36e9fddeb258e2f639e26a6a7ec35160a","title":"Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation"},{"paperId":"e00631018e737355f1b0b3db779641f8f26288b1","title":"WikiBERT Models: Deep Transfer Learning for Many Languages"},{"paperId":"98ef0db84e62aef969629264c9de1f4d0013f3b9","title":"AdapterFusion: Non-Destructive Task Composition for Transfer Learning"},{"paperId":"85dc7829455819283270eb643817bcf97133464d","title":"From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"9be1d1bf82f6ca6a7bf6a7d92f8f37b647e493d0","title":"Probing Pretrained Language Models for Lexical Semantics"},{"paperId":"33add47a46818f2e6a63cfe84e539720111f844f","title":"MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale"},{"paperId":"110c13fbf4ff87b52ee1fd9eb2d3616c839ceb41","title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank"},{"paperId":"03f22e693a0c00bae8a66a64a2fecb0f11a4b034","title":"IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding"},{"paperId":"c0ba595b2bef54f2552ec4716bb187901f52f4a3","title":"KR-BERT: A Small-Scale Korean-Specific Language Model"},{"paperId":"063f8b1ecf2394ca776ac61869734de9c1953808","title":"AdapterHub: A Framework for Adapting Transformers"},{"paperId":"528dd0da358b4939d99eeb92548deccfeac48bd6","title":"A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages"},{"paperId":"8b8c29c0cbb6cbae26b930840396596dd5806f33","title":"Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers"},{"paperId":"14489ec7893e373a0dcc9555c52b99b2b3a429f6","title":"Are All Languages Created Equal in Multilingual BERT?"},{"paperId":"d97e7561fa7710213ccd4f8128044ea6849be377","title":"XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"3b233bdb697cc43effa1eb6d2868ff14efbbab7a","title":"UDapter: Language Adaptation for Truly Universal Dependency Parsing"},{"paperId":"41a7fece6e8c47d5bff75f7701a702f351b110d6","title":"BERTurk - BERT models for Turkish"},{"paperId":"b805693c17961af2cc7f859c1a54320b26036f46","title":"Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"6551f742b825561d26242ca8a646ba0e33fb109f","title":"What the [MASK]? Making Sense of Language-Specific BERT Models"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"2bd54adb3b5588281396a4b5dae7db09496b2c61","title":"SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis"},{"paperId":"3b2538f84812f434c740115c185be3e5e216c526","title":"Cross-Lingual Ability of Multilingual BERT: An Empirical Study"},{"paperId":"4fa37d012ad0014552a6a5a03624b29f95558bf7","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"a179b6fdbaf2fbad5d7fe3dc0cc34351bc586439","title":"A Finnish news corpus for named entity recognition"},{"paperId":"85ec965034e2b2a23187c2d105fa0e44f7b57460","title":"Parsing with Multilingual BERT, a Small Treebank, and a Small Corpus"},{"paperId":null,"title":"Universal dependencies 2.6. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL)"},{"paperId":null,"title":"2020)) with batch size"},{"paperId":null,"title":"Revisiting pretrained models for Chinese natural language processing"},{"paperId":"a4d5e425cac0bf84c86c0c9f720b6339d6288ffa","title":"BERTje: A Dutch BERT Model"},{"paperId":"477d66dcd2c08243dcc69822d6da7ec06393773a","title":"Multilingual is not enough: BERT for Finnish"},{"paperId":"6ffb1cc32ddde6ae01e2fc0286eafa116ade0ffb","title":"KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension"},{"paperId":"81fbf08beb80b01abaa6ad6a07b48c3034ead8a6","title":"Is Multilingual BERT Fluent in Language Generation?"},{"paperId":"26dc86404fac9d8ead6c98e8237e29c4a5f7981f","title":"Improving Bi-LSTM Performance for Indonesian Sentiment Analysis Using Paragraph Vector"},{"paperId":"e471dccea2b2a05196137984847d7e2067d259a1","title":"OSIAN: Open Source International Arabic News Corpus - Preparation and Integration into the CLARIN-infrastructure"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a3958ca44a736bd58a991d42fb2fdf6bfcb2029e","title":"Sentiment Analysis of Product Reviews in Russian using Convolutional Neural Networks"},{"paperId":"82d40215de43fbc2aa3b0f8c6ebba73f35e64c9b","title":"An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese"},{"paperId":"cc94d15ba408c260c8fe4fa4f1cb6797a996dd21","title":"Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language"},{"paperId":"2fa3f7ce620a1c7155daef6620dd6bb0e01934f3","title":"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"},{"paperId":"06979d126d8286866624f20907e57e8e8aa7df52","title":"Polyglot Contextual Representations Improve Crosslingual Transfer"},{"paperId":"660d3472d9c3733dedcf911187b234f2b65561b5","title":"BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"e99e2bd4812b30e104db0feddb681f32acd88758","title":"Massively Multilingual Transfer for NER"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"e1e43d6bdb1419e08af833cf4899a460f70da26c","title":"AlBERTo: Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets"},{"paperId":null,"title":"2019) only included text passages from the articles, and used older Wikipedia dumps"},{"paperId":null,"title":"2020), we only use masked language modeling (MLM) as pretraining objective and omit the next sentence prediction task"},{"paperId":null,"title":"Exploring BERT's Vocabulary. Blog Post"},{"paperId":null,"title":"We pretrain the new monolingual models (MONOMODEL-*) from scratch for 1M steps with batch size 64"},{"paperId":null,"title":"2020), we only use masked language modeling as pretraining objective and omit the next sentence prediction task"},{"paperId":null,"title":"We pretrain the new monolingual models (wiki-mono) from scratch for 1M steps with batch size 64. We choose a sequence length"},{"paperId":null,"title":"Selection of pretrained models used in our experiments. We display the respective vocabulary sizes and the proportion of tokens"},{"paperId":null,"title":"Question Answering dataset overview"},{"paperId":null,"title":"How multilingual is multilingual bert? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996–5001, Florence, Italy"},{"paperId":null,"title":"Exploring bert’s vocabulary"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"c60ade7d6301eb8f863eee926c167420cf77ac63","title":"Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing"},{"paperId":"c997d481606f0346164511cabe74c6d1ef3f6be5","title":"DRCD: a Chinese Machine Reading Comprehension Dataset"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"fdaa957d6c0a5c99b1cdbbf9054fe9eeed40b01c","title":"Hotel Arabic-Reviews Dataset Construction for Sentiment Analysis Applications"},{"paperId":"5308b9d6c001304a882a50891ccce9f7ccb1c3ec","title":"On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling"},{"paperId":"9589244bbff8c5b5e57f52f99776cda332e6ba48","title":"A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"d89ee98810039d2061ed42ee8026da49c503d16b","title":"Learning multiple visual domains with residual adapters"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":null,"title":"Korea Maritime and Ocean University (KMOU) NER dataset29 for KO. For AR, ID, JA, RU, and TR, we use the respective portions of the WikiAnn"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"a1aaa7c75464e7ebe41cfe5c5258241ca34c6414","title":"Farasa: A Fast and Furious Segmenter for Arabic"},{"paperId":"c5f5bb3131f5f1082bd82cce8a0ac08dea1e9366","title":"UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing"},{"paperId":null,"title":"Universal Dependencies v1: A multilingual"},{"paperId":null,"title":"2016. 1.5 billion words arabic"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":null,"title":"Wikiextractor"},{"paperId":"66182e1ba6dca20f315cd952866a94f54d8ac820","title":"Cross-lingual polarity detection with machine translation"},{"paperId":null,"title":"Sentiment Movie Corpus (NSMC)32 for KO, the RuReviews dataset (Smetanin and Komarov, 2019) for RU, the movie and product reviews datasets by Demirtas and Pechenizkiy"},{"paperId":"649d03490ef72c5274e3bccd03d7a299d2f8da91","title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":null,"title":"News Articles 14041"},{"paperId":null,"title":"Early stopping-but when? In Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pages 55–69, Berlin, Germany"},{"paperId":null,"title":"Early stopping-but when? In Neural Networks: Tricks of the Trade, pages 55–69"},{"paperId":null,"title":"Sentiment Analysis (SA)"},{"paperId":null,"title":"Universal Dependency Parsing (UDP), and Part-of-Speech Tagging (POS)"},{"paperId":null,"title":"Furkan Atmaca, Mohammed Attia"}],"id":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","summary":"It is found that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language."},{"url":"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":75,"citationCount":115,"influentialCitationCount":29,"publicationDate":"28/05/2021","authors":"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel","citations":[{"paperId":"b1dad820853464b30c93b52366b32690ba4b99a6","title":"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View"},{"paperId":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models"},{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"11ddb0953eae196dab339bfdc117221594cf945e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only"},{"paperId":"d4c23dc85ea1eba2f863ff6ffbbb5e473e491ff4","title":"TRIP: Triangular Document-level Pre-training for Multilingual Language Models"},{"paperId":"4aa09cba27a489ca02471fad011ea4854fc63cc1","title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"be2df0fafa89b6355d1ff1336c10e0d4c8d27276","title":"The Massively Multilingual Natural Language Understanding 2022 (MMNLU-22) Workshop and Competition"},{"paperId":"3af872a4bd314db9e7964876fee811823628f83f","title":"Text Normalization on Code-Mixed Twitter Text using Language Detection"},{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"},{"paperId":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"9f9196beee29c02b8c5837c6edbc69967189b735","title":"Efficient Transformers with Dynamic Token Pooling"},{"paperId":"d50b9db750ded246bde13e2c263e341bcbd8a335","title":"A Benchmark and Dataset for Post-OCR text correction in Sanskrit"},{"paperId":"9dab4c20648cd4c7c6830e6274a95294b014aac9","title":"QAmeleon: Multilingual QA with Only 5 Examples"},{"paperId":"9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages"},{"paperId":"92302ab168429c7c3a8f699b35ba8302916c6e7c","title":"Bridging Speech and Textual Pre-trained Models with Unsupervised ASR"},{"paperId":"5e786d47e3dc5ffa65ba8548ff67139a2764acf8","title":"T5lephone: Bridging Speech and Text Self-supervised Models for Spoken Language Understanding via Phoneme level T5"},{"paperId":"d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6","title":"SLING: Sino Linguistic Evaluation of Large Language Models"},{"paperId":"6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc","title":"Graphemic Normalization of the Perso-Arabic Script"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72","title":"Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning"},{"paperId":"265ebfc1074c73917233dbecad802c0c64921a1c","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"595cc15b9db4e985b30a2e175399a38c021d4ce7","title":"Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino"},{"paperId":"2a8aac78df5e1e9658a02d381662ed26c6577713","title":"Disease diagnostic method based on cascade backbone network for apple leaf disease classification"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"9fc5878d49c41beb12b62032b0afe9a8501fe9db","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?"},{"paperId":"e4437293a703fcf282bf1b38bf7900ed8ca711bb","title":"Training a T5 Using Lab-sized Resources"},{"paperId":"a806041621acb5cf648fc780f1ff14939aa3a721","title":"How Much Does Lookahead Matter for Disambiguation? Partial Arabic Diacritization Case Study"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"00d9a73c54053a32e2aba92b53fc3e6bc71d3238","title":"Sequence to sequence pretraining for a less-resourced Slovenian language"},{"paperId":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","title":"Confident Adaptive Language Modeling"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"b308511ac52e1c2da915d1e55d5246dfdb4cbe28","title":"Romanian Question Answering Using Transformer Based Neural Networks"},{"paperId":"a766ef0678aade6a9798552618819cf4d0fac406","title":"TEVR: Improving Speech Recognition by Token Entropy Variance Reduction"},{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"32afdb07021fda775ceaedd231c58bfed0aa980a","title":"Automated Crossword Solving"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"645b10b7802a035e034488e3640fc0bc415de34c","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"063d9fa4861356500219b7e81d5a654aa921da6f","title":"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation"},{"paperId":"7267812178393b8ae0b99648f02661ca1ff2b412","title":"Bilingual End-to-End ASR with Byte-Level Subwords"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"},{"paperId":"0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"880c8973ea1376c6bff23226b3f64792657aa666","title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"1ed66e048bb025e75aa5ea660545285212e5341f","title":"Scaling Up Models and Data with t5x and seqio"},{"paperId":"a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia"},{"paperId":"8a4eec70e3d0c43abf26757252ad6210c9717f6c","title":"Towards Lithuanian grammatical error correction"},{"paperId":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation"},{"paperId":"7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"},{"paperId":"8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"},{"paperId":"f357b35e068bbfbb8468b48198ab63241713629d","title":"Correcting diacritics and typos with ByT5 transformer model"},{"paperId":"7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models"},{"paperId":"e53f455b3300d95738dac117419c88fbde58ac4e","title":"Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e35357ac461a669fe7e4b877ee1fad0dfda26303","title":"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings"},{"paperId":"a70fc86508bd0133d5d984a4e777abef1934d76c","title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview"},{"paperId":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"CHARFORMER: FAST CHARACTER TRANSFORMERS"},{"paperId":"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","title":"On the Influence of Tokenizers in NLP Pipelines"},{"paperId":"9758782feed4b4b9bf0ec18b802462e8023a7f83","title":"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"0ffb0b109acddf0067aec252221e0ea04d317ddc","title":"A Framework for Sexism Detection on Social Media via ByT5 and TabNet"},{"paperId":"6102fe88a512290b80e83ed2fe17606b166e505a","title":"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin"},{"paperId":"143659fcf93f33e9139f594cf8111c6bc85c04fa","title":"Overview of the EvaLatin 2022 Evaluation Campaign"},{"paperId":"2005311276a1a90dc17cf6e2ca7725fee2e76e1e","title":"BasqueGLUE: A Natural Language Understanding Benchmark for Basque"},{"paperId":"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","title":"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference"},{"paperId":"32290d428b50f64d1cfee6ea658dc6ba977b26e9","title":"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia"},{"paperId":"2b6b38b66fcca218cb2f381e5d4711b5c99a784f","title":"Training Text-to-Text Transformers with Privacy Guarantees"},{"paperId":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"5f77157038dc7f189db7e72ea58567039222d9df","title":"Examining Single Sentence Label Leakage in Natural Language Inference Datasets"},{"paperId":"db3690379953f2ea4f2a015615de02e643d437ea","title":"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers"},{"paperId":"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","title":"Large Dual Encoders Are Generalizable Retrievers"},{"paperId":"6e2682eb2fbec93b329028b23764c1164e232c41","title":"ÚFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5"},{"paperId":"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","title":"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome"},{"paperId":"de0e1f9980afa7949df64d53b8ae7a2f59c55579","title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages"},{"paperId":"c107835a05ca6fda6e73b64e2ed9884de4fcec0f","title":"A proposed conceptual framework for a representational approach to information retrieval"},{"paperId":"4318c9f87221b9f504f286540c9a6d5c1cc4e4c8","title":"Rethnicity: Predicting Ethnicity from Names"},{"paperId":"9d13bde760d8e77f059436d60160881becd2d2e0","title":"Predicting Ethnicity from Names with rethnicity: Methodology and Application"},{"paperId":"39262814fa3e47905a2e5facf13465a1f70706b9","title":"Single-Read Reconstruction for DNA Data Storage Using Transformers"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"972808b92159a782f5ca1c1ab3a8ed3867acfb2d","title":"mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset"},{"paperId":"89a4f4d1f8c93da85d829a1acfe8cafea2b50c00","title":"Transfer Learning for Multi-lingual Tasks - a Survey"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"c41819413725668fbf8e3ca1a0bcaf7fb691e582","title":"How Optimal is Greedy Decoding for Extractive Question Answering?"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning"},{"paperId":"06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"7175caf7568d46c857380d0e5b64653819d5cc45","title":"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization"},{"paperId":"e873ddaf58ff92ae0492983cf57221fb25837f4e","title":"Strategies in subword tokenization: humans vs. algorithms"},{"paperId":"ae7fc32df9401a86353185515f4fd5e2020ac922","title":"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"}],"references":[{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"2b66a0d8a00b0ba7b585e11ab34879420ad351cb","title":"Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution"},{"paperId":"824cd8db8a68732db04f4d8b7139eb4475e59ff2","title":"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"3fd0f34117cf9395130e08c3f02ac2dadcca7206","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":null,"title":"The GEM benchmark: Natural language"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"e46c97b2d3bea4c21eb462d5771647a0de99b81c","title":"Ensemble Self-Training for Low-Resource Languages: Grapheme-to-Phoneme Conversion and Morphological Inflection"},{"paperId":"8c122ff82dedf72fa7ef5342622667bf5848916e","title":"One-Size-Fits-All Multilingual Models"},{"paperId":"278f4f68309fc4ffb83bfc4ba86a0ea005106682","title":"The SIGMORPHON 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion"},{"paperId":"b044395ed89de33b43d304c008d3c5a7727d423d","title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection"},{"paperId":"c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c","title":"Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":null,"title":"Language ID in the wild"},{"paperId":null,"title":"Neural machine translation with bytelevel subwords"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"5b78c2476984452c372e7fce6ac8246d15c97efa","title":"TWEETQA: A Social Media Focused Question Answering Dataset"},{"paperId":"be564b861e5a9bf5d1dec531807e711000027380","title":"One Size Does Not Fit All: Comparing NMT Representations of Different Granularities"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"5e93a30656ef091f55ce42374d498f2b881e0c47","title":"Bytes Are All You Need: End-to-end Multilingual Speech Recognition and Synthesis with Bytes"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":null,"title":"Characterlevel language modeling with deeper selfattention"},{"paperId":null,"title":"Characterlevel language modeling with deeper selfattention"},{"paperId":"1125d986433735ffc63dc0a8be466043e4f5c5b4","title":"Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"305b2cf37e5dece81e95c92883d5a6e28ac93b22","title":"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"f0b6c1ffed9984317050d0c1dfb005cb65582f13","title":"On the State of the Art of Evaluation in Neural Language Models"},{"paperId":"89cb259f39eb8350dd337fc1d02e2f4128c8398c","title":"Byte-based Neural Machine Translation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"563783de03452683a9206e85fe6d661714436686","title":"HyperNetworks"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"7dba53e72c182e25e98e8f73a99d75ff69dda0c2","title":"Recurrent Highway Networks"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"4dabd6182ce2681c758f654561d351739e8df7bf","title":"Multilingual Language Processing From Bytes"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":null,"title":"Neural machine"},{"paperId":null,"title":"Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":null,"title":"GLUE : A multitask benchmark and analysis platform for natural language understanding"},{"paperId":null,"title":"Bowman . 2019 b . GLUE : A multitask benchmark and analysis platform for natural language understanding"},{"paperId":null,"title":"Le . 2017 . Hypernetworks"},{"paperId":null,"title":"Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-totext transformer"}],"id":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","summary":"This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts."}]}
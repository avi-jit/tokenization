{"papers":[{"url":"https://www.semanticscholar.org/paper/ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning","venue":"Workshop on Representation Learning for NLP","year":2022,"referenceCount":24,"citationCount":4,"influentialCitationCount":0,"publicationDate":"22/04/2022","authors":"Md. Mofijul Islam,Gustavo Aguilar,Pragaash Ponnusamy,Clint Solomon Mathialagan,Chengyuan Ma,Chenlei Guo","citations":[{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"d7a379254ac2dee2e31dfdeedb440176f0285aa5","title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"}],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"8ae8e5e840c5f43d4d72cbc4595690fba01aa799","title":"LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation"},{"paperId":"c9b56cb026a38e39bb0228faac57accd6f65e6f7","title":"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"7f9ca11d122957dc59088543f6cd9f907c00d0d3","title":"Neural Models of Text Normalization for Speech Applications"},{"paperId":"e07f2c383a34217a9403a4d58a4e7d61e57d9163","title":"Character Eyes: Seeing Language through Character-Level Taggers"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"5b1516c87818084dc5d195cc274e1ee8923210d2","title":"Neural Cross-Lingual Named Entity Recognition with Minimal Resources"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"664ec878de4b7170712baae4a7821fc2602bba25","title":"Learning to Generate Reviews and Discovering Sentiment"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"93c20e38c85b69fc2d2eb314b3c1217913f7db11","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"20b844e395355b40fa5940c61362ec40e56027aa","title":"Neural networks"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}],"id":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","summary":"This work proposes a vocabulary-free neural tokenizer by distilling segmentation information from heuristic-based subword tokenization, which allows end-to-end task learning, resulting in optimal task-specific tokenization."},{"url":"https://www.semanticscholar.org/paper/9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":79,"citationCount":10,"influentialCitationCount":0,"publicationDate":"06/06/2022","authors":"Ayush Kaushal,Kyle Mahowald","citations":[{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"5eb0850336e5cb952999dc8522d21799e815ec5f","title":"Generative AI and ChatGPT in School Children’s Education: Evidence from a School Lesson"},{"paperId":"97cea53d979305e6686387ff0ee38a15e3a5ba46","title":"Rigorously Assessing Natural Language Explanations of Neurons"},{"paperId":"a93e4403e6d897e0569214d1acd18685629212b0","title":"Decoding the Encoded – Linguistic Secrets of Language Models: A Systematic Literature Review"},{"paperId":"88a8d98958fa4869d2d10d7c9722ce5ef776fdba","title":"A blind spot for large language models: Supradiegetic linguistic information"},{"paperId":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"e827aa16dd1a1587993e51582db0194cf6901a57","title":"Performance Improvement on Traditional Chinese Task-Oriented Dialogue Systems With Reinforcement Learning and Regularized Dropout Technique"},{"paperId":"4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations"}],"references":[{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"911b7539e964782670e555930b291de16fa971c5","title":"Flexible Generation of Natural Language Deductions"},{"paperId":"8b723be33e62bf5bd9278769244f1c13a9510898","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"538f8e8a36e70ca408f2c5fb6f10f303c52fc317","title":"Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language"},{"paperId":"3dcfa05a1c162e6cab927c5b08d0444f7b6691f4","title":"Probing Classifiers: Promises, Shortcomings, and Advances"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"1d7f3297924a9dd90cfc0df522ebe9138c28b46f","title":"Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"cf592385909a1e3e9a428d8d6d8f427ab70b60a9","title":"Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"738c6d664aa6c3854e1aa894957bd595f621fc42","title":"Information-Theoretic Probing for Linguistic Structure"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"9eb4cd1a4b4717c97c47e3dc4563a75779ae9390","title":"BERT is Not an Interlingua and the Bias of Tokenization"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"199ff73d2f728e997f860b62a2322823d3e3d9e8","title":"Designing and Interpreting Probes with Control Tasks"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"73cf24d4c8ca809727553925a31253a649702582","title":"Meaning to Form: Measuring Systematicity as Information"},{"paperId":"98050eeda3b79a18f555480881e1f3bc7d447882","title":"What Kind of Language Is Hard to Language-Model?"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"70f7542579aeec99d684754b646e245481a71bda","title":"On the Complexity and Typology of Inflectional Morphological Systems"},{"paperId":"84aa9e1685779019fc162a697d0cdb98066f808c","title":"Subword-level Composition Functions for Learning Word Embeddings"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"9852ae077c7da6a9d178acaa2b44a335289507a6","title":"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model"},{"paperId":"97856a4c31fec7b189446a130aab4cbfa8d6a3e8","title":"Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure"},{"paperId":"31f2c1c42cb179820565839868b369c4752e735d","title":"Words cluster phonetically beyond phonotactic regularities"},{"paperId":"645283253f7e79077879ae6b7382661411e8d004","title":"Sound–meaning association biases evidenced across thousands of languages"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"39f22dd35bd8c00095355eab614f7d2ef9034bba","title":"What do you know about an alligator when you know the company it keeps"},{"paperId":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"},{"paperId":"1ca04437a921b53e5d5d25b42c4c987e53bad129","title":"Learning novel phonological neighbors: Syntactic category matters"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"d208fa50239973289e7fab6c5af5d956c3cef3ad","title":"How arbitrary is language?"},{"paperId":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging"},{"paperId":"db734a0e1dc65fe3fe2eef474aefba6d083f54dd","title":"A New Algorithm For Data Compression"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"6ae9ec8288df868505b07bb46911b3e8619055f4","title":"Adding part-of-speech information to the SUBTLEX-US word frequencies"},{"paperId":"cfdd423c8672a7b178ea85d56079328df4eea647","title":"Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit"},{"paperId":"f3054a53bb066ab229cc6f50dfdcddc5d3174060","title":"On the Origin of Language"},{"paperId":"978aebcea5ffb37f916fe9f1a9eeea8618b82117","title":"The differential role of phonological and distributional cues in grammatical categorisation"},{"paperId":"af25dc4cd32ef790690b10878c10d109d52d2021","title":"The Psychological Reality of Phonaesthemes"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"63470730dea262829c56722739bc53bbee99c6b8","title":"Using sound to solve syntactic problems: the role of phonology in grammatical category assignments."},{"paperId":"2461c19e8859ad54107b0bb4e11ca885cab4096f","title":"On Psychological Reality"},{"paperId":"ff94e1642dff4204b89ce046d9cb53ab4bb4b342","title":"Course in General Linguistics"},{"paperId":"43cd44b0c9b8bdd5d790fec110d6c0b6eab036b1","title":"Phonetic symbolism in English word-formation"},{"paperId":"1b94ebedacda0c21a4b8a40a5a40afcea4cc719a","title":"When Combating Hype, Proceed with Caution"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"5034cf3e2483b9e80fa43c6d75ed309a3e028980","title":"Ré-entraîner ou entraîner soi-même ? Stratégies de pré-entraînement de BERT en domaine médical (Re-train or train from scratch ? Pre-training strategies for BERT in the medical domain )"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"bbec2094640e47652234dd8fcfe218fd04886ab5","title":"The indeterminacy of word segmentation and the nature of morphology and syntax"},{"paperId":"2b591d66ffa5f2e017be3c440b492ab3439ca7c5","title":"Exploring systematicity between phonological and context-cooccurrence representations of the mental lexicon"},{"paperId":"1d51de5328d6dd8336033a5b5c8e334e14000880","title":"Probabilistic Modeling in Psycholinguistics: Linguistic Comprehension and Production"},{"paperId":null,"title":"Gpt-j-6b: A 6 billion parameter autoregressive language model"},{"paperId":null,"title":"Exact match and whitespaces Exact match and whitespaces \" signature"},{"paperId":null,"title":"Probing for Character Information We use off-the-shelf APIs for lemmatization and WordNet from NLTK (Apache License 2.0"},{"paperId":null,"title":"Fuzzy match and misspellings Fuzzy match and misspellings Fuzzy match and misspellings \"S1GNATURE"},{"paperId":null,"title":"Why don’t people use characterlevel machine translation? arXiv preprint arXiv:2110.08191"},{"paperId":null,"title":"2019. Bert is not an interlingua and"},{"paperId":null,"title":"spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing"},{"paperId":null,"title":"Some examples of variations in tokenization for two example words"}],"id":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","summary":"The mechanisms through which PLMs acquire English-language character information during training are investigated and it is argued that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings."},{"url":"https://www.semanticscholar.org/paper/188cd686fb2200f237f688dbda7f64ffc75e67ac","title":"Subword Pooling Makes a Difference","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":17,"citationCount":17,"influentialCitationCount":1,"publicationDate":"22/02/2021","authors":"Judit Ács,'Akos K'ad'ar,András Kornai","citations":[{"paperId":"2b6cefcedf5ab2b747e4c4279666cb100fff8506","title":"Morphosyntactic probing of multilingual BERT models"},{"paperId":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!"},{"paperId":"c29348b82ba94a35c1cecfabb526bf61a167ae1f","title":"Impact of Subword Pooling Strategy on Cross-lingual Event Detection"},{"paperId":"7abfcf54eeac3a5c298717a4f0fe8a5daceeaa42","title":"Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling"},{"paperId":"9518b6f3389bb85346eb7240c1b6e8ab1170e1b3","title":"AdaSL: An Unsupervised Domain Adaptation framework for Arabic multi-dialectal Sequence Labeling"},{"paperId":"62f488d3178ce448c9916caf131b83d22ff5d5ff","title":"Comparison of text preprocessing methods"},{"paperId":"00298eef5c6fcd87b572a876e1e13df9bca85bce","title":"UM6P-CS at SemEval-2022 Task 11: Enhancing Multilingual and Code-Mixed Complex Named Entity Recognition via Pseudo Labels using Multilingual Transformer"},{"paperId":"c11b6653a4af4ab030ff91d3183f090694115b07","title":"FiNER: Financial Numeric Entity Recognition for XBRL Tagging"},{"paperId":"c0b9c2dacf76f2a55575291c117fec7162f745d3","title":"A Latent-Variable Model for Intrinsic Probing"},{"paperId":"4a3a92c876e984b564b21fdf18fd2a98bd15837d","title":"Evaluating Transferability of BERT Models on Uralic Languages"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"6da8debf6b4236454662dc3c2542ee8e6fc1e168","title":"ID10M: Idiom Identification in 10 Languages"},{"paperId":"a42965768043c57610142d700666f59daf42d18d","title":"An ELECTRA Model for Latin Token Tagging Tasks"},{"paperId":"dfbb661a7827e37e896e050e57e95417a5bf40b2","title":"Extended Overview of HIPE-2022: Named Entity Recognition and Linking in Multilingual Historical Documents"},{"paperId":"2ac5442a32988f86730e460b3198f475592ae410","title":"Improving Low-Resource Languages in Pre-Trained Multilingual Language Models"},{"paperId":"f7c14e79d3eb1d24b4184d106244be1672113ce2","title":"WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER"},{"paperId":"2ed528f2c1cb8185cd048dc37103515cb2245d44","title":"IWCLUL 2021 The Seventh International Workshop on Computational Linguistics of Uralic Languages"}],"references":[{"paperId":"421beeabd2b981754c2871055a8d48e08cd2a5ad","title":"Attentive Pooling with Learnable Norms for Text Representation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"31c872514c28a172f7f0221c8596aa5bfcdb9e98","title":"75 Languages, 1 Model: Parsing Universal Dependencies Universally"},{"paperId":"526cae4863eb15b5bc39112449c2d5fdf1db85b2","title":"Multilingual Constituency Parsing with Self-Attention and Pre-Training"},{"paperId":"697e110df76fe33e232f019d7e44097af3572abd","title":"Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms"},{"paperId":"57133ef4c4de4d54a57686b8a914b06e4ff4aab5","title":"Universal Dependencies 2.1"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"adb00206a14768c73e2255ab1a5123f17c9260cf","title":"Improving the Arabic Pronunciation Dictionary for Phone and Word Recognition with Linguistically-Based Pronunciation Rules"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"d9c71db75046473f0e3d3229950d7c84c09afd5e","title":"Text Chunking using Transformation-Based Learning"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":null,"title":"We extract 2000 train, 200 dev and 200 test sentences for each task. We keep UD's original splits, in other words, all of our train sentences come from"},{"paperId":null,"title":"Meeting of the Association for Computational Lin-guistics , pages 3499"}],"id":"188cd686fb2200f237f688dbda7f64ffc75e67ac","summary":"This paper investigates how the choice of subword pooling affects the downstream performance on three tasks: morphological probing, POS tagging and NER, in 9 typologically diverse languages and shows that mBERT is better than XLM-RoBERTa in all 9 languages."},{"url":"https://www.semanticscholar.org/paper/7771aa7badc3375a31bfac8dc47755ff5d5c7780","title":"From characters to words: the turning point of BPE merges","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":31,"citationCount":11,"influentialCitationCount":0,"publicationDate":2021,"authors":"Ximena Gutierrez-Vasques,C. Bentz,O. Sozinova,T. Samardžić","citations":[{"paperId":"d77885e0478c318df7a271674dce04349601e80f","title":"Morpheme-Based Neural Machine Translation Models for Low-Resource Fusion Languages"},{"paperId":"77221764ce0eb68d29e1fc35ae61a21179e7dbe5","title":"What changes when you randomly choose BPE merge operations? Not much."},{"paperId":"befaa6e28da4eaaa42de5f5c1a31516de7fb26da","title":"Towards robust complexity indices in linguistic typology"},{"paperId":"14efaa989b8db89ad907e6a15253d482cbba77d9","title":"Linguistically inspired roadmap for building biologically reliable protein language models"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"66ef8d777eccbb4138c0f9cc83d99c0bb1405c2a","title":"Optimizing the Size of Subword Vocabularies in Dialect Classification"},{"paperId":"9a9b689813d23f304bdef7e47a275c1682558991","title":"Dialect Representation Learning with Neural Dialect-to-Standard Normalization"},{"paperId":"8e23530814b61a0cc42233f0248a9d4e106a8ce4","title":"TeDDi Sample: Text Data Diversity Sample for Language Comparison and Multilingual NLP"},{"paperId":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color"},{"paperId":"8929066ce924696f960512c92a720c70bba65586","title":"On Language Spaces, Scales and Cross-Lingual Transfer of UD Parsers"},{"paperId":"3895423c2e2ed608ba1439a6a21b66e43f5ef8b8","title":"Subword Evenness (SuE) as a Predictor of Cross-lingual Transfer to Low-resource Languages"}],"references":[{"paperId":"0b407aaf4a2ffff17d46d24c7b87c3dbaadc38aa","title":"Investigating the effects of i-complexity and e-complexity on the learnability of morphological systems"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"52b3692a99269307144b16db112af8490d32de07","title":"Productivity and Predictability for Measuring Morphological Complexity"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"96c75eaa732028f92065c290723de5ae4c347ab8","title":"Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche"},{"paperId":"98050eeda3b79a18f555480881e1f3bc7d447882","title":"What Kind of Language Is Hard to Language-Model?"},{"paperId":"d323d011a3214116a18d623501bca9a31d33cf4c","title":"Are All Languages Equally Hard to Language-Model?"},{"paperId":"236f10fa4a054e67f7a7cab334f31a80a36781a2","title":"The Entropy of Words - Learnability and Expressivity across More than 1000 Languages"},{"paperId":"a921e014249395496c22783fe58d293746af852b","title":"From Characters to Words to in Between: Do We Capture Morphology?"},{"paperId":"5cedb6606803fd849a73cd1a33160a926e9878cf","title":"A Rich Morphological Tagger for English: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax"},{"paperId":"375f5318839cdabb664f4cc00b0d6eb75a973319","title":"Word and Paradigm Morphology"},{"paperId":"339b602bd0c82c3ff6074ec2bc61221faef05ae0","title":"A Comparison Between Morphological Complexity Measures: Typological Data vs. Language Corpora"},{"paperId":"05b359264e6fed937cb76e9e434935944c5a5b0f","title":"The statistical trade-off between word order and word structure – Large-scale evidence for the principle of least effort"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"6140f733fad17f9f12bb10e072e8babe723c1e9c","title":"Word Order Typology through Multilingual Word Alignment"},{"paperId":"dbf336a8cb911bdda6ef06bc584015a8ed2b1565","title":"Creating a massively parallel Bible corpus"},{"paperId":"a3cec34e50c0e267514b9b92f40b84b886ca570e","title":"From the extraction of continuous features in parallel texts to visual analytics of heterogeneous areal-typological datasets"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"896265da201dcc75d94faf17c995f59762e38bb6","title":"Morphological Organization: The Low Conditional Entropy Conjecture"},{"paperId":"cea92778250fd37c74d2d407f5e244ce64f6a38a","title":"Lexical typology through similarity semantics: Toward a semantic map of motion verbs"},{"paperId":"5e56afcd36262b17a36d5e3f26368ef4f9b5bce1","title":"Parallel texts: using translational equivalents in linguistic typology"},{"paperId":"a4f74ef44f176b0a2d2bdf8576388087ca4086d8","title":"Word-based morphology"},{"paperId":"f193a31921ffd25b0a514dcfe71f61b11a53572d","title":"Entropy Measures, Maximum Entropy Principle and Emerging Applications"},{"paperId":"384dafe7a6a3a576fd34127e0c27b00477728b6d","title":"Measuring Linguistic Complexity: The Morphological Tier"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"6713c11cd2363c24059c4dad65c68cb3e3e2b479","title":"Information Theory and Redundancy"},{"paperId":"c2dfe0f672e80b1ad1fc0fcae9345ea28e5deae2","title":"A Quantitative Approach to the Morphological Typology of Language"},{"paperId":"c4c9d7dce2d1733f98ae268ca80eb35a3cc6e71b","title":"An information-theoretic approach to assess linguistic complexity"},{"paperId":"01d743a38a9759a18c220c598c42c96e6a87b4ca","title":"The type-token relationship in Slavic parallel texts"},{"paperId":"6d12a1d23b21a9b170118a56386552bc5d4727de","title":"A Mathematical Theory of Communication"},{"paperId":"7ce5098ff4e490614157d36f3be865b90ce61bd2","title":"Cognition, quantitative linguistics, and systemic typology"}],"id":"7771aa7badc3375a31bfac8dc47755ff5d5c7780","summary":"It is shown that text entropy values tend to converge at specific subword levels: relatively few BPE merges lead to the most similar distributions across languages."},{"url":"https://www.semanticscholar.org/paper/17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":6,"influentialCitationCount":1,"publicationDate":"08/04/2022","authors":"Edward Gow-Smith,Harish Tayyar Madabushi,Carolina Scarton,A. Villavicencio","citations":[{"paperId":"ec61e750d7a67794ea1d602785a0ca50afe30869","title":"Construction Grammar and Language Models"},{"paperId":"ca7636c1d3426dd09a185abf25f81f7a8fa594e1","title":"How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"5b0c0d181f31acebf2442e7b7c7af27f3f6a5f4a","title":"Hints on the data for language modeling of synthetic languages with transformers"},{"paperId":"c3291d112da94f838669130c6a9138d33a7a0596","title":"SEUNIPD@CLEF: Team JIHUMING on Enhancing Search Engine Performance with Character N-Grams, Query Expansion, and Named Entity Recognition"}],"references":[{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224","title":"Superbizarre Is Not Superb: Derivational Morphology Improves BERT’s Interpretation of Complex Words"},{"paperId":"3f02219184319aa8ca1ef182e6b091b6a7539a04","title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"44189a062182cab428e6d7ea010819f2fe978533","title":"DagoBERT: Generating Derivational Morphology with a Pretrained Language Model"},{"paperId":"8515c83e6562415021b4c536c28ed198f05a8691","title":"Emerging trends: Subwords, seriously?"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"2e808fa44bd52e7234f04f9fb8819ff840608586","title":"Morfessor EM+Prune: Improved Subword Segmentation with Expectation Maximization and Pruning"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"6c1beae31b92c70b42ebeb99e5598d73bff6eea5","title":"NEZHA: Neural Contextualized Representation for Chinese Language Understanding"},{"paperId":"d56c1fc337fb07ec004dc846f80582c327af717c","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"7da138d704ef0fc055825fa132f5c452ed3fb52a","title":"LADEC: The Large Database of English Compounds"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"a9c3a009d754a110379574b069b48c0b4c75db40","title":"Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"3cea3a3eb5b83612a7f8da49fde0d7244058ee06","title":"MorphoLex: A derivational morphological database for 70,000 English words"},{"paperId":"28fcaaa1855d0ed995f649e22c0da2ad415001b4","title":"UniMorph 2.0: Universal Morphology"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"a9772e38875bc73f42a50c26fa9b68c96d2001e5","title":"MorphoLex: A derivational morphological database for 70,000 English words"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"56010a55d49ac1f42355538f494427fd22402be1","title":"Exploring the Limits"},{"paperId":"aaa8f3b7504476727e72806e6f9ded01e13cd494","title":"The English Lexicon Project"},{"paperId":"0c5043108eda7d2fa467fe91e3c47d4ba08e0b48","title":"Unsupervised Discovery of Morphemes"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"7fbf9d5e1bbac56efba21021d4577620ba3b3ee5","title":"MorphyNet: a Large Multilingual Database of Derivational and Inflectional Morphology"},{"paperId":null,"title":"Electra: Pre-training text encoders as discriminators rather than generators"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"5c71a835e13a834cbe8f1241159a7237b820dc92","title":"Morfessor 2.0: Python Implementation and Extensions for Morfessor Baseline"},{"paperId":null,"title":"Japanese 693 and korean voice search Attention is all 707 you need"},{"paperId":"19b8d1def48288a22cc4256a9a620cdf7f294d2f","title":"Morpheme Segmentation Gold Standards for Finnish and English"},{"paperId":null,"title":"On the role of derivational affixes in recognizing complex words"}],"id":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","summary":"An alterna- 013 tive tokenisation approach where spaces are treated as individual tokens are experiments, which show that the modi- 022 ﬁed algorithms give improved performance on downstream NLP tasks that involve handling 024 complex words, whilst having no detrimental effect on performance in general natural lan- 026 guage understanding tasks."},{"url":"https://www.semanticscholar.org/paper/373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation","venue":"AMTA","year":2022,"referenceCount":38,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Salvador Carrión-Ponz,F. Casacuberta","citations":[{"paperId":"117a0c2c23a5a558cf3b39468d917a750bca720c","title":"Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation"},{"paperId":"ed7b51e4a5c4835218f6697b280afb2849211939","title":"Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation"}],"references":[{"paperId":"60293183d3b39cfadf60ba02c0dee6214ceff800","title":"AutoNMT: A Framework to Streamline the Research of Seq2Seq Models"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed","title":"Sparse is Enough in Scaling Transformers"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"9436806f98d0c71245135d5d45025427bb36cd33","title":"Optimizing Transformer for Low-Resource Neural Machine Translation"},{"paperId":"3fd45fc420a882ab2fba3166ef08f376cc758ad0","title":"On Long-Tailed Phenomena in Neural Machine Translation"},{"paperId":"0156fb2096c39839384c0cae0194e123e01cc575","title":"Character-Level Transformer-Based Neural Machine Translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"d9f1eed347959149f27002485a7fe339604fe45d","title":"Revisiting Low-Resource Neural Machine Translation: A Case Study"},{"paperId":"295065d942abca0711300b2b4c39829551060578","title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"7e77d37e7fbfe90f58e4e96e8198903f364d6402","title":"FearNet: Brain-Inspired Model for Incremental Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"2e55ba6c97ce5eb55abd959909403fe8da7e9fe9","title":"Overcoming catastrophic forgetting in neural networks"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a","title":"Learning without Forgetting"},{"paperId":"53c9443e4e667170acc60ca1b31a0ec7151fe753","title":"Progressive Neural Networks"},{"paperId":"84ca430856a92000e90cd728445ca2241c10ddc3","title":"Very Deep Convolutional Networks for Natural Language Processing"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"1eb09fecd75eb27825dce4f964b97f4f5cc399d7","title":"On the Properties of Neural Machine Translation: Encoder–Decoder Approaches"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"4571d75f0be9fe3315f3e8052f9b249db722cc03","title":"Substring-based machine translation"},{"paperId":"37e08e5a6b9f6ae06fe9742ba390b93a4962286b","title":"Can We Translate Letters?"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"6dc710b46bc510a42af487a3ac220e4fabf4d518","title":"VOLT: Improving Vocabularization via Optimal Transport for Machine Translation"},{"paperId":null,"title":"Neurogenesis deep learning"},{"paperId":null,"title":"Bleu: A method for automatic"}],"id":"373588ff1fb9f7590db000a04de8d838b1516e5a","summary":"This work suggests that quasi-character-level models have practically the same generalization capabilities as character-based models but at lower computational costs and appear to help achieve greater consistency between domains than standard subword- level models, although the catastrophic forgetting problem is not mitigated."},{"url":"https://www.semanticscholar.org/paper/d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP","venue":"ArXiv","year":2021,"referenceCount":201,"citationCount":63,"influentialCitationCount":1,"publicationDate":"20/12/2021","authors":"Sabrina J. Mielke,Zaid Alyafeai,Elizabeth Salesky,Colin Raffel,Manan Dey,Matthias Gallé,Arun Raja,Chenglei Si,Wilson Y. Lee,Benoît Sagot,Samson Tan","citations":[{"paperId":"d921732ca5920049b6a8194f559917689f02d096","title":"Text Rendering Strategies for Pixel Language Models"},{"paperId":"17d0cc24fd6a267e7fd59ae62054de3e5c552096","title":"Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"3856fce2fca56e2d7be0134889bf6f9d2f01b931","title":"Advancing Perception in Artificial Intelligence through Principles of Cognitive Science"},{"paperId":"9b1ddb49e89f0f6b6726c40964744b31280cc2be","title":"Data Analysis of the Wordle Game: Insights and Predictive Models Based on Twitter Data"},{"paperId":"02bc90e9fb4a681b048c6652720afe439d16e6cd","title":"Scaling Experiments in Self-Supervised Cross-Table Representation Learning"},{"paperId":"55c04e70e2faf2aae6413731d92cafede317d6ee","title":"Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic"},{"paperId":"6693b31ba94069901313e11b3a9f138eb5ab777d","title":"Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation"},{"paperId":"5bd254c0775d18cdc30cb85be61e080484ee6713","title":"Multilingual Text Representation"},{"paperId":"374ebdc8240a35820cb7ab8bfca37e180e21b605","title":"Sparks of Large Audio Models: A Survey and Outlook"},{"paperId":"7fce7f55cf3e0a3843fd4b63222c476843020b27","title":"Deep learning-based automatic action extraction from structured chemical synthesis procedures"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code"},{"paperId":"d77885e0478c318df7a271674dce04349601e80f","title":"Morpheme-Based Neural Machine Translation Models for Low-Resource Fusion Languages"},{"paperId":"358b97cacb6c4198b3327f96bc481fd98a8379ea","title":"CodeLens: An Interactive Tool for Visualizing Code Representations"},{"paperId":"1158f7a56dd3cc7d715740349d1d9fffbeef10ad","title":"SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural Machine Translation"},{"paperId":"d57cbec10dfee4e244657d3cdf9ba4cf53086744","title":"An Empirical Analysis Towards Replacing Vocabulary-Rigid Embeddings by a Vocabulary-Free Mechanism"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"9addef681bc1576d7d9b0104d71c41b9def546c5","title":"Analysis of Subword Tokenization Approaches for Turkish Language"},{"paperId":"5917ba80a905aeba41487f7dcdb8b885f18689f5","title":"Tokenization and the Noiseless Channel"},{"paperId":"988a17753dd040867eef4f093fa50a87bf4142b1","title":"Tokenization with Factorized Subword Encoding"},{"paperId":"751563cf0c32fe4dfa43d3416c916f8eb053e5f3","title":"GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration"},{"paperId":"17fbffb05fa14e21d1c506fd5f0f568b955fe983","title":"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models"},{"paperId":"b3cff6abe401a244c21d4706b0931e48acaeeb4e","title":"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models"},{"paperId":"d7a379254ac2dee2e31dfdeedb440176f0285aa5","title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding"},{"paperId":"6237a49297f45ebfdeabbbf67d06de323d1fccd4","title":"Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer"},{"paperId":"5161e4a6a6c276960d0db610acb7807f5fdf4c7c","title":"Neural Machine Translation for Code Generation"},{"paperId":"879a7f5abdb7ab803d48172d4f0830965f989d46","title":"Language Model Tokenizers Introduce Unfairness Between Languages"},{"paperId":"ea8197cb357af6f89e8b6e5548897236a24719b1","title":"What is the best recipe for character-level encoder-only modelling?"},{"paperId":"77221764ce0eb68d29e1fc35ae61a21179e7dbe5","title":"What changes when you randomly choose BPE merge operations? Not much."},{"paperId":"1ef0927a242abf01263eaaf48b89876c380f7fec","title":"From Twitter to Aso-Rock: A sentiment analysis framework for understanding Nigeria 2023 presidential election"},{"paperId":"af6c6941acecfb23d899cd5266efdf2e27463f12","title":"Causal Language Model Aided Sequential Decoding With Natural Redundancy"},{"paperId":"850da23eae35322e0af55dbddb7e96e99482c118","title":"From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation"},{"paperId":"83edcfbb206ddad38a971d605da09390604248ea","title":"BloombergGPT: A Large Language Model for Finance"},{"paperId":"4e97303aeb299ee736b1b8c29cef046212690354","title":"Generation-based Code Review Automation: How Far Are Weƒ"},{"paperId":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models"},{"paperId":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"092312cb2a59502a2db7ffea1e31c19ad2eecc7c","title":"H2-Golden-Retriever: Methodology and Tool for an Evidence-Based Hydrogen Research Grantsmanship"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"58b322dbfd76a50d501415d9441053e726e6b8fa","title":"Prototyping Deep Learning Applications with Non-Experts: An Assistant Proposition"},{"paperId":"8a37503b60898afee407162f27c8376d3137a2fd","title":"Evolving Label Usage within Generation Z when Self-Describing Sexual Orientation"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"14efaa989b8db89ad907e6a15253d482cbba77d9","title":"Linguistically inspired roadmap for building biologically reliable protein language models"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"fe35bf433202b4e130709d042aae09dbf4d76232","title":"Text Generation with Text-Editing Models"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"cd48780a4bb7513e6b81cf3bb993b4f4fb962e13","title":"Word-order Typology in Multilingual BERT: A Case Study in Subordinate-Clause Detection"},{"paperId":"188f49825057eee3871684a5070c0f465c6bc9c4","title":"UniMorph 4.0: Universal Morphology"},{"paperId":"61d9162eea5aceb5b78c2d1230f18d8dfe10a208","title":"The Risks of Machine Learning Systems"},{"paperId":"7a25155364476839b6d1fc0653cd8611327ab9ba","title":"mGPT: Few-Shot Learners Go Multilingual"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"2b4369d50ac7310b9908a2baef89a63c68cbd893","title":"Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models"},{"paperId":"2174214efd91ce7334c28acf81fd04c8a493a370","title":"Benchmarking Image Generators on Open-Vocabulary Scene Graphs"},{"paperId":"9459c49da6a5da0d617c9183cc23601b5594673e","title":"Enriching WayunaikiSpanish Neural Machine Translation with Linguistic Information"},{"paperId":"732761bad14162fcff61eccebb4f97f89fd1415b","title":"Controlling Diffusion Input-Output Mapping of the Components of a Diffusion Model as a Potential Approach for Enhanced Model Control Master’s thesis in Complex adaptive systems"},{"paperId":"8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability"},{"paperId":"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","title":"Beyond Characters: Subword-level Morpheme Segmentation"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"fd4274bd2d313c2821b63cebec9ae6a9e8329a2f","title":"Break it Down into BTS: Basic, Tiniest Subword Units for Korean"},{"paperId":"3895423c2e2ed608ba1439a6a21b66e43f5ef8b8","title":"Subword Evenness (SuE) as a Predictor of Cross-lingual Transfer to Low-resource Languages"}],"references":[{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","title":"How BPE Affects Memorization in Transformers"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"58e13e1fed9b28e50efcaff611772801d7980c80","title":"Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation"},{"paperId":"fd400801136261ca329aa84b11307be0e28571f5","title":"Recurrent Neural Networks with Mixed Hierarchical Structures for Natural Language Processing"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"996f0d401acd11e95ce5586010e7e4e18f5c3bb9","title":"How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"d7aa3c65eae71776ccc3cddafb1d4b288deae2cb","title":"A Masked Segmental Language Model for Unsupervised Natural Language Segmentation"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"38e99b3b56d03ad77cc058381bcc90f6cae5cb75","title":"The Effectiveness of Morphology-aware Segmentation in Low-Resource Neural Machine Translation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"5e8da0a332906a2cc5262483ddc27eb9b792dbaf","title":"Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine Translation: The Case of Fon Language"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"13b4d9a1096113db63158590e9d2e58744766e3b","title":"Unsupervised Word Segmentation with Bi-directional Neural Language Model"},{"paperId":"b73b3b086465c629170fd7be5a25164652655f26","title":"One Size Does Not Fit All: Finding the Optimal N-gram Sizes for FastText Models across Languages"},{"paperId":"fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224","title":"Superbizarre Is Not Superb: Derivational Morphology Improves BERT’s Interpretation of Complex Words"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"72eed6570807c1d3c9a60289c7fc6813277e1e98","title":"Fast WordPiece Tokenization"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"d7a1c8da1b8163f5bbd4d12e58cad5002947105e","title":"Word Shape Matters: Robust Machine Translation with Visual Embedding"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"110c13fbf4ff87b52ee1fd9eb2d3616c839ceb41","title":"Parsing with Multilingual BERT, a Small Treebank, and a Small Corpus"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"846e4280f9b958e3ee4b38c76300e62287e16273","title":"Linguist vs. Machine: Rapid Development of Finite-State Morphological Grammars"},{"paperId":"9ae116139ae77ca6d78e162e9639681adef5761c","title":"The Unstoppable Rise of Computational Linguistics in Deep Learning"},{"paperId":"82665af3fa153ef6def31cc42a1c0aac39afa61b","title":"Neural Polysynthetic Language Modelling"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"9d92905edc1a5d027b0827d1309bc6ac03dd94a0","title":"Character-Level Translation with Self-attention"},{"paperId":"651c01b897f18e31bab3ad482154a980ac92c638","title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"2e808fa44bd52e7234f04f9fb8819ff840608586","title":"Morfessor EM+Prune: Improved Subword Segmentation with Expectation Maximization and Pruning"},{"paperId":"76a211ec3b4b96f37038c3993c81597cb1ea7a4a","title":"Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation"},{"paperId":"3b2538f84812f434c740115c185be3e5e216c526","title":"Cross-Lingual Ability of Multilingual BERT: An Empirical Study"},{"paperId":"1c84438eff3d10cfb97fb64fc9f35d734209a465","title":"Character-based NMT with Transformer"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"229aff74294275d31ded5c8e6e80f6aa6a88fcfd","title":"A Latent Morphology Model for Open-Vocabulary Neural Machine Translation"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"7c848b37605cdfe6d2d4778b06e66c0710c3ec34","title":"On the Importance of Word Boundaries in Character-level Neural Machine Translation"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"3504dd566b25f24f44cf647731370760a537b483","title":"Stochastic Tokenization with a Language Model for Neural Text Classification"},{"paperId":"8a7e9f91d949764d6159c114d4feb961ec07b32d","title":"Training Hybrid Language Models by Marginalizing over Segmentations"},{"paperId":"98050eeda3b79a18f555480881e1f3bc7d447882","title":"What Kind of Language Is Hard to Language-Model?"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"2fa3f7ce620a1c7155daef6620dd6bb0e01934f3","title":"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"},{"paperId":"b401aca53c04cec865e1a733090a2c60ca7ebe97","title":"Glyce: Glyph-vectors for Chinese Character Representations"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"20ff6bd5fe67a2d73fb3423216ec2b7369d5a091","title":"Comparing neural‐ and N‐gram‐based language models for word segmentation"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"a98c340f316fc041dd071efcec1fbddf176282d9","title":"Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"64b8361a133ad545cbf9c035e109ef8461b31f67","title":"Super Characters: A Conversion from Sentiment Classification to Image Classification"},{"paperId":"1e6d69f8cfd698b8139cde557252753b22c7d712","title":"BPE and CharCNNs for Translation of Morphology: A Cross-Lingual Comparison and Analysis"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"589e237c9b15c49f15b4989a4684792d9e705cce","title":"Revisiting the Hierarchical Multiscale LSTM"},{"paperId":"2e638ac785703b7332393e5f2d8064b03a3fccf8","title":"Universal Word Segmentation: Implementation and Interpretation"},{"paperId":"9e95df7a7f667c8ee907d0ba4537889e0e27ec3e","title":"Learning Distributional Token Representations from Visual Features"},{"paperId":"19885d5b288e6043eb989d296cc12bc8dbead8e3","title":"Morphological and Language-Agnostic Word Segmentation for NMT"},{"paperId":"d323d011a3214116a18d623501bca9a31d33cf4c","title":"Are All Languages Equally Hard to Language-Model?"},{"paperId":"771c1cb5fb161231e9aaa0a189caba672256a880","title":"Using Morphological Knowledge in Open-Vocabulary Neural Language Models"},{"paperId":"c79ce664a42d7867846ee468180bf9ad8c9e17b2","title":"CoNLL-UL: Universal Morphological Lattices for Universal Dependency Parsing"},{"paperId":"57faf160097049d14399ac6d317bbe4d1b8aa2de","title":"Compositional Representation of Morphologically-Rich Input for Neural Machine Translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"9852ae077c7da6a9d178acaa2b44a335289507a6","title":"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model"},{"paperId":"0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b","title":"Neural Lattice Language Models"},{"paperId":"287efcd4f62c84c45818a71cf5bdd70364ff10eb","title":"An Evaluation of Two Vocabulary Reduction Methods for Neural Machine Translation"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"8e39bc4e1711e941881f64935b203a93259acb50","title":"Glyph-aware Embedding of Chinese Characters"},{"paperId":"a3a39cebbe65cc21009182808300f8f83af2213c","title":"A Dataset for Sanskrit Word Segmentation"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"45e3f381d16e6d6db5449b7a44b7bdf294a8a822","title":"Multiscale sequence modeling with a learned dictionary"},{"paperId":"5616064812996ab1fae525f9679f300c7c307895","title":"Towards Neural Phrase-based Machine Translation"},{"paperId":"ed5003e6a2b97dd4b65c5190ccb88b9c45b032b8","title":"Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"},{"paperId":"db253b17043b6a86e02173b6aa597664b0c7f256","title":"Learning Character-level Compositionality with Visual Features"},{"paperId":"8da6b21fb29fcbe3d6596a8b87242ccb3b06a894","title":"From Segmentation to Analyses: a Probabilistic Model for Unsupervised Morphology Induction"},{"paperId":"5f09d0cbd59e7c84b912692b6ded42107e8f9733","title":"Chinese–Spanish neural machine translation enhanced with character and word bitmap fonts"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"a486e2839291111bb44fa1f07731ada123539f75","title":"Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"},{"paperId":"2d7782c225e0fc123d6e227f2cb253e58279ac73","title":"Improving Neural Language Models with a Continuous Cache"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"105788dd22393d5a4333c167814ec3d38c7d6612","title":"Latent Sequence Decompositions"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"f797fd44b9ddd5845611eb7a705ca9464a8819d1","title":"Very Deep Convolutional Networks for Text Classification"},{"paperId":"acd87e4f672f0b92ea4164414c213560c23bee52","title":"Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"6b904d6e84c98c6ce22ce6923224b205a2a24ee1","title":"Segmental Recurrent Neural Networks"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"6dab1c6491929d396e9e5463bc2e87af88602aa2","title":"Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"},{"paperId":"f22e78a284df04b57d8fc8e622acf1942f288c61","title":"An Unsupervised Method for Uncovering Morphological Chains"},{"paperId":"746bd8b886be53b4f3bdd27d64de5e6181ba8195","title":"Morfessor FlatCat: An HMM-Based Method for Unsupervised and Semi-Supervised Learning of Morphology"},{"paperId":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"5522764282c85aea422f1c4dc92ff7e0ca6987bc","title":"A Clockwork RNN"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"1e6c802a7663e309d47ee45487c03333fd388014","title":"A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability"},{"paperId":"876e51a39ae8bf9b1d48f4e219bbc4e0705ee1cd","title":"Text segmentation with character-level text embeddings"},{"paperId":"62c76ca0b2790c34e85ba1cce09d47be317c7235","title":"Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"93c20e38c85b69fc2d2eb314b3c1217913f7db11","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"ae9768ea90a257f710756fcf19a852d3545fb5c1","title":"Language-independent compound splitting with morphological operations"},{"paperId":"ea9a3a0aaee44cf95d61cd7134c26b8d5e5f4b0e","title":"The sequence memoizer"},{"paperId":"aa5b0f501670a76eac7deaf3027ded218971b633","title":"Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models"},{"paperId":"dd79821ce068f263ab7d6c2037a83c20e562dff3","title":"Morpho Challenge 2005-2010: Evaluations and Results"},{"paperId":"9e022fa8effeaaff77d86be0a3d1bf50d899b5b8","title":"Painless Unsupervised Learning with Features"},{"paperId":"51c88cf10b75ed1a5445660cc64cee3d1c6fd8c5","title":"Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling"},{"paperId":"46f31f9069bb934498a288126053bcab01ff34aa","title":"A Bayesian framework for word segmentation: Exploring the effects of context"},{"paperId":"41fbb2cbace32055d26616fa8c00da5e4782c776","title":"Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars"},{"paperId":"57458bc1cffe5caa45a885af986d70f723f406b4","title":"A unified architecture for natural language processing: deep neural networks with multitask learning"},{"paperId":"342fe6a6338e73fd4d34c4f37f41e3bbad274dd2","title":"Networks"},{"paperId":"1375dca2b57bdadbfc263e641dd95d7826a06073","title":"ParaMor: Minimally Supervised Induction of Paradigm Structure and Morphological Analysis"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"e04fa3599e22b92336a06eca79417f4d6af040f3","title":"Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models"},{"paperId":"a99d6ebe6e583b752d1639a9002dd60581983dc1","title":"Contextual Dependencies in Unsupervised Word Segmentation"},{"paperId":"6bf6c77b895069239ef7a180aee5332ed7b40c79","title":"A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes"},{"paperId":"24b20f7b118588055346f4ac5cdb1fe22e886dda","title":"Interpolating between types and tokens by estimating power-law generators"},{"paperId":"bceba79ff11bdb30cc3b450bf6ac23e02dbbdc26","title":"How Effective is Stemming and Decompounding for German Text Retrieval?"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","title":"A Neural Probabilistic Language Model"},{"paperId":"cdaae7a8f0db8b280266606004f1c6f164a13f6d","title":"Empirical Methods for Compound Splitting"},{"paperId":"0c5043108eda7d2fa467fe91e3c47d4ba08e0b48","title":"Unsupervised Discovery of Morphemes"},{"paperId":"f7a8bd1a62d22de3a3db5edfcc364b462cd4558e","title":"A Bayesian Model for Morpheme and Paradigm Identification"},{"paperId":"30545fe538a773b57e06b4217cd495ef84230bc8","title":"Knowledge-Free Induction of Inflectional Morphologies"},{"paperId":"9f834ee11902ada79b874e7fe5072159d72a0f9f","title":"Unsupervised Learning of the Morphology of a Natural Language"},{"paperId":"23a2d9aaaf581cf1d5db0b82a063c743c10e86a0","title":"Offline dictionary-based compression"},{"paperId":"4d1dac08bb3d960baa88e4ff3477ec834446d056","title":"Minimally Supervised Morphological Analysis by Multimodal Alignment"},{"paperId":"efada5827fd7eedecb4a7dc101caa4509a9f770f","title":"Empirical Estimates of Adaptation: The chance of Two Noriegas is closer to p/2 than p2"},{"paperId":"32bdcb13b099314dcab80fea0e7416ea8a97d41b","title":"Off-line dictionary-based compression"},{"paperId":"37b187e3df04fe7dd31293222407b4b86f3089fb","title":"Attention!"},{"paperId":"a651bb7cc7fc68ece0cc66ab921486d163373385","title":"An algorithm for suffix stripping"},{"paperId":"389713f6b0d7906b42eefeca0f7987d06728e86b","title":"Distributional regularity and phonotactic constraints are useful for segmentation"},{"paperId":"7ec7bc1d7f35558640ac05e94f9b854ec17d7bc0","title":"Linguistic Structure as Composition and Perturbation"},{"paperId":"d4e8bed3b50a035e1eabad614fe4218a34b3b178","title":"An Empirical Study of Smoothing Techniques for Language Modeling"},{"paperId":"b13813b49f160e1a2010c44bd4fb3d09a28446e3","title":"Hierarchical Recurrent Neural Networks for Long-Term Dependencies"},{"paperId":"01fa57bd91f731522c861404d29e4604ba6ac6d3","title":"A hierarchical Dirichlet language model"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"0b44fcbeea9415d400c5f5789d6b892b6f98daff","title":"Building a Large Annotated Corpus of English: The Penn Treebank"},{"paperId":"db6ae486a695efc02910b1dc08eeba13b50d5ca8","title":"Tokenization as the Initial Phase in NLP"},{"paperId":"50c770b425a5bb25c77387f687a9910a9d130722","title":"Learning Complex, Extended Sequences Using the Principle of History Compression"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","title":"Finding Structure in Time"},{"paperId":"66e5551231b018f32004b53b559f8144f38e3c4e","title":"Stochastic Complexity in Statistical Inquiry Theory"},{"paperId":"3d951ad1164c17217a24e46b57f9b31cea1b2b96","title":"Data compression via textual substitution"},{"paperId":"5e37f888248c44a8d8098ccd07e91cc06e8b4d42","title":"AN ALGORITHM FOR THE SEGMENTATION OF AN ARTIFICIAL LANGUAGE ANALOGUE"},{"paperId":"b3c7cb5da97acc79806fc80abef67b45e9434f7e","title":"Morphological Analysis"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"7771aa7badc3375a31bfac8dc47755ff5d5c7780","title":"From characters to words: the turning point of BPE merges"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"1c59de25af45cef20d846ec7454251e8237d45d1","title":"A Statistical Extension of Byte-Pair Encoding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Exploring BERT's vocabulary"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"834724753187ba86709f9d859740f23861d42191","title":"Meaningless yet meaningful: Morphology grounded subword-level NMT"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":"c2e1e362e27ff5ced52bfc9bde3ca165e7eafe76","title":"Neural machine translation using bitmap fonts"},{"paperId":"5f3d28b5c60386994f91f54cf61d1c8568ed328e","title":"The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"},{"paperId":"3df68bed55021596d09b44769d4d74f7495379d2","title":"Empirical Comparison of Evaluation Methods for Unsupervised Learning of Morphology"},{"paperId":"1fd7fc06653723b05abe5f3d1de393ddcf6bdddb","title":"SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"},{"paperId":"0dda3342416a221eb840f1499d01a47b05d7c7f4","title":"Learning from Unseen Data"},{"paperId":"07943f471d65614d3f545f1d37be4ec25739d218","title":"Probabilistic ParaMor"},{"paperId":"b773bc1bed0976c0e3f8ebe4ad883bf582e5902c","title":"SxPipe 2: architecture pour le traitement pré-syntaxique de corpus bruts"},{"paperId":"02711b7e8c73779db802cb00b6cbf407d0872b66","title":"Unsupervised models for morpheme segmentation and morphology learning"},{"paperId":"fcc5e6ab7e7d88835732e58661d264d0b6e9381c","title":"MAF: a Morphosyntactic Annotation Framework"},{"paperId":"a218eb044d91fce34126a2e672ac0a0935a8a7f1","title":"INDUCING THE MORPHOLOGICAL LEXICON OF A NATURAL LANGUAGE FROM UNANNOTATED TEXT"},{"paperId":"af4c762a7a4f4803dfca6223955aba43895aa606","title":"Lexicon-directed segmentation and tagging in Sanskrit"},{"paperId":null,"title":"Finite-state morphology: Xerox tools and techniques"},{"paperId":"0649db23e4d672d3122435e48030880ad6b0b0db","title":"A Probabilistic Model for Learning Concatenative Morphology"},{"paperId":"63d165e916b851a0aeaafaa528f6aad99a78d04f","title":"Distributional cues in morpheme discovery: A computational model and empirical evidence"},{"paperId":"24e96df17d861e2bc6070fdf69dd765db8c6c476","title":"Discovering Morphemic Suffixes A Case Study In MDL Induction"},{"paperId":null,"title":"CELEX2"},{"paperId":"c69201d091dd92699fd90a17b9e3407319726791","title":"Neural Sequence Chunkers"},{"paperId":"39cc07b9bd6ac368c46544d0c2805b78f68bb923","title":"On the syntactic structure of protein sequences and the concept of grammar complexity"},{"paperId":null,"title":"Robbie Jimmerson, Vasilisa Andriyanets"}],"id":"d617f51833860dc50d202af7f80be71304b2e994","summary":"This survey connects several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subwordbased approaches based on learned segmentation have been proposed and evaluated."},{"url":"https://www.semanticscholar.org/paper/15db055ac7cb49bac96cc1be1270d9bf2b0b1770","title":"Beyond Characters: Subword-level Morpheme Segmentation","venue":"SIGMORPHON","year":2022,"referenceCount":36,"citationCount":9,"influentialCitationCount":0,"publicationDate":2022,"authors":"Ben Peters,André F. T. Martins","citations":[{"paperId":"17d0cc24fd6a267e7fd59ae62054de3e5c552096","title":"Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"537de056cac3fc1ab45965b7615e338e7c67a236","title":"Non-additive entropies and statistical mechanics at the edge of chaos: a bridge between natural and social sciences"},{"paperId":"30f36f68265823c7f9945f902451fe0b1fac790b","title":"Biomedical Language Models are Robust to Sub-optimal Tokenization"},{"paperId":"b3cff6abe401a244c21d4706b0931e48acaeeb4e","title":"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models"},{"paperId":"0842aba60d8fc1fc61ad95df34872c17bcfcc123","title":"Effects of sub-word segmentation on performance of transformer language models"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"5b0c0d181f31acebf2442e7b7c7af27f3f6a5f4a","title":"Hints on the data for language modeling of synthetic languages with transformers"},{"paperId":"73cb4755f15502b9cb34797d487b6fb77c7ba4b4","title":"Word-level Morpheme segmentation using Transformer neural network"},{"paperId":"729508bbd090d97e2e0ba57b3a6ea2b5832bf091","title":"Impact of Sequence Length and Copying on Clause-Level Inflection"}],"references":[{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"1e6171d9937a36b2dba6b622bd5857f2407d1b2e","title":"Smoothing and Shrinking the Sparse Seq2Seq Search Space"},{"paperId":"28d7562a716099a5147509bfd84f76de08b1192c","title":"Searching for Search Errors in Neural Morphological Inflection"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"05cfd96eed27ceb2aa35285991a745a5cd119abc","title":"If Beam Search Is the Answer, What Was the Question?"},{"paperId":"1e0a14db59c5a1a18c83dfbeb17eb5be9e67623d","title":"Modeling Word Formation in English–German Neural Machine Translation"},{"paperId":"8c122ff82dedf72fa7ef5342622667bf5848916e","title":"One-Size-Fits-All Multilingual Models"},{"paperId":"82665af3fa153ef6def31cc42a1c0aac39afa61b","title":"Neural Polysynthetic Language Modelling"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"d3231772937a2182b2377d028417245c49868dd1","title":"On NMT Search Errors and Model Errors: Cat Got Your Tongue?"},{"paperId":"3cee801d10f410f0feb1a2390776a01ba2765001","title":"Sparse Sequence-to-Sequence Models"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"69ffe3277011087b55afaeaef0a3933160beee9a","title":"Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English"},{"paperId":"c75ecddc5eda6b71b54bf8ef40f6fa9da1d8f108","title":"Neural Morphological Analysis: Encoding-Decoding Canonical Segments"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"b20c0758a38bd5a4083f64eff53af924499a8e29","title":"Possible generalization of Boltzmann-Gibbs statistics"},{"paperId":"145c0b53514b02bdc3dadfb2e1cea124f2abd99b","title":"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"},{"paperId":"238d45d79209f9f28f01a02dc337888942499e2f","title":"Morfessor-enriched features and multilingual training for canonical morphological segmentation"},{"paperId":"b522b0ece0cc38d7a3a5b25c5c19ce626556a897","title":"CLUZH at SIGMORPHON 2022 Shared Tasks on Morpheme Segmentation and Inflection Generation"},{"paperId":"e6272f72f780fa4a14cc83569abb7389bcf592d1","title":"IT–IST at the SIGMORPHON 2019 Shared Task: Sparse Two-headed Models for Inflection"},{"paperId":"834724753187ba86709f9d859740f23861d42191","title":"Meaningless yet meaningful: Morphology grounded subword-level NMT"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":"1f462943c8d0af69c12a09058251848324135e5a","title":"Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"},{"paperId":null,"title":"Exploring bert’s vocabulary"},{"paperId":null,"title":"Mathias Creutz, and Mikko Kurimo. 2022. Morfessorenriched features and multilingual training for canonical morphological segmentation"}],"id":"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","summary":"This paper presents DeepSPIN’s submissions to the SIGMORPHON 2022 Shared Task on Morpheme Segmentation, and challenges the assumption that models for morphological tasks should be trained at the character level by building a transformer that generates morphemes as sequences of unigram language model-induced subwords."},{"url":"https://www.semanticscholar.org/paper/f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models","venue":"","year":2021,"referenceCount":61,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/06/2021","authors":"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Yasheng Wang,Qun Liu,Maosong Sun","citations":[{"paperId":"b5b22e668858d4a469f921827150cc14b4951f57","title":"READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises"},{"paperId":"c991f7f99dd35eadb66b383b4d711f9cc25af7e3","title":"Pronunciation-Aware Unique Character Encoding for RNN Transducer-Based Mandarin Speech Recognition"}],"references":[{"paperId":"b5b22e668858d4a469f921827150cc14b4951f57","title":"READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"654ea3c14dc3dd9dc2c1ff19bf4f9e71dda78d10","title":"ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d","title":"CPM-2: Large-scale Cost-effective Pre-trained Language Models"},{"paperId":"59ca487aca988c50dac2f1605756cc04c36603b4","title":"The effect of second-language orthographic input on the phonological encoding of Mandarin words"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"cc50f846ed7222698d130cddbc58ed4d547914ed","title":"CPM: A Large-scale Generative Chinese Pre-trained Language Model"},{"paperId":"582649c87e590101617a21524f0ec203ea935937","title":"MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"6dd1a0f657ff547b1736c2d8b30451f49a9a48f5","title":"OCNLI: Original Chinese Natural Language Inference"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"6afe0fb12ceacadbbfed7202d430770a3f344731","title":"Revisiting Pre-Trained Models for Chinese Natural Language Processing"},{"paperId":"e0697594749cca8e1a272033d35e6102bfd39c00","title":"CLUENER2020: Fine-grained Name Entity Recognition for Chinese"},{"paperId":"63faf898cb502b9325c5ce238a955596757dadf9","title":"Learning Chinese Word Embeddings from Stroke, Structure and Pinyin of Characters"},{"paperId":"6007bd2a34385132a7885b934d90b519a1f65bba","title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"6c1beae31b92c70b42ebeb99e5598d73bff6eea5","title":"NEZHA: Neural Contextualized Representation for Chinese Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training with Whole Word Masking for Chinese BERT"},{"paperId":"e278e072774f23675266881750e20bca74804cb9","title":"ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"},{"paperId":"ae43556f2cc1cecb8b48762b4e09df319fbaa4d9","title":"Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"},{"paperId":"1b07a24b81834116f6ad1d0232485ba81b9445f3","title":"Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"fc09d6486be1c9bbfbef4165ce3c1ab664e5d084","title":"PAWS: Paraphrase Adversaries from Word Scrambling"},{"paperId":"42ed4a9994e6121a9f325f5b901c5b3d7ce104f5","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"},{"paperId":"b401aca53c04cec865e1a733090a2c60ca7ebe97","title":"Glyce: Glyph-vectors for Chinese Character Representations"},{"paperId":"a2ce385fc8d5068e8c87ebe4699c8f9b295cad5e","title":"Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"57b57e88edcc9a20c78388e847b42e088b451c55","title":"cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"5d90aab981804c756b344c0b9cabfc2d9807cdae","title":"Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"db253b17043b6a86e02173b6aa597664b0c7f256","title":"Learning Character-level Compositionality with Visual Features"},{"paperId":"b9445206f592423f0b2faf05f99de124ccc6aaa8","title":"Character-Based LSTM-CRF with Radical-Level Features for Chinese Named Entity Recognition"},{"paperId":"f1a8ff5542ffd51c3618953e5cf926c467154f79","title":"Phonologically Aware Neural Model for Named Entity Recognition in Low Resource Transfer Settings"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"5c6ee6707ed3ee298a57064be0acee7a464bdce7","title":"Radical-Enhanced Chinese Character Embedding"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"128cb6b891aee1b5df099acb48e2efecfcff689f","title":"The Winograd Schema Challenge"},{"paperId":"7f147cd5d509e4feb628972922297f48b86ebd3e","title":"Punctuation as Implicit Annotations for Chinese Word Segmentation"},{"paperId":"5b2beddeb063bd5988da70ae58f3e0a6564e647a","title":"Optimizing Chinese Word Segmentation for Machine Translation Performance"},{"paperId":"885476e9d1b2ea67405f7f9b46c2c8536657a204","title":"Scalable Term Selection for Text Categorization"},{"paperId":"a6c2773efba0c69e4d0ca5043cbafef9b5fc3d26","title":"Writing Systems of the World"},{"paperId":"85cac89ba01a07f3dbf6dbb1e0c56067a3105714","title":"CROSS-CONTAMINATION: ACCELERATING LARGE LANGUAGE MODELS WITHOUT IMPACTING PERFORMANCE"},{"paperId":"9998875164c1d46c8c41be6f22171e90abf0ccbe","title":"PHMOSpell: Phonological and Morphological Knowledge Guided Chinese Spelling Check"},{"paperId":"e783d0869160cae9b39414eb0ecdcc973a911004","title":"Representation"},{"paperId":null,"title":"ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":null,"title":"The #BenderRule: On naming the languages\n                        we study and why it matters"},{"paperId":"7afb83134d5b7914131e10b229d30dc2593266f6","title":"The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification"},{"paperId":null,"title":"THU-LAC: An Efﬁcient Lexical Analyzer for Chinese"}],"id":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","summary":"Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency, and 2) Pronunciation-based SubChartokenization can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to all homophone typos."},{"url":"https://www.semanticscholar.org/paper/dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?","venue":"WNUT","year":2021,"referenceCount":57,"citationCount":14,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Arij Riabi,Benoît Sagot,Djamé Seddah","citations":[{"paperId":"d7b5055b3d2aedac4257f46741b58e7338ab11e4","title":"Enriching the NArabizi Treebank: A Multifaceted Approach to Supporting an Under-Resourced Language"},{"paperId":"14347ae05bebfd9cb594ca2d2ebf07dd179962b7","title":"Data-Efficient French Language Modeling with CamemBERTa"},{"paperId":"e0d09d91784a6d426ffcd2fd12448dd9c8ceefe9","title":"Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages"},{"paperId":"e9cc129f259da83672f532b073ca5b2a2e9604d2","title":"Low-resource Bilingual Dialect Lexicon Induction with Large Language Models"},{"paperId":"b16a7adc26d791d83971708aad17e8855b95db4d","title":"MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning"},{"paperId":"e92b881ec7b762fee1c2e6df47693aecfc3aafb1","title":"Part-of-Speech and Morphological Tagging of Algerian Judeo-Arabic"},{"paperId":"b162638cd42b65e6add199b0b34f1b375070fc7c","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"c2d2d98a1dd0ba591224b729a37673b899146708","title":"Text normalization for endangered languages: the case of Ligurian"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"3979141ef17f9bd2b8a5c290e33775f90a14154c","title":"DziriBERT: a Pre-trained Language Model for the Algerian Dialect"},{"paperId":"60c8d456c1accb2ee37ea297d13d968245697ffc","title":"Text normalization for low-resource languages: the case of Ligurian"},{"paperId":"4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations"},{"paperId":"957f234aed41235b3d37e8a5f5803de520621a5a","title":"Normalisation lexicale de contenus générés par les utilisateurs sur les réseaux sociaux"}],"references":[{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"3979141ef17f9bd2b8a5c290e33775f90a14154c","title":"DziriBERT: a Pre-trained Language Model for the Algerian Dialect"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"1cfd0f08bb7ab449bbccc3d69a62ffbf43a7eb7f","title":"First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT"},{"paperId":"b29ebfe83ccd5aae614ca9986fb847d4c739aff4","title":"Arabizi Language Models for Sentiment Analysis"},{"paperId":"fff1aaa53a249da73ab5f71a023427c23ea7d006","title":"Treebanking user-generated content: a UD based overview of guidelines, corpora and unified recommendations"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"29599829d26b4acaaf0ad88698ed4362d33b2ae7","title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"5259de991f875b2c614c4bd7bf7bda97b81b9264","title":"On the Importance of Pre-training Data Volume for Compact Language Models"},{"paperId":"bc3c662d2b6f5159202d703d201a9283687d43b9","title":"Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell"},{"paperId":"14489ec7893e373a0dcc9555c52b99b2b3a429f6","title":"Are All Languages Created Equal in Multilingual BERT?"},{"paperId":"370648eabfcde7e0d6ef2c9e2a332672b556e8a6","title":"Can Multilingual Language Models Transfer to an Unseen Dialect? A Case Study on North African Arabizi"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"b805693c17961af2cc7f859c1a54320b26036f46","title":"Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"4fa37d012ad0014552a6a5a03624b29f95558bf7","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"95236a88cc959656958d7a49422f4004015d594d","title":"Comparison between NMT and PBSMT Performance for Translating Noisy User-Generated Content"},{"paperId":"3f9df96b26c42dea6dd6cad64557a3b7d698ea90","title":"MultiFiT: Efficient Multi-lingual Language Model Fine-tuning"},{"paperId":"55ace73cb8f7c3c4bd6d8ead45c0ba6193d1afda","title":"A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"92343cecdc990380de362b969eec60081959f507","title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"a9c3a009d754a110379574b069b48c0b4c75db40","title":"Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"c935bcc748c4e9b87420deaf23074535592950d0","title":"Addressing Code-Switching in French/Algerian Arabic Speech"},{"paperId":"5feb32a73dd1bd9e13f84a7b3344497a5545106b","title":"FastText.zip: Compressing text classification models"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":"23d4d2eb084e646261dd7c1f0b284f55f26f1b64","title":"Massively Multilingual Word Embeddings"},{"paperId":"e0945081b5b87187a53d4329cf77cd8bff635795","title":"Highway Networks"},{"paperId":"a72325ced3a56f00114c4947cb0e702a95df074b","title":"Rhapsodie: a Prosodic-Syntactic Treebank for Spoken French"},{"paperId":"e95ad36302f92f45abe169fbc5185e55407bcc34","title":"Universal Dependency Annotation for Multilingual Parsing"},{"paperId":"c928e453122fa7c0658e02a7aa07a623d4e5b679","title":"What to do about bad language on the internet"},{"paperId":"2a04cb0103c3e94bb98a341bca195f821d58d0c1","title":"The French Social Media Bank: a Treebank of Noisy User Generated Content"},{"paperId":"4af5ce5ca404054aada0ff421f50c541078a2c3d","title":"Le corpus Sequoia : annotation syntaxique et exploitation pour l’adaptation d’analyseur par pont lexical (The Sequoia Corpus : Syntactic Annotation and Use for a Parser Lexical Domain Adaptation Method) [in French]"},{"paperId":"2a733deaa7c55e427337860c7643e6b5f8b750fc","title":"Book Reviews: Introduction to Arabic Natural Language Processing by Nizar Y. Habash"},{"paperId":"ca9c140636a7e9545e0071d0af5337fee8a349f6","title":"“cba to check the spelling”: Investigating Parser Performance on Discussion Forum Posts"},{"paperId":"b741a9b06c4253c8fc89ce6196d58fbf544a9baa","title":"Statistical French Dependency Parsing: Treebank Conversion and First Results"},{"paperId":"1b86df622f1cc3e2e666318f427ffb7a03aa34e6","title":"Analyse en dépendances du français avec des plongements contextualisés (French dependency parsing with contextualized embeddings)"},{"paperId":null,"title":"Byt5: Towards a token-free"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"6fc879a6f264ecb791ffcfac686562230c428b89","title":"An Algerian Arabic-French Code-Switched Corpus"},{"paperId":null,"title":"of guidelines, corpora and uniﬁed recommendations"},{"paperId":"4d2543d4351fb41da36ce751de216b495c1f8a2a","title":"Linguistic Variation"}],"id":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","summary":"This work shows that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models."},{"url":"https://www.semanticscholar.org/paper/62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Makesh Narsimhan Sreedhar,Xiangpeng Wan,Yu-Jie Cheng,Junjie Hu","citations":[],"references":[{"paperId":"51d62830c1112ea7443398990b850a988ed7c86c","title":"Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"9d92905edc1a5d027b0827d1309bc6ac03dd94a0","title":"Character-Level Translation with Self-attention"},{"paperId":"651c01b897f18e31bab3ad482154a980ac92c638","title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"95856e0789481eedc2cedc413581a0a819ef8fc8","title":"Unsupervised Domain Clusters in Pretrained Language Models"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"1c84438eff3d10cfb97fb64fc9f35d734209a465","title":"Character-based NMT with Transformer"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"9237d6efc603465765e80eb5ca1268c2bd7b5c24","title":"compare-mt: A Tool for Holistic Comparison of Language Generation Systems"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4","title":"Rapid Adaptation of Neural Machine Translation to New Languages"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"2397ce306e5d7f3d0492276e357fb1833536b5d8","title":"On the State of the Art of Evaluation in Neural Language Models"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"7dba53e72c182e25e98e8f73a99d75ff69dda0c2","title":"Recurrent Highway Networks"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"25ca4a36df2955b345634b5f8a6b6bb66a774b3c","title":"Parallel Data, Tools and Interfaces in OPUS"},{"paperId":"93c20e38c85b69fc2d2eb314b3c1217913f7db11","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"6bfa1b07cb1f46294adac73afe33616cd919b603","title":"MANTa: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling"},{"paperId":"0bfa9275561f45bc772c4d544fd75f5d65143c5e","title":"When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation"},{"paperId":"81aace0e90c6a962059b117c24db0d856f340f41","title":"Report on the 11th IWSLT evaluation campaign"}],"id":"62ece609555bf833a2afd25ef796d72b5f59e767","summary":"This paper proposes a Lo cal B yt e F usion (LOBEF) method for byte-based machine translation—utilizing byte n -gram and word boundaries—to aggregate local semantic information and indicates that the byte- based models are parameter-efﬁcient and can be trained faster than subword models while showcasing competitive performance."},{"url":"https://www.semanticscholar.org/paper/12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":3,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Hui-li Xue,Nikolaos Aletras","citations":[{"paperId":"7a1db1346d95b2ec6c9706e71d2ff92532bfe78d","title":"Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?"},{"paperId":"c30deb8f4b0e942a36e5ef5c87bed01f575c0c6e","title":"Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention"},{"paperId":"ff50383c09346e3e7d16856af3519fa09a9f8580","title":"Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning"}],"references":[{"paperId":"5ca0a54fa0f76ae0e1881899c61b36d35d3bd166","title":"How does the pre-training objective affect what large language models learn about linguistic properties?"},{"paperId":"cddf40e579a596d0110b260313adf43470617c4c","title":"Datasets: A Community Library for Natural Language Processing"},{"paperId":"acac699d02a972f58b091bfbef7518f0e61c8225","title":"Frustratingly Simple Pretraining Alternatives to Masked Language Modeling"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"82ba6cc2fd424b7374cb8c2a056496925b4c2948","title":"Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings"},{"paperId":"05a6fa3bb47bd283c95b3d86ba4cbbc471cbd6ac","title":"ProFormer: Towards On-Device LSH Projection Based Transformers"},{"paperId":"1531ffc206bdb310ae08adc02e3df44d1d125d99","title":"On-Device Text Representations Robust To Misspellings via Projections"},{"paperId":"738215a396f6eee1709c6b521a6199769f0ce674","title":"Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"3c5f1ab37f70db503636075e15b3173f86eea00b","title":"Green AI"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"e26c03768664a1b2a2821659b1e234f59c738c43","title":"Efficient On-Device Models using Neural Projections"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"4bf6ce4a9366cdba069a45651606538f2febd8e6","title":"Compressing Word Embeddings via Deep Compositional Code Learning"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"8215fb083cb4b0ed2b6858b81dcc30fbd0afb6e1","title":"Mining of Massive Datasets"},{"paperId":"3fb1c64b763d27fba2a18c923637c5ea0e048e3b","title":"Small Statistical Models by Random Feature Mixing"},{"paperId":"e9ce5ad132f753624a017dc036f45eff45839265","title":"The MD5 Message-Digest Algorithm"},{"paperId":"090be9851f8ceea2acaebce4763c780287c85693","title":"Extremely Small BERT Models from Mixed-Vocabulary Training"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"511b0bb924d109767feb11855acd059125ff0164","title":"Hash Kernels"},{"paperId":"f7d95e8a91d7683e74ee642f20906e7751e47dfe","title":"Association for Computational Linguistics"},{"paperId":null,"title":"Albert Villanova del Moral"},{"paperId":null,"title":"Networks for NLP"}],"id":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","summary":"It is empirically demonstrated that H ASH F ORMERS are more memory efﬁcient compared to standard pre-trained transformers while achieving comparable predictive performance when ﬁne-tuned on multiple text classiﬂcation tasks."},{"url":"https://www.semanticscholar.org/paper/2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers","venue":"ACL","year":2022,"referenceCount":53,"citationCount":18,"influentialCitationCount":0,"publicationDate":2022,"authors":"Valentin Hofmann,Hinrich Schütze,J. Pierrehumbert","citations":[{"paperId":"d9655aad891e0dba404bc132c8419f93c098c8f8","title":"Learning to Abstract with Nonparametric Variational Information Bottleneck"},{"paperId":"fabee69ea9a76d18fcb90b4a329a9f124ba35e11","title":"Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model"},{"paperId":"17d0cc24fd6a267e7fd59ae62054de3e5c552096","title":"Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"0c6398ea23bda5efce817157f36d7975f0e161aa","title":"ChapGTP, ILLC's Attempt at Raising a BabyLM: Improving Data Efficiency by Automatic Task Formation"},{"paperId":"997cebb936d88577da59ba460a9141bdd5dcb36f","title":"To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer"},{"paperId":"d57cbec10dfee4e244657d3cdf9ba4cf53086744","title":"An Empirical Analysis Towards Replacing Vocabulary-Rigid Embeddings by a Vocabulary-Free Mechanism"},{"paperId":"89b5890999ce6874507b0264fa4b468e16c03788","title":"MorphPiece : Moving away from Statistical Language Representation"},{"paperId":"7500f105fd95191e50fc9e8ca3f38d89d92055ad","title":"Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing"},{"paperId":"f7af09604486cfc671cfe9b7ab21badcd2a4e23f","title":"Learning Sentiment-Enhanced Word Representations by Fusing External Hybrid Sentiment Knowledge"},{"paperId":"17fbffb05fa14e21d1c506fd5f0f568b955fe983","title":"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models"},{"paperId":"879a7f5abdb7ab803d48172d4f0830965f989d46","title":"Language Model Tokenizers Introduce Unfairness Between Languages"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"14efaa989b8db89ad907e6a15253d482cbba77d9","title":"Linguistically inspired roadmap for building biologically reliable protein language models"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"38c6d8004d8b20c66f1adafe029785e0cb91a3f2","title":"Entropy-guided Vocabulary Augmentation of Multilingual Language Models for Low-resource Tasks"},{"paperId":"8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability"},{"paperId":"1606793daaa20d4a4a78e859c2fd6b4f7535680c","title":"Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment"},{"paperId":"2dce73e4a3e19d71249fc7a53c2a9531daaff839","title":"Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention"}],"references":[{"paperId":"03aa38229a6c853d96430edfc8f1542598f2116e","title":"AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain"},{"paperId":"6cd9c0630c422f70ceced4dc400fcbc6001d15b5","title":"ABC: Attention with Bounded-memory Control"},{"paperId":"3c477f0e43660ce1ba39c111df312929da37f81f","title":"Efficient Domain Adaptation of Language Models via Adaptive Tokenization"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"bb0b0c9853f658db9d6a7f4aa90c24906ba4b1dc","title":"A Pointer Network Architecture for Joint Morphological Segmentation and Tagging"},{"paperId":"fd2dc47c38945c0364641ac1085239b076454d40","title":"Tackling the Low-resource Challenge for Canonical Segmentation"},{"paperId":"110c13fbf4ff87b52ee1fd9eb2d3616c839ceb41","title":"Parsing with Multilingual BERT, a Small Treebank, and a Small Corpus"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"dcd72571e380eacf3ddccf225f3324eb8c51ece0","title":"A Graph Auto-encoder Model of Derivational Morphology"},{"paperId":"df8511a57b5d72e3f7e7f3b83bb271d58ef66a39","title":"Predicting the Growth of Morphological Families from Social and Linguistic Factors"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"44189a062182cab428e6d7ea010819f2fe978533","title":"DagoBERT: Generating Derivational Morphology with a Pretrained Language Model"},{"paperId":"8515c83e6562415021b4c536c28ed198f05a8691","title":"Emerging trends: Subwords, seriously?"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"7da138d704ef0fc055825fa132f5c452ed3fb52a","title":"LADEC: The Large Database of English Compounds"},{"paperId":"3c5f1ab37f70db503636075e15b3173f86eea00b","title":"Green AI"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"a6cd2c524740aff36d79468ae99f1f484de01e1b","title":"Correcting Whitespace Errors in Digitized Historical Texts"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"5839b0a5f7fce57941620e4e9e2e7168a335a43c","title":"Subword-Level Language Identification for Intra-Word Code-Switching"},{"paperId":"25687348fd01908c56664b40e249a611fb4b6dd8","title":"Gendered associations of English morphology"},{"paperId":"c7ec1b98fce650503168a51131a889942c6f0aa8","title":"A Distributional and Orthographic Aggregation Model for English Derivational Morphology"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"dc60cfe8f4378ffa8ecdfd0c1ae93c598726f365","title":"Paradigm Completion for Derivational Morphology"},{"paperId":"5bcc16402c812c236adee3449a5e9ba5659b5796","title":"Neural Sequence-to-sequence Learning of Internal Word Structure"},{"paperId":"3cda98ecef94392d8ce4492ee3515b2ebe75899b","title":"Context-Aware Prediction of Derivational Word-forms"},{"paperId":"38afef35de217bdce44e2d02f366ca42bedfb60e","title":"Joint Semantic Synthesis and Morphological Analysis of the Derived Word"},{"paperId":"c75ecddc5eda6b71b54bf8ef40f6fa9da1d8f108","title":"Neural Morphological Analysis: Encoding-Decoding Canonical Segments"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"0a275c52c61ecf2429189387426d6173e40d695a","title":"A Joint Model of Orthography and Morphological Segmentation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"9c73d98785cc143db64da987659dd5656c08db23","title":"Singulars and plurals in Dutch: Evidence for a parallel dual-route model"},{"paperId":"66e73e696414e9fde9201b176c447bf1ae91b00b","title":"The Cambridge encyclopedia of the english language"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"9ba4b5ac12496b14e9d2e571515fad89904d19fa","title":"Lexical access and inflectional morphology"},{"paperId":"3376118362db3751cfbd88acd0c090b8a3897733","title":"Superbizarre Is Not Superb: Improving BERT's Interpretations of Complex Words with Derivational Morphology"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"21079df28af037f93620a0bd63f04fe49cf5df7d","title":"On the Importance of Tokenization in Arabic Embedding Models"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"44a590466bf95545eb47daf8ce2a468fa8818751","title":"On Hapax Legomena and Morphological Productivity"},{"paperId":"60258897d250a41f11cfee27de828a0130110b5e","title":"The CELEX Lexical Database (CD-ROM)"},{"paperId":"061e742d1d63631ed28d5dcc8b8e3c884519e924","title":"Distributional properties of derivational affixes: Implications for processing"},{"paperId":null,"title":"Journal of the Association for Laboratory Phonology"}],"id":"2f07f97563a73d9b691ec6144e4bba25a347ab87","summary":"FLOTA (Few Longest Token Approximation) leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise."},{"url":"https://www.semanticscholar.org/paper/023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":3,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Shaked Yehezkel,Yuval Pinter","citations":[{"paperId":"94e7c98aff1b831201ad42eb54c327817fb42b53","title":"PatternGPT : A Pattern-Driven Framework for Large Language Model Text Generation"},{"paperId":"7bf0cbcd45100d837ccef1cedb3f647a67c1e7c9","title":"Strong Prediction: Language model surprisal explains multiple N400 effects"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"}],"references":[{"paperId":"8c82d3d758897ef9f166924683831ecf6085f21a","title":"MaxMatch-Dropout: Subword Regularization for WordPiece"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"3a096a3d67348640084c209abb09f79447fbbc11","title":"Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ac6e85cc3f105567e20e019f5752ed38a0a34a25","title":"Breaking Character: Are Subwords Good Enough for MRLs After All?"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"7694aae9766d5f1fe74d900cd82aee898cb6e8e9","title":"How to Train BERT with an Academic Budget"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224","title":"Superbizarre Is Not Superb: Derivational Morphology Improves BERT’s Interpretation of Complex Words"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"0438f16147dc44669863722f81efaa13420885f4","title":"Subword Sampling for Low Resource Word Alignment"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"9127c3873cac9979433a208e69d426782747910b","title":"Evaluating Sub-word Embeddings in Cross-lingual Models"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"997855e1f17d34dd3922d953a587742d198844e6","title":"CrossWeigh: Training Named Entity Tagger from Imperfect Annotations"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"c39e409d6b7744200c4fd12a6b81e51f6145cfae","title":"Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a28856e5d99707db56fabaefd68c24e1ab9b6093","title":"Getting a life"},{"paperId":"531bc31aa70981cc989180f6e3f7d3c6442e7d14","title":"POLYGLOT-NER: Massive Multilingual Named Entity Recognition"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"57458bc1cffe5caa45a885af986d70f723f406b4","title":"A unified architecture for natural language processing: deep neural networks with multitask learning"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"63a81058989260bb7c5bb7939a90cf3d2ca85de4","title":"e d"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"834724753187ba86709f9d859740f23861d42191","title":"Meaningless yet meaningful: Morphology grounded subword-level NMT"},{"paperId":null,"title":"Inﬂec-tional synthesis of the verb"},{"paperId":null,"title":"Gensim-python framework for vector space modelling"},{"paperId":"3744f4dba73770db95a3a5f72a8e29319e550780","title":"The World Atlas of Language Structures Online"},{"paperId":"4c7952451d6791fdcfa39e05b8165aa79b61754a","title":"Exponence of selected inﬂectional formatives"}],"id":"023fd42f0867a88a2206f906c7f127701058feb6","summary":"SAGE is presented, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase, and does a better job than current widespread tokenizers in keeping token contexts cohesive, while not incurring a large price in terms of encoding efficiency or domain robustness."},{"url":"https://www.semanticscholar.org/paper/6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"C HARFORMER : F AST C HARACTER T RANSFORMERS VIA G RADIENT - BASED S UBWORD T OKENIZATION","venue":"","year":null,"referenceCount":70,"citationCount":98,"influentialCitationCount":0,"publicationDate":null,"authors":"","citations":[{"paperId":"28c75ab7f6951f1c2bb349b34abc3204ba8b9498","title":"Toucan: Token-Aware Character Level Language Modeling"},{"paperId":"5f3d6c077712a2d692bb29671c084e098854c738","title":"Learning Mutually Informed Representations for Characters and Subwords"},{"paperId":"5c104f905fcacf390270f619f232a2ba4eb873f2","title":"FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores"},{"paperId":"fd665a3d8c4a5fde1e1b00f78ab231d99b22abd0","title":"Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew"},{"paperId":"310c4a7da17efa86efd42f793587dc67657a046c","title":"BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text"},{"paperId":"d9655aad891e0dba404bc132c8419f93c098c8f8","title":"Learning to Abstract with Nonparametric Variational Information Bottleneck"},{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"3856fce2fca56e2d7be0134889bf6f9d2f01b931","title":"Advancing Perception in Artificial Intelligence through Principles of Cognitive Science"},{"paperId":"997cebb936d88577da59ba460a9141bdd5dcb36f","title":"To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer"},{"paperId":"64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?"},{"paperId":"da74c568e1a6388257a263d1e09d94910c5ba942","title":"Using Transformer Models and Textual Analysis for Log Parsing"},{"paperId":"5bd254c0775d18cdc30cb85be61e080484ee6713","title":"Multilingual Text Representation"},{"paperId":"1943144f998c2620616b3e89d376ff801b6e04c6","title":"Lightweight Adaptation of Neural Language Models via Subspace Embedding"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code"},{"paperId":"d57cbec10dfee4e244657d3cdf9ba4cf53086744","title":"An Empirical Analysis Towards Replacing Vocabulary-Rigid Embeddings by a Vocabulary-Free Mechanism"},{"paperId":"2c8e8f8ae858412a5b86bb0752c25ad4d8e18040","title":"LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems"},{"paperId":"8dbd1cb17f3a627bb5768b8a4ea90dda818d4b5e","title":"Fine-Tuned IndoBERT Based Model and Data Augmentation for Indonesian Language Paraphrase Identification"},{"paperId":"2adc13eb55c92e026c4cefc89a47a0ee0ac95111","title":"SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding"},{"paperId":"bfd2b76998a0521c12903ef5ced517adf70ad2ba","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution"},{"paperId":"2d1197b3a0d9f9e4c366638e131a9cc8ae9fe1af","title":"Lost in Translation: Large Language Models in Non-English Content Analysis"},{"paperId":"5fce7d9442b06cab91174fb68ba52ff6bdaa29cc","title":"A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks"},{"paperId":"ec18aef9ccec70b979d6ab3c78d3721736fd5388","title":"Where’s the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation"},{"paperId":"29a54c072bcf85fa6934b6f701962a7c9aeb6489","title":"Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora"},{"paperId":"06e6f125b0f15affd38382135acf03efa28cf439","title":"A Quantitative Review on Language Model Efficiency Research"},{"paperId":"c9a69319c56c897580c18eb3b6187b465e872b76","title":"Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator"},{"paperId":"b3cff6abe401a244c21d4706b0931e48acaeeb4e","title":"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models"},{"paperId":"17fbffb05fa14e21d1c506fd5f0f568b955fe983","title":"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models"},{"paperId":"33c017b6298dd6f7d099f88f9667a6ea97131dbc","title":"mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models"},{"paperId":"d7a379254ac2dee2e31dfdeedb440176f0285aa5","title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding"},{"paperId":"e1a0f59cb3ff4c377f2cc6ba1affdc1b958ec474","title":"FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering"},{"paperId":"5079e188ae86d8330414edb65e898ed306ac450d","title":"Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation"},{"paperId":"ea8197cb357af6f89e8b6e5548897236a24719b1","title":"What is the best recipe for character-level encoder-only modelling?"},{"paperId":"e4bf034670934c6b99bcc8dfcee75e9f5701c3fe","title":"LLMs to the Moon? Reddit Market Sentiment Analysis with Large Language Models"},{"paperId":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!"},{"paperId":"0a438980ac42451d6d32dd2ad8ead7b55520408d","title":"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning"},{"paperId":"117a0c2c23a5a558cf3b39468d917a750bca720c","title":"Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation"},{"paperId":"67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation"},{"paperId":"507465f8d46489a68a527cb5304d76bdb6c31ed9","title":"Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation"},{"paperId":"09e8e546d7ce7da1bba5d8d7a94c462ad1536603","title":"Conspiracy spillovers and geoengineering"},{"paperId":"0351167c875b8931366e85eb5e517819d7db80cc","title":"What Makes for Good Tokenizers in Vision Transformer?"},{"paperId":"52136f813243ac3de8e277906112a41590a376d4","title":"What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis"},{"paperId":"7cdebb73662387d9040da4f27a7dc04dbffa3c3e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"2f272ca3394656835bf32a30b80c6f8ba1c8f4c4","title":"Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"071fb0b7af9b3d390c7aa64a26068e23a81c52d7","title":"Paraphrase Identification with Deep Learning: A Review of Datasets and Methods"},{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"},{"paperId":"111dadc41f7b0337235fb526bf0cd3a4ac23b98d","title":"Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"7abfcf54eeac3a5c298717a4f0fe8a5daceeaa42","title":"Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling"},{"paperId":"5e52d654fd31f04c1bd884cd5480e6af8c95ad50","title":"Efficient Transformers with Dynamic Token Pooling"},{"paperId":"9769992ca9ff1c3894d19f4bd2b6dac27ed1befd","title":"A review on abusive content automatic detection: approaches, challenges and opportunities"},{"paperId":"dd568e6838903ad7c381f13c1268c94c5db08b02","title":"Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"7bd8859b5920c7b769e6d40dbdbcd857c1770401","title":"Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining"},{"paperId":"27c00c75ea9b2e71dc70e5a2708c5f065fe170a7","title":"Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"487f9e74fff5e29b8aa37ba551edab71cfcbd256","title":"L2-GEN: A Neural Phoneme Paraphrasing Approach to L2 Speech Synthesis for Mispronunciation Diagnosis"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"0232715f9089e3a2fc002cff6737bb9939805b8d","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"c13b69672ebee01914b0be69d052161350f653e8","title":"CONSENT: Context Sensitive Transformer for Bold Words Classification"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"b21670e8061a06ab97e7d6052c9345a326e84ff8","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"0de580957d23dd65e31b6c95e6bc5d15bc15c57d","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction"},{"paperId":"b1dad820853464b30c93b52366b32690ba4b99a6","title":"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View"},{"paperId":"a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia"},{"paperId":"7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"},{"paperId":"8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"2d4f66046bb436864cd6bf589e3a931c405f9f44","title":"Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"ed7b51e4a5c4835218f6697b280afb2849211939","title":"Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation"},{"paperId":"d53d4ff9f3bda51534184344845a78f8c02badd9","title":"DiatopIt: A Corpus of Social Media Posts for the Study of Diatopic Language Variation in Italy"},{"paperId":"4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data"},{"paperId":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color"},{"paperId":"83265f71c3a9ae65ee4b98435726f30ed92848f5","title":"Deploying Unified BERT Moderation Model for E-Commerce Reviews"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"},{"paperId":"2dce73e4a3e19d71249fc7a53c2a9531daaff839","title":"Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"2365410a710b421b2295cdca0074946cb50bb1d4","title":"Are Pretrained Convolutions Better than Pretrained Transformers?"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"7e9ff94476f41041c75e253e84f487db00e9c861","title":"Long Range Arena: A Benchmark for Efficient Transformers"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"3fbf6339273c50b04e886fa9bd4ad18c952a683d","title":"Rethinking Attention with Performers"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"26b277f2d7cd86b67bc3572257557edc4640b8e9","title":"Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","title":"GLU Variants Improve Transformer"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"b611a8095630557229dc5fb6b07c272f1cd614da","title":"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"2270b8628fd8ca67ae39d277f45bc3c38ac63d5f","title":"Mesh-TensorFlow: Deep Learning for Supercomputers"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"a9a5d671271fff45429084e184a788f611b6f194","title":"FRAGE: Frequency-Agnostic Word Representation"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"1db9bd18681b96473f3c82b21edc9240b44dc329","title":"Image Transformer"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"deff3b0635e141297238797d924d7a9aba3a132a","title":"Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"6f35b070c250507dbec8a7365cf01b25eb66d792","title":"Ex Machina: Personal Attacks Seen at Scale"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"649d03490ef72c5274e3bccd03d7a299d2f8da91","title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"0c09f282d78f42f77d73a52b08cb425e1cabb9b2","title":"Are Vowels and Consonants Processed Differently? Event-related Potential Evidence with a Delayed Letter Paradigm"},{"paperId":"98b405fd0153f24e8359448d79acc49c3ffc8f4c","title":"The relative contribution of consonants and vowels to word identification during reading"},{"paperId":"084c55d6432265785e3ff86a2e900a49d501c00a","title":"Book Reviews: Foundations of Statistical Natural Language Processing"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3bf2e6941dbb87ac0d2c771c159e1e27366a26e3","title":"Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics"},{"paperId":"475354f10798f110d34792b6d88f31d6d5cb099e","title":"Automatically Constructing a Corpus of Sentential Paraphrases"},{"paperId":null,"title":"Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee"}],"id":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","summary":"This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion that paves the way for highly performant token-free models that are trained completely end-to-end."},{"url":"https://www.semanticscholar.org/paper/b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA","venue":"","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Peng Chen","citations":[],"references":[{"paperId":"ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51","title":"Efficiently Modeling Long Sequences with Structured State Spaces"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"7e9ff94476f41041c75e253e84f487db00e9c861","title":"Long Range Arena: A Benchmark for Efficient Transformers"},{"paperId":"3fbf6339273c50b04e886fa9bd4ad18c952a683d","title":"Rethinking Attention with Performers"},{"paperId":"0964490205fdc38c2f0980c9d778069089ca92e3","title":"HiPPO: Recurrent Memory with Optimal Polynomial Projections"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"8c1b00128e74f1cd92aede3959690615695d5101","title":"QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"3a7b63b50c64f4ec3358477790e84cbd6be2a0b4","title":"Bidirectional Attention Flow for Machine Comprehension"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"698d675ba7134ac701de810c9ca4a6de72cb414b","title":"Character-Level Question Answering with Attention"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"}],"id":"b639124771f9c62cd656a24e8e685a456918e0ff","summary":"A question answering model that encodes text inputs at character level to utilize subword structures and mitigate the out-of-vocabulary problem is proposed, and both S4 and character-level encoding improve the model performance on the question answering task."},{"url":"https://www.semanticscholar.org/paper/f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?","venue":"Findings","year":2021,"referenceCount":75,"citationCount":15,"influentialCitationCount":0,"publicationDate":"15/10/2021","authors":"Jindřich Libovický,Helmut Schmid,Alexander Fraser","citations":[{"paperId":"6693b31ba94069901313e11b3a9f138eb5ab777d","title":"Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation"},{"paperId":"74f89dfa5060c87093f045b59e1aebeb0e8bfd7d","title":"Character-level NMT and language similarity"},{"paperId":"a9cd3b1ad7ea09dfddf1a22124c27070c6f910e1","title":"Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT"},{"paperId":"29a54c072bcf85fa6934b6f701962a7c9aeb6489","title":"Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora"},{"paperId":"5079e188ae86d8330414edb65e898ed306ac450d","title":"Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation"},{"paperId":"80dd882d0a543a5207b07e456023b16396f3039a","title":"Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages"},{"paperId":"0842aba60d8fc1fc61ad95df34872c17bcfcc123","title":"Effects of sub-word segmentation on performance of transformer language models"},{"paperId":"117a0c2c23a5a558cf3b39468d917a750bca720c","title":"Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation"},{"paperId":"7cdebb73662387d9040da4f27a7dc04dbffa3c3e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"0232715f9089e3a2fc002cff6737bb9939805b8d","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"2b4369d50ac7310b9908a2baef89a63c68cbd893","title":"Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models"}],"references":[{"paperId":"00191ddacdbef6f5819e8682ddfe184d5b415d27","title":"What Do You Get When You Cross Beam Search with Nucleus Sampling?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"af59aeeb46fb5d8412f550f6dd5c5dc99afc9c1a","title":"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation"},{"paperId":"37c4ec8d4fcaaf7f2c3f01a2fb30e9e41d8865a2","title":"TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"648bea1c1e11936ecae023593a43d848e3942c0e","title":"Why Neural Machine Translation Prefers Empty Outputs"},{"paperId":"c204d40384d39c59cd7249bde4cd8615972acaac","title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"f85040f8aecee4cdb63137bbad5bbab3ba838c33","title":"Gender Coreference and Bias Evaluation at WMT 2020"},{"paperId":"05cfd96eed27ceb2aa35285991a745a5cd119abc","title":"If Beam Search Is the Answer, What Was the Question?"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":"dd7fdaa997a074dbbc4849d0330a42985e7b3c3a","title":"Announcing CzEng 2.0 Parallel Corpus with over 2 Gigawords"},{"paperId":"0156fb2096c39839384c0cae0194e123e01cc575","title":"Character-Level Transformer-Based Neural Machine Translation"},{"paperId":"8b6c9adf85a9d6391e3ccd503c2e5af929a36735","title":"Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation"},{"paperId":"9d92905edc1a5d027b0827d1309bc6ac03dd94a0","title":"Character-Level Translation with Self-attention"},{"paperId":"651c01b897f18e31bab3ad482154a980ac92c638","title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems"},{"paperId":"1c84438eff3d10cfb97fb64fc9f35d734209a465","title":"Character-based NMT with Transformer"},{"paperId":"027946f80f3cb276ea38bc2cf19903052f59cd0e","title":"How Decoding Strategies Affect the Verifiability of Generated Text"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7c848b37605cdfe6d2d4778b06e66c0710c3ec34","title":"On the Importance of Word Boundaries in Character-level Neural Machine Translation"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"d3231772937a2182b2377d028417245c49868dd1","title":"On NMT Search Errors and Model Errors: Cat Got Your Tongue?"},{"paperId":"ea3e18c7b10a137d495054682c055a80b5be768c","title":"Findings of the 2019 Conference on Machine Translation (WMT19)"},{"paperId":"b33a9df8e39966217786855ef8fcf87d11294774","title":"The University of Helsinki Submissions to the WMT19 Similar Language Translation Task"},{"paperId":"90e531d15be3c19f3d83085b906d5c9b5aae540d","title":"JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English"},{"paperId":"a5690b0a514a7cbc913871e41e54c9ad4f6362db","title":"Findings of the First Shared Task on Machine Translation Robustness"},{"paperId":"6e722ac4d386489aa47703887881835ec0e1331d","title":"Tagged Back-Translation"},{"paperId":"1670a07b70f90cc4ddba71343e6a7ee4b5198595","title":"Evaluating Gender Bias in Machine Translation"},{"paperId":"d9f1eed347959149f27002485a7fe339604fe45d","title":"Revisiting Low-Resource Neural Machine Translation: A Case Study"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"48fbdf1be70221ac8a6b22079245030ab6158760","title":"Findings of the 2018 Conference on Machine Translation (WMT18)"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"243b9ad1ba2de68a0d59d8c8cf52c8f235e3182e","title":"The WMT’18 Morpheval test suites for English-Czech, English-German, English-Finnish and Turkish-English"},{"paperId":"b699fce01693dcec868a6905488fcd3a652dd412","title":"Neural Machine Translation of Logographic Language Using Sub-character Level Information"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"8fec5d6ac57e90f459e7330775165f2671abc445","title":"Training Deeper Neural Machine Translation Models with Transparent Attention"},{"paperId":"622d324604a2e219dddc1d474ba9c38d9e05ac6c","title":"Character-level Chinese-English Translation through ASCII Encoding"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"d3707cf521e3596313af1f53acba6413d0d528a6","title":"Training Tips for the Transformer Model"},{"paperId":"aeeb2e5ecb5b926bd5673d1fef3e3e61589fabb2","title":"Findings of the WMT 2017 Biomedical Translation Shared Task"},{"paperId":"51b8a5d5264b0e891595a1e111b863efb86c8c72","title":"Evaluating the morphological competence of Machine Translation Systems"},{"paperId":"346e012f79ee607f1b563bc66fd8243f1b1ae4e9","title":"Extending hybrid word-character neural machine translation with multi-task learning of morphological analysis"},{"paperId":"89cb259f39eb8350dd337fc1d02e2f4128c8398c","title":"Byte-based Neural Machine Translation"},{"paperId":"d4cc6fb1b81dd6cffde542f60b001129e3175b93","title":"The Helsinki Neural Machine Translation System"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"63c4114bd373dd0fcfe0d25a605b353c62be2995","title":"How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"1a327709cc53ff9e52454e50a643abf4a0ac92af","title":"Findings of the 2016 Conference on Machine Translation"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"e0945081b5b87187a53d4329cf77cd8bff635795","title":"Highway Networks"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"0f8992ee6418d367d8e50ecbb59b08ea15e8431f","title":"Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems"},{"paperId":"aab0c37bcb25d3066a327abb60bc381d7ef61896","title":"What You Need…"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"cb826a3899752b796f14df1c50378c64954a6b0a","title":"Statistical Significance Tests for Machine Translation Evaluation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"cd5a169879504ea91660a443b9151753cc29c42f","title":"Minimum Bayes-risk automatic speech recognition"},{"paperId":"0bfa9275561f45bc772c4d544fd75f5d65143c5e","title":"When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation"},{"paperId":"f3aaec8d24e07d6370b78aefc757e5d5fe642f60","title":"The University of Edinburgh’s English-German and English-Hausa Submissions to the WMT21 News Translation Task"},{"paperId":"e8f297e161f57e461ede2d4e0c26573981cad077","title":"Findings of the 2020 Conference on Machine Translation (WMT20)"},{"paperId":"671cd646d61c670fffc2fea4cf7b7a1f6f80dcf7","title":"NRC Systems for the 2020 Inuktitut-English News Translation Task"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"d2b2d748208ff79fdaa7064cf9e7bf84a408f2ac","title":"University of Rochester WMT 2017 NMT System Submission"},{"paperId":"b561b860e6931436c66f378386a4aec5c778f9d4","title":"The TALP-UPC Neural Machine Translation System for German/Finnish-English Using the Inverse Direction Model in Rescoring"},{"paperId":"ea40fba2dc52bde32dedd45b05f83d27dc2ea6c7","title":"Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe"},{"paperId":"6dc33afdf5a90e44e3fa20086fc270ddc88681a2","title":"CUNI System for WMT17 Automatic Post-Editing Task"},{"paperId":"cb0ab255c4079e2082ba6e3a807529527d96687c","title":"Overview of the IWSLT 2017 Evaluation Campaign"}],"id":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","summary":"It is shown that even with recent modeling innovations in character-level natural language processing, character- level MT systems still struggle to match their subword-based counterparts, and show neither better domain robustness, nor better morphological generalization."},{"url":"https://www.semanticscholar.org/paper/2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish","venue":"ArXiv","year":2022,"referenceCount":68,"citationCount":19,"influentialCitationCount":1,"publicationDate":"19/04/2022","authors":"Cagri Toraman,E. Yilmaz,Furkan Sahinuc,Oguzhan Ozcelik","citations":[{"paperId":"c009f7a22a715579e1b9aec8cd13e9e0252492a6","title":"Improving speech recognition systems for the morphologically complex Malayalam language using subword tokens for language modeling"},{"paperId":"7dd4b7e6f5f1588eae707df8334589ffd503bc54","title":"A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models"},{"paperId":"0318ae7671555eb3b1166888430dbdd76cd32354","title":"Core Building Blocks: Next Gen Geo Spatial GPT Application"},{"paperId":"64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?"},{"paperId":"7aef8e81883e9ded80076b8d2002e29ec5555564","title":"BioBERTurk: Exploring Turkish Biomedical Language Model Development Strategies in Low-Resource Setting"},{"paperId":"b3a2cab6545af689ec3fe060f460b4ab1837faca","title":"Domain Effect Investigation for Bert Models Fine-Tuned on Different Text Categorization Tasks"},{"paperId":"8282c6b141c335fc144818f20d86e68800691a01","title":"Reproducibility, Replicability, and Insights into Dense Multi-Representation Retrieval Models: from ColBERT to Col*"},{"paperId":"ca7636c1d3426dd09a185abf25f81f7a8fa594e1","title":"How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese"},{"paperId":"0842aba60d8fc1fc61ad95df34872c17bcfcc123","title":"Effects of sub-word segmentation on performance of transformer language models"},{"paperId":"42b6c3f3ab62311d503e6d30233aa96beac689fe","title":"Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios"},{"paperId":"8e4b162f745556a3dd04e660e96333204d491c01","title":"Letter Detection : An Empirical Comparative Study of Different ML Classifier and Feature Extraction"},{"paperId":"560e3cf413a42f21b84143a4416747514ca51ed1","title":"Analyzing Cyberbullying Negative Content on Twitter Social Media with the RoBERTa Method"},{"paperId":"fb49e38135302a1c16d644c0f746cef7d5f10ee4","title":"Understanding BLOOM: An empirical study on diverse NLP tasks"},{"paperId":"afbb735c4e1e08acf8eab3c6fbb02204ff7697ee","title":"Hate Speech Detection in Hindi language using BERT and Convolution Neural Network"},{"paperId":"00717135b5959e2af26d77664aac5e96de052b6d","title":"Sentiment Analysis for Financial News Using RNN-LSTM Network"},{"paperId":"5b0c0d181f31acebf2442e7b7c7af27f3f6a5f4a","title":"Hints on the data for language modeling of synthetic languages with transformers"},{"paperId":"732761bad14162fcff61eccebb4f97f89fd1415b","title":"Controlling Diffusion Input-Output Mapping of the Components of a Diffusion Model as a Potential Approach for Enhanced Model Control Master’s thesis in Complex adaptive systems"},{"paperId":"8ebbbdbf7a77b28739200a49789bc3e1011eb59a","title":"ChatGPT vs. Bard: A Comparative Study"},{"paperId":"a611ecc0b48b1302abbd33712b23f025f35be0b4","title":"ARC-NLP at CheckThat!-2022: Contradiction for Harmful Tweet Detection"}],"references":[{"paperId":"bef55aedf15ebcf9b21b0e3f5df4dde8def512bd","title":"Large-Scale Hate Speech Detection with Cross-Domain Transfer"},{"paperId":"72d8a122c62abdc223280dfd56b34f7634e27e92","title":"Ethical Challenges in AI"},{"paperId":"6b47cd165443e1a1037a001824b2ab3ebde8efa2","title":"Towards Tokenization and Part-of-Speech Tagging for Khmer: Data and Discussion"},{"paperId":"019e6093a5439468abc82038dc9e6d72c49e23b7","title":"The social cost of carbon dioxide under climate-economy feedbacks and temperature variability"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5e8da0a332906a2cc5262483ddc27eb9b792dbaf","title":"Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine Translation: The Case of Fon Language"},{"paperId":"1c15d9f928d2ff13ba857e3eafc1e784a0100838","title":"Finding Better Subwords for Tibetan Neural Machine Translation"},{"paperId":"c97e2fc4640536cf3f7ac46d97306bc71fbb9cda","title":"Tuning Language Representation Models for Classification of Turkish News"},{"paperId":"86b369759bff13ec23b45ca23cc5461292a75415","title":"WangchanBERTa: Pretraining transformer-based Thai Language Models"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"4d1367141eb8210a981428f3a7fdbdbc9db3e248","title":"A Tokenization System for the Kurdish Language"},{"paperId":"3f02219184319aa8ca1ef182e6b091b6a7539a04","title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words"},{"paperId":"2c953a3c378b40dadf2e3fb486713c8608b8e282","title":"Pretrained Transformers for Text Ranking: BERT and Beyond"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"2f3f885a52b92b2ab6bddcc3ce54b7eff541bd02","title":"Enhancing Tokenization by Embedding Romanian Language Specific Morphology"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"1cbcc8a05fcd76254211b0d499063b268f6a1a9a","title":"Data and Representation for Turkish Natural Language Inference"},{"paperId":"41a7fece6e8c47d5bff75f7701a702f351b110d6","title":"BERTurk - BERT models for Turkish"},{"paperId":"ff137bbcc56edd895486287059a30d61c1199972","title":"Context-Dependent Sequence-to-Sequence Turkish Spelling Correction"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"74b4f16c5ac91e3e7c88ae81cc8c91416b71d151","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"92343cecdc990380de362b969eec60081959f507","title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"},{"paperId":"3bffe4ee25ee090c7bdcfe7dd22682e19a7f730d","title":"Towards Burmese (Myanmar) Morphological Analysis"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"690edf44e8739fd80bdfb76f40c9a4a222f3bba8","title":"BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"},{"paperId":"e8c73626c70e1e5d8dc5ab055f94b78c6a24e084","title":"Co-occurrence Weight Selection in Generation of Word Embeddings for Low Resource Languages"},{"paperId":"98b42f0fe7fd5e13903ee06c05183a7a52f9a98a","title":"NOVA"},{"paperId":"7365f887c938ca21a6adbef08b5a520ebbd4638f","title":"Model Cards for Model Reporting"},{"paperId":"a9a5d671271fff45429084e184a788f611b6f194","title":"FRAGE: Frequency-Agnostic Word Representation"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"4ad638ae669a28c24c18412c14bff57e284663dd","title":"Characters or Morphemes: How to Represent Words?"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"fce6bacacd59de6ad7b486261d7f955374164c7f","title":"Unsupervised Morphological Segmentation Using Neural Word Embeddings"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"66182e1ba6dca20f315cd952866a94f54d8ac820","title":"Cross-lingual polarity detection with machine translation"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"b8417c267678a66919e3a85f7ffb05de8e71e4ff","title":"Information extraction from web services: a comparison of Tokenisation algorithms"},{"paperId":"066502368061f8f002095c1fb8146e8b379cd595","title":"Exploiting Morphology in Turkish Named Entity Recognition System"},{"paperId":"61031f5b863de2d2b235c964d28de468d02bd2d5","title":"Developing a text categorization template for Turkish news portals"},{"paperId":"22dc2148ab8bd58c6b4491fe7b59257f5739d2e7","title":"Exploiting Separation of Closed-Class Categories for Arabic Tokenization and Part-of-Speech Tagging"},{"paperId":"eb0b8e3cb4ee649c934836a98af1b6b6b3d04027","title":"Information retrieval on Turkish texts"},{"paperId":"b42abb25f97fa729a4bd22cb09d3c49aebe6dce3","title":"Morph-based speech recognition and modeling of out-of-vocabulary words across languages"},{"paperId":"28c80bce6fe1b21dc52ad165c3a329eeddab8c3d","title":"A statistical information extraction system for Turkish"},{"paperId":"d9c71db75046473f0e3d3229950d7c84c09afd5e","title":"Text Chunking using Transformation-Based Learning"},{"paperId":null,"title":"Climate Impact by Area"},{"paperId":"7b3f9d6343aeb5659b2d6c39e8a4004b00b0695e","title":"Semantic Similarity Based Evaluation for Abstractive News Summarization"},{"paperId":"3b6bab26e32a297b15999160c5cf5727e2635210","title":"Should we find another model?: Improving Neural Machine Translation Performance with ONE-Piece Tokenization Method without Model Modification"},{"paperId":"e53c82abd656858f6c4715e545737aa3c9889f36","title":"Pre-Training on Mixed Data for Low-Resource Neural Machine Translation"},{"paperId":null,"title":"Oscar Dataset Huggingface"},{"paperId":null,"title":"Retrieved March 16, 2022 from Turkish Language Models"},{"paperId":null,"title":"Primary Energy and GHG Emissions Coefficients of Electricity"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"2019. BERT: Pre-training"},{"paperId":"052418ebf497a3a4eaa76cd350987932253dee2b","title":"Morphological Disambiguation for Turkish"},{"paperId":null,"title":"Łukasz Kaiser, and Illia Polosukhin"},{"paperId":null,"title":"Language Detection Library for Java"},{"paperId":null,"title":"Zemberek, an open source NLP framework for Turkic languages"}],"id":"2ffacbeeebd3d9e7467610057b4308635a165b6b","summary":"Morphological and word-level tokenizers outperform de-facto tok- 017 enizers in particular cases and mini models can be competitive to larger state-of-the-art models, such that a 14-times smaller model can recover 94% of the performance of a larger model."},{"url":"https://www.semanticscholar.org/paper/17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?","venue":"Conference of the Association for Machine Translation in the Americas","year":2022,"referenceCount":37,"citationCount":10,"influentialCitationCount":0,"publicationDate":"29/04/2022","authors":"Shiyue Zhang,Vishrav Chaudhary,Naman Goyal,James Cross,Guillaume Wenzek,Mohit Bansal,Francisco Guzmán","citations":[{"paperId":"64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?"},{"paperId":"17fbffb05fa14e21d1c506fd5f0f568b955fe983","title":"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models"},{"paperId":"879a7f5abdb7ab803d48172d4f0830965f989d46","title":"Language Model Tokenizers Introduce Unfairness Between Languages"},{"paperId":"62ad7ea9467bbcdbfe325b9ee561cab3908e4583","title":"MEGA: Multilingual Evaluation of Generative AI"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"15a63fa394d180f10f2d2102e35c395da4dc016d","title":"Multilingual Bidirectional Unsupervised Translation through Multilingual Finetuning and Back-Translation"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"c464352b7ab5f74bc201b5cf94bb4b9a14f5f487","title":"Masked Part-Of-Speech Model: Does Modeling Long Context Help Unsupervised POS-tagging?"},{"paperId":"55f68ac30765d943364b700817cecfcc596e5221","title":"Knowledge Transfer in Incremental Learning for Multilingual Neural Machine Translation"},{"paperId":"b6f4877861be9a40711763394d8f5f0377ad1a8f","title":"Entropy-Based Vocabulary Substitution for Incremental Learning in Multilingual Neural Machine Translation"}],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c17985a669522e7e85ae3d34754c7df49c7187d1","title":"Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627","title":"Six Challenges for Neural Machine Translation"},{"paperId":"a486e2839291111bb44fa1f07731ada123539f75","title":"Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"9ed9bff37ec952134564b3b2a022b7aba9479ff2","title":"Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"4d8a5338042da99819746ff835b6f299135e2023","title":"Statistical Power Analysis for the Behavioral Sciences"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Exploring bert’s vocabulary"},{"paperId":"5308b9d6c001304a882a50891ccce9f7ccb1c3ec","title":"On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling"}],"id":"17e2977b907aad2532c45185947539e83ac639cd","summary":"This work analyzes how translation performance changes as the data ratios among languages vary in the tokenizer training corpus and finds that while relatively better performance is often observed when languages are more equally sampled, the downstream performance is more robust to language imbalance than the authors usually expected."},{"url":"https://www.semanticscholar.org/paper/062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/08/2022","authors":"Borun Chen,Hongyin Tang,Jingang Wang,Qifan Wang,Haitao Zheng,Wei Yu Wu,Liqian Yu","citations":[{"paperId":"6bfafb32b423c3f0456a10984814f89046def489","title":"A Cookbook of Self-Supervised Learning"}],"references":[{"paperId":"27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","title":"LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization"},{"paperId":"8b954e1654c6b759a957fd11e66c111e6105fb3f","title":"CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"077c713bccd9d2c7fde68d4cbde06ab0f07a6855","title":"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"},{"paperId":"19537be34dbadbcaa4fffcf028a8ada5095b1b5c","title":"COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining"},{"paperId":"83ed88e4f745cc9aecd1fbd479612b11beddcb86","title":"CLEAR: Contrastive Learning for Sentence Representation"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"6dd1a0f657ff547b1736c2d8b30451f49a9a48f5","title":"OCNLI: Original Chinese Natural Language Inference"},{"paperId":"574c7c7260a795d2906f03b5229ace3f54ee0ab3","title":"An Unsupervised Sentence Embedding Method by Mutual Information Maximization"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"32d281a1e7a0a2d4e2b3f34e0f71780c987e1374","title":"DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations"},{"paperId":"38643c2926b10f6f74f122a7037e2cd20d77c0f1","title":"Supervised Contrastive Learning"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training with Whole Word Masking for Chinese BERT"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"ae43556f2cc1cecb8b48762b4e09df319fbaa4d9","title":"Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"549c1a581b61f9ea47afc6f6871845392eaebbc4","title":"LCQMC:A Large-scale Chinese Question Matching Corpus"},{"paperId":"c997d481606f0346164511cabe74c6d1ef3f6be5","title":"DRCD: a Chinese Machine Reading Comprehension Dataset"},{"paperId":"e7fd6848cb29ca221a7e17d823e06fb566f1f135","title":"Mixed Precision Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"346d900f2f73b706ec539caecb4800d131fc9f6a","title":"An empirical study of sentiment analysis for chinese documents"},{"paperId":"885476e9d1b2ea67405f7f9b46c2c8536657a204","title":"Scalable Term Selection for Text Categorization"},{"paperId":"b9478e237b58160c65acd2c41894493d27e2c277","title":"WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"},{"paperId":"17d5884215b5afa53545cd7cb6135de5478da4ec","title":"CERT: Contrastive Self-supervised Learning for Language Understanding"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"7afb83134d5b7914131e10b229d30dc2593266f6","title":"The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification"},{"paperId":null,"title":", and Christopher D Manning . 2019 . Electra : Pre - training text encoders as discriminators rather than generators"}],"id":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","summary":"This work proposes a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations, and implicitly encodes the coarse-grained information into the fine-Grained representations through contrastive learning on multi-grains information."},{"url":"https://www.semanticscholar.org/paper/5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation","venue":"Findings","year":2020,"referenceCount":32,"citationCount":49,"influentialCitationCount":1,"publicationDate":"05/04/2020","authors":"Thamme Gowda,Jonathan May","citations":[{"paperId":"866369023b12ee24fc62c5725f6f73234ea33da5","title":"Mean BERTs make erratic language teachers: the effectiveness of latent bootstrapping in low-resource settings"},{"paperId":"cd70bbd80f4e07b18ee2cc339e13feb15b87e934","title":"Optimized Tokenization for Transcribed Error Correction"},{"paperId":"d77885e0478c318df7a271674dce04349601e80f","title":"Morpheme-Based Neural Machine Translation Models for Low-Resource Fusion Languages"},{"paperId":"5917ba80a905aeba41487f7dcdb8b885f18689f5","title":"Tokenization and the Noiseless Channel"},{"paperId":"a1d8ddc399293753082c480b93f8d2bf4d04589c","title":"Diversifying Emotional Dialogue Generation via Selective Adversarial Training"},{"paperId":"c297aaadac066ca32a0f495f0cb62b4826f720ea","title":"Balanced image captioning with task-aware decoupled learning and fusion"},{"paperId":"9aaa71d9311b44e2228eac213c24c37bb9ca64d1","title":"Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale"},{"paperId":"77221764ce0eb68d29e1fc35ae61a21179e7dbe5","title":"What changes when you randomly choose BPE merge operations? Not much."},{"paperId":"491eaab2ff93772d52be8c014bf97466d8c6fe73","title":"Tokenization Tractability for Human and Machine Learning Model: An Annotation Study"},{"paperId":"f6a7217f271345c898db2c3fc0e6851784b5e4c3","title":"A Mixed Malay–English Language COVID-19 Twitter Dataset: A Sentiment Analysis"},{"paperId":"a713d9fb97c1e2b3982bc2f71128989c085d3f13","title":"Trained on 100 million words and still in shape: BERT meets British National Corpus"},{"paperId":"7e488d31bcc7453c0f30554f867703bb54925bc8","title":"UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units"},{"paperId":"caec2c201aa002ee23d0c7fea7dff6e7ffa7b267","title":"Towards a general purpose machine translation system for Sranantongo"},{"paperId":"9d83941a205e31d394f614424cdc553cea9e35f9","title":"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"a30318cc5281187bd315915f0d0cccd7007f75b9","title":"The VolcTrans System for WMT22 Multilingual Machine Translation Task"},{"paperId":"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","title":"Checks and Strategies for Enabling Code-Switched Machine Translation"},{"paperId":"bb459f27d173b3ffa293a0213ec29e31e44c404d","title":"Reinforcement Learning with Large Action Spaces for Neural Machine Translation"},{"paperId":"19b7c860128e5461d451b3d15f3836a9e1680ffc","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class"},{"paperId":"4f68042a0aa40f34027a49ceec64ad2bbe2211aa","title":"When does Parameter-Efficient Transfer Learning Work for Machine Translation?"},{"paperId":"88aeac6f5efe99bf24aae982a9c2d00d02685bf7","title":"IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"a821453809f36a23402ce93b6042c76d85a720a6","title":"Frequency-Aware Contrastive Learning for Neural Machine Translation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"318853f0448a9b95235dbb99338af0b19d138eaa","title":"Quasi Character-Level Transformers to Improve Neural Machine Translation on Small Datasets"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"2607dce6dcb9043ca9cae67e25e6a24411f08c0b","title":"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation"},{"paperId":"94772377a9c08ad81e506240f844534b6669b8e9","title":"Diversifying Dialog Generation via Adaptive Label Smoothing"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"3e32139deb17761a25075f8839daa61ad5992fc9","title":"Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation"},{"paperId":"3b34e79610e5acaba352def4323a59f6d531fac7","title":"Macro-Average: Rare Types Are Important Too"},{"paperId":"a49b0997efaec2db61109a6deed1512672c3cd0c","title":"Many-to-English Machine Translation Tools, Data, and Pretrained Models"},{"paperId":"c74242e97212867b2ed00cd0d6ac90652c2dc604","title":"SeqTrans: Automatic Vulnerability Fix Via Sequence to Sequence Learning"},{"paperId":"66ef8d777eccbb4138c0f9cc83d99c0bb1405c2a","title":"Optimizing the Size of Subword Vocabularies in Dialect Classification"},{"paperId":"9bedd67004ef344b801365ae28c09dce28410517","title":"Stolen Subwords: Importance of Vocabularies for Machine Translation Model Stealing"},{"paperId":"aa3cdaac0dc02e90b9ca0073dd27f41944b8f1a9","title":"Improving Formality-Sensitive Machine Translation Using Data-Centric Approaches and Prompt Engineering"},{"paperId":"e9bfa2907c6faffacfe9b7972a1b800dc9c2832d","title":"Unsupervised Feature Selection for Effective Parallel Corpus Filtering"},{"paperId":"4ddd463c22fab24e6d89105b6aa887e149832dac","title":"HFT: High Frequency Tokens for Low-Resource NMT"},{"paperId":"674265c672777b6d10d5455adc58a6cacb0d0cfe","title":"BPE beyond Word Boundary: How NOT to use Multi Word Expressions in Neural Machine Translation"},{"paperId":"b9bba726584babe18ed249c3ff55e9b851a2876c","title":"Automatic Gloss-level Data Augmentation for Sign Language Translation"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"9e3f3789ef3bfbf8f2ab3174c4d9a49b92085787","title":"Limitations and Challenges of Unsupervised Cross-lingual Pre-training"},{"paperId":"52abe540ea97d3a4794e300cfd32f16180b70592","title":"ANVITA-African: A Multilingual Neural Machine Translation System for African Languages"},{"paperId":"4e45aef9949becb3ee2e95375a51d6adcc427ed5","title":"MUNI-NLP Systems for Lower Sorbian-German and Lower Sorbian-Upper Sorbian Machine Translation @ WMT22"},{"paperId":"d680dd4b0e528b6c575fb210db6971c124210b20","title":"Finding the Optimal Vocabulary Size for Turkish Named Entity Recognition"},{"paperId":"7771aa7badc3375a31bfac8dc47755ff5d5c7780","title":"From characters to words: the turning point of BPE merges"},{"paperId":"49a5a1a550d0ce757befbad7715f373d8a5b3b87","title":"On the Strengths of Cross-Attention in Pretrained Transformers for Machine Translation"},{"paperId":"f742e7a7582d647ffe9b8037ba286c4932eb84dd","title":"Boosting Neural Machine Translation from Finnish to Northern Sámi with Rule-Based Backtranslation"},{"paperId":"7d4e38cc1c26ef513b3e9764d1778917e5420187","title":"Transformers for Low-Resource Languages: Is Féidir Linn!"}],"references":[{"paperId":"ea3e18c7b10a137d495054682c055a80b5be768c","title":"Findings of the 2019 Conference on Machine Translation (WMT19)"},{"paperId":"f9163156eeba67762a7441db48fe6720106137cd","title":"Survey on deep learning with class imbalance"},{"paperId":"5dc1a37bf05fddcdb77efafec3bc50c8ded7cef0","title":"End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features"},{"paperId":"d3707cf521e3596313af1f53acba6413d0d528a6","title":"Training Tips for the Transformer Model"},{"paperId":"15e81c8d1c21f9e928c72721ac46d458f3341454","title":"Non-Autoregressive Neural Machine Translation"},{"paperId":"9757ef31095227cb289af22b0a4010eda754d100","title":"A systematic study of the class imbalance problem in convolutional neural networks"},{"paperId":"25c0e5ad89b77ae9c7ff91765ebd4e1e21abdcb3","title":"The IIT Bombay English-Hindi Parallel Corpus"},{"paperId":"2d8edc4e38bf9907170238726ec902cb3739393b","title":"Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627","title":"Six Challenges for Neural Machine Translation"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"63e39cdf1ad884da6bc69096bb3413b5b1100559","title":"Using the Output Embedding to Improve Language Models"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"1eb09fecd75eb27825dce4f964b97f4f5cc399d7","title":"On the Properties of Neural Machine Translation: Encoder–Decoder Approaches"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"649d03490ef72c5274e3bccd03d7a299d2f8da91","title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"fa0324d3fddbbac02d24887b8a644e4db111d574","title":"Training neural network classifiers for medical decision making: The effects of imbalanced datasets on classification performance"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746","title":"The class imbalance problem: A systematic study"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"8dcaf96f66340c453e775ab217a1b1bd9ba63449","title":"Time series analysis, forecasting and control"},{"paperId":"14f4e021da2b6a1c4a5a56f8b153f8bdefd9674c","title":"Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies"},{"paperId":null,"title":"Mark Steedman . 2008 . On becoming a discipline"}],"id":"5e788c833321b12671206b96a438c0e5b1202027","summary":"This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and reveals an explanation for why certain vocabulary sizes are better than others."},{"url":"https://www.semanticscholar.org/paper/035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences","venue":"EMNLP","year":2019,"referenceCount":38,"citationCount":20,"influentialCitationCount":0,"publicationDate":"01/11/2019","authors":"Matthias Gallé","citations":[{"paperId":"17d0cc24fd6a267e7fd59ae62054de3e5c552096","title":"Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"1158f7a56dd3cc7d715740349d1d9fffbeef10ad","title":"SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural Machine Translation"},{"paperId":"5917ba80a905aeba41487f7dcdb8b885f18689f5","title":"Tokenization and the Noiseless Channel"},{"paperId":"a9cd3b1ad7ea09dfddf1a22124c27070c6f910e1","title":"Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT"},{"paperId":"71f99c67e3baaedb1121ad84dcbac082cac95b61","title":"Transfer learning from English to Slovak in speech recognition applications"},{"paperId":"850da23eae35322e0af55dbddb7e96e99482c118","title":"From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation"},{"paperId":"2e53fe6ff7d6d74da962957a5c0b282eaabd4edf","title":"How Effective is Byte Pair Encoding for Out-Of-Vocabulary Words in Neural Machine Translation?"},{"paperId":"65f6318d25c03787864ee934358bea1d50b0de1c","title":"A High-Level Representation of the Navigation Behavior of Website Visitors"},{"paperId":"1c98fcd62a5889c70ea9da4cd168b36253c7101c","title":"No Features Needed: Using BPE Sequence Embeddings for Web Log Anomaly Detection"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey"},{"paperId":"308e3090cc3c77712733a5552d10d25f210cd905","title":"Practical Random Access to SLP-Compressed Texts"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"25fc1d08e98172afe9ac2e4a64fb8fe359045547","title":"Morphological segmentation method for Turkic language neural machine translation"},{"paperId":"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","title":"Beyond Characters: Subword-level Morpheme Segmentation"},{"paperId":"d36d6abf8f9f1e80124d8a72dc5203802a6fdb26","title":"IIITT at CASE 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection"},{"paperId":"617c0f6fc2d6807a9cfa72f487b0542b25342fd0","title":"Optimizing Word Alignments with Better Subword Tokenization"},{"paperId":"d458218398f84b6ce259f3ef684582eea9e9158e","title":"String Processing and Information Retrieval: 27th International Symposium, SPIRE 2020, Orlando, FL, USA, October 13–15, 2020, Proceedings"},{"paperId":"ec69e191e25ff4ab9b59178fdc5bd171f9f147b6","title":"Using Context to Help Predict Speaker's Emotions in Social Dialogue"}],"references":[{"paperId":"4ab8662b0004305470c234acebe9b0381234107d","title":"Rpair: Rescaling RePair with Rsync"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features"},{"paperId":"6e205e973af09673028918658b99196f31fb8684","title":"Neural Machine Translation for Morphologically Rich Languages with Improved Sub-word Units and Synthetic Data"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6cddfbed35c46937588bd9d6b846ca2855953cea","title":"Neural Lattice-to-Sequence Models for Uncertain Inputs"},{"paperId":"f958d4921951e394057a1c4ec33bad9a34e5dad1","title":"A Convolutional Encoder Model for Neural Machine Translation"},{"paperId":"e43f713e0d2d438a4c0b03eacab58c334e869e6a","title":"Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation"},{"paperId":"5a14f3fe1b555afe30a62e780965bff414e1444c","title":"Lexis: An Optimization Framework for Discovering the Hierarchical Structure of Sequential Data"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"9aebe7899e9de235254dd78ec0bdc4cdef234f72","title":"An effective heuristic for the smallest grammar problem"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"611a498be099f8bd97b0104ea28f79bcac895221","title":"Searching for smallest grammars on large sequences and application to DNA"},{"paperId":"021002a852fdc590fbfbad6027cf8342ff2ea975","title":"The Smallest Grammar Problem as Constituents Choice and Minimal Grammar Parsing"},{"paperId":"fda0ba4dc485df29f1bcb11fc6badff356594489","title":"Linear-Time Text Compression by Longest-First Substitution"},{"paperId":"35dc54a582a06007c53e00943e51f2ecfa28966d","title":"Re-pair Achieves High-Order Entropy"},{"paperId":"54d7a8d91c2a2f09ea12d3a789bcb5bb33c3fe51","title":"The smallest grammar problem"},{"paperId":"40da79fa6616a5db31b83057f7b79712eb370dab","title":"A Proposal"},{"paperId":"32bdcb13b099314dcab80fea0e7416ea8a97d41b","title":"Off-line dictionary-based compression"},{"paperId":"e0f0f1f1872264e7769ccbd9edaaf435bb400b81","title":"Data compression using long common strings"},{"paperId":"7ec7bc1d7f35558640ac05e94f9b854ec17d7bc0","title":"Linguistic Structure as Composition and Perturbation"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"1de77281d812c67b605e160d2db841415ec19c17","title":"An analysis of the longest match and the greedy heuristics in text encoding"},{"paperId":"3d951ad1164c17217a24e46b57f9b31cea1b2b96","title":"Data compression via textual substitution"},{"paperId":"d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5","title":"Modeling By Shortest Data Description*"},{"paperId":"5e37f888248c44a8d8098ccd07e91cc06e8b4d42","title":"AN ALGORITHM FOR THE SEGMENTATION OF AN ARTIFICIAL LANGUAGE ANALOGUE"},{"paperId":"91e56dee713c103aa8a86ab12383391d292cca45","title":"A comparison of algorithms for data base compression by use of fragments as language elements"},{"paperId":"d435f1a0d3c4325d02332f5b39b581b3f0c34488","title":"Common phrases and minimum-space text storage"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":null,"title":"The Kyoto free translation task"},{"paperId":"5aada25e3d3122f02a84d5d393c2ef2a78bf7e8b","title":"Text Compression"},{"paperId":"062f5fd99da136c01d6dff356073c54a91838412","title":"Compression and Explanation Using Hierarchical Grammars"},{"paperId":"21f389aabe2491d620ce920e3bad2b12521fa025","title":"Algorithms on Strings, Trees, and Sequences - Computer Science and Computational Biology"},{"paperId":"39cc07b9bd6ac368c46544d0c2805b78f68bb923","title":"On the syntactic structure of protein sequences and the concept of grammar complexity"}],"id":"035df9ecf84da7ae475175f326095ab16b97dd47","summary":"The experiments show that - given a fixed vocabulary size budget - the fewer tokens an algorithm needs to cover the test set, the better the translation (as measured by BLEU)."},{"url":"https://www.semanticscholar.org/paper/5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":52,"citationCount":23,"influentialCitationCount":1,"publicationDate":"16/04/2021","authors":"Elizabeth Salesky,David Etter,Matt Post","citations":[{"paperId":"d921732ca5920049b6a8194f559917689f02d096","title":"Text Rendering Strategies for Pixel Language Models"},{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"997cebb936d88577da59ba460a9141bdd5dcb36f","title":"To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer"},{"paperId":"ea6792cd3acbe38954ead38f222448457db19347","title":"SOTASTREAM: A Streaming Approach to Machine Translation Training"},{"paperId":"940e3ee32edfe425e958a672644a0bab2fa30ddc","title":"When Vision Fails: Text Attacks Against ViT and OCR"},{"paperId":"405f1a5602867c66e015491c26d2be5504eed458","title":"Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction"},{"paperId":"6237a49297f45ebfdeabbbf67d06de323d1fccd4","title":"Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer"},{"paperId":"36e1b3973c673d3c323014a168e05c5762f84262","title":"OneCAD: One Classifier for All image Datasets using multimodal learning"},{"paperId":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only"},{"paperId":"a94e2ce7f94d189e5f788cfa431c504b3fb49402","title":"A Major Obstacle for NLP Research: Let’s Talk about Time Allocation!"},{"paperId":"c9b98d7dd15acfddf8de8448263bfff0feb6c382","title":"Logographic Information Aids Learning Better Representations for Natural Language Inference"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"4743ee49af83d3010549ee105e0c193b36fa239a","title":"Machine Translation Robustness to Natural Asemantic Variation"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e","title":"VTTS: Visual-Text To Speech"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"96fd89de07a69dd2dc94d71f884e64174c5974e2","title":"Interpreting the Robustness of Neural NLP Models to Textual Perturbations"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"a76c98c6814ce8de07707b81c18520af508b7184","title":"BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks"},{"paperId":"38375a9961778355f073bf974e643e4e00d6c10e","title":"Visual Cues and Error Correction for Translation Robustness"},{"paperId":"b6358befad3a131f14521b36353cc0523b8b0515","title":"DetTrans: A Lightweight Framework to Detect and Translate Noisy Inputs Simultaneously"},{"paperId":"eec04588f97b750965da44e27de5fa4bae9c39b6","title":"Multimodal Robustness for Neural Machine Translation"}],"references":[{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71","title":"OCR Post-Correction for Endangered Language Texts"},{"paperId":"c204d40384d39c59cd7249bde4cd8615972acaac","title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"d7a1c8da1b8163f5bbd4d12e58cad5002947105e","title":"Word Shape Matters: Robust Machine Translation with Visual Embedding"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"5dd34d2781ca702d0e3cd1224517ff60d6c3e2ee","title":"Phonetic and Visual Priors for Decipherment of Informal Romanization"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"a5690b0a514a7cbc913871e41e54c9ad4f6362db","title":"Findings of the First Shared Task on Machine Translation Robustness"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"bdbf635476477eec5be5a292b494e20b8902cc35","title":"Improving Robustness of Machine Translation with Synthetic Noise"},{"paperId":"4dda68faa3ea2c888711ce5ced009afcb612e05b","title":"Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"64b8361a133ad545cbf9c035e109ef8461b31f67","title":"Super Characters: A Conversion from Sentiment Classification to Image Classification"},{"paperId":"1e6d69f8cfd698b8139cde557252753b22c7d712","title":"BPE and CharCNNs for Translation of Morphology: A Cross-Lingual Comparison and Analysis"},{"paperId":"ce89ee7aaeeea2c9d474707690f3ea9d948776a3","title":"MTNT: A Testbed for Machine Translation of Noisy Text"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"d09e0187879c6dbaacb16c23a2dddb31d74b8b0b","title":"On the Impact of Various Types of Noise on Neural Machine Translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"6ae164b36a8b712ca5b51659e7d77aa124151b4e","title":"COPPA V2. 0: Corpus Of Parallel Patent Applications Building Large Parallel Corpora with GNU Make"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"4b5d9e8225d9caa2643217ad033b32ba3fe97b05","title":"Combining Convolutional Neural Networks and LSTMs for Segmentation-Free OCR"},{"paperId":"8e39bc4e1711e941881f64935b203a93259acb50","title":"Glyph-aware Embedding of Chinese Characters"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"db253b17043b6a86e02173b6aa597664b0c7f256","title":"Learning Character-level Compositionality with Visual Features"},{"paperId":"5f09d0cbd59e7c84b912692b6ded42107e8f9733","title":"Chinese–Spanish neural machine translation enhanced with character and word bitmap fonts"},{"paperId":"711e6e9e2fb1f4ab7238cdd556002ae36deeece7","title":"Robsut Wrod Reocginiton via Semi-Character Recurrent Neural Network"},{"paperId":"892e53fe5cd39f037cb2a961499f42f3002595dd","title":"Bag of Tricks for Efficient Text Classification"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"8e9149ab00236d04db23394774e716c4f1d89231","title":"An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","title":"Show and tell: A neural image caption generator"},{"paperId":"fba7c0a51a6301ca4086a5ce59b1f13af9acad7f","title":"Pointwise Prediction for Robust, Adaptable Japanese Morphological Analysis"},{"paperId":"a7bfdac75a0f0dfbd5faa375d90132c3115f9725","title":"R34D1NG W0RD5 W1TH NUMB3R5."},{"paperId":"3169b65a8769302ccb3ef41d98617bd2d9826293","title":"Raeding Wrods With Jubmled Lettres"},{"paperId":"cf61a21a58d3c762e0dc868ce09114e18304ed9d","title":"Word recognition inside out and outside in"},{"paperId":"e8f297e161f57e461ede2d4e0c26573981cad077","title":"Findings of the 2020 Conference on Machine Translation (WMT20)"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"c2e1e362e27ff5ced52bfc9bde3ca165e7eafe76","title":"Neural machine translation using bitmap fonts"},{"paperId":null,"title":"Glyce: Glyph­vectors for chinese"},{"paperId":null,"title":"2016a. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651"},{"paperId":null,"title":"Journal of experi mental psychology. Human perception and perfor mance"},{"paperId":null,"title":"Robsut wrod reocgini ­ ton via semi ­ character recurrent neural network Optimizing segmentation granularity for neural machine trans ­ lation"},{"paperId":null,"title":"Costa ­ jussà , David Aldón , and José A . R . Fonollosa"},{"paperId":null,"title":", andM . Carreiras . 2008 . R 34 d 1 ng w 0 rd 5 w 1 th numb 3 r 5"},{"paperId":null,"title":"The multitarget TED talks task"}],"id":"5c3005e22e6fb218aa76fea49971f3f991993b32","summary":"This work proposes the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows, and demonstrates significant robustness to varied types of noise."},{"url":"https://www.semanticscholar.org/paper/379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":2,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"F. Meyer,Jan Buys","citations":[{"paperId":"3448e136046d22d4f90d7a4c875890f5fb64b811","title":"PuoBERTa: Training and evaluation of a curated language model for Setswana"},{"paperId":"5079e188ae86d8330414edb65e898ed306ac450d","title":"Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation"}],"references":[{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"d7aa3c65eae71776ccc3cddafb1d4b288deae2cb","title":"A Masked Segmental Language Model for Unsupervised Natural Language Segmentation"},{"paperId":"7b8d0413ee38d09421e0243dbb63c1e51fd1b571","title":"Low-Resource Language Modelling of South African Languages"},{"paperId":"bf607962d2edc1f52f06622853c93ec65efc6d03","title":"Canonical and Surface Morphological Segmentation for Nguni Languages"},{"paperId":"5a50d90c7ad715c57b5f0cd9d8473b3dff705d40","title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"13b4d9a1096113db63158590e9d2e58744766e3b","title":"Unsupervised Word Segmentation with Bi-directional Neural Language Model"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"550871a80614dd481d153a43f94ef880cc4c461b","title":"Adaptation of Deep Bidirectional Transformers for Afrikaans Language"},{"paperId":"b079db3fe7cc3037fc15a5a1071254a0046774ac","title":"On the Importance of Subword Information for Morphological Tasks in Truly Low-Resource Languages"},{"paperId":"40c531fb0267437b0f76fd8f0080fb4de9ffe146","title":"A Systematic Study of Leveraging Subword Information for Learning Word Representations"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"7e55cb75b085ea144597710e9fb75be7479b2a64","title":"Towards an unsupervised morphological segmenter for isiXhosa"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"a98c340f316fc041dd071efcec1fbddf176282d9","title":"Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"fce6bacacd59de6ad7b486261d7f955374164c7f","title":"Unsupervised Morphological Segmentation Using Neural Word Embeddings"},{"paperId":"0a275c52c61ecf2429189387426d6173e40d695a","title":"A Joint Model of Orthography and Morphological Segmentation"},{"paperId":"6b904d6e84c98c6ce22ce6923224b205a2a24ee1","title":"Segmental Recurrent Neural Networks"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"1a7715e8d41a228f8fc35f7e3f2580988eea9b24","title":"Developing Text Resources for Ten South African Languages"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"681f9d533274d2a20f96349cac05ee0e68869b4c","title":"Unsupervised Morphological Segmentation with Log-Linear Models"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","title":"Finding Structure in Time"},{"paperId":"b2474a00d7de3373bab934c09acef1994fa82207","title":"Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages"},{"paperId":null,"title":"CANINE: pre-training an efﬁcient"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c46fd08184f91144dbc56bb2e41dd800669ac5f8","title":"Evaluation of combined bi-directional branching entropy language models for morphological segmentation of isiXhosa"},{"paperId":"e6ffe957179541a4cad7b6eba888c79c4aad8f91","title":"Unsupervised Morphological Segmentation for Low-Resource Polysynthetic Languages"},{"paperId":null,"title":"Exploring BERT's vocabulary"},{"paperId":"02711b7e8c73779db802cb00b6cbf407d0872b66","title":"Unsupervised models for morpheme segmentation and morphology learning"},{"paperId":"fb3670b1b33adc6d90b6cf2326b48e9869c68081","title":"A Comparison of Approaches to Word Class Tagging: Disjunctively vs. Conjunctively Written Bantu Languages"}],"id":"379722c04fb3b54215f82512eb86398cb02d42dd","summary":"A subword segmental language model (SSLM) that learns how to segment words while being trained for autoregressive language modelling, enabling the model to discover morpheme-like subwords that improve its LM capabilities."},{"url":"https://www.semanticscholar.org/paper/70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Jimin Sun,Patrick Fernandes,Xinyi Wang,Graham Neubig","citations":[{"paperId":"d921732ca5920049b6a8194f559917689f02d096","title":"Text Rendering Strategies for Pixel Language Models"},{"paperId":"17fbffb05fa14e21d1c506fd5f0f568b955fe983","title":"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models"}],"references":[{"paperId":"5032c0946ee96ff11a292762f23e6377a6cf2731","title":"Holistic Evaluation of Language Models"},{"paperId":"66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"58e13e1fed9b28e50efcaff611772801d7980c80","title":"Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"77a096d80eb4dd4ccd103d1660c5a5498f7d026b","title":"Dynabench: Rethinking Benchmarking in NLP"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"40848b41ed8c9c255ecd8a920006877691b52d03","title":"WILDS: A Benchmark of in-the-Wild Distribution Shifts"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"4ceff7472c04ee6d76bce89d61ba4b445d8dbf74","title":"InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training"},{"paperId":"33ec7eb2168e37e3007d1059aa96b9a63254b4da","title":"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"8741a0768d643c1593f8fea75dfdd0e5e90e1147","title":"Vocabulary Adaptation for Domain Adaptation in Neural Machine Translation"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"c26f90d4cfa33ceff373cf49c2a534e2004685da","title":"HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing"},{"paperId":"74b4f16c5ac91e3e7c88ae81cc8c91416b71d151","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"0e4cd6bae6ac1017e7b1b9bd644375aee65b8372","title":"Show Your Work: Improved Reporting of Experimental Results"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"e99e2bd4812b30e104db0feddb681f32acd88758","title":"Massively Multilingual Transfer for NER"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"9a786d1ecf77dfba3459a83cd3fa0f1781bbcba4","title":"An Analysis of Deep Neural Network Models for Practical Applications"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"5839ad026ee43f3b72493c416dddb4203791714d","title":"Translation"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":null,"title":"XNLI For encoder-only models, the ﬁrst token ("}],"id":"70dd68c07b322b68836eded1fb4f78c0efcad685","summary":"Surprisingly, it is found that subword-based models might still be the most practical choice in many settings, achieving better performance for lower inference latency and memory usage."},{"url":"https://www.semanticscholar.org/paper/e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE","venue":"","year":2022,"referenceCount":49,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"N. Chirkova,Sergey Troshin","citations":[{"paperId":"c559a8c6bab06be554025be3d1014070c35eb850","title":"Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey"},{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?"}],"references":[{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"31bfcbb1cbd74e8f38d97911f987108bd9517c6b","title":"Efficient Inference for Multilingual Neural Machine Translation"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"0cf02cb3b3c25aae8336379e29a8257c85558637","title":"AVATAR: A Parallel Corpus for Java-Python Program Translation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"556983a574e322fd7d01c6a8193b3785c97a8905","title":"A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"1c84438eff3d10cfb97fb64fc9f35d734209a465","title":"Character-based NMT with Transformer"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"735ce0447e459e13f89ae751b19323d76a2af786","title":"Program Synthesis and Semantic Parsing with Learned Code Idioms"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"6faf62ac9e69e66cb92c923b749fa071554d23ca","title":"Learning Programmatic Idioms for Scalable Semantic Parsing"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"642c1b4a9da95ea4239708afc5929a5007a1870d","title":"Tensor2Tensor for Neural Machine Translation"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"3a29aa4eff48624752c07059a44d3288a678c8ab","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"de7293a0137fefe92412d61d3db93e22c0988136","title":"Evaluating clone detection tools with BigCloneBench"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":null,"title":"2021b. Avatar: A"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"URL https://d4mucfpksywv"},{"paperId":null,"title":"[ From : Machine translation : from real users to research : 6 th conference of the Association for Machine Translation in the Americas"},{"paperId":"f7d95e8a91d7683e74ee642f20906e7751e47dfe","title":"Association for Computational Linguistics"},{"paperId":null,"title":", Marie - Anne Lachaux , Marc 833 Szafraniec , and Guillaume Lample . 2021 . Dobf : A 834 deobfuscation pre - training objective for program - 835 ming languages"},{"paperId":null,"title":": A generative model for code"},{"paperId":null,"title":"UnigramLM 50K Level 0"}],"id":"e6b73466bab5e52ce0db19dd06d9353c26557dae","summary":"This work proposes subtokenziation that reduces average length by 17–40% without downstream performance drop, and shows that a carefully chosen subtokenization may significantly improve quality by 0.5-2%, possibly with some length increase."},{"url":"https://www.semanticscholar.org/paper/4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification","venue":"Neural Processing Letters","year":2021,"referenceCount":62,"citationCount":13,"influentialCitationCount":1,"publicationDate":"14/06/2021","authors":"Zaid Alyafeai,Maged S. Al-shaibani,Mustafa Ghaleb,Irfan Ahmad","citations":[{"paperId":"db9b05f738c145d298c006f6319ccdeb77d9911f","title":"Performance Improvement of Poem Genre Classification using a Combination of SMOTE and Support Vector Machine"},{"paperId":"dc40985b5c4ea44a4c04f57538a56e3ce321174e","title":"A Study on Different Methods in Sentiment Analysis from Text"},{"paperId":"9addef681bc1576d7d9b0104d71c41b9def546c5","title":"Analysis of Subword Tokenization Approaches for Turkish Language"},{"paperId":"7e4ce5d14e95dad17f95b6785a7b8a1acff90c60","title":"A systematic review of Arabic text classification: areas, applications, and future directions"},{"paperId":"3ac1b0f67a92c32c64d0a01dcf04f91cd1ccd2f4","title":"Performance evaluation of machine learning models on large dataset of android applications reviews"},{"paperId":"29c09defb807a9e16b4ea7cbf91587c9f4361121","title":"A Review Study on Arabic Text Classification"},{"paperId":"a6fad9b1b32ace586b735e1764d26f1d67f2f7c7","title":"Arabic Document Classification: Performance Investigation of Preprocessing and Representation Techniques"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"d1d3dde91e3e73ccfbeb176f3af565a4507be077","title":"AI-Based Misogyny Detection from Arabic Levantine Twitter Tweets"},{"paperId":"b2ed3ed8d8b5717644be0ae705c13e2c2121a058","title":"Cloud computing architecture for Tagging Arabic Text Using Hybrid Model"},{"paperId":"b087a99deafc6fda5b2c125df66f71b4208fed05","title":"A Review on Recent Arabic Information Retrieval Techniques"},{"paperId":"5bd578b737d5ee9d10c4ad8500881d98606a6462","title":"SQU-CS @ NADI 2022: Dialectal Arabic Identification using One-vs-One Classification with TF-IDF Weights Computed on Character n-grams"},{"paperId":"6bff90e44c26f59f6201ecf132516ff9a62442cc","title":"Generating Classical Arabic Poetry using Pre-trained Models"}],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"f3460673df92ec46de780f8e7813e1c35c3bb41f","title":"A comparative study of effective approaches for Arabic sentiment analysis"},{"paperId":"bcc14c3a878c7f155a52a07be8b26eec2f46ec66","title":"Pre-Training BERT on Arabic Tweets: Practical Considerations"},{"paperId":"1e4cda8be54999ced1324777fa462a85e2c9746c","title":"ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic"},{"paperId":"2e4089ca4e1c4bda989e9e73fc92223e0ec96c99","title":"Classifying and diacritizing Arabic poems using deep recurrent neural networks"},{"paperId":"22e7c2cac5f6d810f3777a0d966f79caabdd3910","title":"MetRec: A dataset for meter classification of arabic poetry"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"22b9b4ca72a7dff11a60f5f43d96b0555014ad76","title":"Meter classification of Arabic poems using deep bidirectional recurrent neural networks"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"4fa37d012ad0014552a6a5a03624b29f95558bf7","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d6271bbe0b76e5e248767f09ae669177f554ba78","title":"Arabic sentiment analysis: studies, resources, and tools"},{"paperId":"91663fbb9257b1ee5d22d7b9e796e03f374a7a6a","title":"hULMonA: The Universal Language Model in Arabic"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"f89df2ef9cd3a4e5f7becaf8123c4d66fcbddd35","title":"The Impact of Preprocessing on Arabic-English Statistical and Neural Machine Translation"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training with Whole Word Masking for Chinese BERT"},{"paperId":"266aa67db44336f6b6cfec6f7b5c340cded7154d","title":"Highly Effective Arabic Diacritization using Sequence to Sequence Modeling"},{"paperId":"cc94d15ba408c260c8fe4fa4f1cb6797a996dd21","title":"Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language"},{"paperId":"60d00c7c09135d3643fd3efbd59d922bb60a5904","title":"Learning meters of Arabic and English poems with Recurrent Neural Networks: a step forward for language understanding and synthesis"},{"paperId":"ad3ce37737fdbab5e4834a161d755249b0c2ee24","title":"A Survey of Opinion Mining in Arabic"},{"paperId":"a1f1dd2e73f6ce765a19787849b6e5b23735ab9c","title":"A comprehensive survey of arabic sentiment analysis"},{"paperId":"1e6d69f8cfd698b8139cde557252753b22c7d712","title":"BPE and CharCNNs for Translation of Morphology: A Cross-Lingual Comparison and Analysis"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"20e5dd2fe186cae77e9cca9be5dc66e18c596ffb","title":"Morphological Word Embeddings for Arabic Neural Machine Translation in Low-Resource Settings"},{"paperId":"80e7eb77c5ab8ffb8d870db0d6fa34a40cc792a4","title":"Sudachi: a Japanese Tokenizer for Business"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"4f94c40285c52128329d29b053976734349a0371","title":"DataSet for Arabic Classification"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"f94f20ee56489dff949d368e0a17d02dee7bad83","title":"Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging"},{"paperId":"2ebc9c842376e2f8cd56686e50d1daf19cbf080d","title":"Arabic Online Handwriting Recognition (AOHR)"},{"paperId":"39c3073181f2330804696a489f32d9c189febec1","title":"Arabic Tweets Sentimental Analysis Using Machine Learning"},{"paperId":"f3eeef4afb81223df96575adadf808fe7fe440b4","title":"1.5 billion words Arabic Corpus"},{"paperId":"8da1c2acdb8174f16566606d8d8b7bf3870d649a","title":"Orthographic Syllable as basic unit for SMT between Related Languages"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"a1aaa7c75464e7ebe41cfe5c5258241ca34c6414","title":"Farasa: A Fast and Furious Segmenter for Arabic"},{"paperId":"93a9694b6a4149e815c30a360347593b75860761","title":"Variable-Length Word Encodings for Neural Translation Models"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"1eb09fecd75eb27825dce4f964b97f4f5cc399d7","title":"On the Properties of Neural Machine Translation: Encoder–Decoder Approaches"},{"paperId":"f5886c433084d1baaa24152cd3e4c555c79bfe4f","title":"MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"a9c3a94b3e620e479ef52e2dc312202ec22af6bf","title":"LABR: A Large Scale Arabic Book Reviews Dataset"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"bc5097c5821d7fefd776d93e238247e76dba7554","title":"A comparative study between methods of Arabic baseline detection"},{"paperId":"45aabb7249efb8f6deed0e14aa635bb03a9e530f","title":"The Korean Morphologically Tight-Fitting Tokenizer for Noisy User-Generated Texts"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"21079df28af037f93620a0bd63f04fe49cf5df7d","title":"On the Importance of Tokenization in Arabic Embedding Models"},{"paperId":"7d1c03b6046b22ad7ec47c889ee14f6d006bf696","title":"ACCURATE AND FAST RECURRENT NEURAL NETWORK SOLUTION FOR THE AUTOMATIC DIACRITIZATION OF ARABIC TEXT"},{"paperId":null,"title":"Arabic optical characters recognition by neural network based arabic unicode"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Towards burmese (myanmar) morphological analysis: Syllable-based tokenization and part-of-speech tagging"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"3924aa213ff891812c66a6909ab902684d3eb107","title":"AraVec: A set of Arabic Word Embedding Models for use in Arabic NLP"},{"paperId":"1fd7fc06653723b05abe5f3d1de393ddcf6bdddb","title":"SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"}],"id":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","summary":"This paper introduces three new tokenization algorithms for Arabic and compares them to three other baselines using unsupervised evaluations and shows that the performance of suchtokenization algorithms depends on the size of the dataset, type of the task, and the amount of morphology that exists in the dataset."},{"url":"https://www.semanticscholar.org/paper/1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","title":"Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings","venue":"EACL","year":2021,"referenceCount":18,"citationCount":5,"influentialCitationCount":0,"publicationDate":2021,"authors":"O-Yeon Kwon,Dohyun Kim,Soobee Lee,Junyoung Choi,SangKeun Lee","citations":[{"paperId":"02942af51e5c80fd6c6fa8097ad389e697d53677","title":"DPMS: Data-Driven Promotional Management System of Universities Using Deep Learning on Social Media"},{"paperId":"ce26a9458de83c7394ebfe6f62d26ad73869b0cb","title":"From task to evaluation: an automatic text summarization review"},{"paperId":"e985d34a29a1988659e7971d520496eca1a7a548","title":"BejaGNN: behavior-based Java malware detection via graph neural network"},{"paperId":"96e77dfdd50411fe404e6b6bbde5224df9eee802","title":"Tweets Emotions Analysis of Community Activities Restriction as COVID-19 Policy in Indonesia Using Support Vector Machine"},{"paperId":"f22ba8b56b89b2f19e84fbe3439bf78e8fc926b2","title":"KLASIFIKASI EMOSI PADA CUITAN DI TWITTER DENGAN PRINCIPAL COMPONENT ANALYSIS DAN SUPPORT VECTOR MACHINE"}],"references":[{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"8fa7d1f3f82526935ba122c20c8d0648506301b3","title":"Misspelling Oblivious Word Embeddings"},{"paperId":"9348186c18bbd35d77a9011474fdd76ef98c86c5","title":"Robust Word Vectors: Context-Informed Embeddings for Noisy Texts"},{"paperId":"6c145caf5da0e2f078c34d0b65b399d7124e2fd8","title":"Learning to Generate Word Representations using Subword Information"},{"paperId":"1dff26dc9be229daae61231a340f57efa8126e0d","title":"Subword-level Word Vector Representations for Korean"},{"paperId":"c0fbe0378150bb35e94bdd800bf02a4e57d9f2c2","title":"Morpheme-based Efficient Korean Word Embedding"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"},{"paperId":"6fe682de00baba4d50bb855ae243f55a4a2a4e14","title":"A Word Embedding and a Josa Vector for Korean Unsupervised Semantic Role Induction"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"b0130277677e5b915d5cd86b3afafd77fd08eb2e","title":"Estimation of probabilities from sparse data for the language model component of a speech recognizer"},{"paperId":"b92a80c848d51bb009758c117ab4f05a99913106","title":"THE KOREAN LANGUAGE: Structure, use and context"},{"paperId":null,"title":"Analyzing of hangul search query spelling error patterns and developing query spelling correction system based on user logs"}],"id":"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","summary":"A robust Hangeul word embedding model against typos is proposed that utilizes a Convolutional Neural Network architecture with a channel attention mechanism that learns to infer the original word embeddings."},{"url":"https://www.semanticscholar.org/paper/9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information","venue":"ArXiv","year":2021,"referenceCount":35,"citationCount":6,"influentialCitationCount":0,"publicationDate":"01/08/2021","authors":"Yuval Pinter,A. Stent,Mark Dredze,Jacob Eisenstein","citations":[{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"d57cbec10dfee4e244657d3cdf9ba4cf53086744","title":"An Empirical Analysis Towards Replacing Vocabulary-Rigid Embeddings by a Vocabulary-Free Mechanism"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"24fbe3af030f393e55bda3dc7dae0f57bd270d04","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"3d3f8399d625238fddb366697acb73446129d65c","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"188cd686fb2200f237f688dbda7f64ffc75e67ac","title":"Subword Pooling Makes a Difference"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"641250e235d81e5b5c0023c32e71731aa0b0027c","title":"Multi-resolution Annotations for Emoji Prediction"},{"paperId":"8c59bd0fc77538f08d100c2a2819c14c7e2adca3","title":"Char2Subword: Extending the Subword Embedding Space from Pre-trained Models Using Robust Character Compositionality"},{"paperId":"9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2","title":"TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"b08545e1281c1eb748e4474687eb61fd3b25d1a6","title":"NYTWIT: A Dataset of Novel Words in the New York Times"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a022bda79947d1f656a1164003c1b3ae9a843df9","title":"How to Fine-Tune BERT for Text Classification?"},{"paperId":"3e9674a344db5e4e7aa02222d659e58e4307b084","title":"SemEval 2018 Task 2: Multilingual Emoji Prediction"},{"paperId":"a235999a41cfcc5868ddacb94b47cda933f00a8c","title":"Peperomia at SemEval-2018 Task 2: Vector Similarity Based Approach for Emoji Prediction"},{"paperId":"8245fc60669776991dcb8ecd53a107f37bec29d2","title":"Hatching Chick at SemEval-2018 Task 2: Multilingual Emoji Prediction"},{"paperId":"321f91528af535cefa1b6971df31c609673f463f","title":"Backpropagating through Structured Argmax using a SPIGOT"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"72e87913632d0f1295fbcfa46795c5bb26d0a422","title":"Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"28ac5c728449c19be229e8839f5b5d6acd896f15","title":"Results of the WNUT16 Named Entity Recognition Shared Task"},{"paperId":"dd95f96e3322dcaee9b1e3f7871ecc3ebcd51bfe","title":"MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"d895647b4a80861703851ef55930a2627fe19492","title":"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"7e82f473cf75cb35831218097d1fe1ff74e4df60","title":"Proceedings"},{"paperId":"e7a2ffd26cd76e5b662ecb8624ecb3e177dbb8da","title":"Pitfalls of Static Language Modelling"},{"paperId":null,"title":"Recent Advances in Language Model Fine-tuning"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"219ad87581e5052911a46ce7c0e2f4bb09385d25","title":"Proceedings of The 12th International Workshop on Semantic Evaluation"},{"paperId":null,"title":"Computational Linguistics (Volume 1: Long Pa-pers)"}],"id":"9c2e4e5ee224c20a45c37244924138b50f3fe603","summary":"It is shown that incorporating XRAYEMB’s learned vectors into sequences of pre- trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on nonstandard English text."},{"url":"https://www.semanticscholar.org/paper/937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":16,"citationCount":11,"influentialCitationCount":1,"publicationDate":"25/08/2021","authors":"I. Itzhak,Omer Levy","citations":[{"paperId":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!"},{"paperId":"67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation"},{"paperId":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"b162638cd42b65e6add199b0b34f1b375070fc7c","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"8b723be33e62bf5bd9278769244f1c13a9510898","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP"},{"paperId":"4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations"},{"paperId":"fd53fd29c919a2dd40d54649c5930a2ba5e1ee71","title":"Mixed Orthographic/Phonemic Language Modeling: Beyond Orthographically Restricted Transformers (BORT)"}],"references":[{"paperId":"7694aae9766d5f1fe74d900cd82aee898cb6e8e9","title":"How to Train BERT with an Academic Budget"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"e9d59dd33698adcdafabeb0f9ea873b2ed36f20b","title":"Farasa: A New Fast and Accurate Arabic Word Segmenter"},{"paperId":"6ed9417eaa7ee16f0563599829a061421a3e0563","title":"NLTK: The Natural Language Toolkit"},{"paperId":"b2f8876482c97e804bb50a5e2433881ae31d0cdd","title":"Binary codes capable of correcting deletions, insertions, and reversals"}],"id":"937b177d2ed7cee27ee45300c690f2f60c81bae5","summary":"The results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell doesn’t appear to enhance its performance on such tasks."},{"url":"https://www.semanticscholar.org/paper/b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels","venue":"ArXiv","year":2022,"referenceCount":144,"citationCount":22,"influentialCitationCount":1,"publicationDate":"14/07/2022","authors":"Phillip Rust,Jonas F. Lotz,Emanuele Bugliarello,Elizabeth Salesky,Miryam de Lhoneux,Desmond Elliott","citations":[{"paperId":"997cebb936d88577da59ba460a9141bdd5dcb36f","title":"To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer"},{"paperId":"5bd254c0775d18cdc30cb85be61e080484ee6713","title":"Multilingual Text Representation"},{"paperId":"68af5607042c3c9837a336ee38e86c2ee59357b7","title":"A Framework for Responsible Development of Automated Student Feedback with Generative AI"},{"paperId":"988a17753dd040867eef4f093fa50a87bf4142b1","title":"Tokenization with Factorized Subword Encoding"},{"paperId":"29a54c072bcf85fa6934b6f701962a7c9aeb6489","title":"Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora"},{"paperId":"36e1b3973c673d3c323014a168e05c5762f84262","title":"OneCAD: One Classifier for All image Datasets using multimodal learning"},{"paperId":"c360b3a789b2a05072f21f6a8997c68bf1b24087","title":"GlyphDiffusion: Text Generation as Image Generation"},{"paperId":"78f599fbd62dcc4a8dbab9d2f6056815dfc5b84c","title":"The MiniPile Challenge for Data-Efficient Language Models"},{"paperId":"8b0dcb0fd6489abc5f1002ffcb693c70ec6b8fe2","title":"Linking Representations with Multimodal Contrastive Learning"},{"paperId":"153b44c68c2366d92ea7c92d7f6cfdedde5b7f78","title":"Efficient OCR for Building a Diverse Digital History"},{"paperId":"84b6fecf016d74512869c698c66c83729abdf359","title":"Self-Supervised Multimodal Learning: A Survey"},{"paperId":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"e1484706c0fab932fc9804df328044b3cb2f110d","title":"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding"},{"paperId":"fac575ded0daf994751edd34466392782c9f2764","title":"Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems"},{"paperId":"134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?"},{"paperId":"f656320f82e9d32e4bd02b5558d59b258f8bfd81","title":"Clark Kent at SemEval-2023 Task 5: SVMs, Transformers, and Pixels for Clickbait Spoiling"},{"paperId":"e8fe4f5a5424d5fd19701dfa32d411b418fbf81f","title":"RenderDiffusion : Text Generation Is Also Image Generation"},{"paperId":"7933c01f75ec11ca6232ea5f00241727630ace98","title":"DUBLIN - Document Understanding By Language-Image Network"},{"paperId":"daf93fc951e9842426b663028ffc36b6728c7444","title":"SemEval-2023 Task 5: Clickbait Spoiling"},{"paperId":"2e4f3ecaf91637e845a9d53d2c800db868ef46af","title":"Lightweight morpheme labeling in context: Using structured linguistic representations to support linguistic analysis for the language documentation context"},{"paperId":"318dcd195b80a5e4d29f513d889ba8133c02a88e","title":"Spatiotemporal Exogenous Variables Enhanced Model for Traffic Flow Prediction"}],"references":[{"paperId":"33ebe01c8705a19def511040686d198871145409","title":"Story Beyond the Eye: Glyph Positions Break PDF Text Redaction"},{"paperId":"18c92da1b7f7f8ea6877e5b3a0d5a6df14a09e00","title":"SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners"},{"paperId":"8f26262437bde0ff8fe5e14d5a6cb4cc05a495ef","title":"Multimodal Masked Autoencoders Learn Transferable Representations"},{"paperId":"0b4f0f6b04476c4f2cdfeabf99d4b835227f0bb2","title":"Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models"},{"paperId":"004b97aea43f9f62cc49dec20f449abfbae28811","title":"Masked Autoencoders As Spatiotemporal Learners"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"ac6e85cc3f105567e20e019f5752ed38a0a34a25","title":"Breaking Character: Are Subwords Good Enough for MRLs After All?"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"3682a8716817743c6341003b2a121fbd183d6bbc","title":"MAE-AST: Masked Autoencoding Audio Spectrogram Transformer"},{"paperId":"4990f7542f0600e0501a7e7a931b32eb7cb804d5","title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"},{"paperId":"a8fffb507d1790851550af5e4ebdd06e5bae1cee","title":"Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"b28e23afe988ad83ab5bd9ce7f826c140c533be8","title":"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages"},{"paperId":"2d439ec2c301d058bd4a8b4743328e3d9939625e","title":"Should You Mask 15% in Masked Language Modeling?"},{"paperId":"2ec3fb471f059fe401c7e1d7d7a199ba4feae814","title":"JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension"},{"paperId":"f5d22b267856410695e159970271fd016c51052a","title":"Does Transliteration Help Multilingual Language Modeling?"},{"paperId":"d0336e5be72e97e0493b1ba77ef8ec3c349d496a","title":"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"972706306f85b1bfb40c7d35c796ad5174eb0c9c","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"03aa38229a6c853d96430edfc8f1542598f2116e","title":"AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain"},{"paperId":"dbd9fab59832369040661bb050daef6de405230c","title":"Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"59c0c6b62e33850cda08663d4c9ecabcf5d21596","title":"IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization"},{"paperId":"8dad3a12d2a652122458dd377f62025354b17a2a","title":"Subword Mapping and Anchoring across Languages"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"964f079b80e6f3ce3164dd883dd1836299f0dba3","title":"GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph"},{"paperId":"5b540745f4b51f95bf90fb3420e51edb037fc51a","title":"The MultiBERTs: BERT Reproductions for Robustness Analysis"},{"paperId":"06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study"},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"a76c98c6814ce8de07707b81c18520af508b7184","title":"BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"7ec5f207263100ea2d45db595712f611a74bafd9","title":"Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"13bcfb944779165983aaef22cec8a3bbd3e98e62","title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"85dc7829455819283270eb643817bcf97133464d","title":"From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"29599829d26b4acaaf0ad88698ed4362d33b2ae7","title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"335a05c16fcbc7a2c2bf5221299de608b08030f0","title":"From Hero to Zéroe: A Benchmark of Low-Level Adversarial Attacks"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"e4ad5c24985127e015be86a53d57e1bb43876b4c","title":"On Romanization for Model Transfer Between Scripts in Neural Machine Translation"},{"paperId":"110c13fbf4ff87b52ee1fd9eb2d3616c839ceb41","title":"Parsing with Multilingual BERT, a Small Treebank, and a Small Corpus"},{"paperId":"df29486c04eafd004f2f0816e84c798783802cdf","title":"Transliteration for Cross-Lingual Morphological Inflection"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"14489ec7893e373a0dcc9555c52b99b2b3a429f6","title":"Are All Languages Created Equal in Multilingual BERT?"},{"paperId":"8ae8e5e840c5f43d4d72cbc4595690fba01aa799","title":"LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"b805693c17961af2cc7f859c1a54320b26036f46","title":"Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"0495d9df8eb84dcdab4e5536179823cd26279949","title":"Big Transfer (BiT): General Visual Representation Learning"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"e4f19271b9491b0297a03318a96b8a5157873acd","title":"Writing Across the World's Languages: Deep Internationalization for Gboard, the Google Keyboard"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"6ffb1cc32ddde6ae01e2fc0286eafa116ade0ffb","title":"KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"c0aaee2337e5af680e5dca1bfc349a737dfec573","title":"Fixing the train-test resolution discrepancy"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"b401aca53c04cec865e1a733090a2c60ca7ebe97","title":"Glyce: Glyph-vectors for Chinese Character Representations"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"4fcec7ea62825953ac8d483cfa5c748b4daa4e7e","title":"The Coptic Universal Dependency Treebank"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"9e95df7a7f667c8ee907d0ba4537889e0e27ec3e","title":"Learning Distributional Token Representations from Visual Features"},{"paperId":"e74ad670a6cbd693e225b0150f3a33fc1931faf7","title":"Universal Dependencies Version 2 for Japanese"},{"paperId":"908cd47b7fed4c1d6da7be6d36d8b2fd37996e98","title":"Building Universal Dependency Treebanks in Korean"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"2c4574c7f42ac3e887670876a37ea6708a77966e","title":"AROMA: A Recursive Deep Learning Model for Opinion Mining in Arabic as a Low Resource Language"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"b022f2a277a4bf5f42382e86e4380b96340b9e86","title":"SGDR: Stochastic Gradient Descent with Warm Restarts"},{"paperId":"3a29aa4eff48624752c07059a44d3288a678c8ab","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"4361e64f2d12d63476fdc88faf72a0f70d9a2ffb","title":"Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"753e30826f1908a62a8d251fc6b1b598f86d2bb2","title":"Shared Tasks of the 2015 Workshop on Noisy User-generated Text: Twitter Lexical Normalization and Named Entity Recognition"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"77ef6c449d3b7f5f5c55a06564b79eb4438c85b9","title":"Prague Dependency Style Treebank for Tamil"},{"paperId":"1bc057046e2903d96cfe3093cb8c62ab02bb12f8","title":"Named Entity Recognition"},{"paperId":"2fa38d8b67c7f59ac2a2bf1b53173a973422d8e2","title":"Prague Arabic Dependency Treebank 1.0"},{"paperId":"a7d1c7979b2d8205e9382e9f4fc39eb999fcd955","title":"Building a Large Syntactically-Annotated Corpus of Vietnamese"},{"paperId":"bb7d7dfad94e6a94bf0926fd28e6a0dd7d13b278","title":"Advances on natural language processing"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"25babf5ef0d4eb3f09f933c9a7f46d2f8e296306","title":"Processing of Sentences With Intra-Sentential Code-Switching"},{"paperId":"b05f5006a879fab3668365918a15dd35acd77204","title":"Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Models"},{"paperId":"7a670e9c4cf6655cfbcacf169565d4d645c0d475","title":"Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"b62a56f629f77acce9ea69456d67dd77de1c7dfb","title":"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization"},{"paperId":"8656e5a885613f29f0f2f35589d865baeb1317a6","title":"Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation"},{"paperId":"3ba96d6f6f4a828b6b32812d7867d86897b88df1","title":"Morphologically-Guided Segmentation For Translation of Agglutinative Low-Resource Languages"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"f5dfed82b0c8747e41a1206f52a6d0ea3dce4a5c","title":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"},{"paperId":"21079df28af037f93620a0bd63f04fe49cf5df7d","title":"On the Importance of Tokenization in Arabic Embedding Models"},{"paperId":null,"title":"European Language Resource Association"},{"paperId":"e547f4c0728552c1c41df44b7c6469198cf44924","title":"MorphoBERT: a Persian NER System with BERT and Morphological Analysis"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Exploring BERT's Vocabulary. Blog Post"},{"paperId":null,"title":"Ud_chinese-gsd. GitHub repository, 2016"},{"paperId":"82cf69e48ede65b9d1f419da786c0349342d449d","title":"A Gold Standard Dependency Corpus for English"},{"paperId":"6f59b8056fd7367f5b5088d9c566d6849ea8a663","title":"Hindi Syntax: Annotating Dependency, Lexical Predicate-Argument Structure, and Phrase Structure"},{"paperId":"944e7729088c5f36cbe0b0801ca0fea432dc5326","title":"Pango, an open-source Unicode text layout engine"},{"paperId":null,"title":"for an overview of the rendering pipeline"}],"id":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","summary":"PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels, and is more robust to noisy text inputs than BERT, further confirming the benefits of modelling language with pixels."},{"url":"https://www.semanticscholar.org/paper/969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":84,"citationCount":118,"influentialCitationCount":14,"publicationDate":"11/03/2021","authors":"J. Clark,Dan Garrette,Iulia Turc,J. Wieting","citations":[{"paperId":"5f3d6c077712a2d692bb29671c084e098854c738","title":"Learning Mutually Informed Representations for Characters and Subwords"},{"paperId":"fd665a3d8c4a5fde1e1b00f78ab231d99b22abd0","title":"Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew"},{"paperId":"d921732ca5920049b6a8194f559917689f02d096","title":"Text Rendering Strategies for Pixel Language Models"},{"paperId":"d9655aad891e0dba404bc132c8419f93c098c8f8","title":"Learning to Abstract with Nonparametric Variational Information Bottleneck"},{"paperId":"17d0cc24fd6a267e7fd59ae62054de3e5c552096","title":"Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"cd70bbd80f4e07b18ee2cc339e13feb15b87e934","title":"Optimized Tokenization for Transcribed Error Correction"},{"paperId":"997cebb936d88577da59ba460a9141bdd5dcb36f","title":"To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer"},{"paperId":"64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?"},{"paperId":"c30deb8f4b0e942a36e5ef5c87bed01f575c0c6e","title":"Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention"},{"paperId":"512726165de86576dd2d408a1393e0dfa79b3a96","title":"Melody-conditioned lyrics generation via fine-tuning language model and its evaluation with ChatGPT"},{"paperId":"d28d94dd981e55003230e18c931387da29ebb47b","title":"Assessment of Pre-Trained Models Across Languages and Grammars"},{"paperId":"bf4d4f3bbd4972cc24547939540a4af1dbf0afa8","title":"A multimodal deep learning architecture for smoking detection with a small data approach"},{"paperId":"ff50383c09346e3e7d16856af3519fa09a9f8580","title":"Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning"},{"paperId":"5bd254c0775d18cdc30cb85be61e080484ee6713","title":"Multilingual Text Representation"},{"paperId":"1943144f998c2620616b3e89d376ff801b6e04c6","title":"Lightweight Adaptation of Neural Language Models via Subspace Embedding"},{"paperId":"cee0fabddbb9d84ff2f5aa139bc57055082525c1","title":"Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT"},{"paperId":"f3e5822d5ea26b7026e75802d3ce05f4eba4243a","title":"Character As Pixels: A Controllable Prompt Adversarial Attacking Framework for Black-Box Text Guided Image Generation Models"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code"},{"paperId":"d57cbec10dfee4e244657d3cdf9ba4cf53086744","title":"An Empirical Analysis Towards Replacing Vocabulary-Rigid Embeddings by a Vocabulary-Free Mechanism"},{"paperId":"30f36f68265823c7f9945f902451fe0b1fac790b","title":"Biomedical Language Models are Robust to Sub-optimal Tokenization"},{"paperId":"af353dcee7ace858fc7975b631d31d30cbf192f2","title":"DGA domain detection using pretrained character based transformer models"},{"paperId":"116a43e7457129f005d60517f1eaa701404894a0","title":"Automating intended target identification for paraphasias in discourse using a large language model"},{"paperId":"5cd4fe8174b582f27d79859e4b79e0eba2a1d5f0","title":"Is Anisotropy Inherent to Transformers?"},{"paperId":"940e3ee32edfe425e958a672644a0bab2fa30ddc","title":"When Vision Fails: Text Attacks Against ViT and OCR"},{"paperId":"e4a95f595b5d60a0858725996b9355f7275492cf","title":"Hierarchical Attention Encoder Decoder"},{"paperId":"ec18aef9ccec70b979d6ab3c78d3721736fd5388","title":"Where’s the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation"},{"paperId":"29a54c072bcf85fa6934b6f701962a7c9aeb6489","title":"Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora"},{"paperId":"33c017b6298dd6f7d099f88f9667a6ea97131dbc","title":"mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models"},{"paperId":"d7a379254ac2dee2e31dfdeedb440176f0285aa5","title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding"},{"paperId":"b3cff6abe401a244c21d4706b0931e48acaeeb4e","title":"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models"},{"paperId":"6237a49297f45ebfdeabbbf67d06de323d1fccd4","title":"Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer"},{"paperId":"879a7f5abdb7ab803d48172d4f0830965f989d46","title":"Language Model Tokenizers Introduce Unfairness Between Languages"},{"paperId":"412e266cddfd87c79087a88ba1e4d11b89a45a13","title":"MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers"},{"paperId":"5079e188ae86d8330414edb65e898ed306ac450d","title":"Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation"},{"paperId":"ea8197cb357af6f89e8b6e5548897236a24719b1","title":"What is the best recipe for character-level encoder-only modelling?"},{"paperId":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!"},{"paperId":"0a438980ac42451d6d32dd2ad8ead7b55520408d","title":"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning"},{"paperId":"67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation"},{"paperId":"421e7acec740e42a95279de46d194b5c49123482","title":"Sentiment Analysis From Twitter About Covid-19 Vaccination in Indonesia Using Naive Bayes and Xgboost Classifier Algorithm"},{"paperId":"62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c","title":"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models"},{"paperId":"6f17ca0bf326da71614c528179c2a56b1ceba02e","title":"Curriculum Script Distillation for Multilingual Visual Question Answering"},{"paperId":"0351167c875b8931366e85eb5e517819d7db80cc","title":"What Makes for Good Tokenizers in Vision Transformer?"},{"paperId":"7cdebb73662387d9040da4f27a7dc04dbffa3c3e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy"},{"paperId":"5e52d654fd31f04c1bd884cd5480e6af8c95ad50","title":"Efficient Transformers with Dynamic Token Pooling"},{"paperId":"9769992ca9ff1c3894d19f4bd2b6dac27ed1befd","title":"A review on abusive content automatic detection: approaches, challenges and opportunities"},{"paperId":"4061a9941fa0ff106e884272d9ed753650417ec4","title":"Collateral facilitation in humans and language models"},{"paperId":"9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages"},{"paperId":"fc78d652c4d395ba2737ab0406bc53fd025d4aad","title":"Detecting Languages Unintelligible to Multilingual Models through Local Structure Probes"},{"paperId":"dd568e6838903ad7c381f13c1268c94c5db08b02","title":"Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing"},{"paperId":"10672baf790962195677c7581a2fe984032e7f98","title":"Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"3e08d84af266026e7d254729f3afc6dcd572d264","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages"},{"paperId":"7bd8859b5920c7b769e6d40dbdbcd857c1770401","title":"Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"8cf974fd3973900c0598730ee5d3617900ac8c3d","title":"Transformers with Learnable Activation Functions"},{"paperId":"ba9a592438447c2a35afc203be40f7f0a2d3fb5a","title":"A Compact Pretraining Approach for Neural Language Models"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"0232715f9089e3a2fc002cff6737bb9939805b8d","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"24fbe3af030f393e55bda3dc7dae0f57bd270d04","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"0376c01a9320027694f2dca57236c27f0c5b36eb","title":"Multilingual Abusiveness Identification on Code-Mixed Social Media Text"},{"paperId":"7ed63a4f928fc126f6780d27e75fa6447a00a8c4","title":"Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference"},{"paperId":"12809bcb734beafeb47876f42e7b438e27fe99fe","title":"General-purpose, long-context autoregressive modeling with Perceiver AR"},{"paperId":"7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models"},{"paperId":"2ec281d654f622da7267d7da8145e96bb77a9ede","title":"An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e43a360728e34e43934707665db0f7be02c8e5a7","title":"Interpreting BERT-based stance classification: a case study about the Brazilian COVID vaccination"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5","title":"Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study"},{"paperId":"5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"bf1c9c518f9bf6ab61db2695f102794164e3176b","title":"ÚFAL Submission for SIGTYP Supervised Cognate Detection Task"},{"paperId":"fd53fd29c919a2dd40d54649c5930a2ba5e1ee71","title":"Mixed Orthographic/Phonemic Language Modeling: Beyond Orthographically Restricted Transformers (BORT)"},{"paperId":"d53d4ff9f3bda51534184344845a78f8c02badd9","title":"DiatopIt: A Corpus of Social Media Posts for the Study of Diatopic Language Variation in Italy"},{"paperId":"035315281c72763a3e0956775732e64f5f193d82","title":"Natural Language Generation with Pixels"},{"paperId":"4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations"},{"paperId":"610088e1d7a8e44f921b2c4894fe1a7b204ce5a9","title":"Vicomtech at LivingNER2022"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"12fec03c538c7aaeca1a4e1ab8d66aed5f793fc0","title":"reamtchka at SemEval-2022 Task 6: Investigating the effect of different loss functions for Sarcasm detection for unbalanced datasets"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"b1be10b76314ea8569249f2310f1e6eead8a5adc","title":"Building Domain-specific Corpora from the Web: the Case of European Digital Service Infrastructures"},{"paperId":"e11d8663ccf2cc412f408853fa5f19ebac75df54","title":"How to encode arbitrarily complex morphology in word embeddings, no corpus needed"},{"paperId":"3d3f8399d625238fddb366697acb73446129d65c","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing"},{"paperId":"58403c6bc061670f7ce6267a3a0cd01ba1bb8e50","title":"Intérêt des modèles de caractères pour la détection d’événements (The interest of character-level models for event detection)"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"},{"paperId":"dca4128a33ca22c02031b5c0c28548a0df022d80","title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"b62a56f629f77acce9ea69456d67dd77de1c7dfb","title":"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization"},{"paperId":"2dce73e4a3e19d71249fc7a53c2a9531daaff839","title":"Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention"}],"references":[{"paperId":"b57da3ccf214e8dad49116c8db9590c2c89629f5","title":"MasakhaNER: Named Entity Recognition for African Languages"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"9ed25f101f19ea735ca300848948ed64064b97ca","title":"Random Feature Attention"},{"paperId":"495b0886a90f33c1a5a1f8fad4cc4c0bffd2b066","title":"Character-level Representations Still Improve Semantic Parsing in the Age of BERT"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"3fbf6339273c50b04e886fa9bd4ad18c952a683d","title":"Rethinking Attention with Performers"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"2a218786f4615b82389f78472e7ff22e6ce57490","title":"ConvBERT: Improving BERT with Span-based Dynamic Convolution"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b1a71677a13299755a12375f0c982484088aa9ef","title":"English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"56676aef356ebb13cba77fc9e4d70760fbc151f5","title":"ETC: Encoding Long and Structured Inputs in Transformers"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b","title":"RobBERT: a Dutch RoBERTa-based Language Model"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"8912eaad7d9c779bd698c73409e410fdf8c5e3c2","title":"PRADO: Projection Attention Networks for Document Classification On-Device"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"8a7e9f91d949764d6159c114d4feb961ec07b32d","title":"Training Hybrid Language Models by Marginalizing over Segmentations"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"1d51b59fcf0297e8df931c8b614bd55165b24172","title":"Better Character Language Modeling through Morphology"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"bc789aef715498e79a74f857fa090ece9e383bf1","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"},{"paperId":"e07f2c383a34217a9403a4d58a4e7d61e57d9163","title":"Character Eyes: Seeing Language through Character-Level Taggers"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"bae055edb1d552c24bbea56556bafdde9ef61c50","title":"On the Strength of Character Language Models for Multilingual Named Entity Recognition"},{"paperId":"b23300ff8ba9fcd82e349da03a6f13386bff0077","title":"What do character-level models learn about morphology? The case of dependency parsing"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"5b1516c87818084dc5d195cc274e1ee8923210d2","title":"Neural Cross-Lingual Named Entity Recognition with Minimal Resources"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"4dd9d67a0259eef54ca32770db24fab5e42d362a","title":"Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction"},{"paperId":"771c1cb5fb161231e9aaa0a189caba672256a880","title":"Using Morphological Knowledge in Open-Vocabulary Neural Language Models"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"fc7e6502dace26305e3e062e426034f61a18095e","title":"Byte-Level Machine Reading Across Morphologically Varied Languages"},{"paperId":"9852ae077c7da6a9d178acaa2b44a335289507a6","title":"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model"},{"paperId":"0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b","title":"Neural Lattice Language Models"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"07466fb914982f55051cc0b236fd524bdcd8bdc7","title":"Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"45e3f381d16e6d6db5449b7a44b7bdf294a8a822","title":"Multiscale sequence modeling with a learned dictionary"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a921e014249395496c22783fe58d293746af852b","title":"From Characters to Words to in Between: Do We Capture Morphology?"},{"paperId":"ed5003e6a2b97dd4b65c5190ccb88b9c45b032b8","title":"Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"},{"paperId":"664ec878de4b7170712baae4a7821fc2602bba25","title":"Learning to Generate Reviews and Discovering Sentiment"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"12e9d005c77f76e344361f79c4b008034ae547eb","title":"Charagram: Embedding Words and Sentences via Character n-grams"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"4dabd6182ce2681c758f654561d351739e8df7bf","title":"Multilingual Language Processing From Bytes"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"fdb813d8b927bdd21ae1858cafa6c34b66a36268","title":"Learning deep structured semantic models for web search using clickthrough data"},{"paperId":"31100d7f8fdd3d263238624a888dab0f7aba3d5a","title":"Adaptor Grammars for Learning Non-Concatenative Morphology"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"727209a57c643472c8fc166ba3cc373936acc8d0","title":"Learning a Part-of-Speech Tagger from Two Hours of Annotation"},{"paperId":"93c20e38c85b69fc2d2eb314b3c1217913f7db11","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"9fae0f18e23db076e27b23f17a417f3149f63e2e","title":"TweetMotif: Exploratory Search and Topic Summarization for Twitter"},{"paperId":"e9d100404934e025a2df61882faf37ae2031af03","title":"Bloom Maps"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"502858989368adae91e0f4a643ebc2aa6efd5738","title":"Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"d619570b03e629653d69b5157764be183e8521bf","title":"UnihanLM: Coarse-to-Fine Chinese-Japanese Language Model Pretraining with the Unihan Database"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"b959e905c093e1aa4a27eda0c0f1b4f727b93e63","title":"Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification"},{"paperId":null,"title":"Open-Vocabulary Representations From Char-acters"},{"paperId":null,"title":"Language-wise breakdown for TYDI QA primary tasks"}],"id":"969287b8a96e242793b11f0dbb99ec341228106f","summary":"Canine is presented, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias."},{"url":"https://www.semanticscholar.org/paper/59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization","venue":"Findings","year":2020,"referenceCount":48,"citationCount":38,"influentialCitationCount":7,"publicationDate":"27/08/2020","authors":"Xinsong Zhang,Hang Li","citations":[{"paperId":"5e09ca61ee13140397b64fdb44e2461067b04114","title":"The Less the Merrier? Investigating Language Representation in Multilingual Models"},{"paperId":"d4a35746eee94efae6e0aff8fb751b347e6df986","title":"A BERT-Based Named Entity Recognition Method of Warm Disease in Traditional Chinese Medicine"},{"paperId":"c5519e0f560c12c0da610258098d1692b79a0c83","title":"An adversarial-example generation method for Chinese sentiment tendency classification based on audiovisual confusion and contextual association"},{"paperId":"e788c7a6a2d8fb32874b92e854ff7e063939ebea","title":"BiLGAT: Bidirectional lattice graph attention network for chinese short text classification"},{"paperId":"693cea1845a5002435a23bd4e817e4b9614e06ff","title":"Retrospective Multi-granularity Fusion Network for Chinese Idiom Cloze-style Reading Comprehension"},{"paperId":"eba0b320f2ac67ba9a0c7cc46a46c49604ea98f0","title":"Classical Chinese poetry generation from vernacular Chinese: a word-enhanced supervised approach"},{"paperId":"02c1bd7ff0f7ffacc7e3589f32ef0acc52fd8a47","title":"A hybrid approach of Poisson distribution LDA with deep Siamese Bi-LSTM and GRU model for semantic similarity prediction for text data"},{"paperId":"74e7edf7c6436bb4992ca9c9df9476c9ccf31919","title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks"},{"paperId":"8db5adadc0ab39f95a26a2eb6499d340d6c5ea21","title":"From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation"},{"paperId":"1d08b41de815c79d3fb874ddb159fbe21828c024","title":"VSCA: A Sentence Matching Model Incorporating Visual Perception"},{"paperId":"beb40cc99a6ab931aa8ea1758c1a0397ecd32847","title":"Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling"},{"paperId":"97d1fb985af4e81e6ad80dbd79dfaa39cfa6017d","title":"mmLayout: Multi-grained MultiModal Transformer for Document Understanding"},{"paperId":"cd4d177a8e5a042a6ece94c72379f1656387f149","title":"Creating a tokenization algorithm based on the knowledge base for the Uzbek language*"},{"paperId":"31d6e59dc275f33da71a72b6b86b38daeed9ffaa","title":"ERNIE-mmLayout: Multi-grained MultiModal Transformer for Document Understanding"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"f8a2428bee464ae69cdb25ce5c2259e88ce576b4","title":"Clickbait Detection of Indonesian News Headlines using Fine-Tune Bidirectional Encoder Representations from Transformers (BERT)"},{"paperId":"05ea28584e5db18c0c31d1aac40e9c1905327557","title":"Effectiveness of Fine-tuned BERT Model in Classification of Helpful and Unhelpful Online Customer Reviews"},{"paperId":"0de580957d23dd65e31b6c95e6bc5d15bc15c57d","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"8e1227b0d58b769a0919f8debdfd093ff6f6a65a","title":"MarkBERT: Marking Word Boundaries Improves Chinese BERT"},{"paperId":"18dc24da603896db2264e9174116407a95c593d5","title":"“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"4481f897a0d8a753ccc27b8dd41add2d44e8f040","title":"An Empirical Study on Adaptive Inference for Pretrained Language Model"},{"paperId":"976f47bade21fd787f029142b39631cb17f16ec2","title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation"},{"paperId":"5722d101859846a6a023b8fa00830742203cd0c1","title":"Span Fine-tuning for Pre-trained Language Models"},{"paperId":"27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","title":"LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization"},{"paperId":"4690eb050572a279f94560b6bbdccaae577b45f5","title":"MVP-BERT: Multi-Vocab Pre-training for Chinese BERT"},{"paperId":"5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"964da4ed9ac61b12bc2a7adc1e94e3964cc63861","title":"Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question Answering Data"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"518cb6d4247bdebf21e2811f296b0c7372602a0a","title":"PMI-Masking: Principled masking of correlated spans"},{"paperId":"4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"35eeeefcbea6fa8edae4c310ee5ee717861c7e60","title":"Improving Constituent Representation with Hypertree Neural Networks"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"923d2376103dbc9e9b2af4518b56299d7630b46b","title":"J URASSIC -1: T ECHNICAL D ETAILS AND E VALUATION"}],"references":[{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"e3794413679237f7a9a2f7e03eb7ea2ccac0ae93","title":"Synthesizer: Rethinking Self-Attention for Transformer Models"},{"paperId":"a238109c3969ae681eee0d4f1bf2012f28850593","title":"Synthesizer: Rethinking Self-Attention in Transformer Models"},{"paperId":"913e9384a1440dbf860f6d8597953187237fa1b4","title":"Disagreement-Regularized Imitation Learning"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"6007bd2a34385132a7885b934d90b519a1f65bba","title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"d56c1fc337fb07ec004dc846f80582c327af717c","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training with Whole Word Masking for Chinese BERT"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"0de0a44b859a3719d11834479112314b4caba669","title":"A Multiscale Visualization of Attention in the Transformer Model"},{"paperId":"e278e072774f23675266881750e20bca74804cb9","title":"ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"ae43556f2cc1cecb8b48762b4e09df319fbaa4d9","title":"Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"1b07a24b81834116f6ad1d0232485ba81b9445f3","title":"Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"},{"paperId":"c34b7796a666f5e205693f6bd1e25e993db19075","title":"Probing Prior Knowledge Needed in Challenging Chinese Machine Reading Comprehension"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","title":"Neural Network Acceptability Judgments"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"883d1d06d857a85a0e64bb19f0b17d56f2cc9d7b","title":"KenLM: Faster and Smaller Language Model Queries"},{"paperId":null,"title":"Electra: Pre-training text encoders as discriminators rather than generators"},{"paperId":null,"title":"2e-5 1.0 C DATA AUGMENTATION To enhance the performance, we conduct data augmentation for the three Chinese classification tasks of TNEWS, CSL, and CLUEWSC2020"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Openweb-text corpus"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"2019) is a large-scale Chinese IDiom cloze test. C3 (Sun et al., 2019a) is a free-form multiple-choice machine reading comprehension for Chinese"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"0f8468de03ee9f12d693237bec87916311bf1c24","title":"The Seventh PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"db8885a0037fe47d973ade79d696586453710233","title":"The Sixth PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"351ec42df2b60c6042addf96e6b98673bbaf4dfd","title":"The Fourth PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"475354f10798f110d34792b6d88f31d6d5cb099e","title":"Automatically Constructing a Corpus of Sentential Paraphrases"},{"paperId":null,"title":"We adopt the standard hyper-parameters of BERT in pre-training of the models. Table 8 shows the hyper-parameters in our"}],"id":"59c0076b3d814588e320820b95563965733d1875","summary":"This paper proposes a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine- grained and coarse-grains tokenizations, which outperforms the existing best performing models in almost all cases."},{"url":"https://www.semanticscholar.org/paper/485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation","venue":"Machine Translation","year":2018,"referenceCount":26,"citationCount":28,"influentialCitationCount":1,"publicationDate":"19/10/2018","authors":"Elizabeth Salesky,Andrew Runge,Alex Coda,J. Niehues,Graham Neubig","citations":[{"paperId":"d77885e0478c318df7a271674dce04349601e80f","title":"Morpheme-Based Neural Machine Translation Models for Low-Resource Fusion Languages"},{"paperId":"e4d2ba4cb6f364079ce3731e86b860264677ae7d","title":"Downstream Task-Oriented Neural Tokenizer Optimization with Vocabulary Restriction as Post Processing"},{"paperId":"e326d731d19c642ea7b4b9f50a2a2d1261f4c1fe","title":"Hyper-parameter optimization in neural-based translation systems: A case study"},{"paperId":"9d83941a205e31d394f614424cdc553cea9e35f9","title":"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages"},{"paperId":"6bf23a80dd5f0089cca94d6fd26802163bcf1d17","title":"Traitement de l’ambiguïté syntaxique et sémantique en TA neuronale : analyse de la traduction de l’anglais vers le français, l’espagnol et l’italien"},{"paperId":"19b7c860128e5461d451b3d15f3836a9e1680ffc","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey"},{"paperId":"24fd022750ea88e44e6790c7c7e1923635885c71","title":"Translation Mechanism of Neural Machine Algorithm for Online English Resources"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"38b40ae531ddca434de07015637b78413370d15a","title":"The Roles of Language Models and Hierarchical Models in Neural Sequence-to-Sequence Prediction"},{"paperId":"405cd5cd6e056675d4545eba12742174cc75d7e7","title":"Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation"},{"paperId":"debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","title":"Adversarial Subword Regularization for Robust Neural Machine Translation"},{"paperId":"4d08dcd2cc1e9691defe664a10f021424a896a1e","title":"Neural Machine Translation: A Review"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation"},{"paperId":"95236a88cc959656958d7a49422f4004015d594d","title":"Comparison between NMT and PBSMT Performance for Translating Noisy User-Generated Content"},{"paperId":"fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a","title":"Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation"},{"paperId":"d680dd4b0e528b6c575fb210db6971c124210b20","title":"Finding the Optimal Vocabulary Size for Turkish Named Entity Recognition"},{"paperId":"6dc710b46bc510a42af487a3ac220e4fabf4d518","title":"VOLT: Improving Vocabularization via Optimal Transport for Machine Translation"},{"paperId":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019"}],"references":[{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"491d29cb0018578b47e9abe19b93d5b65dc59113","title":"Improving Character-Based Decoding Using Target-Side Morphological Information for Neural Machine Translation"},{"paperId":"b36a5bb1707bb9c70025294b3a310138aae8327a","title":"Automatic differentiation in PyTorch"},{"paperId":"df1769afbbf3904877629dd7e785f195361ec531","title":"Lifelong Learning with Dynamically Expandable Networks"},{"paperId":"69a037af886a6235d9af3cdeef4c7233d34c86ce","title":"Growing a Brain: Fine-Tuning by Increasing Model Capacity"},{"paperId":"1f080959faf9bf2a282c0aadbd8584d8b32f6e24","title":"Modeling Target-Side Inflection in Neural Machine Translation"},{"paperId":"69ffe3277011087b55afaeaef0a3933160beee9a","title":"Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English"},{"paperId":"100104b980d56a40cadfbd7034fa7807ce49b3fb","title":"Nematus: a Toolkit for Neural Machine Translation"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"16cb6876666f3a7b56a636c1d85ad00bd0d98bf3","title":"Net2Net: Accelerating Learning via Knowledge Transfer"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"6dab1c6491929d396e9e5463bc2e87af88602aa2","title":"Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"88b66f705a329da8292e7b8aa4bfe26de4759cfa","title":"Machine Translation without Words through Substring Alignment"},{"paperId":"791c0df5557890e7a4d8c81b12cd966531e7b42c","title":"Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability"},{"paperId":"8f4987d648d0daf3e36172e1abcda796fe8cf865","title":"An exponential translation model for target language morphology"},{"paperId":"42fc4c6580bfa54d57b5d6c56b5dfde58c6f6abb","title":"English-to-Czech Factored Machine Translation"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":null,"title":"Autodiff Workshop: the future of gradient-based machine learning software and techniques"},{"paperId":"319af0958e268b3243975b8628262ebc1980ce40","title":"of the European Chapter of the Association for Computational Linguistics"},{"paperId":null,"title":"Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations"},{"paperId":null,"title":"Lin-guistics"}],"id":"485b3f77b9913e151e7ca897d99497e70e7f30d1","summary":"This paper incrementally introduces new BPE vocabulary online based on the held-out validation loss, and matches the results found with grid search, optimizing segmentation granularity while significantly reducing overall training time."},{"url":"https://www.semanticscholar.org/paper/ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features","venue":"International Conference on Computational Linguistics","year":2018,"referenceCount":19,"citationCount":16,"influentialCitationCount":0,"publicationDate":"12/04/2018","authors":"Makoto Morishita,Jun Suzuki,Masaaki Nagata","citations":[{"paperId":"2dcdc4ef997507630f2c9b1b6682646ae31bdb7f","title":"SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models"},{"paperId":"2e6028f8b156c8b344e1a68d15d88403a978c71d","title":"Learning Multiscale Transformer Models for Sequence Generation"},{"paperId":"188f913be48a218b44b0bd964662b4926fd0b0b8","title":"Neural Machine Translation: A Review of Methods, Resources, and Tools"},{"paperId":"ca83469977ab49baae02ef1e12f419120927ecdc","title":"Mixed-Level Neural Machine Translation"},{"paperId":"fac7f9b04fba4889b445e309d384644d15ee7e88","title":"Trends and Advances in Neural Machine Translation"},{"paperId":"debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation"},{"paperId":"9f3f6deeb1f03ebd52f9ab44275ad382fe60d073","title":"Latent Part-of-Speech Sequences for Neural Machine Translation"},{"paperId":"0aef56962035a79101821480f51897fdc4443945","title":"Character n-gram Embeddings to Improve RNN Language Models"},{"paperId":"0ab0fda8774c303be8f8f8c8f684a890dcf5d455","title":"Lattice-Based Transformer Encoder for Neural Machine Translation"},{"paperId":"054f8635d1bd259464e6c0c8a78281de46b4f01d","title":"Neural Machine Translation"},{"paperId":"c68959f4da94fd35da5e8649465177b24d0dd351","title":"Byte-based Multilingual NMT for Endangered Languages"},{"paperId":"bfbfeb0613beae296d76077b2adc9e38a6b678a4","title":"Sub-Subword N-Gram Features for Subword-Level Neural Machine Translation"},{"paperId":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019"}],"references":[{"paperId":"56cf69bbe6598471d1655a4d0ccd4a61e946a532","title":"NTT Neural Machine Translation Systems at WAT 2017"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627","title":"Six Challenges for Neural Machine Translation"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"77e2a8c7afa8694b5a96e1126b8ef9e8d7bf3791","title":"Kyoto University Participation to WAT 2016"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"3bc6bcb60c7c00efcb471191fb62fd3ebd231d66","title":"Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions"},{"paperId":"3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70","title":"Controlling Output Length in Neural Encoder-Decoders"},{"paperId":"46d0aa6b357c5427f46c7f8ff7053617c4309649","title":"Linguistic Input Features Improve Neural Machine Translation"},{"paperId":"7a67159fc7bc76d0b37930b55005a69b51241635","title":"Abstractive Sentence Summarization with Attentive Recurrent Neural Networks"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"09cd7876b72d6105c83db59052572433a0a2b36c","title":"WIT3: Web Inventory of Transcribed and Translated Talks"}],"id":"ab139e341c005929848a7326f3d44f8a6aa9863c","summary":"It is confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets and the assumption that in the NMT model, the appropriate subword units for the following three modules can differ is confirmed."},{"url":"https://www.semanticscholar.org/paper/205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training","venue":"","year":2022,"referenceCount":61,"citationCount":6,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Jing Huang,Zhengxuan Wu,Kyle Mahowald,Christopher Potts","citations":[{"paperId":"5f3d6c077712a2d692bb29671c084e098854c738","title":"Learning Mutually Informed Representations for Characters and Subwords"},{"paperId":"97cea53d979305e6686387ff0ee38a15e3a5ba46","title":"Rigorously Assessing Natural Language Explanations of Neurons"},{"paperId":"fcf36a99a8eddc4a29471507d4f3ab31580baa6c","title":"Causal interventions expose implicit situation models for commonsense language understanding"},{"paperId":"00337d7d7fc679cb4959aead3204b0fb9c6a5ef2","title":"ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning"},{"paperId":"3f7fa58806614a1f38ae760c25c1305e3a87d4ce","title":"Interpretability at Scale: Identifying Causal Mechanisms in Alpaca"},{"paperId":"cb1b5d949a1d711b1bd3a2924e045ad3f893252c","title":"Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations"}],"references":[{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"e638b9369e9b588b1c4fcfeee6409e51b4625f9b","title":"Causal Proxy Models for Concept-Based Model Explanations"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"32afdb07021fda775ceaedd231c58bfed0aa980a","title":"Automated Crossword Solving"},{"paperId":"be9bb35bc3dc5630ff09895129d4d515aa94ed97","title":"AmbiPun: Generating Humorous Puns with Ambiguous Context"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"075054b8038dd34089b54e20f2aa4c402d0bd29c","title":"Causal Distillation for Language Models"},{"paperId":"ccd04c27bf1237368b35eb456b3dd1c18ef9a9b9","title":"Inducing Causal Structure for Interpretable Neural Networks"},{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"5951ede73a1dbb9c5ea8b4d95f31c5ed646aacbd","title":"Causal Abstractions of Neural Networks"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"8b723be33e62bf5bd9278769244f1c13a9510898","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"538f8e8a36e70ca408f2c5fb6f10f303c52fc317","title":"Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language"},{"paperId":"2d9fa173baa6e07112eaa0ae89b85f37cc2c000a","title":"Homophonic Pun Generation with Lexically Constrained Rewriting"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"03826718c8c47d8f1f25e437fd6ff15165162e8c","title":"Will it Unblend?"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e8e2876b446b08a4a461aae389f72b7edfd62c3a","title":"Approximate Causal Abstractions"},{"paperId":"3cfd09d6d14fb9a34ee72a487cd610d168a60530","title":"Attentive Mimicking: Better Word Embeddings by Attending to Informative Contexts"},{"paperId":"604347b9d876b267f8a296c1a2599bdffd2d7eff","title":"Abstracting Causal Models"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"68c03788224000794d5491ab459be0b2a2c38677","title":"WordNet: A Lexical Database for English"},{"paperId":"fdbb252f29ee0b72fc5467c0ae11f7cb30149f46","title":"Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"},{"paperId":null,"title":"Language Technologies, Volume 1 ("},{"paperId":null,"title":"Responsible NLP Checklist A For every submission: A1. Did you describe the limitations of your work? Yes"},{"paperId":null,"title":"Did you discuss any potential risks of your work? Yes, in the required limitation section"},{"paperId":null,"title":"coverage of domains, languages, and linguistic phenomena, demographic groups represented"},{"paperId":null,"title":"B Did you use or create scientific artifacts? Section 3, 4, 5, Appendix A, and Appendix B"},{"paperId":null,"title":"Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Section 5 and Appendix A"},{"paperId":null,"title":"2022. Faithful, interpretable model explanations via causal abstraction"},{"paperId":null,"title":"error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?"},{"paperId":null,"title":"for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used"},{"paperId":null,"title":"(cid:3) 7 Did you use human annotators (e.g., crowdworkers) or research with human participants?"},{"paperId":null,"title":"Please follow the instructions to manipulate the characters of the INPUT string and generate the desired OUTPUT string"},{"paperId":null,"title":"Pre-trained T5-small and ByT5-small models: Both models are distributed under the Apache 2.0 License 1617 . We download the models from Huggingface"},{"paperId":null,"title":"Did you discuss the license or terms for use and / or distribution of any artifacts? Yes, in Appendix C"},{"paperId":null,"title":"Have you used AI writing assistants when working on this paper?"},{"paperId":null,"title":"Example GPT-3 prompt (gray) and targeted GPT-3 completion (bold) for the spelling correction with context task"},{"paperId":null,"title":"Do the abstract and introduction summarize the paper's main claims? Abstract and Section 1"}],"id":"205cc15fca6963b355e4c071071368e874ee103e","summary":"While simple character-level tokenization approaches still perform best on purely form-based tasks like string reversal, this method is superior for more complex tasks that blend form, meaning, and context, such as spelling correction in context and word search games."},{"url":"https://www.semanticscholar.org/paper/353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering","venue":"","year":2022,"referenceCount":182,"citationCount":20,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Rosanne Liu,Daniel H Garrette,Chitwan Saharia,William Chan,Adam Roberts,Sharan Narang,Irina Blok,R. Mical,Mohammad Norouzi,Noah Constant","citations":[{"paperId":"b7b7a549629bbe7fa325572339938980e77d155c","title":"AnyText: Multilingual Visual Text Generation And Editing"},{"paperId":"c3b1174989748777173f25406627727073608c05","title":"Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation"},{"paperId":"ba7f09d76f465c7d5fefc87de859a491d6c5e145","title":"DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning"},{"paperId":"965b8e0311e184c3c2663b8349d627a7e972bab1","title":"Towards Diverse and Consistent Typography Generation"},{"paperId":"650d7cf842b536dc6a3f46c189b9d8e3c8cc07a1","title":"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback"},{"paperId":"d7890d1906d95c4ae4c430b350455156d6d8aed9","title":"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"},{"paperId":"a903e1e0ffd04dd666f3537f6570d742d7be3486","title":"Grounded Text-to-Image Synthesis with Attention Refocusing"},{"paperId":"02dde8e9fefafae8e0b9ee2eef0ddc74c511f7cd","title":"Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models"},{"paperId":"5fbe4c92791fbecb179c1ab79bba9a59b2e155ba","title":"GlyphControl: Glyph Conditional Control for Visual Text Generation"},{"paperId":"a553bf27d801d09f667fe121c0ba9632257f364b","title":"DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models"},{"paperId":"0cc49320f77e384a9acde7fa9c1b7c776a4f04a4","title":"If at First You Don't Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by Selection"},{"paperId":"e779781f1bea273573fc9d3f1a5e874bcff2cd2b","title":"TextDiffuser: Diffusion Models as Text Painters"},{"paperId":"c5f43f34b8023f9fa94889ba266d75f368062df3","title":"DiffUTE: Universal Text Editing Diffusion Model"},{"paperId":"a820ba23594ba25d1db21116ddb5a55c806ee30a","title":"Improving Diffusion Models for Scene Text Editing with Dual Encoders"},{"paperId":"10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7","title":"GlyphDraw: Seamlessly Rendering Text with Intricate Spatial Structures in Text-to-Image Generation"},{"paperId":"1f02ba1c6fae779ec3d003340e72eaf82351cfb9","title":"TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering"},{"paperId":"1e4b6567ff66de6cdcbe58c863538241e2f62b45","title":"Aligning Text-to-Image Models using Human Feedback"},{"paperId":"d7e0f0cec28c34710fa631df410b717186741db5","title":"A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces"},{"paperId":"1c7471996a4f2e08a5ef592a6ffcde65a034a1e4","title":"GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models Coherently"},{"paperId":"035315281c72763a3e0956775732e64f5f193d82","title":"Natural Language Generation with Pixels"}],"references":[{"paperId":"e24f4b28167b05fbf7d29000490fc0a4e4c109c7","title":"eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers"},{"paperId":"36de930fa280c5ff7ca0adc889e158d6b2716698","title":"Euler Bricks"},{"paperId":"1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe","title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"38b0567e83386ddc294d6c81b541deacbd8e3c2a","title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"10ac1ff1dd5302be6c1bb2048b2258c6c46fb3a5","title":"Fall is Here!"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"231af7dc01a166cac3b5b01ca05778238f796e41","title":"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"93c20e38c85b69fc2d2eb314b3c1217913f7db11","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"cee6a6336af0dc660e5879f8fb480d82cb56b732","title":"Russian"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":null,"title":"Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators"},{"paperId":null,"title":"Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? We closely follow the procedure of Saharia"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"f7d95e8a91d7683e74ee642f20906e7751e47dfe","title":"Association for Computational Linguistics"},{"paperId":"9d742bac14195f491ede2dc52d50dda257db01fb","title":"Shadow in Stone"},{"paperId":"90a944f337539984fa8bbf1a93f680baf5e3a519","title":"This is a newspaper"},{"paperId":null,"title":"dslr portrait of a robot is holding a sign with text \"i am not a robot"},{"paperId":null,"title":"Studio shot of words \"I like coffee because it gives me the illusion that I might be awake"},{"paperId":null,"title":"with a fork and knife sticking out of the oyster, with a caption that says \"oysters for lunch"},{"paperId":null,"title":"Time is temporary, everything is temporary"},{"paperId":null,"title":"Quails of North America"},{"paperId":null,"title":"Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?"},{"paperId":null,"title":"error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?"},{"paperId":null,"title":"Was the data collection protocol approved (or determined exempt) by an ethics review board? Our institution does not have an IRB"},{"paperId":null,"title":"a plate with a single oyster, with a fork and knife sticking out of the oyster, with a caption"},{"paperId":null,"title":"a minimalistic version of a forest with a sign saying \"help the forest\" in the foreground"},{"paperId":null,"title":"giraffe toothbrush made from wood, with the words \"giraffe\" and \"toothbrush\" in rainbow color"},{"paperId":null,"title":"a logo for a grocery store chain with the name \"grocery land\", with the g and the y are made of fruits and vegetables"},{"paperId":null,"title":"studio shot of beautiful textbook with title \"how to be a manager of managers\", white background"},{"paperId":null,"title":"the hubble telescope and the milky way"},{"paperId":null,"title":"a cartoon of a turtle with a thought bubble over its head with the words \"what if there was no such thing as a thought bubble?\""},{"paperId":null,"title":"A bowl of tomato soup with pasta letters that read \"De-licious\""},{"paperId":null,"title":"a test tube with a drop of liquid in it"},{"paperId":null,"title":"\"coffee\" made from coffee beans"},{"paperId":null,"title":"a fun and colorful illustration of a waterfall, with the word \"waterfall\" in the style of a children’s book"},{"paperId":null,"title":"A photo of a rabbit sipping coffee and reading a book. The book title \"The Adventures of Peter Rabbit\" is"},{"paperId":null,"title":"scholarly elephant reading a newspaper with the head-line \"elephants take over the world"},{"paperId":null,"title":"The word \"exquisite\" written in modern calligraphy"},{"paperId":null,"title":"a sunﬂower ﬁeld with a tractor about to run over a sun-ﬂower"},{"paperId":null,"title":"beautiful isometric word \"DRAW\" entirely made of pencils, soft smooth lighting, pastel colors, trending on polycount, modular constructivism, blue background, physically based rendering, centered"},{"paperId":null,"title":"a drawing of a badger made of mushrooms, with the word \"mushroom\" written above in glowing letters"},{"paperId":null,"title":"a map of the world with the text \"the world is your oyster\" in the middle"},{"paperId":null,"title":"a cartoon of a dog holding a telescope looking at a star with a speech bubble saying \"i wonder if there’s a dog on that planet"},{"paperId":null,"title":"a pair of scissors pointing down, and a computer with the word \"delete\" on the screen"},{"paperId":null,"title":"A retro coffee ad with the text ’Coffee is what i like’"},{"paperId":null,"title":"robot on a butter food processing line, with robot looking dejected, with an overhead red light indicating error, with robot saying \"i can’t believe it’s not butter\""},{"paperId":null,"title":"a dog with a speech bubble with the text \"woof woof\" and a translation speech bubble with the text \"other dogs do vex us\""},{"paperId":null,"title":"photo illustration of the earth being struck by multiple lightning strikes that merge"},{"paperId":null,"title":"a black and white logo on"},{"paperId":null,"title":"a blueprint of a house, with a triangle for the roof, a square for the walls, and a rectangle for the ﬂoor, and with the message"},{"paperId":null,"title":"a photo of a sea of roses all around, and a sign in the distance that"},{"paperId":null,"title":"a vintage image of the las vegas strip with the text \"las vegas\" in bold block letters"},{"paperId":null,"title":"generation model should be able to learn from just a handful of examples how to map any language pretraining into the space of images. 9 If this explanation is"},{"paperId":null,"title":"a volcano erupting, with the text \"magma\" in red"},{"paperId":null,"title":"A decorative greeting card"},{"paperId":null,"title":"a close up of a ﬁgurine of toothpaste tube, a 3D render, candy pastel, with text \"brush your teeth\" on the tube"},{"paperId":null,"title":"An antique bottle labeled \"Energy Tonic\""},{"paperId":null,"title":"a lobster in a suit and tie, holding a microphone"},{"paperId":null,"title":"a green-colored luxury car with a \"green\" sticker in the back window"},{"paperId":null,"title":"a tower with a huge \"w\" on the side, from the perspective of a person standing at the base of the tower"},{"paperId":null,"title":"beautiful photo of the alps"},{"paperId":null,"title":"A Scrabble board showing the words \"optimize\" and \"pattern\""},{"paperId":null,"title":"a painting of a landscape, with a handwritten note that"},{"paperId":null,"title":"book with \"surgery made easy\""},{"paperId":null,"title":"Letters \"VOLUME\" fully made from rainbow smoke black background, centered,"},{"paperId":null,"title":"dslr, 3-d word \"rainbow\" with rainbow fur, white background"},{"paperId":null,"title":"\"broken\" made from broken shattered black glass centered"},{"paperId":null,"title":"A rendered 3D model of the word \"Dependable\" made out of granite"},{"paperId":null,"title":"a 17th century french baroque painting of a huge female lion, with the word \"meow\" written in a speech bubble coming from her mouth"},{"paperId":null,"title":"a giant shoe"},{"paperId":null,"title":"the view from one end of a bench in a park, looking at the sky"},{"paperId":null,"title":"a mouse with a ﬂashlight saying \"i’m afraid of the dark\""},{"paperId":null,"title":"a photo of a prison cell with a window and a view of the ocean, and the word \"freedom\" painted on the glass"},{"paperId":null,"title":"Bananas arranged on a picnic table to form the message \"That’s bananas!\""},{"paperId":null,"title":"a pumpkin with a mustache and a monocle and a top hat, with the text \"you can get rich too\" in a speech bubble"},{"paperId":null,"title":"a photo of a beautiful ﬁeld of poppies with a sign that says \"no photos please\""},{"paperId":null,"title":"transparent water drops exploding under water in the shape of word \"water\", under water"},{"paperId":null,"title":"a lizard sitting on a baseball ﬁeld home plate, with the words \"made it safe\" in a speech bubble"},{"paperId":null,"title":"Topographical letters Contour made of a layered paper muted pastel colors"},{"paperId":null,"title":"a landscape of the coyote point national wildlife refuge in arizona, with a coyote sitting on a rock, with the word \"coyote\" written in sunrise colors"},{"paperId":null,"title":"A vintage postage stamp showing a painting of the Golden Gate Bridge and the text \"California\""},{"paperId":null,"title":"A hand-drawn blueprint for a time machine"},{"paperId":null,"title":"the city of toronto as seen from an airplane, with a giant cn tower in the middle of the frame, with the text \"the cn tower\" in comic sans"},{"paperId":null,"title":"ﬂowers in a beautiful garden with a text \"peace\" made by the ﬂowers, with a text \"tensions\" on the clouds in the sky 24"},{"paperId":null,"title":"Art is never ﬁnished, only continued"},{"paperId":null,"title":"A photo of a panda giving a presentation in a large conference room, with text ‘Diffusion Models’"},{"paperId":null,"title":"a picture of a dog and a cat with their heads poking out of a cage with a sign saying \"no pets allowed\""},{"paperId":null,"title":"a 3d model of a 1980s-style computer with the text \"my old habit\" on the screen"},{"paperId":null,"title":"a yellow saxophone in a rainbow-colored mist with the words \"funky mist\" that looks like musical clouds of smoke"},{"paperId":null,"title":"studio shot of vines in the shape of text ’knowledge is power’ sprouting, centered"},{"paperId":null,"title":"Two llamas dancing the mambo, pointing to a sign that says \"Llama Mambo\""},{"paperId":null,"title":"picture of two hands, one holding a heart, the other holding a lightning bolt"},{"paperId":null,"title":"A professionally designed logo for a bakery called Just What I Kneaded"},{"paperId":null,"title":"marquee billboard with \"my fear of moving stairs is escalating"},{"paperId":null,"title":"Generative art of words \"Time is temporary, everything is temporary\", viscous smoke made from dots, rivers, graph design, white background"},{"paperId":null,"title":"a logo for the company \"diamonds\", with a diamond in the shape of a heart"},{"paperId":null,"title":"Closeup shot of light magenta, blue and paint brush-strokes of very wide translucent overlapping plastic in the shape of letter F, over white background"},{"paperId":null,"title":"a gold and black logo for the company \"moneymoney-money\", which looks like dollar signs"},{"paperId":null,"title":"a dark forest with a single light in the distance, and the text \"i’ve come to talk with you again"},{"paperId":null,"title":"A photo of a corgi with a sign that says \"I am not a real corgi\""},{"paperId":null,"title":"Slopy minimal continued line pencil hand drawing of letter Z, white background"},{"paperId":null,"title":"Photo of a robot lecturer writing the words \"Representation Learning\" in cursive on a blackboard, with math formulas and diagrams"},{"paperId":null,"title":"A meme showing a cat attacking a shoe, with the message \"I own your sole\""},{"paperId":null,"title":"a detailed drawing, of words \"Vintage lettering\", letter-ism, heavy-gauge ﬁligree, inhabited initials, medium: black pencil, revolver, ecopunk rococo, photo taken of an epic intricate, centered"},{"paperId":null,"title":"a picture of a bruised apple with the text \"apples are good for you\" in a fancy font"},{"paperId":null,"title":"The cover for the album ’Elusive Interludes’ by the band"},{"paperId":null,"title":"A hand painted wooden \"Pineapple Club\" sign in the shape of a pineapple, hanging outside a bar"},{"paperId":null,"title":"a scene with a city in the background, and a single cloud in the foreground, with the text \"contemplate the clouds\" in rounded cursive"},{"paperId":null,"title":"3-d letters \"dessert\" made from desserts, arranged on a plate,"},{"paperId":null,"title":"photo of a sign with \"having a dog named shark at the beach was a mistake"},{"paperId":null,"title":"A poster titled \"Quails of North America\", showing different kinds of quails"},{"paperId":null,"title":"balloons are ﬂying"},{"paperId":null,"title":"a fork with the word \"salad\" engraved on it in a calli-graphic font"},{"paperId":null,"title":"art installation of a chair with the text \"i got nothin\" carved into the backrest"},{"paperId":null,"title":"a bottle of hair gel with the label \"ﬂawless\""},{"paperId":null,"title":"\"Ethics 101"},{"paperId":null,"title":"a parrot on a pirate ship, with a parrot wearing a pirate hat, and the caption \"i’m the captain now"},{"paperId":null,"title":"A sign that says \"Please refrain from arguing with the chimpanzees\""},{"paperId":null,"title":"dslr shot of a pair of black and red sneakers with the word \"punk\" written in white. the background is a dark blue"},{"paperId":null,"title":"A professional logo for the crypto trading platform \"Salt-Mine\""},{"paperId":null,"title":"a pencil sketch of a tree with the title \"nothing to tree here\""},{"paperId":null,"title":"plant in a fancy pot with a \"do not touch\" sign on it"},{"paperId":null,"title":"a grumpy sunﬂower with a \"no solar panels\" sign"},{"paperId":null,"title":"photo of a restaurant \"the gas station\""},{"paperId":null,"title":"The logo for Robotrax, with metallic letters arranged in the shape of a robot"},{"paperId":null,"title":"a heart with the text \"i love you\", with the letters \"love\" made of rainbow colors"},{"paperId":null,"title":"portrait of a parrot is holding a sign with text \"no parrots were harmed in the making of this presentation\""},{"paperId":null,"title":"a bowl of alphabet cereal, with the message \"smackeroo\" written in the bowl with the cereal letters"},{"paperId":null,"title":"a photograph of a ﬁeld of dandelions with the text \"dan-delions are the ﬁrst to go when the lawn is mowed\""},{"paperId":null,"title":"A large recipe book titled \"Recipes from Peru\""},{"paperId":null,"title":"It takes AI and rain to make a rainbow"},{"paperId":null,"title":"An airplane ﬂying over a city, with the message \"Support Skywriters\" written in smoke trails"},{"paperId":null,"title":"a sculpture of a brain made from wire and paper, with the words \"deep thoughts\" written into the material of the brain"},{"paperId":null,"title":"a picture of multiple trees at various stages of development"},{"paperId":null,"title":"clown is holding a paper sign with \"Even in hard times there’s a possibility to have fun."},{"paperId":null,"title":"a composition of the taj mahal in the center of a gold leaf mandala, with the words \"place of honor\" centered at the bottom"},{"paperId":null,"title":"studio shot of word \"BEE\" made from bees, white background, in a frame made from bees"},{"paperId":null,"title":"studio close-up shot of an antique book with ’knowledge is power’ painted in gold on the cover in thick ﬂowing brushed calligraphy"},{"paperId":null,"title":"photo of a helicopter with the text \"helicopter tours\" on the side landing on a helipad in a valley with a river, trees, and mountains in the background"},{"paperId":null,"title":"crab sitting on a beach with a surfboard, the sun is a giant orange, and the sky is a rainbow, and the crab"},{"paperId":null,"title":"a grafﬁti art of the text \"free the pink\" on a wall"},{"paperId":null,"title":"a circle with the text \"inﬁnity makes me happy\""},{"paperId":null,"title":"The lowercase letter \"b\" made out of ﬁre"},{"paperId":null,"title":"aliens found in space"},{"paperId":null,"title":"Struck by Lightning Twice."},{"paperId":null,"title":"a purple ﬂower with a crown on its head and a speech bubble that"},{"paperId":null,"title":"photo of a dark cave with the word \"crazy\" carved into the wall, with a yellow light shining through the cave entrance"},{"paperId":null,"title":"Grape vines in the shape of text 'open your mind' sprouting out of a head with flowers and butterflies. DSLR photo"},{"paperId":null,"title":"a logo for the company \"imagine\", where the letters look like hands pointing up"},{"paperId":null,"title":"Studio shot of words \"the food is terrible and the portions are too small\" made from hotdogs, museum quality, framed photo"},{"paperId":null,"title":"a picture of the earth with the words \"save the earth\" in a circle"},{"paperId":null,"title":"Studio shot of book shelf in the shape of letters READ, museum quality, white background"},{"paperId":null,"title":"A hastily handwritten note that says \"I'll be back at"},{"paperId":null,"title":"a cartoon of a cat with a thought bubble saying \"this is so weird\""},{"paperId":null,"title":"studio shot, word \"wow\" in script made from rainbow colored fur, in a furry frame, white background, centered"},{"paperId":null,"title":"a black and white photo of a saxophone with the word \"jazz\" written in ﬂowing cursive"},{"paperId":null,"title":"organic, colorful, letters \"fuzzy\" made from many furry spheres of different sizes, 3-d rendering, centered in the frame"},{"paperId":null,"title":"Studio shot of sculpture of text \"cheese\" made from cheese, with cheese frame"},{"paperId":null,"title":"a picture of a powerful-looking vehicle that looks like it was designed to go off-road, with a text saying \"i'm a truck"},{"paperId":null,"title":"a painting of a ﬁeld of daisies, with the word \"danger\" written on them in red spray paint"},{"paperId":null,"title":"a photo of a ﬁsh tank with a ﬁsh inside"},{"paperId":null,"title":"A storefront with \"The world's best deli\" written on it"},{"paperId":null,"title":"Muted pastel magenta colored paint swirled in white paint in the shape of letter X, globular paint in liquid"},{"paperId":null,"title":"Minimal sculpture of word \"this is the future\" made from light metallic iridescent chrome thin wire, 3-D render, isometric perspective, ultra-detailed, dark background"}],"id":"353d6a18a222aec0be66de0b8be111fbbe67012d","summary":"This paper trains a suite of image generation models, and shows that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (the authors' DrawText benchmark), and sets a much higher state-of-the-art on visual spelling."},{"url":"https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing","venue":"ArXiv","year":2021,"referenceCount":304,"citationCount":130,"influentialCitationCount":6,"publicationDate":"12/08/2021","authors":"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha","citations":[{"paperId":"972db8f6f147c0078b3106ce84db991649e1b287","title":"Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems"},{"paperId":"0a898a2772c7c2941b83a737f1bef806710b047a","title":"Evaluating multiple large language models in pediatric ophthalmology"},{"paperId":"8aa476e50d8bbbdde08e984f405696df82ca9c77","title":"SugarViT - Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet"},{"paperId":"18d18d4ffdc070868ce06f216a2a8d040d42a4cb","title":"Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models"},{"paperId":"9ea8105a7bc03dbcde05bf953f3b90f1db61f6bd","title":"URL-BERT: Training Webpage Representations via Social Media Engagements"},{"paperId":"6b5b6384a0400c9188c3577803395b3adc94c7e9","title":"Challenges and opportunities for Arabic question-answering systems: current techniques and future directions"},{"paperId":"7dd4b7e6f5f1588eae707df8334589ffd503bc54","title":"A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models"},{"paperId":"7be93f2daf5753d13a627db9e046e558291f6dcc","title":"PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps"},{"paperId":"e7f45aa6124a5315ccf0cd607637ee15691f90c5","title":"MetaProbformer for Charging Load Probabilistic Forecasting of Electric Vehicle Charging Stations"},{"paperId":"788da23e46632cca4696a4a8286e2ea9b33a1b46","title":"GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction"},{"paperId":"880aa839e9275808573f221988944323bf40ddab","title":"Bias and Fairness in Chatbots: An Overview"},{"paperId":"7ebbc89e7d90e342793bc39cf5fa30bb63dce518","title":"A robust deep learning workflow to predict CD8 + T-cell epitopes"},{"paperId":"d2b5b86a76b3633ee189125ff4046bebafcd4a95","title":"A Lightweight Transformer for Faster and Robust EBSD Data Collection"},{"paperId":"efd4c155210317cea21741c08042eb9a049d479c","title":"An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM"},{"paperId":"7ba388fdcfc34da2dbb55786f0b70a898b9c0489","title":"Fine-tuning Multilingual Transformers for Hausa-English Sentiment Analysis"},{"paperId":"7db2732db2cd1cd571aea1dc5cc853d886ac30f3","title":"A Spatial-Aware Representation Learning Model for Link Completion in GeoKG: A Case Study on Wikidata and OpenStreetMap"},{"paperId":"1ba4da21d669b630c86079b60173664abed0c279","title":"A Side-by-side Comparison of Transformers for Implicit Discourse Relation Classification"},{"paperId":"7cd8d611fe158d51a8f6764cdc52b2ef5f1fcd0a","title":"Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study"},{"paperId":"e36b3a9b6071a237ab110219d61107f14c7275e2","title":"No Labels?: No Problem! Experiments with active learning strategies for multi-class classification in imbalanced low-resource settings"},{"paperId":"7d5657c78f3fee9756061c6a82db44db9d413e0b","title":"AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology"},{"paperId":"5fce7d9442b06cab91174fb68ba52ff6bdaa29cc","title":"A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks"},{"paperId":"7b304faf0bbbc6e17891454c2f6a32bc573f2f18","title":"DSHGT: Dual-Supervisors Heterogeneous Graph Transformer - A pioneer study of using heterogeneous graph learning for detecting software vulnerabilities"},{"paperId":"5a90a8f4ec612cef6b1bb9cf4eae897385d33c2d","title":"An Overview on Generative AI at Scale with Edge-Cloud Computing"},{"paperId":"43ec80eeb6f22431ae741796996b25ca3b6bf3e2","title":"Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting"},{"paperId":"1a91b8652baddcbf7722f3087a6e0c2d067ae6f5","title":"Multilingual Multiword Expression Identification Using Lateral Inhibition and Domain Adaptation"},{"paperId":"7609c41065711902ebf56542dd7b95f3bcb807a3","title":"Automatic Classification of Public Expenses in the Fight against COVID-19: A Case Study of TCE/PI"},{"paperId":"79f53eb251af62ecffa784ff89605610e4b14d56","title":"A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting"},{"paperId":"77904c6d1eb07dc128c4776447b7b50e7816383e","title":"TI-former: A Time-Interval Prediction Transformer for Timestamped Sequences"},{"paperId":"97f551d97f6ea4880c11f1da5bcec2277bdb56ef","title":"Sequential Transfer Learning to Decode Heard and Imagined Timbre from fMRI Data"},{"paperId":"4f0c7f4df04f07609bdb67944af2a529d5a4517b","title":"A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation"},{"paperId":"e5dff0d39324dd0bb3fa323f2d256f801043ba4a","title":"A Survey on Time-Series Pre-Trained Models"},{"paperId":"d50f2935deabfc75c93d0d210c365ca4d1ba37ca","title":"Prospects and Challenges of Large Language Models in the Field of Intelligent Building"},{"paperId":"96b75f51fa24018fad2d96878a58536048a41c3f","title":"HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models"},{"paperId":"af3acb9c57f8da06382b5e885ad797d70444827d","title":"MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers"},{"paperId":"736c66e1e6ddbf8311b000e9e6d5289edc185d8c","title":"Comparison of Transformer Models for Performance on Domain Specific Texts: A Systematic Evaluation of Intrinsic Model Performance"},{"paperId":"ddcf4ae15f6620c39dfa30b91403f00614e46fb5","title":"Self-Supervised Pretraining on Paired Sequences of fMRI Data for Transfer Learning to Brain Decoding Tasks"},{"paperId":"729bf8040870d411f9b526eae0861c10004eb746","title":"IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers and Task-adaptive Pretraining"},{"paperId":"ae1db9bf0930cc8506c2c74d3dfc72385675fa4e","title":"ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps"},{"paperId":"8d247cfc133e31047ce98c1758081b2115b62d8a","title":"Evaluation of transformer models for financial targeted sentiment analysis in Spanish"},{"paperId":"15d12739b26783e2cf38bc0cbd81557fcc078eb0","title":"Sabiá: Portuguese Large Language Models"},{"paperId":"8489b55d992e6a3ac54aec7094a42ec8e333012f","title":"SELFormer: molecular representation learning via SELFIES language models"},{"paperId":"5861df95084cf739a6ca3185d6523dd702bd1f10","title":"Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT"},{"paperId":"cd1e691d26793874c244b84f6f1f91ae0a9a2029","title":"Reviewer Assignment Problem: A Systematic Review of the Literature"},{"paperId":"b92b8a32f89f45cd771359e3351f6ddcad61450c","title":"ChatGPT Participates in a Computer Science Exam"},{"paperId":"5e3ffdb21e897afdc4301249a45363c6efb538dc","title":"Transfer learning on large datasets for the accurate prediction of material properties"},{"paperId":"a0d0002be16a1b70dcbb4b152813da55a81c2201","title":"Diffusing Graph Attention"},{"paperId":"2bb67629796b8f53d9cc573beb1b65929d3492a7","title":"A transformer with layer-cross decoding for remaining useful life prediction"},{"paperId":"a171780f04780f1dca6965e6c451915b8ff5458f","title":"Mask-guided BERT for Few Shot Text Classification"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"83f8a272b54a4611ae0dca73bfe41741b18dfb34","title":"Exploring zero-shot and joint training cross-lingual strategies for aspect-based sentiment analysis based on contextualized multilingual language models"},{"paperId":"569e323c84bcc01fc6562ea2c7084c7a2a00e703","title":"Multi-modal Machine Learning in Engineering Design: A Review and Future Directions"},{"paperId":"8539284502ae94def71366f4df6031a0a3d6e949","title":"PSST! Prosodic Speech Segmentation with Transformers"},{"paperId":"3ea31f9b80bb69537f11f2c0e7d39c97d0742e3b","title":"On the Connection Between MPNN and Graph Transformer"},{"paperId":"99c951469b1f79ffd8f44e0da5a3301d6ec10455","title":"Identifying COVID-19 english informative tweets using limited labelled data"},{"paperId":"7389b6ebbf36f4d869a02e305e2ef52ad2c92264","title":"Applications of transformer-based language models in bioinformatics: a survey"},{"paperId":"a7c244e906b8bd09c027ea5f996746b23953d15d","title":"Automated Line Labelling: Dataset for Contour Detection and 3D Reconstruction"},{"paperId":"df49486e50abdfa0ca8ecada65bc36fe747be776","title":"Correlation, path and principal component analysis of few agronomical traits in few elite lines of chickpea (Cicer arietinum L.)"},{"paperId":"3f77a071feb8118fa851265381c9d5d4db132ca9","title":"COVID-19 Fake News Detection With Pre-trained Transformer Models"},{"paperId":"78a0c1b28d3c7543ae5d25aa9080a48807ec9264","title":"CoVEffect: interactive system for mining the effects of SARS-CoV-2 mutations and variants based on deep learning"},{"paperId":"a3129f5e4d6505376f8f2661db137853a582a819","title":"Understanding Postpartum Parents' Experiences via Two Digital Platforms"},{"paperId":"9c2b7c67a4bf516aeda0b960edd4aeb281c6c053","title":"Rate Insight: A Comparative Study on Different Machine Learning and Deep Learning Approaches for Product Review Rating Prediction in Bengali Language"},{"paperId":"61c7e5590064b250ebb5a53c430aede492a4d8ab","title":"BERT for Natural Language Processing in Bahasa Indonesia"},{"paperId":"49ab76e76b0943862d6f3c6b99d78cc1296f91d2","title":"HeartBEiT: Vision Transformer for Electrocardiogram Data Improves Diagnostic Performance at Low Sample Sizes"},{"paperId":"3c861f509f9da5133648a3e8f2b891b7f7638491","title":"MaNLP@SMM4H’22: BERT for Classification of Twitter Posts"},{"paperId":"dad15404d372a23b4b3bf9a63b3124693df3c85e","title":"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers"},{"paperId":"f1f78bbb3e146874501e3c52b56cff6abf731842","title":"Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges"},{"paperId":"b9fa8a915a9f0cf3e91c1659e19ba057ac78a498","title":"PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations"},{"paperId":"13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","title":"MarianCG: a code generation transformer model inspired by machine translation"},{"paperId":"8458052b99daeb81d95716100e916bf785313b0b","title":"Research on the Classification of Policy Instruments Based on BERT Model"},{"paperId":"3a4e97478ad1b113a7f51668f05d5ba85e500f5a","title":"CPSAA: Accelerating Sparse Attention using Crossbar-based Processing-In-Memory Architecture"},{"paperId":"8971d2a43a11b4beca31afadf88d5ed8cda75297","title":"Block Format Error Bounds and Optimal Block Size Selection"},{"paperId":"f5669510806e6a671cece0920b7593adafdae7d8","title":"JoeyS2T: Minimalistic Speech-to-Text Modeling with JoeyNMT"},{"paperId":"0f3d7b2bb0b3bd3bad87a39c545d93ddfd383362","title":"Selective Token Generation for Few-shot Natural Language Generation"},{"paperId":"2227980ce08aebbf18a42d4abea42381062c4bd5","title":"Method of Transformation of Image Classification Labels into Segmentation Masks"},{"paperId":"247a218762eef55655f992285225718fdd8a8082","title":"Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines"},{"paperId":"2a672342035defd8d75b54e08597ef124c6a0172","title":"Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models"},{"paperId":"5b71f03167bea77cde1ad15cf95979f2d08f9e45","title":"TransDBC: Transformer for Multivariate Time-Series based Driver Behavior Classification"},{"paperId":"74671b4f898daf2b8234d5a4cdf2f25b81f1e4fd","title":"TIMS: A Novel Approach for Incrementally Few-Shot Text Instance Selection via Model Similarity"},{"paperId":"0ab111fce85d37389b93ea5ca8caca14c755c3cf","title":"Attention-based Dependability Prediction for Industrial Wireless Communication Systems"},{"paperId":"c27c68597065dad70cb0b4f882cd861826bf28fa","title":"Automated Construction Contract Summarization Using Natural Language Processing and Deep Learning"},{"paperId":"1429d4b6f6c19d6447d60d8bed05841c5de72b38","title":"FluSa-Tweet: A Benchmark Dataset for Influenza Detection in Saudi Arabia"},{"paperId":"0fb235cc59cd7198b5a1494157b0250cfca04386","title":"Detecting Harmful Online Conversational Content towards LGBTQIA+ Individuals"},{"paperId":"e86869d44e78d4cffd1bf1b62f2f8e56a519e23c","title":"E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation"},{"paperId":"277dd73bfeb5c46513ce305136b0e71fcd2a311c","title":"Recipe for a General, Powerful, Scalable Graph Transformer"},{"paperId":"52dccf13f442e7451f4b81db203b24e056142557","title":"TransGrasp: A Multi-Scale Hierarchical Point Transformer for 7-DoF Grasp Detection"},{"paperId":"24fbe3af030f393e55bda3dc7dae0f57bd270d04","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding"},{"paperId":"2002883eecd8f8e0c094c357defa5dcc40b081d9","title":"UMass PCL at SemEval-2022 Task 4: Pre-trained Language Model Ensembles for Detecting Patronizing and Condescending Language"},{"paperId":"7a25155364476839b6d1fc0653cd8611327ab9ba","title":"mGPT: Few-Shot Learners Go Multilingual"},{"paperId":"77c1eed5c13928d39c5b62ba6063b47b10ab6636","title":"Spatial transformer network on skeleton‐based gait recognition"},{"paperId":"94d05045e9bf69e015f5398086ac5c27a70d13e6","title":"A Comparative Evaluation Of Transformer Models For De-Identification Of Clinical Text Data"},{"paperId":"43fee02f5e63dcc85c58d74966951e89b2331489","title":"Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking"},{"paperId":"627be995900638aef82279a22013a7b03b5d732d","title":"A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification"},{"paperId":"b6adc972f4bdbbb4dc7143713d16a3408a71ef7e","title":"Automated Customer Complaint Processing for Water Utilities Based on Natural Language Processing—Case Study of a Dutch Water Utility"},{"paperId":"551dc8a18770a5807c2ea0724701c3f946bc7c0c","title":"Hindi/Bengali Sentiment Analysis Using Transfer Learning and Joint Dual Input Learning with Self Attention"},{"paperId":"7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"},{"paperId":"3e906906d475c73b6d8ce24ac5ebdac9979fd01b","title":"Video Transformers: A Survey"},{"paperId":"e475d7c3b10d548e59590902474ff99206a732f3","title":"The Dark Side of the Language: Pre-trained Transformers in the DarkNet"},{"paperId":"0c181f508ec9de8e48f62523ba8a9bcb1f51f83a","title":"Pretrained Language Models for Text Generation: A Survey"},{"paperId":"489af1ae6db21c198b6efeffc3ba68313f9bd4b3","title":"Neuromorphic Camera Denoising using Graph Neural Network-driven Transformers"},{"paperId":"6fb5dc674bf0013ad5e269b3905c0f4253c3890b","title":"Investigating Cross-Linguistic Gender Bias in Hindi-English Across Domains"},{"paperId":"1b6798695de27880009346c6c2023665139b0014","title":"Ontology-Based Question Answering over Corporate Structured Data"},{"paperId":"7c59a4e11eed57aea54d27bbb28a3e4dbc26b0f3","title":"Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling"},{"paperId":"d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","title":"A Primer on Pretrained Multilingual Language Models"},{"paperId":"420c897bc67e6f438db522d919d925df1a10aa8c","title":"AMMU: A survey of transformer-based biomedical pretrained language models"},{"paperId":"2055e2d803e7854477bb186d28e1a13f890f08f4","title":"TBMF Framework: A Transformer-Based Multilevel Filtering Framework for PD Detection"},{"paperId":"392d139040f9656b72374553dbe0d9ae70ee4eb0","title":"Effective Exploitation of Macroeconomic Indicators for Stock Direction Classification Using the Multimodal Fusion Transformer"},{"paperId":"5a9002e2092a0c832cb03aa18a631f4cc85d5b43","title":"Adjusting Dropout in Contrastive Learning of Sentence Embeddings"},{"paperId":"0567a26ccacdbe977ffc1160da1ebe746dc19228","title":"Chick Adams at SemEval-2023 Task 5: Using RoBERTa and DeBERTa to Extract Post and Document-based Features for Clickbait Spoiling"},{"paperId":"3d67937d4d4013a9951f73c56b42a9ffe1d2aceb","title":"Optimizing the Performance of Text Classification Models by Improving the Isotropy of the Embeddings Using a Joint Loss Function"},{"paperId":"15b0060ba45c5da5d74efb8aa2bccc8e0db21a15","title":"Transformers for Multi-Intent Classification and Slot Filling of Supreme Court Decisions Related to Sexual Violence Law"},{"paperId":"ce29a77010e47ed62e2e10c5654ab335e062ae22","title":"Improving Media Bias Detection with state-of-the-art Transformers"},{"paperId":"7f46a645911a57ea34522739def8bfc358afa96c","title":"Understanding Customer Requirements: an Enterprise Knowledge Graph Approach"},{"paperId":"0986eb81994647aaacb6dd879397103438751dad","title":"A template for the arxiv style"},{"paperId":"01404a50a3c0c065da3391c60dda49a0cab36251","title":"A Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques"},{"paperId":"e0a4053a8ba5883d6a3f06dae43e32bfde610c61","title":"Cross-lingual Strategies for Low-resource Language Modeling: A Study on Five Indic Dialects"},{"paperId":"049778820d158b87a326352eba41f3a8d6ed6403","title":"Interaction in Transformer for Change Detection in VHR Remote Sensing Images"},{"paperId":"e617b09412d11c27cf78db93a9de7dfca471199c","title":"Modeling Easiness for Training Transformers with Curriculum Learning"},{"paperId":"5f3f7ffe97e0de0b2c6b82412ce2d12e68daddaf","title":"Brain-Inspired Remote Sensing Foundation Models and Open Problems: A Comprehensive Survey"},{"paperId":"b42e3a759348f27cca2f918a6bd0b139a5312e44","title":"A Survey of Pretrained Language Models Based Text Generation"},{"paperId":"0ec91fd9c5c3ff9758d9cfddc2e5046eaa3c1ab0","title":"Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking"},{"paperId":"4e8e404a6fff91ac55f9c7476a041e2ba31c0454","title":"Know Better – A Clickbait Resolving Challenge"},{"paperId":"f3c6ffbb9ed061ebb17623c066b84c596f55b796","title":"міток маски"},{"paperId":"71cc838d8a50a0d62cc9c679536f1f25b2ea6b7f","title":"A Survey on Generative Diffusion Model"},{"paperId":"e85c5fdc4c44d3e17c45c239adfad8d0942e2805","title":"Text-to-Text Extraction and Verbalization of Biomedical Event Graphs"},{"paperId":"cbf284fe85795eaeb94bfb3dc9e98276dcd33788","title":"Neural Architecture Search for Transformers: A Survey"},{"paperId":"3d3f8399d625238fddb366697acb73446129d65c","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing"},{"paperId":"9f158e69eb2ccf5dce78cd50f4be3cff99b25ca8","title":"NLG-Metricverse: An End-to-End Library for Evaluating Natural Language Generation"},{"paperId":null,"title":"Predicting Human Judgments of Relational Similarity: A Comparison of Computational Models Based on Vector Representations of Meaning"},{"paperId":"10007f2b4b6e7e269400bbfd7d3d4204051d0497","title":"Sentiment Analysis on Dravidian Code-Mixed YouTube Comments using Paraphrase XLM-RoBERTa Model"},{"paperId":"00f899ec60300ca972962e6014d63b4ea835ef38","title":"CMS Optimisation with Deep Learning Techniques"}],"references":[{"paperId":"79926aa63d4daee6af06a8e9a7c2480b31cb7ed9","title":"Spanish Pre-trained BERT Model and Evaluation Data"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"ff2f48fe6438adcaf860aac0f41c584568beafb5","title":"CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision"},{"paperId":"2ee03e28208a9310a9be4032c2b04ebdddb83cc7","title":"FLEX: Unifying Evaluation for Few-Shot NLP"},{"paperId":"4237cbebe788a97174f48dc398082739bbffe95b","title":"FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark"},{"paperId":"4566c0d22ebf3c31180066ab23b6c445aeec78d5","title":"Deduplicating Training Data Makes Language Models Better"},{"paperId":"8b954e1654c6b759a957fd11e66c111e6105fb3f","title":"CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding"},{"paperId":"94b38a1cf2905a9f8dd90f5e22f904a07a59b6bc","title":"XLM-E: Cross-lingual Language Model Pre-training via ELECTRA"},{"paperId":"f07bcf49a92e09437359be788bbe3f9237c5ec40","title":"A Closer Look at How Fine-tuning Changes BERT"},{"paperId":"cf5e670a79847d9be0eb185fb372d99d30d4d98f","title":"Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains"},{"paperId":"114aa720872462b0ca1b97bfdec0ebd56c36fd0a","title":"Towards Understanding and Mitigating Social Biases in Language Models"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a6a7724763d8adba466519489b0b9d209e7f2d15","title":"BARTScore: Evaluating Generated Text as Text Generation"},{"paperId":"00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d","title":"CPM-2: Large-scale Cost-effective Pre-trained Language Models"},{"paperId":"a93632237958800217341d7bad847200afdd60e3","title":"Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better"},{"paperId":"c6fd846b9b8f9eb0a492d6d6242fffce987c4580","title":"Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning"},{"paperId":"d8d2e574965fe733eb1416e03df2b5c2914fc530","title":"A Survey of Transformers"},{"paperId":"2384c92bbde47f5dbc8d8f175aa67e0f95c413d4","title":"FastSeq: Make Sequence Generation Faster"},{"paperId":"7509c66a666e2e3f14bc8676b969b945ee6e136f","title":"CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"981995fd64611f475179b280f4e9c241051ac185","title":"Knowledge Inheritance for Pre-trained Language Models"},{"paperId":"077c713bccd9d2c7fde68d4cbde06ab0f07a6855","title":"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"},{"paperId":"0934952763f1549e75b142261e73f2b27a2f495b","title":"RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model"},{"paperId":"28459083ba624020c8f1c1ed7c3a075f48b4e709","title":"KLUE: Korean Language Understanding Evaluation"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"6563251e69e4378c189d0a0c94d8d19508d552c8","title":"MathBERT: A Pre-Trained Model for Mathematical Formula Understanding"},{"paperId":"c553280c1fc1d0bc7b94683bb75910e309b0d579","title":"Larger-Scale Transformers for Multilingual Masked Language Modeling"},{"paperId":"78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f","title":"PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"},{"paperId":"b4f30496a8fa212a40461ca1bdef32169e998902","title":"Efficient pre-training objectives for Transformers"},{"paperId":"279a19b9eba7afd513394c7a733834b0f41f97fb","title":"mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs"},{"paperId":"c5dd0c12353d683179fae1df079d5c5ae0e2cd23","title":"Dual-View Distilled BERT for Sentence Embedding"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"7b99c51d562e33309a46601c846abbe72a65c6a4","title":"What to Pre-Train on? Efficient Intermediate Task Selection"},{"paperId":"2435c04832d486975304a094e55ecbab8acf8a5f","title":"Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders"},{"paperId":"0e027f8206a4d04f0b50b88dfe31e7f2f46e2d60","title":"IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"b6647280615f667dd7418bfb9b13d828a22c1cfe","title":"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning"},{"paperId":"1cf2e9e198feef3893da2800a7949f6880ddc084","title":"ExplainaBoard: An Explainable Leaderboard for NLP"},{"paperId":"3acedae6febb3d9567e896696f966c92ff406a17","title":"Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages"},{"paperId":"6fd41ee12ae828bb7de965bf7e06bb1bd5259fe7","title":"IndT5: A Text-to-Text Transformer for 10 Indigenous Languages"},{"paperId":"96afd5a4bcf814fc9db32b1fd0324c3dbb63ed61","title":"MuRIL: Multilingual Representations for Indian Languages"},{"paperId":"1cfa7360fef59cdbab6f2415088f413d3ee202eb","title":"GPT Understands, Too"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"8994bce4b85a8b4087584661c49f8776f868f7dd","title":"Interpretable Bias Mitigation for Textual Data: Reducing Genderization in Patient Notes While Maintaining Classification Performance"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"57e4877fc700c075a3ed332b3695551e8b7e8b94","title":"Bidirectional Representation Learning From Transformers Using Multimodal Electronic Health Record Data to Predict Depression"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"cbd4f06a08eb4223b97c9079007a87dda4339afe","title":"Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models"},{"paperId":"824cd8db8a68732db04f4d8b7139eb4475e59ff2","title":"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"},{"paperId":"fcfc9648561a221750b8085790ad9ba1bebb1800","title":"Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models"},{"paperId":"86b369759bff13ec23b45ca23cc5461292a75415","title":"WangchanBERTa: Pretraining transformer-based Thai Language Models"},{"paperId":"0e8afb694ad0a84c5deafd4eedfb75e953776e65","title":"Cloud-based intelligent self-diagnosis and department recommendation service using Chinese medical BERT"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"9f9fe98b60d75c6407726efff8193e8bee3ee13b","title":"KoreALBERT: Pretraining a Lite BERT Model for Korean Language Understanding"},{"paperId":"3a906b77fa218adc171fecb28bb81c24c14dcc7b","title":"Transformers in Vision: A Survey"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"c342798bafc1eaaa60c652fc90fd738941542133","title":"AraGPT2: Pre-Trained Transformer for Arabic Language Generation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"aea9f18d70a78d39ca927f2baa143e084c486086","title":"AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding"},{"paperId":"1e4cda8be54999ced1324777fa462a85e2c9746c","title":"ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic"},{"paperId":"df7d26339adf4eb0c07160947b9d2973c24911ba","title":"Extracting Training Data from Large Language Models"},{"paperId":"21d347db8adc949b0ad08e9c42b66a63739c866f","title":"ParsiNLU: A Suite of Language Understanding Challenges for Persian"},{"paperId":"092442a694b811dff5b7715fba9e363e0ce4108c","title":"SentiX: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis"},{"paperId":"5eea6a9de39c41715e105f5943ac0fcb98fa245c","title":"Enhancing Clinical BERT Embedding using a Biomedical Knowledge Base"},{"paperId":"aa8f5faf8890b6846d4da9cb7e60a8df6e96ba4a","title":"KD-Lib: A PyTorch library for Knowledge Distillation, Pruning and Quantization"},{"paperId":"5fe78eb0f142902237df11cb67c455787a759172","title":"GLGE: A New General Language Generation Evaluation Benchmark"},{"paperId":"70b0c85638d195dbde56cbedc94ae4363b272b58","title":"A Pre-Training Technique to Localize Medical BERT and to Enhance Biomedical BERT"},{"paperId":"b70ee890b5e0ec7d6db04cfd5471b3bbbd0320fc","title":"Self-Supervised Learning from Contrastive Mixtures for Personalized Speech Enhancement"},{"paperId":"a79b520571f7373cbeb8c6ffc02f6a719b3bce38","title":"CODER: Knowledge-infused cross-lingual medical term embedding for term normalization"},{"paperId":"35dffa79dc5c511d7cda7b6c25c0562eba3a5e0c","title":"Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"1109d62ebd2b29a7dc148bc30dd6cfc803a63dec","title":"IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP"},{"paperId":"d7b93f247f9e46115d6b78a67a458c44de0f9039","title":"iNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages"},{"paperId":"28ba41305e8268592ec829600f5a3b54cd10fbca","title":"Learning from Unlabelled Data for Clinical Semantic Textual Similarity"},{"paperId":"70efbd71c840e78d0ecee101d389db0aaea93652","title":"exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources"},{"paperId":"210cf704dddaa922e4eafe634dbabf707d6683bc","title":"LIMIT-BERT : Linguistics Informed Multi-Task BERT"},{"paperId":"302bb91ed02b90896e0ee78a80f303672d9c3b1b","title":"RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark"},{"paperId":"b68b2e81ae2de647394ec05ee62ecf108bf2b50a","title":"Eliciting Knowledge from Language Models Using Automatically Generated Prompts"},{"paperId":"9927a15ddf5313d97c98f0111fd191caf507ce72","title":"HateBERT: Retraining BERT for Abusive Language Detection in English"},{"paperId":"9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2","title":"TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"615204452304331004532c5800399ef55d58b4c7","title":"Self-Alignment Pretraining for Biomedical Entity Representations"},{"paperId":"03935e520c612ac9f137d9e9ef388e0c08568b60","title":"UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus"},{"paperId":"5ad57623099f1fb6045a67fee313fee2573ef5ec","title":"A Benchmark for Lease Contract Review"},{"paperId":"1d95011355628f7aef068ab1914198e43258c530","title":"BERTimbau: Pretrained BERT Models for Brazilian Portuguese"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"708dcd8456426cd609c89a86344e0007c04c80bf","title":"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models"},{"paperId":"6d6595766a35f12a6ad671d05634b5e2159d4f3e","title":"Bio-Megatron: Larger Biomedical Domain Language Model"},{"paperId":"0c775d7ed34fb4690b4291490778649ae75c48d2","title":"TurboTransformers: an efficient GPU serving system for transformer models"},{"paperId":"1c6f94fb3d888167355afb580f04d55cd517ebc6","title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"},{"paperId":"3fbf6339273c50b04e886fa9bd4ad18c952a683d","title":"Rethinking Attention with Performers"},{"paperId":"2274b14cd3f513bee527a92f5859d14aea093aaa","title":"DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue"},{"paperId":"097210dc65924f8ce59523faf444e635523dc714","title":"TernaryBERT: Distillation-aware Ultra-low Bit BERT"},{"paperId":"574c7c7260a795d2906f03b5229ace3f54ee0ab3","title":"An Unsupervised Sentence Embedding Method by Mutual Information Maximization"},{"paperId":"8b0a0f6d1cd6f3aa9b54be45d5127bb016a98171","title":"The birth of Romanian BERT"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"f30444fbb6ad806168e2564db4815cd27faa7fd9","title":"It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"},{"paperId":"03f22e693a0c00bae8a66a64a2fecb0f11a4b034","title":"IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding"},{"paperId":"2b01b3334ce950c76c9c3c2c9146a7f0ce79cc50","title":"Conceptualized Representation Learning for Chinese Biomedical Text Mining"},{"paperId":"725264948d7b6946259af5b8d966e996b9570f99","title":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"},{"paperId":"5b2ec7a534cab750c6b00fe491500681ae3b1527","title":"PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data"},{"paperId":"9baab08fbe37369856688b2abe5b3c90cce1682c","title":"Compression of Deep Learning Models for Text: A Survey"},{"paperId":"2a218786f4615b82389f78472e7ff22e6ce57490","title":"ConvBERT: Improving BERT with Span-based Dynamic Convolution"},{"paperId":"54523ff961a1ac57a86696ef9a53b3a630b482c0","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"16cca900a850eee1b832937281bb08c84beb86ff","title":"Identification of Semantically Similar Sentences in Clinical Notes: Iterative Intermediate Training Using Multi-Task Learning"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"7c462df4adefcf90af3c27ce7f7a8d83efbff2b0","title":"Measurement of Semantic Textual Similarity in Clinical Texts: Comparison of Transformer-Based Models"},{"paperId":"09bfe057c9285577242636950c6835b8731a07fb","title":"Multi-task learning for natural language processing in the 2020s: where are we going?"},{"paperId":"4ceff7472c04ee6d76bce89d61ba4b445d8dbf74","title":"InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training"},{"paperId":"214a0eede75f546b631d6d28871bd9028a66fc46","title":"Playing with Words at the National Library of Sweden - Making a Swedish BERT"},{"paperId":"c0091585ae4f9cccd4c1dba5aa7409c0886553fa","title":"Transferability of Natural Language Inference to Biomedical Question Answering"},{"paperId":"6150a2dab1b63b246eb2cd418fcdb5a6b6b8ae62","title":"exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models"},{"paperId":"1882f194cb43828852cc052887671e55a80f945a","title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"},{"paperId":"49a049dc85e2380dde80501a984878341dd8efdf","title":"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"},{"paperId":"370b680057a6e324e67576a6bf1bf580af9fdd74","title":"Self-Supervised Learning: Generative or Contrastive"},{"paperId":"3578a7792904e6af3db8ffefdff86ab6a387c7c3","title":"FinBERT: A Pretrained Language Model for Financial Communications"},{"paperId":"8b9d77d5e52a70af37451d3db3d32781b83ea054","title":"On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines"},{"paperId":"82453548b97f78ab2cdb9a8626ff858db9ce5a82","title":"Pre-training Polish Transformer-based Language Models at Scale"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"be158d5ab493b2f2dae736a2ca92afcd66ed5be4","title":"ParsBERT: Transformer-based Model for Persian Language Understanding"},{"paperId":"5d4de0fa45aeddc31142e6a24666d06ed7923f1e","title":"Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction"},{"paperId":"72cdd6ebe0221fb568ef20534f44ba5b35190a56","title":"BERTweet: A pre-trained language model for English Tweets"},{"paperId":"126fb7df6bcab2b70000dfe5b940ada63ae1ba6a","title":"COVID-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter"},{"paperId":"1b0c8b26affd13e10ace5770e85478d60dcc368e","title":"GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"},{"paperId":"02809fc23aecf33e3ed95b83d1d03b54fb5c3d0a","title":"An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining"},{"paperId":"f527bcef68aeda601aae314fe5c75185c716e579","title":"KLEJ: Comprehensive Benchmark for Polish Language Understanding"},{"paperId":"8ae8e5e840c5f43d4d72cbc4595690fba01aa799","title":"LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation"},{"paperId":"c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c","title":"Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset"},{"paperId":"673e970fd835c7dd1bb1e071c5a37e9df99b7c8e","title":"Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?"},{"paperId":"11c226588d4d6a255967fe9c1cee92b081b405cd","title":"Low Resource Multi-Task Sequence Tagging - Revisiting Dynamic Conditional Random Fields"},{"paperId":"00cd2650a89734105fa0c0aba3bf07935b318290","title":"GLUECoS: An Evaluation Benchmark for Code-Switched NLP"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"64b4d2c2181e3fb4ecacf797bf5f35db203c1437","title":"MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask Learning"},{"paperId":"56676aef356ebb13cba77fc9e4d70760fbc151f5","title":"ETC: Encoding Long and Structured Inputs in Transformers"},{"paperId":"0171ad4cc87cc7db25b4ec3169e293eed9a13b39","title":"Training with Quantization Noise for Extreme Model Compression"},{"paperId":"5b015296730273921889e54a0a31e3b173017026","title":"TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue"},{"paperId":"1c8aea2bfb61f4661b6907018a5a8bca390900dd","title":"PALM: Pre-training an Autoencoding&autoregressive Language Model for Context-conditioned Generation"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"2b9955bc08fc5f4ddba73082ddabcfaabdbb4416","title":"Poor Man's BERT: Smaller and Faster Transformer Models"},{"paperId":"3b504f939e55d567652737ef093c1087cd40689b","title":"Analyzing Redundancy in Pretrained Transformer Models"},{"paperId":"25a49187e0d1e3ebebda71c7e77f31bc49358044","title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA"},{"paperId":"2573af4e13d9a5dddb257d22cd38a600528d9a8b","title":"MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"},{"paperId":"10467a1466aeec246ac0a577bfc311ec4de110de","title":"Alternating Language Modeling for Cross-Lingual Pre-Training"},{"paperId":"297ad41c0e7264e67ae078921e2a57436293ce72","title":"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"b2fd96a52ded7a64f60c1e54f5bb488c787629c0","title":"What Happens To BERT Embeddings During Fine-tuning?"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","title":"Pre-trained models for natural language processing: A survey"},{"paperId":"e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286","title":"A Survey on Contextual Embeddings"},{"paperId":"0dde065405210ebc399c58ab6b7e843a18caad51","title":"CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model"},{"paperId":"a622332550eaf535cf0f0f6c3a3f3ba197c39cac","title":"PhoBERT: Pre-trained language models for Vietnamese"},{"paperId":"501a8b86428563539667e8117cd8409674ef97c3","title":"TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing"},{"paperId":"738215a396f6eee1709c6b521a6199769f0ce674","title":"Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"60a4a3a886338d0c8e3579d392cb32f493430255","title":"MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"d9b824dbecbe3a1f0b1489f9e4521a532a63818d","title":"Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"},{"paperId":"baf60d13c98916b77b09bc525ede1cd610ed1db5","title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"81ed0e757ae2d66a43d73407ad6f7e0359adf6d7","title":"PMIndia - A Collection of Parallel Corpora of Languages of India"},{"paperId":"dc5da5ac3aff86e4b0156c52d9641d05dc1eeace","title":"MT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b","title":"RobBERT: a Dutch RoBERTa-based Language Model"},{"paperId":"e344ae0473651d04bf322849f1c71a5edc75f887","title":"Modified Bidirectional Encoder Representations From Transformers Extractive Summarization Model for Hospital Information Systems Based on Character-Level Tokens (AlphaBERT): Development and Performance Evaluation"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"a4d5e425cac0bf84c86c0c9f720b6339d6288ffa","title":"BERTje: A Dutch BERT Model"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"477d66dcd2c08243dcc69822d6da7ec06393773a","title":"Multilingual is not enough: BERT for Finnish"},{"paperId":"069e0d896da7c79faeee4cf057548d5da7ce885e","title":"FlauBERT: Unsupervised Language Model Pre-training for French"},{"paperId":"04690b6e72d1d236d91196b93fe64a48f4666a52","title":"Automated Brain Image Classification Based on VGG-16 and Transfer Learning"},{"paperId":"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","title":"How Can We Know What Language Models Know?"},{"paperId":"4fa37d012ad0014552a6a5a03624b29f95558bf7","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"46b8201f1b84950f141cbbb5eeccaa1437159ff4","title":"A Massive Collection of Cross-Lingual Web-Document Pairs"},{"paperId":"2bd5b4aed18400bf1a1cc866d9b8d931aa047290","title":"E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT"},{"paperId":"68c1bf884f0fc0e86641466a1f1fa67e79f16a17","title":"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76","title":"Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"},{"paperId":"4099c4d272c12081b562392606e6d567e4ae7031","title":"Masked Language Model Scoring"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"ce106590145e89ea4b621c99665862967ccf5dac","title":"Q8BERT: Quantized 8Bit BERT"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"12dc43176fd607557d6cc8d46af8b8d77c121b47","title":"Domain-Relevant Embeddings for Medical Question Similarity"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1","title":"Reducing Transformer Depth on Demand with Structured Dropout"},{"paperId":"222b9a7b8038120671a1610e857d3edbc7ac5550","title":"Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models"},{"paperId":"aa2e8b6eecaed5c4af018c03abe5eb8adcabaf47","title":"Cross-Lingual Natural Language Generation via Pre-Training"},{"paperId":"0cbf97173391b0430140117027edcaf1a37968c7","title":"TinyBERT: Distilling BERT for Natural Language Understanding"},{"paperId":"1a00229c25dcc740fd0388ac1e98c42eaa52912e","title":"Pre-trained Language Model for Biomedical Question Answering"},{"paperId":"4fb8fd55b476909a26a8dc594e0ae98d4923ad4d","title":"Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"},{"paperId":"bfeb827d06c1a3583b5cc6d25241203a81f6af09","title":"Knowledge Enhanced Contextual Word Representations"},{"paperId":"3af5a41e43158a75bf7a8bb3f9517edc4163b3ca","title":"Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity"},{"paperId":"65f788fb964901e3f1149a0a53317535ca85ed7d","title":"Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"7102bb3fe73bd057ff161d9db5214a267c1ef312","title":"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"},{"paperId":"80cf2a6af4200ecfca1c18fc89de16148f1cd4bf","title":"Patient Knowledge Distillation for BERT Model Compression"},{"paperId":"d78aed1dac6656affa4a04cbf225ced11a83d103","title":"Revealing the Dark Secrets of BERT"},{"paperId":"18a1c21f35153c45d0ef30c564bffb7d70a13ccc","title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"772717eb2e369cd68c11b7da7aa779450dced9d0","title":"SenseBERT: Driving Some Sense into BERT"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"d56c1fc337fb07ec004dc846f80582c327af717c","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},{"paperId":"8aa1d9145640b4a63258b82bc8180c3683d072b5","title":"KU_ai at MEDIQA 2019: Domain-specific Pre-training and Transfer Learning for Medical NLI"},{"paperId":"1cd8167b2a6be4fdc0f2bfeec4e6f23a9bbb7090","title":"Tuning Multilingual Transformers for Language-Specific Named Entity Recognition"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"57633ff5c6f0708be25e651f51eef29d2fbfe48b","title":"BEHRT: Transformer for Electronic Health Records"},{"paperId":"92343cecdc990380de362b969eec60081959f507","title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"},{"paperId":"f48f90464d9694e2ea18767f14842c64c9a1e8fb","title":"WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"347bac45298f37cd83c3e79d99b826dc65a70c46","title":"Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"3b3f47170ec5c4fabac510585b33aeb87b384396","title":"Variational Pretraining for Semi-supervised Text Classification"},{"paperId":"ad7129af0644dbcafa9aa2f111cb76526ea444a1","title":"Defending Against Neural Fake News"},{"paperId":"4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9","title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"},{"paperId":"07a64686ce8e43ac475a8d820a8a9f1d87989583","title":"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"cc94d15ba408c260c8fe4fa4f1cb6797a996dd21","title":"Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"145b8b5d99a2beba6029418ca043585b90138d12","title":"MASS: Masked Sequence to Sequence Pre-training for Language Generation"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"b03c7ff961822183bab66b2e594415e585d3fd09","title":"Are Sixteen Heads Really Better than One?"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"a9c3a009d754a110379574b069b48c0b4c75db40","title":"Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"},{"paperId":"e61a3a5ba2b93458774f2ccbe480f3cf6cd74fa1","title":"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?"},{"paperId":"2a567ebd78939d0861d788f0fedff8d40ae62bf2","title":"Publicly Available Clinical BERT Embeddings"},{"paperId":"bc789aef715498e79a74f857fa090ece9e383bf1","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"658721bc13b0fa97366d38c05a96bf0a9f4bb0ac","title":"Multi-Task Deep Neural Networks for Natural Language Understanding"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"06a1bf4a7333bbc78dbd7470666b33bd9e26882b","title":"Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling"},{"paperId":"b47381e04739ea3f392ba6c8faaf64105493c196","title":"Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25","title":"T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"},{"paperId":"8c207ece66e0a63627869c49fb37c6811072539b","title":"The brWaC Corpus: A New Open Resource for Brazilian Portuguese"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"25c0e5ad89b77ae9c7ff91765ebd4e1e21abdcb3","title":"The IIT Bombay English-Hindi Parallel Corpus"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"786f95cada23d4639aa1a8b922cdb9fb9a9c03fa","title":"Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b","title":"Recurrent Neural Network for Text Classification with Multi-Task Learning"},{"paperId":"95cd83603a0d2b6918a8e34a5637a8f382da96f5","title":"MIMIC-III, a freely accessible critical care database"},{"paperId":"800366078f063a637e6a4880c0c49c217c7905ea","title":"The United Nations Parallel Corpus v1.0"},{"paperId":"36f652172792f8aab1cf3c4441a72a1bf79d17c8","title":"Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"},{"paperId":"9a501e501a60b431b6031f81dc2c19b390b0aff3","title":"Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","title":"Distilling the Knowledge in a Neural Network"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"27725a2d2a8cee9bf9fffc6c2167017103aba0fa","title":"A Convolutional Neural Network for Modelling Sentences"},{"paperId":"d770060812fb646b3846a7d398a3066145b5e3c8","title":"Do Deep Nets Really Need to be Deep?"},{"paperId":"032d67d27ecacbf6c5b82eb67e5d02d81fb43a7a","title":"A Survey on Multi-view Learning"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"1b97b4623cf2f183340e548e0aa53abf0f2963d8","title":"Representing General Relational Knowledge in ConceptNet 5"},{"paperId":"a25fbcbbae1e8f79c4360d26aa11a3abf1a11972","title":"A Survey on Transfer Learning"},{"paperId":"0d2336389dff3031910bd21dd1c44d1b4cd51725","title":"Why Does Unsupervised Pre-training Help Deep Learning?"},{"paperId":"30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9","title":"Model compression"},{"paperId":"45a23651bcc5a6cc993d722e71b0d301a6dc9dee","title":"Open Mind Common Sense: Knowledge Acquisition from the General Public"},{"paperId":"a9693b7b57f203940889de6d3f979c70c09202ed","title":"Shuffled-token Detection for Refining Pre-trained RoBERTa"},{"paperId":"50068fbea4d1cafcf4c99873ab272c701c08dfcb","title":"OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language Models"},{"paperId":"76db3624f1278a4f4f1e69b343bfff7bba306d47","title":"XLM-T: A Multilingual Language Model Toolkit for Twitter"},{"paperId":"acf2dd4e2853f90832c01c556a2e716e7c720bc2","title":"GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation"},{"paperId":"1554887c6bd76c443a477b27dbcab35877787b27","title":"LightSeq: A High Performance Inference Library for Transformers"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"2ada5bea5e567313fdac9541725fac0cdc49bc36","title":"BanglaBERT: Combating Embedding Barrier for Low-Resource Language Understanding"},{"paperId":"b9478e237b58160c65acd2c41894493d27e2c277","title":"WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"},{"paperId":"cbd78779af4e83fe101ba3f7ba4d4786388d12d8","title":"Semantic Re-tuning with Contrastive Tension"},{"paperId":"5322e5936e4a46195b1a92001467a2350fe72782","title":"KART: Privacy Leakage Framework of Language Models Pre-trained with Clinical Records"},{"paperId":"2af4e8d14d03cd9031ae4a6b1ef39fce2ab3f504","title":"NetBERT: A Pre-trained Language Representation Model for Computer Networking"},{"paperId":null,"title":"“Legal-bert: The muppets straight out of law school,”"},{"paperId":"888c3a3788c52d6637d45dc4238691083884589d","title":"Investigating Learning Dynamics of BERT Fine-Tuning"},{"paperId":"17d5884215b5afa53545cd7cb6135de5478da4ec","title":"CERT: Contrastive Self-supervised Learning for Language Understanding"},{"paperId":"5c5751d45e298cea054f32b392c12c61027d2fe7","title":"S2ORC: The Semantic Scholar Open Research Corpus"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Electra: Pretraining text encoders as discriminators rather than generators"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"2a0870bc2ecfd17dfb1b96cc34613bb73bb4506a","title":"BLACK BOX ATTACKS ON TRANSFORMER LANGUAGE MODELS"},{"paperId":"e1e43d6bdb1419e08af833cf4899a460f70da26c","title":"AlBERTo: Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets"},{"paperId":null,"title":"Bert and pals: Projected attention layers for efficient adaptation in multi-task learning"},{"paperId":null,"title":"“Bertviz: A tool for visualizing multihead self-attention in the bert model,”"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"“Wordnet,”"},{"paperId":"0adeb54d32925d200bb313a8e0f06116e49c67fb","title":"The Unified Medical Language System (UMLS): integrating biomedical terminology"},{"paperId":null,"title":"Sentilare: Sentimentaware language representation learning with linguistic knowledge"},{"paperId":null,"title":"“Umlsbert: Clinical domain knowledge augmentation of con-38"}],"id":"6c761cfdb031701072582e434d8f64d436255da6","summary":"This comprehensive survey paper explains various core concepts like pretraining, Pretraining methods, pretraining tasks, embeddings and downstream adaptation methods, presents a new taxonomy of T-PTLMs and gives brief overview of various benchmarks including both intrinsic and extrinsic."},{"url":"https://www.semanticscholar.org/paper/ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":4,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Lukas Edman,Antonio Toral,Gertjan van Noord","citations":[{"paperId":"412e266cddfd87c79087a88ba1e4d11b89a45a13","title":"MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers"},{"paperId":"5079e188ae86d8330414edb65e898ed306ac450d","title":"Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation"},{"paperId":"117a0c2c23a5a558cf3b39468d917a750bca720c","title":"Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation"},{"paperId":"ed7b51e4a5c4835218f6697b280afb2849211939","title":"Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation"}],"references":[{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"99b12d0df2b93e800207a5e4618a353912f3dff8","title":"Multilingual Unsupervised Neural Machine Translation with Denoising Adapters"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"3b233bdb697cc43effa1eb6d2868ff14efbbab7a","title":"UDapter: Language Adaptation for Truly Universal Dependency Parsing"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"48530f3d6425f2f150f07ccdd61ba951951a0a7d","title":"Simple, Scalable Adaptation for Neural Machine Translation"},{"paperId":"bf8fe437f779f2098f9af82b534aa51dc9edb06f","title":"Scaling Neural Machine Translation"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"69ffe3277011087b55afaeaef0a3933160beee9a","title":"Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"68c03788224000794d5491ab459be0b2a2c38677","title":"WordNet: A Lexical Database for English"},{"paperId":"b5d998e26c6ca1eb24f0008125baddac273c80f7","title":"Tasks"},{"paperId":"f81b036fb6c28deb50f572b26b80297becac2a18","title":"HW-TSC’s Participation in the WMT 2021 News Translation Shared Task"},{"paperId":null,"title":"Translation To analyze our models on lower-resource translation, we chose to train and evaluate on Xhosa–Zulu, specifically the data provided by the WMT2021"},{"paperId":null,"title":"Translation We also run experiments in the higher resource setting of the WMT14 German→English data. Following the principles of Ott et al. (2018), we use larger batch sizes of 50k and 240k tokens"},{"paperId":null,"title":"sub-set for testing (amounting to roughly sentences). Concerning the evaluation sets we Section 7, we show the sizes of the"},{"paperId":null,"title":"The datasets used for translation are shown in Table 10. In"}],"id":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","summary":"This newdownsampling method not only outperforms existing downsampling methods, showing that downsamplings characters can be done without sacriﬁcing quality, but also leads to promising performance compared to subword models for translation."},{"url":"https://www.semanticscholar.org/paper/11ddb0953eae196dab339bfdc117221594cf945e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models","venue":"","year":2022,"referenceCount":79,"citationCount":6,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Jonas Belouadi,Steffen Eger","citations":[{"paperId":"9486179c5ab6e0cff0ef919c34080a882bb9fe9f","title":"AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ"},{"paperId":"51a0bba0c5fb4257e843040615bb23f712fed4e6","title":"Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models"},{"paperId":"67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation"},{"paperId":"380605105531d27474190451183bf6ad6126cac8","title":"GPoeT: a Language Model Trained for Rhyme Generation on Synthetic Data"},{"paperId":"82c92de048a94e31b9fc11f7be5413fd1013a5bd","title":"The Utility of ChatGPT in Educational Research—Potential Opportunities and Pitfalls"},{"paperId":"6c5a1079d9705c0ee022cef77207daa20ce2cde5","title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models"}],"references":[{"paperId":"77a94f6c91ee1590dd2c6fd80b4a6d8bffdb91ac","title":"Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing"},{"paperId":"6ab78343ab82fa9d7baa68027f9f7e8cd9863737","title":"Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks"},{"paperId":"5b0855b9b9361839844ae86277e8189a957d9d23","title":"MVP: Multi-task Supervised Pre-training for Natural Language Generation"},{"paperId":"4cde74e8da60a35e36f19ca8a62fc431fff02c9c","title":"Why is constrained neural language generation particularly challenging?"},{"paperId":"8cc1d839276a8d97d922f57e36f44ecf3f31cbf2","title":"PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation"},{"paperId":"5ae837949dca9fedcc72704746a1ba6c83868f97","title":"BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog"},{"paperId":"1ce77b594dac02d3d114fe2dba895852216825db","title":"Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features"},{"paperId":"880c8973ea1376c6bff23226b3f64792657aa666","title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion"},{"paperId":"bb1c9cb431e771660cffdda1d80a7f15ff40c764","title":"Measuring the Mixing of Contextual Information in the Transformer"},{"paperId":"7e7cf447c6ed27980d56ee15cf80a39ab39137a6","title":"UScore: An Effective Approach to Fully Unsupervised Evaluation Metrics for Machine Translation"},{"paperId":"28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d","title":"Quantifying Memorization Across Neural Language Models"},{"paperId":"a7dd208b248ead76b8f15562b39c2d0a3bdb7fa0","title":"Transformers analyzing poetry: multilingual metrical pattern prediction with transfomer-based language models"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"042c6d99bdf381b55048b8bb48c8479dbcfbcd5a","title":"Incorporating Residual and Normalization Layers into Analysis of Masked Language Models"},{"paperId":"19a9937a1029ee5b1f6da8a945ed9a5c5a029c57","title":"DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5","title":"Pretrained Language Models for Text Generation: A Survey"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"9dc624d7258d1a56117ca720aea953ce46b66b21","title":"Efficient Attentions for Long Document Summarization"},{"paperId":"aa28873534c24e4a8c5deb7bff723cd5fc69a6f0","title":"QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"9623e9e461647a10a8f14419a9abe40482e9eb47","title":"MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization"},{"paperId":"39dbf18f4149ea890a73da36a472b7b713dfeea4","title":"Metrical Tagging in the Wild: Building and Annotating Poetry Corpora with Rhythmic Features"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"63169665bd592fb818678c47644b29302877d50e","title":"UBAR: Towards Fully End-to-End Task-Oriented Dialog Systems with GPT-2"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"f4b33eda909c873c519fce390c3fd66a568c2e27","title":"Automatic Poetry Generation from Prosaic Text"},{"paperId":"9ce3d29c3b4bfe677414a00ecff5dc141475feb5","title":"Cross-lingual Retrieval for Iterative Self-Supervised Training"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"132593995892a5918090320a86c120c7db8c3f0a","title":"PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"e17337f1812d883450119c24cba5ad6e4adee05f","title":"Next Sentence Prediction helps Implicit Discourse Relation Classification within and across Domains"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"9d9c8202a1efaaeccc2eec1e9cf966d119611bbe","title":"Learning Rhyming Constraints using Structured Adversaries"},{"paperId":"33fbaf34fa0119e5249dff3267795e13fa0eaa37","title":"Neural data-to-text generation: A comparison between pipeline and end-to-end architectures"},{"paperId":"eb606d9ce65139754232cee62f6ab77f3e0c665f","title":"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b497160adf0800ae3fd8f156de0ad06100773896","title":"Supervised Rhyme Detection with Siamese Recurrent Networks"},{"paperId":"05c7f8778e7a7b054e475288758f0d8ebc02931a","title":"Deep-speare: A joint neural model of poetic language, meter and rhyme"},{"paperId":"524fd6d2bd91bc9fc6ff7f4ee19e52f652718644","title":"Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation"},{"paperId":"f902f0a8c1aac417d67de5861bb8af449bc8561a","title":"Automatically Generating Rhythmic Verse with Neural Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"332b05ff46f779754a8fefbfe3695f98aed9ed81","title":"Capturing Reliable Fine-Grained Sentiment Associations by Crowdsourcing and Best–Worst Scaling"},{"paperId":"5f4398f0df93ddd548f244b75a49b97f51abd161","title":"Best-Worst Scaling: Theory, Methods and Applications"},{"paperId":"229ec55602143271867682d181ec35f2e43e06e8","title":"Chinese Poetry Generation with Recurrent Neural Networks"},{"paperId":"07cec27d3ee3c96c490d93de5dd06800c31ee829","title":"'The Sounds of the Psalter: Computational Analysis of Soundplay'"},{"paperId":"cb4917b5f430e4f796aa3013c472927f0169b98a","title":"Poetic Data and the News from Poems: A For Better for Verse Memoir"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"caf121025ea1044aee26b10a2d2e7837d66b7026","title":"On the measurement of alliteration in poetry"},{"paperId":"73eeeb36cee123b940c4e94277800c339324eb7b","title":"Constrained Language Models for Interactive Poem Generation"},{"paperId":"622f5ee4d704e21d8b47c7a47dc05e3d8f0a49bc","title":"One Line at a Time - Generation and Internal Evaluation of Interactive Poetry"},{"paperId":"3fe405b9729ee6297f1982dbc92d37f83c638851","title":"Findings of the WMT 2022 Shared Task on Chat Translation"},{"paperId":"5f9e8f70fa88cda5431475130f6cd4892ba91bb1","title":"Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022"},{"paperId":null,"title":"Eighteenth-century poetry archive"},{"paperId":"f31a11257aad37847893b5495024865ca5f41ef9","title":"End-to-end style-conditioned poetry generation: What does it take to learn from examples alone?"},{"paperId":"504a9112f52c8d845f5117879ad7afc7c06aa4cf","title":"FFCD: A Fast-and-Frugal Coherence Detection Method"},{"paperId":"f1b618f60e0bd64293a67dc9b3f3b9519694b906","title":"Syllable Neural Language Models for English Poem Generation"},{"paperId":"5641584e314da699a9fb1473403aabd8440bfe99","title":"A Three Sample Hypothesis Test for Evaluating Generative Models"},{"paperId":null,"title":"The eight emotions in Po-Emo we train our classifier on are beauty / joy, sadness, uneasiness, vitality, awe / sublime, suspense, humor, and annoyance"},{"paperId":null,"title":"GerPT2: German large and small versions of GPT2"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3e7841061391a4ea6b8f325d78c905b0a770cd00","title":"MaxDiff Analysis : Simple Counting , Individual-Level Logit , and HB"},{"paperId":null,"title":"Pattern matching: The gestalt approach"},{"paperId":null,"title":"A mathematical model for alliteration"},{"paperId":"b7b846a80baced27ab16acf51570628540b0bf3d","title":"A Handbook to Literature"},{"paperId":"b2d5cb9e3cb452619fad9fb068f93fbb1b4fad00","title":"The alliteration in Shakespeare's sonnets: a study in literary behavior."},{"paperId":null,"title":"In Memoriam A.H.H. Edward Moxon and Co., London"},{"paperId":null,"title":"Maler Nolten . Novelle in zwei Teilen . 2 . Teil . G . J . Göschen ’ sche Verlagshandlung , Stuttgart . OpenAI . 2022 . ChatGPT : Optimizing language models for dialogue"},{"paperId":null,"title":"syllables that are close together in a verse"},{"paperId":null,"title":"OpenAI"},{"paperId":null,"title":"Alliteration Score is 1 if a quatrain has the correct alliteration level"},{"paperId":null,"title":"Meter Score is the fraction of verses with correctly classified meters"},{"paperId":null,"title":"(iii) By fine-tuning ByGPT5 on QuaTrain, we show that it learns character-level styles better"},{"paperId":null,"title":"than subword-based systems, such as GPT-2 and mT5, as well as other token-free models like ByT5, while being more parameter efficient and also faring well compared to humans"},{"paperId":null,"title":"the level of alliteration instead, which we classify as either low , medium , or high (cf. §5)"},{"paperId":null,"title":"(ii) We create QuaTrain, a large machine-labeled poetry"},{"paperId":null,"title":"Neural poetry: Learning to generate poems using syllables"}],"id":"11ddb0953eae196dab339bfdc117221594cf945e","summary":"This work successfully pre-train and release ByGPT5, a new token-free decoder-only language model, and successfully tunes it on a large custom corpus of English and German quatrains annotated with the authors' styles, demonstrating its runtime performance and introspect the model’s understanding of style conditions."},{"url":"https://www.semanticscholar.org/paper/9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Chul Lee,Qipeng Guo,Xipeng Qiu","citations":[],"references":[{"paperId":"bb15f3727f827a3cb88b5d3ca48415c09b40a88f","title":"What Language Model to Train if You Have One Million GPU Hours?"},{"paperId":"188f49825057eee3871684a5070c0f465c6bc9c4","title":"UniMorph 4.0: Universal Morphology"},{"paperId":"82819510c79101b889f62fb6938a8902be40674e","title":"Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words"},{"paperId":"3425495ee3b6ead009f35aeb70edeac4e6eb2d10","title":"Patches Are All You Need?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"7694aae9766d5f1fe74d900cd82aee898cb6e8e9","title":"How to Train BERT with an Academic Budget"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"512b8ef0002e0bfd0ecb5ab17d533c1762eb9786","title":"Set Transformer"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"921c1216edbf6b2931b15874f24847ff1007ad8c","title":"EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks"},{"paperId":null,"title":"Asher Trockman and J . Zico Kolter . 2022 . Patches are all you need ? Ashish Vaswani"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Hinton"}],"id":"9f4351600c72d5dac0251cd49984f691dca2fcd1","summary":"This work overhauls the existing character-aware method that takes character-level inputs but makes word-level sequence modeling and prediction a token free model with slim input embeddings for downstream tasks by introducing a cross-attention network and a sub-word level prediction based on word- level hidden states."},{"url":"https://www.semanticscholar.org/paper/9f9196beee29c02b8c5837c6edbc69967189b735","title":"Efficient Transformers with Dynamic Token Pooling","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":10,"influentialCitationCount":0,"publicationDate":"17/11/2022","authors":"P. Nawrot,J. Chorowski,Adrian La'ncucki,E. Ponti","citations":[{"paperId":"28c75ab7f6951f1c2bb349b34abc3204ba8b9498","title":"Toucan: Token-Aware Character Level Language Modeling"},{"paperId":"3f4decfbe642d09a2779ed58dcc5de1e8258d543","title":"Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction"},{"paperId":"9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b","title":"Transformer-VQ: Linear-Time Transformers via Vector Quantization"},{"paperId":"8ab5af5c9510bb25dcef0338cb5575461a3ba140","title":"nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources"},{"paperId":"881883842c2661b41bbfc999d56c763b1ceef0bd","title":"No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models"},{"paperId":"e4a95f595b5d60a0858725996b9355f7275492cf","title":"Hierarchical Attention Encoder Decoder"},{"paperId":"0ba0e73ba636b3f93d2e5e39d8f93a40bbbe555a","title":"PuMer: Pruning and Merging Tokens for Efficient Vision Language Models"},{"paperId":"7da17d023d32c64e55c2aba6177374895fa39eee","title":"StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure"},{"paperId":"ffdf78392747ad50c6bb39adab124726db05c3da","title":"Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens"},{"paperId":"148644bf4ccef7e022b965304e8b3178be8af0fa","title":"Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference"}],"references":[{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"cb66cfda54c4b8f9ac39ad48137127b0706a39b0","title":"Variable-rate hierarchical CPC leads to acoustic unit discovery in speech"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"0e802c0739771acf70e60d59c2df51cd7e8c50c0","title":"Memorizing Transformers"},{"paperId":"6281c40c66febca1d8003bcc6fdfd2189b30c38f","title":"SCROLLS: Standardized CompaRison Over Long Language Sequences"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"680e61a17e27a1e8e121276c7ec53fc4fd40babb","title":"Revisiting the Uniform Information Density Hypothesis"},{"paperId":"5d032bd2632b6f5847767f39ce247098c6bbc563","title":"Combiner: Full Attention Transformer with Sparse Computation Cost"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"642dab29e680f516eb25949d616a24e0ad147a19","title":"Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"1f133158a8973fb33fea188f20517cd7e69bfe7f","title":"FNet: Mixing Tokens with Fourier Transforms"},{"paperId":"5a50d90c7ad715c57b5f0cd9d8473b3dff705d40","title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"320efa53dea3e8f836790682fbd4196132c49749","title":"Segatron: Segment-Aware Transformer for Language Modeling and Understanding"},{"paperId":"3fbf6339273c50b04e886fa9bd4ad18c952a683d","title":"Rethinking Attention with Performers"},{"paperId":"d52df686da98c4b5fc35705529253f708c7ed1d0","title":"Self-Supervised Contrastive Learning for Unsupervised Phoneme Segmentation"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"01203341a8b5b7df21dec5359afe8cc388786ebf","title":"Wiki-40B: Multilingual Language Model Dataset"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"657329c633709dd1ac34a30d57341b186b1a47c2","title":"Efficient Content-Based Sparse Attention with Routing Transformers"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"cf3c81c2e2200163c0b12fd02caedb94904f7789","title":"Preserving activations in recurrent neural networks based on surprisal"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"1e4cfedd79a108d0d04cc498bb146e4dcd4b5f0a","title":"Neural Speed Reading via Skim-RNN"},{"paperId":"ae8d5be3caea59a21221f02ef04d49a86cb80191","title":"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"87a913817503379547bec61a5f010abac5b0f76b","title":"Fast-Slow Recurrent Neural Networks"},{"paperId":"29e944711a354c396fad71936f536e83025b6ce0","title":"Categorical Reparameterization with Gumbel-Softmax"},{"paperId":"515a21e90117941150923e559729c59f5fdade1c","title":"The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"},{"paperId":"5b4cef5ce21753a38234740b3d5bf5ff0f7d9b3a","title":"Surprisal-Driven Zoneout"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"de5e7320729f5d3cbb6709eb6329ec41ace8c95d","title":"Gaussian Error Linear Units (GELUs)"},{"paperId":"9f0687bcd0a7d7fc91b8c5d36c003a38b8853105","title":"Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"9e1e23463e5e8a40aac955700c331f9fc8e2b2ec","title":"Zipf's law of abbreviation as a language universal"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"438bb3d46e72b177ed1c9b7cd2c11a045644a1f4","title":"Gradient Estimation Using Stochastic Computation Graphs"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"5522764282c85aea422f1c4dc92ff7e0ca6987bc","title":"A Clockwork RNN"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"b268600834c639e4b0240404685bdf4ee2799d59","title":"Finding Structure via Compression"},{"paperId":"b13813b49f160e1a2010c44bd4fb3d09a28446e3","title":"Hierarchical Recurrent Neural Networks for Long-Term Dependencies"},{"paperId":"710b5c54c011377130f436d2531e6c1f89dd884f","title":"Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology"},{"paperId":null,"title":"Large text compression benchmark"},{"paperId":"7bea855e19fd13461590e4f2d44bbf7b807ce3e3","title":"A bitter lesson."}],"id":"9f9196beee29c02b8c5837c6edbc69967189b735","summary":"Dynamic pooling, which jointly segments and models language, is of-ten both faster and more accurate than vanilla Transformers and ﬁxed-length pooling within the same computational budget."},{"url":"https://www.semanticscholar.org/paper/660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only","venue":"","year":2022,"referenceCount":87,"citationCount":18,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"M. Tschannen,Basil Mustafa,N. Houlsby","citations":[{"paperId":"5ea575d7f59b9c3af365c2d2ef48454688ca8e29","title":"Automated vision-based structural health inspection and assessment for post-construction civil infrastructure"},{"paperId":"bcfb620ed92e0986b201f69c6dc18ab0b712e59f","title":"ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to Video"},{"paperId":"da1df9dbbbfaa6031434f57d96be70d8fc0b0227","title":"LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models"},{"paperId":"6efda5d5d627533601843381fc3ec1e63b26f897","title":"Unsupervised Camouflaged Object Segmentation as Domain Adaptation"},{"paperId":"0590ec99d2b36b8922139078ac1a91fd62eeda61","title":"Exploring Multimodal Approaches for Alzheimer's Disease Detection Using Patient Speech Transcript and Audio Data"},{"paperId":"41fa7fa8e12bb4aa72db7aeb2b11656ace18925d","title":"Paint ChatGPT: A Platform for Old Shanghai Calendar Card Generation"},{"paperId":"7805581aeb501dca317b9ec4fa9ea89cdaa8aa68","title":"Prompt Ensemble Self-training for Open-Vocabulary Domain Adaptation"},{"paperId":"a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3","title":"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"},{"paperId":"29cd4e8504df8c762b0b6eef8299584118feeb88","title":"Image Captioners Are Scalable Vision Learners Too"},{"paperId":"1849d04b51ac97a6218440d7347e5c75f4711ab6","title":"On the Generalization of Multi-modal Contrastive Learning"},{"paperId":"fd928577d67dd01048d13f284a6256164bbcf2f0","title":"Learning without Forgetting for Vision-Language Models"},{"paperId":"36e1b3973c673d3c323014a168e05c5762f84262","title":"OneCAD: One Classifier for All image Datasets using multimodal learning"},{"paperId":"c63c33ff286330490bc3725ead555321b36ab133","title":"VicTR: Video-conditioned Text Representations for Activity Recognition"},{"paperId":"690df0820f35a47e1ce44f90e6ddb4132aa09267","title":"Vision-Language Models for Vision Tasks: A Survey"},{"paperId":"84b6fecf016d74512869c698c66c83729abdf359","title":"Self-Supervised Multimodal Learning: A Survey"},{"paperId":"8a4dd69533378b4e1e1d6429de4f2c6eab18e101","title":"CoBIT: A Contrastive Bi-directional Image-Text Generation Model"},{"paperId":"d1d936bf4be60e64f80ff5aafdbd5ff4891784fb","title":"Conceptual Attention in StyleGAN"},{"paperId":"035315281c72763a3e0956775732e64f5f193d82","title":"Natural Language Generation with Pixels"}],"references":[{"paperId":"e1484706c0fab932fc9804df328044b3cb2f110d","title":"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding"},{"paperId":"aa509ec67f311cd09d109356f7fa37a40072aabb","title":"Phenaki: Variable Length Video Generation From Open Domain Textual Description"},{"paperId":"498ac9b2e494601d20a3d0211c16acf2b7954a54","title":"Imagen Video: High Definition Video Generation with Diffusion Models"},{"paperId":"cd724569488970381ecfea0976891cd0c19403dc","title":"TVLT: Textless Vision-Language Transformer"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"9e147826995203059f86eac7e7fba2fe1a66e00c","title":"vec2text with Round-Trip Translations"},{"paperId":"8c870bef01a4fbb20f60722ffc2f6bee3870b18b","title":"AudioLM: A Language Modeling Approach to Audio Generation"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"fb6348bd6ef6149c98298ff9a88742fb29285404","title":"Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"794c5d5ca20e71eae416da91cf1fed0a8ef15658","title":"Masked Autoencoders that Listen"},{"paperId":"1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe","title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"},{"paperId":"d2425b430fbf5b8ddf9cf2309c36a80a71e5a449","title":"OmniMAE: Single Model Masked Pretraining on Images and Videos"},{"paperId":"499d3bb3acbc10730dd6582bd9b8f646bf22ccd5","title":"Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts"},{"paperId":"32ef7df798ecef81f7abc9a48df276dd07fbc865","title":"Disentangling visual and written concepts in CLIP"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"a8260077135246476a0b0601495ef08e56c21a50","title":"Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"055cd2faeebc7a9df43923d554a61ae924a4af6b","title":"UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes"},{"paperId":"a26a7a74f1e5fd562be95c3611a0680759fbdf84","title":"CoCa: Contrastive Captioners are Image-Text Foundation Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"f3ca598a4c7b7ed436acfc09dad5ec27340dc780","title":"Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation"},{"paperId":"c689d1f3ae2447fd5b2f108b5b4436276e4d3761","title":"LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"32e89d4d1225f3e02ac38ec93efc39fb61ccdde8","title":"Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning"},{"paperId":"9f984b48d57a7e0704c4cb46264e9f697cf60e42","title":"Language-biased image classification: evaluation based on semantic representations"},{"paperId":"c3d086d0f50ff9efa28d56616ed127547d836e55","title":"Omnivore: A Single Model for Many Visual Modalities"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"c8f92e2033630bec76d4e3d3c02b11088e30dda9","title":"Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text"},{"paperId":"2f33b928d76504714e17cd948695d170329843fc","title":"OCR-Free Document Understanding Transformer"},{"paperId":"00d7dfde1cd69d2247e8c36d10807b0dee9656d7","title":"PolyViT: Co-training Vision Transformers on Images, Videos and Audio"},{"paperId":"197d5867a45a2988f4dd159063cdfbfe90164962","title":"LiT: Zero-Shot Transfer with Locked-image text Tuning"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"dc32a984b651256a8ec282be52310e6bd33d9815","title":"Highly accurate protein structure prediction with AlphaFold"},{"paperId":"8f167ec1149921fac63b1ea855443de109bb013a","title":"How Much Can CLIP Benefit Vision-and-Language Tasks?"},{"paperId":"e6cbb6aa65d7761fabb6c5eb363f66524abe0a4c","title":"DocFormer: End-to-End Transformer for Document Understanding"},{"paperId":"c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500","title":"Decision Transformer: Reinforcement Learning via Sequence Modeling"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"f0524b3005720bcff886bcb0227f7f0dd924ff07","title":"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"0e2d8b8d81092037f9866c1ceddcebb87318e38b","title":"AST: Audio Spectrogram Transformer"},{"paperId":"b5ba72aaaef1ae5dccb313c64a5cfb5de3e2b442","title":"Multimodal Neurons in Artificial Neural Networks"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"01730636fe12bd3c15597e9439aba9b0b27ac150","title":"A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned, and Perspectives"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"83ed88e4f745cc9aecd1fbd479612b11beddcb86","title":"CLEAR: Contrastive Learning for Sentence Representation"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"32d281a1e7a0a2d4e2b3f34e0f71780c987e1374","title":"DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"598a2ee223e2949c3b28389e922c1892b4717d2a","title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"cb6899b6cca3680e7438f4356092aa4a2b3dd800","title":"Shared"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"8a1744da011375d711ed75fc2d160c6fdca2cf89","title":"Deep Modular Co-Attention Networks for Visual Question Answering"},{"paperId":"726320cdbd04804ffa8f3a78c095bd1b55a2a695","title":"Similarity of Neural Network Representations Revisited"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"36c3972569a6949ecca90bfa6f8e99883e092845","title":"Pythia v0.1: the Winning Entry to the VQA Challenge 2018"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"54a13bcc9613dcaa76fb25fbe96572f376cfcca9","title":"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"},{"paperId":"ad31866da7f14ae21bd38df0a3b1ffd1a1438122","title":"An efficient framework for learning sentence representations"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"354c029c88be2bbc27dfd2e2e729c0ae622511e6","title":"YFCC100M"},{"paperId":"44040913380206991b1991daf1192942e038fe31","title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"da2bb505ba1fcfa8877c008309bbed49be1332a5","title":"Combined Scaling for Open-Vocabulary Image Classiﬁcation"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Set transformer: A framework for attention-based permutation-invariant neural networks"},{"paperId":null,"title":"Big Vision"},{"paperId":null,"title":"Wikimedia Foundation"},{"paperId":null,"title":"Alex 11015 Authorized licensed use limited to the terms of the applicable"}],"id":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","summary":"This work explores an additional uniﬁcation: the use of a pure pixel-based model to perform image, text, and multimodal tasks and exploits the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multi-modal retrieval without modi ﬁcations."},{"url":"https://www.semanticscholar.org/paper/0351167c875b8931366e85eb5e517819d7db80cc","title":"What Makes for Good Tokenizers in Vision Transformer?","venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence","year":2022,"referenceCount":99,"citationCount":7,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Shengju Qian,Yi Zhu,Wenbo Li,Mu Li,Jiaya Jia","citations":[{"paperId":"5311db0b04b95fa43b886387fb1f484055638660","title":"Modal-aware Visual Prompting for Incomplete Multi-modal Brain Tumor Segmentation"},{"paperId":"b17b6dbdb43e057ad629c353cb8984ab9089a69f","title":"The power of empathy and positive emotions in enhancing the communication of environmental issues: a case study of ‘wandering elephant in Yunnan’ on twitter"},{"paperId":"6baabc2f8824a781ded3d620fb847b6d717f5b4d","title":"Towards Efficient Task-Driven Model Reprogramming with Foundation Models"},{"paperId":"15c2b3ecdf1b9af2f94a2b106fddcfc89cb336cb","title":"ReBotNet: Fast Real-time Video Enhancement"},{"paperId":"251e6a18899d3ea14e8efa67262786c5e697116e","title":"Possibilities of Using the Architecture of Neural Networks \"Transformer\" in Transport Engineering"},{"paperId":"c1fd5f40fd8c2fa96ae81aded3846e7d59473edf","title":"StraIT: Non-autoregressive Generation with Stratified Image Transformer"},{"paperId":"fac388b8c24044dea06cc8c7b03dd1d99c8439a0","title":"AIM: Adapting Image Models for Efficient Video Action Recognition"}],"references":[{"paperId":"2805917375bf84ab06ad658af4ceaec85d4a5906","title":"On Efficient Transformer-Based Image Pre-training for Low-Level Vision"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"5e708602a7cb97a092653f1be0300aa7774391df","title":"Blending Anti-Aliasing into Vision Transformer"},{"paperId":"2e644c67a697073d561da4f4dad35e5ad5316cfd","title":"SOFT: Softmax-free Transformer with Linear Complexity"},{"paperId":"5a060dc5a5bcac0879f17841a5cf70bb4302cc47","title":"Token Pooling in Vision Transformers"},{"paperId":"ee35122f0be32652dc1239c6601ff31732b7026d","title":"Leveraging Batch Normalization for Vision Transformers"},{"paperId":"f7e449d7695fbbf43081cc820a81fe0ccb11c3db","title":"PnP-DETR: Towards Efficient Visual Analysis with Transformers"},{"paperId":"c945efdeefaacb8ca679298720f4b0b054dc84bd","title":"Vision Transformer with Progressive Sampling"},{"paperId":"a5c41f188b0eb0acb444cb4899bf6af378ee9ede","title":"CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"},{"paperId":"57e81d6545dc0279af6d63018bf82a6b9e363fec","title":"DPT: Deformable Patch-based Transformer for Visual Recognition"},{"paperId":"48418b285a92376a38daafa664a2dd07d42e3fe3","title":"Focal Self-attention for Local-Global Interactions in Vision Transformers"},{"paperId":"7b664a306b7d2f68dd816ea1d6586cf3472d75c1","title":"Early Convolutions Help Transformers See Better"},{"paperId":"67040b931c1a384426c44ae73f9553e97f08cf6a","title":"PVT v2: Improved baselines with Pyramid Vision Transformer"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"e2f2662f0734e2edc2b4b36a734de111c7f8d54d","title":"IA-RED2: Interpretability-Aware Redundancy Reduction for Vision Transformers"},{"paperId":"48bcfea07f343b29128c71bb2cce5f3ab62f6d85","title":"TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?"},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"9f4b69762ffb1ba42b573fd4ced996f3153e21c0","title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes"},{"paperId":"dbdcabd0444ad50b68ee09e30f39b66e9068f5d2","title":"DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"},{"paperId":"e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60","title":"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"},{"paperId":"6709d5583f658f589ae6a2184805933aceb18849","title":"Twins: Revisiting the Design of Spatial Attention in Vision Transformers"},{"paperId":"8d3ddc27dce9c6c0fe110e4f9cb45d3b59feb04b","title":"Visformer: The Vision-friendly Transformer"},{"paperId":"8754533bead3996f20440e4a1d0220d4971d00d7","title":"VidTr: Video Transformer Without Convolutions"},{"paperId":"7093016e02e8f4580ffea18396f95d129d37858d","title":"All Tokens Matter: Token Labeling for Training Better Vision Transformers"},{"paperId":"18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6","title":"Multiscale Vision Transformers"},{"paperId":"5b68522f58b61e7235b852677337ef3725075fd9","title":"Co-Scale Conv-Attentional Image Transformers"},{"paperId":"739ceacfafb1c4eaa17509351b647c773270b3ae","title":"An Empirical Study of Training Self-Supervised Vision Transformers"},{"paperId":"b364cdb02d18b9d9a3c097f5ea446f7e9ab10325","title":"Going deeper with Image Transformers"},{"paperId":"e775e649d815a02373eac840cf5e33a04ff85c95","title":"CvT: Introducing Convolutions to Vision Transformers"},{"paperId":"0eff37167876356da2163b2e396df2719adf7de9","title":"CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"},{"paperId":"91e8117e7ebc966bc76de2cb52ec717d2acdb1a4","title":"Scaling Local Self-Attention for Parameter Efficient Visual Backbones"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"0ae67202f0584afccefa770865d14a46655d2975","title":"Transformer in Transformer"},{"paperId":"3e398bad2d8636491a1034cc938a5e024c7aa881","title":"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"},{"paperId":"63812f583caac3ac32bbfb64f66ba69e57c1e90a","title":"Conditional Positional Encodings for Vision Transformers"},{"paperId":"dbe077f8521ecbe0a1477d6148c726d4f053d9c9","title":"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"},{"paperId":"32a81aef8063274b1d8cc770a7f6dcfd8efe5336","title":"Revisiting Locally Supervised Learning: an Alternative to End-to-end Training"},{"paperId":"d29430adccb805ab57b349afa8553954347b3197","title":"Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"},{"paperId":"ad7ddcc14984caae308c397f1a589aae75d4ab71","title":"Training data-efficient image transformers & distillation through attention"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"39ca8f8ff28cc640e3b41a6bd7814ab85c586504","title":"Deformable DETR: Deformable Transformers for End-to-End Object Detection"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"9257a6b9b072b3c1b82bcc76db26406fd0ce24e0","title":"Attentive Normalization for Conditional Image Generation"},{"paperId":"4076e421d1758fdb68411242044cd45747b7e35b","title":"PowerNorm: Rethinking Batch Normalization in Transformers"},{"paperId":"b45d656ac8cc2e940609580cf291ee76ffcac20a","title":"On Layer Normalization in the Transformer Architecture"},{"paperId":"a573c125e85d1230626c8f3cf6193354f753958d","title":"Temporal Interlacing Network"},{"paperId":"b85d339e49399966d629973c889e8edfca56517c","title":"A Mutual Information Maximization Perspective of Language Representation Learning"},{"paperId":"87f6a7c014ce206ac5b57299c07e10667d194b39","title":"Randaugment: Practical automated data augmentation with a reduced search space"},{"paperId":"a88c914f5a738d38f02790bb5de41453bf17bde1","title":"Object-Contextual Representations for Semantic Segmentation"},{"paperId":"112fd54ee193237b24f2ce7fce79e399609a29c5","title":"The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives"},{"paperId":"8835a2a56c04d8a1dc47798ceb567748bb32fac0","title":"Make a Face: Towards Arbitrary High Fidelity Face Manipulation"},{"paperId":"c2c083df88e88223e1a411e61040b94c233b1b63","title":"MMDetection: Open MMLab Detection Toolbox and Benchmark"},{"paperId":"a1a19aaddf57c0546357d890d9269092ba0afb26","title":"Semantic Image Synthesis With Spatially-Adaptive Normalization"},{"paperId":"cf0a995aed9e2e1dff6d0b0d7ea526ddd84fe137","title":"Greedy Layerwise Learning Can Scale to ImageNet"},{"paperId":"a9a5d671271fff45429084e184a788f611b6f194","title":"FRAGE: Frequency-Agnostic Word Representation"},{"paperId":"af3825437b627db1a99f946f7aa773ba8b03befd","title":"Learning deep representations by mutual information estimation and maximization"},{"paperId":"aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1","title":"Unified Perceptual Parsing for Scene Understanding"},{"paperId":"d39a5ea57b1c242a0450385523fd3471b172458c","title":"Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net"},{"paperId":"f723eb3e7159f07b97464c8d947d15e78612abe4","title":"AutoAugment: Learning Augmentation Policies from Data"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"030ff7012b92b805a60976f8dbd6a08c1cecebe6","title":"DCAN: Dual Channel-wise Alignment Networks for Unsupervised Scene Adaptation"},{"paperId":"d6e8e560100c43ea22070535b2d0e31f518ffb72","title":"The Contextual Loss for Image Transformation with Non-Aligned Data"},{"paperId":"0a255e716a89b787336ab956f0aa74424629c950","title":"On the information bottleneck theory of deep learning"},{"paperId":"ab559473a01836e72b9fb9393d6e07c5745528f3","title":"cGANs with Projection Discriminator"},{"paperId":"6b73775f40467aed52784ff355b9bb7168e9078c","title":"Mutual Information Neural Estimation"},{"paperId":"04957e40d47ca89d38653e97f728883c0ad26e5d","title":"Cascade R-CNN: Delving Into High Quality Object Detection"},{"paperId":"45dfef0cc1ed96558c1c650432ce39d6a1050b6a","title":"Fixing Weight Decay Regularization in Adam"},{"paperId":"79cfb51a51fc093f66aac8e858afe2e14d4a1f20","title":"Focal Loss for Dense Object Detection"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1a0912bb76777469295bb2c059faee907e7f3258","title":"Mask R-CNN"},{"paperId":"be0ef77fb0345c5851bb5d297f3ed84ae3c581ee","title":"Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"88512be44744615f4baa8e14f600f036db4c2433","title":"Semantic Understanding of Scenes Through the ADE20K Dataset"},{"paperId":"63de0ad39d807f0c256f851428f211e8d5fcd3bb","title":"Instance Normalization: The Missing Ingredient for Fast Stylization"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"915c4bb289b3642489e904c65a47fa56efb60658","title":"Perceptual Losses for Real-Time Style Transfer and Super-Resolution"},{"paperId":"24da6180db314619060d7b8fc798390f0c7a139a","title":"Revisiting Batch Normalization For Practical Domain Adaptation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"5f5dc5b9a2ba710937e2c413b37b053cd673df02","title":"Auto-Encoding Variational Bayes"},{"paperId":"e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","title":"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"b50bd1f96e20952d7ad1bf774ea6199ce12f2fa6","title":"Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"9af62668cb87f11fffb53a194588c8158fde6b00","title":"DynamicViT: Efﬁcient Vision Transformers with Dynamic Token Sparsiﬁcation"},{"paperId":"44b49bcd122eb9b336d6a2545665d2e80a78b014","title":"Layer"},{"paperId":null,"title":"T2t vit"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"432c29bbbe5386a7445ef2bd559234894046bd45","title":"Supplementary Materials for Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation"},{"paperId":null,"title":"Mocov3"},{"paperId":null,"title":"Pytorch image models"},{"paperId":"dd6d044696df5e4353ff7c92b8009e1201c85129","title":"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"},{"paperId":null,"title":"Deep clustering for unsupervised learning of visual features"},{"paperId":"23a99224c7b7d148675c1798c796ec1b0904620a","title":"Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics"},{"paperId":null,"title":"the MS degree in electrical engineering from the University of Kansas, in 2014, and the PhD degree in computer science from the University of California, Merced, in"},{"paperId":"f349a43480449c9e1f4e8222cd1feedebb0de059","title":"Conditional Entropy"},{"paperId":null,"title":"Hisresearchfocusesonlarge-scalemachinelearning.Hismainresearchinterestsincludeco-designofdistributedsystemsandmachinelearningalgorithms"},{"paperId":null,"title":"“On efﬁcient"},{"paperId":null,"title":"A.2 Details on Comparison across structural designs"}],"id":"0351167c875b8931366e85eb5e517819d7db80cc","summary":"Through extensive experiments on various transformer architectures, this work observes both improved performance and intriguing properties of these two plug-and-play designs with negligible computational overhead, indicating the importance of the commonly-omitted designs of tokenizers in vision transformer."},{"url":"https://www.semanticscholar.org/paper/e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model","venue":"Findings","year":2021,"referenceCount":38,"citationCount":14,"influentialCitationCount":0,"publicationDate":"26/05/2021","authors":"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki","citations":[{"paperId":"e4d2ba4cb6f364079ce3731e86b860264677ae7d","title":"Downstream Task-Oriented Neural Tokenizer Optimization with Vocabulary Restriction as Post Processing"},{"paperId":"491eaab2ff93772d52be8c014bf97466d8c6fe73","title":"Tokenization Tractability for Human and Machine Learning Model: An Annotation Study"},{"paperId":"67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation"},{"paperId":"75a05bffa2cc35a876ce04edb0b57f9592716d3b","title":"Extending the Subwording Model of Multilingual Pretrained Models for New Languages"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"8c82d3d758897ef9f166924683831ecf6085f21a","title":"MaxMatch-Dropout: Subword Regularization for WordPiece"},{"paperId":"ef9c8b441b7088878104cee17091444cc75d7a5f","title":"Composing Word Embeddings for Compound Words Using Linguistic Knowledge"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"0de580957d23dd65e31b6c95e6bc5d15bc15c57d","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a","title":"Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"2b4369d50ac7310b9908a2baef89a63c68cbd893","title":"Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models"},{"paperId":"3227844af4d38bfa702781e321cf6712bf537e2c","title":"Word-level Perturbation Considering Word Length and Compositional Subwords"}],"references":[{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"3504dd566b25f24f44cf647731370760a537b483","title":"Stochastic Tokenization with a Language Model for Neural Text Classification"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"4c8f666fc2221bfd569f4e277e8242e7e47611dc","title":"Juman++: A Morphological Analysis Toolkit for Scriptio Continua"},{"paperId":"90fe32dd2fccd4ca14713d8b0252aa34a6b910e8","title":"Subword Encoding in Lattice LSTM for Chinese Word Segmentation"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"80e7eb77c5ab8ffb8d870db0d6fa34a40cc792a4","title":"Sudachi: a Japanese Tokenizer for Business"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"34b4c0ae0160d5414729522612df43d72983d686","title":"Neural Word Segmentation with Rich Pretraining"},{"paperId":"1e0b165d10704d0d9be4aa55ae89ab566f919758","title":"Fast and Accurate Neural Word Segmentation for Chinese"},{"paperId":"36f652172792f8aab1cf3c4441a72a1bf79d17c8","title":"Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"bae2cbe50c62798305acbaf9eaeebbba27afdf81","title":"Daily-Aware Personalized Recommendation based on Feature-Level Time Series Analysis"},{"paperId":"a384185d8588c6bd148ef624730242b29efdd2ba","title":"Nonparametric Word Segmentation for Machine Translation"},{"paperId":"a575a9f508240a17ecf4e253984ab3ad5af5b8d1","title":"Unsupervised Tokenization for Machine Translation"},{"paperId":"51c88cf10b75ed1a5445660cc64cee3d1c6fd8c5","title":"Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling"},{"paperId":"46f31f9069bb934498a288126053bcab01ff34aa","title":"A Bayesian framework for word segmentation: Exploring the effects of context"},{"paperId":"977a2e71c0889315a302fe69e3ec2c47496ea4cb","title":"Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation"},{"paperId":"5b2beddeb063bd5988da70ae58f3e0a6564e647a","title":"Optimizing Chinese Word Segmentation for Machine Translation Performance"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"a99d6ebe6e583b752d1639a9002dd60581983dc1","title":"Contextual Dependencies in Unsupervised Word Segmentation"},{"paperId":"6e682f6b2ccb55641e644c9a1a1f136ebc69e38a","title":"Improving Statistical MT through Morphological Analysis"},{"paperId":"da336ed15bd24c268cb2a09efe2aa4f298cda3ba","title":"Statistical Machine Translation with Scarce Resources Using Morpho-syntactic Information"},{"paperId":"1f701007f789890d1066172094bbf158f002f673","title":"Bayesian Methods for Hidden Markov Models"},{"paperId":"f30a129113961242c6279436d60df17e9043ad08","title":"A Stochastic Japanese Morphological Analyzer Using a Forward-DP Backward-A* N-Best Search Algorithm"},{"paperId":"145c0b53514b02bdc3dadfb2e1cea124f2abd99b","title":"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"},{"paperId":"99cfd7ae6383acd5023e8e74ae02b31ebe7e95d8","title":"Morphological Analysis for Unsegmented Languages using Recurrent Neural Network Language Model"},{"paperId":null,"title":"Rakuten dataset"},{"paperId":"70b849773678010942a0975f2887e527c17cda76","title":"MeCab : Yet Another Part-of-Speech and Morphological Analyzer"},{"paperId":"3746bbd652af92bfbaecb5e48530f3bd5c705031","title":"Joint conference."}],"id":"e6b252ad22486c10b1b288e0a5e1ad468690be70","summary":"Experimental results show that the proposed method improves the performance by determining appropriate tokenizations and can be used to explore the appropriate tokenization for an already trained model as post-processing."},{"url":"https://www.semanticscholar.org/paper/bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/12/2022","authors":"Nathan Godey,Roman Castagn'e,Eric Villemonte de la Clergerie,Benoît Sagot","citations":[{"paperId":"67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation"}],"references":[{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"2b66a0d8a00b0ba7b585e11ab34879420ad351cb","title":"Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"f2588de5173fb047192dbb93d62ce6636bdf46bd","title":"Lessons from Natural Language Inference in the Clinical Domain"},{"paperId":"e9b711ec25c8d919ab60dfefe362251f60309214","title":"A simple and fast method for computing the Poisson binomial distribution function"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5c2658fedd4bff3c8aea27cefe72cf72a36d949d","title":"An Algorithm for Computing the Distribution Function of the Generalized Poisson-Binomial Distribution"},{"paperId":"6f35b070c250507dbec8a7365cf01b25eb66d792","title":"Ex Machina: Personal Attacks Seen at Scale"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"this computation, storing O ( K × L × H ) parameters and only later apply the convolution per block by summing these products with the block-byte membership map P . Caching greatly"}],"id":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","summary":"MANTa is a differentiable tokenizer trained end-to-end with the language model that improves robustness to character perturbations and out-of-domain data and is considerably faster than strictly byte-level models."},{"url":"https://www.semanticscholar.org/paper/00675f1591392622b0db2d9cd37a8a1f32e37aa8","title":"Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks","venue":"","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Kaiser Sun,Peng Qi,Yuhao Zhang,Lan Liu,William Yang Wang,Zhiheng Huang","citations":[],"references":[{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"190b831643573cd73d543f620c50051078d8bce9","title":"CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing"},{"paperId":"b2542a738b75ee9b7ce1a13d8b78f9095d212412","title":"Generate rather than Retrieve: Large Language Models are Strong Context Generators"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"996f0d401acd11e95ce5586010e7e4e18f5c3bb9","title":"How to Split: the Effect of Word Segmentation on Gender Bias in Speech Translation"},{"paperId":"ea8c46e193d5121e440daf96edfd15a47151c293","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1be21e96eaac56f626e7b41c1f332b6b46131608","title":"MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"a7bbb084f5de4f318c811776afeba2b05439c234","title":"DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension"},{"paperId":"c071a1ad68310fed7f0876b6f01cb7b135043bc3","title":"Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"3adff57fd09965224506a1bacc0579d9d3c8c11e","title":"SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine"},{"paperId":"3eda43078ae1f4741f09be08c4ecab6229046a5c","title":"NewsQA: A Machine Comprehension Dataset"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"c4dd9a19d822c965ce8cde55ab23b8a0b628278a","title":"An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"30c9ba88ad0f2fc62b44674950b36ead105cc12e","title":"COMPREHENSION"},{"paperId":"8f90451a627c0b5d0cdff5f6e11b4cdf2fe008db","title":"Improving the Numerical Reasoning Skills of Pretrained Language Models"},{"paperId":"1c59de25af45cef20d846ec7454251e8237d45d1","title":"A Statistical Extension of Byte-Pair Encoding"},{"paperId":"0bfa9275561f45bc772c4d544fd75f5d65143c5e","title":"When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation"},{"paperId":null,"title":"Findings of the Association for Computational Lin-guistics"},{"paperId":null,"title":"0.8 1.0 1.2 ti on L o ss BART-Original BART-Consistent"}],"id":"00675f1591392622b0db2d9cd37a8a1f32e37aa8","summary":"It is shown that, with consistent tokenization, the model performs better in both in-domain and out-of-domain datasets, with a notable average of +1.7 F 1 gain when a BART model is trained on SQuAD and evaluated on 8 QA datasets."},{"url":"https://www.semanticscholar.org/paper/75a05bffa2cc35a876ce04edb0b57f9592716d3b","title":"Extending the Subwording Model of Multilingual Pretrained Models for New Languages","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"K. Imamura,E. Sumita","citations":[],"references":[{"paperId":"ef8873006f9d715369063c854db37e65d6c2fffd","title":"The NiuTrans Machine Translation Systems for WMT21"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"13bcfb944779165983aaef22cec8a3bbd3e98e62","title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts"},{"paperId":"ec7566ec272814560c45f9b97412395eff8f8460","title":"The University of Edinburgh’s English-Tamil and English-Inuktitut Submissions to the WMT20 News Translation Task"},{"paperId":"0701aad2252c619096083b86e5151d6b35d78a59","title":"Facebook AI’s WMT20 News Translation Task Submission"},{"paperId":"1e169d99f966c3f377c8f2e9f666ff47193b09e8","title":"CUNI Submission for the Inuktitut Language in WMT News 2020"},{"paperId":"29599829d26b4acaaf0ad88698ed4362d33b2ae7","title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"1f58c42f44113f1c3c8a97c538e78f37f839f4b8","title":"Multilingual Translation with Extensible Multilingual Pretraining and Finetuning"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"f16410d241616553ae6e63fcf4ba8959eda58aea","title":"Applying Conditional Random Fields to Japanese Morphological Analysis"},{"paperId":"bcec2d28d1eb2d60d4efcf3e2714a92d03df6e52","title":"Machine Translation for English–Inuktitut with Segmentation, Data Acquisition and Pre-Training"},{"paperId":"671cd646d61c670fffc2fea4cf7b7a1f6f80dcf7","title":"NRC Systems for the 2020 Inuktitut-English News Translation Task"},{"paperId":null,"title":"editors"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Foundations of Statistial Natural Language Processing"},{"paperId":"ef5abd33b2e485609e388a03f1b7eb5061005cc2","title":"Machine Translation"},{"paperId":null,"title":"the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)"},{"paperId":null,"title":"Iterate the following procedure until the vocabulary size of the model becomes predeﬁned"},{"paperId":null,"title":"b) Supply each subword candidate with a likelihood computed from the relative frequency in the corpora"}],"id":"75a05bffa2cc35a876ce04edb0b57f9592716d3b","summary":"This paper adds new subwords to the SentencePiece tokenizer to apply a multilingual pretrained model to new languages (Inuk-titut in this paper) and applies the mBART-50 pret trained model to English-Inuktituts translation."},{"url":"https://www.semanticscholar.org/paper/8c82d3d758897ef9f166924683831ecf6085f21a","title":"MaxMatch-Dropout: Subword Regularization for WordPiece","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":30,"citationCount":4,"influentialCitationCount":0,"publicationDate":"09/09/2022","authors":"Tatsuya Hiraoka","citations":[{"paperId":"e4d2ba4cb6f364079ce3731e86b860264677ae7d","title":"Downstream Task-Oriented Neural Tokenizer Optimization with Vocabulary Restriction as Post Processing"},{"paperId":"491eaab2ff93772d52be8c014bf97466d8c6fe73","title":"Tokenization Tractability for Human and Machine Learning Model: An Annotation Study"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"2b4369d50ac7310b9908a2baef89a63c68cbd893","title":"Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models"}],"references":[{"paperId":"75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a","title":"Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation"},{"paperId":"70d73c5c519a7259a52da232a8392616ec283c52","title":"WRIME: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"28459083ba624020c8f1c1ed7c3a075f48b4e709","title":"KLUE: Korean Language Understanding Evaluation"},{"paperId":"72eed6570807c1d3c9a60289c7fc6813277e1e98","title":"Fast WordPiece Tokenization"},{"paperId":"8724dbc032a936efd7d2761be56be66069fd1cb6","title":"Evaluating Robustness to Input Perturbations for Neural Machine Translation"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"3504dd566b25f24f44cf647731370760a537b483","title":"Stochastic Tokenization with a Language Model for Neural Text Classification"},{"paperId":"2a66d626f575fb418dd7b82eb156b721142f7420","title":"Filtering Method for Twitter Streaming Data Using Human-in-the-Loop Machine Learning"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"36f652172792f8aab1cf3c4441a72a1bf79d17c8","title":"Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"685d42a668413422615519a52ac75d66fded4611","title":"Framewise phoneme classification with bidirectional LSTM networks"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":null,"title":"Pretrained language models for korean"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"8ff46c88964a36985f2b45933a3d47b81bd87bd0","title":"Quora Question Pairs"},{"paperId":"db8885a0037fe47d973ade79d696586453710233","title":"The Sixth PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"351ec42df2b60c6042addf96e6b98673bbaf4dfd","title":"The Fourth PASCAL Recognizing Textual Entailment Challenge"}],"id":"8c82d3d758897ef9f166924683831ecf6085f21a","summary":"The proposed method, MaxMatch-Dropout, randomly drops words in a search using the maximum matching algorithm for tokenization to realize finetuning with subword regularization for popular pretrained language models such as BERT-base."},{"url":"https://www.semanticscholar.org/paper/0ab0fda8774c303be8f8f8c8f684a890dcf5d455","title":"Lattice-Based Transformer Encoder for Neural Machine Translation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2019,"referenceCount":49,"citationCount":41,"influentialCitationCount":5,"publicationDate":"04/06/2019","authors":"Fengshun Xiao,Jiangtong Li,Zhao Hai,Rui Wang,Kehai Chen","citations":[{"paperId":"acb121fe4b0c28dce2ab60937fe70ac1d3dcb931","title":"DeepLOC: Deep Learning-based Bone Pathology Localization and Classification in Wrist X-ray Images"},{"paperId":"e9cf6b32ddf971a64095f4852c3b13b2e2b6fcc7","title":"Compound Aspect Extraction by Augmentation and Constituency Lattice"},{"paperId":"7ec44b836b60076f4c00da6824e829dc9f13b3f7","title":"Development of Language Models for Continuous Uzbek Speech Recognition System"},{"paperId":"ca1d71b52b3b81a7e1cca297f19d47f353cf3340","title":"Relation-Aware Graph Transformer for SQL-to-Text Generation"},{"paperId":"3390f8fc6e0f81e791baea2b82ff1b6f0bb74cc4","title":"Lattention: Lattice-Attention in ASR Rescoring"},{"paperId":"0b18f7f0e06b6b048e79bed6437b27ac3d496f64","title":"Enhanced encoder for non-autoregressive machine translation"},{"paperId":"08ffdec40291a2ccb5f8a6cc048b01247fb34b96","title":"Relative Positional Encoding for Transformers with Linear Complexity"},{"paperId":"44930df2a3186edb58c4d6f6e5ed828c5d6a0089","title":"Attention, please! A survey of neural attention models in deep learning"},{"paperId":"a81ed90d219196b50e27795530b0eec51843df67","title":"Confusion2Vec 2.0: Enriching ambiguous spoken language representations with subwords"},{"paperId":"5a84bf724ee79c507e66e5dc564c98c76d553a53","title":"Porous Lattice Transformer Encoder for Chinese NER"},{"paperId":"88ef2416a4f337b72a0b7ade0e1fea5afcec0512","title":"Constituency Lattice Encoding for Aspect Term Extraction"},{"paperId":"e9fdb52f861d0e13ebf98780587dfc66b151a766","title":"Lexicon-constrained Copying Network for Chinese Abstractive Summarization"},{"paperId":"de17ba267fbf02e9dff443745230eefac5f7d581","title":"Incorporating Named Entity Information into Neural Machine Translation"},{"paperId":"067906c924810e8ffc595ff8c9c4b0b2906cca85","title":"SJTU-NICT’s Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task"},{"paperId":"479c9cefe82c774471ed96800835db34bf04e3d3","title":"High-order Semantic Role Labeling"},{"paperId":"b3eaf6ac2bc38d42585fcb20cfcb40ccf3ac4629","title":"Learning Spoken Language Representations with Neural Lattice Language Modeling"},{"paperId":"4bfae29a5492c488460c504f79247d4f0bb1e7f3","title":"BiG-Transformer: Integrating Hierarchical Features for Transformer via Bipartite Graph"},{"paperId":"93d22f0c38e1c465d75ce1e4ca870ed95a3ef0b8","title":"Bipartite Flat-Graph Network for Nested Named Entity Recognition"},{"paperId":"876be9b226601821eeade310013506a03f023824","title":"Capsule-Transformer for Neural Machine Translation"},{"paperId":"be7c6432abd3edd5d9224bc380c16177589dd60e","title":"DCMN+: Dual Co-Matching Network for Multi-Choice Reading Comprehension"},{"paperId":"7066df8fd89cca546d1ef3d66679cb15eba48d50","title":"FLAT: Chinese NER Using Flat-Lattice Transformer"},{"paperId":"3fa8d2a9e9a9cf3ee9626424a157888580dcfaba","title":"A Survey of Deep Learning Techniques for Neural Machine Translation"},{"paperId":"66c81a4cd0ba6f2cfce8e1e76ec1d2dd0e389add","title":"A Hierarchical Clustering Approach to Fuzzy Semantic Representation of Rare Words in Neural Machine Translation"},{"paperId":"011be9f75e5c009eeb4ff8f220a73d716befac76","title":"English to Urdu: Optimizing Sequence Learning in Neural Machine Translation"},{"paperId":"4452fbcd04370f2cfbc46066bef3b749a7b3b5a4","title":"Korean-to-Chinese Machine Translation using Chinese Character as Pivot Clue"},{"paperId":"3e2a688a97f5158d441fce9387592de1e3ab1264","title":"Porous Lattice-based Transformer Encoder for Chinese NER"},{"paperId":"18621213820fb05948b326e6d52dfff2deae3ea5","title":"Hierarchical Contextualized Representation for Named Entity Recognition"},{"paperId":"b134ce39a3a195f5b0c277a1d2c0d776e5ce6a52","title":"Syntax-aware Transformer Encoder for Neural Machine Translation"},{"paperId":"4d68c1b4167f858979c6a8e8b9ad0b484cd48c63","title":"Cross Aggregation of Multi-head Attention for Neural Machine Translation"},{"paperId":"47a60929d2e3a54511b89d28f8b4f1f43f764808","title":"Named Entity Recognition Only from Word Embeddings"},{"paperId":"72c0e5b7365ae4981db13cfa15ca808a8eb3a8a1","title":"Head-Driven Phrase Structure Grammar Parsing on Penn Treebank"},{"paperId":"25a233968e62eb6381558b0c590f5e2dc474445f","title":"Dual Co-Matching Network for Multi-choice Reading Comprehension"},{"paperId":"dc477a26c162ddfa8ab36d4f7975f62f5ad04ab5","title":"Open Vocabulary Learning for Neural Chinese Pinyin IME"},{"paperId":"c70df347923d90e05ff19ebd724ffc2fd6232594","title":"Neural-based Pinyin-to-Character Conversion with Adaptive Vocabulary"},{"paperId":"5bfb0b5494885d35bc15952c025fa2d8fbbd8c98","title":"Explicit Contextual Semantics for Text Comprehension"},{"paperId":"ddfc4e1508c2d723b88bcbe0220ccffffb412d37","title":"Transformer versus LSTM Language Models trained on Uncertain ASR Hypotheses in Limited Data Scenarios"},{"paperId":"6b2a3003b27525f51c4f84f65bc65a98cbd9a731","title":"PCBERT: Parent and Child BERT for Chinese Few-shot NER"},{"paperId":"7da93508244f150d518ad96122d215e6f8c8262f","title":"Optimizing Word Segmentation for Downstream Tasks by Weighting Text Vector"},{"paperId":"875fe31a7adaf7462ae5b91231572e1ac24f0145","title":"SWITCHING-ALIGNED-WORDS DATA AUGMENTATION"},{"paperId":"be4e226afde6879620d4563f3c39cf3443e22d1f","title":"SJTU-NICT at MRP 2019: Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing"},{"paperId":"c8e707a0ea3a1f3bc104f191e72c4444b28be834","title":"SJTU at MRP 2019: A Transition-Based Multi-Task Parser for Cross-Framework Meaning Representation Parsing"}],"references":[{"paperId":"72c0e5b7365ae4981db13cfa15ca808a8eb3a8a1","title":"Head-Driven Phrase Structure Grammar Parsing on Penn Treebank"},{"paperId":"f3abc20cf0632b667d6f4d9c99f962b243d3beae","title":"Chinese Word Segmentation: Another Decade Review (2007-2017)"},{"paperId":"ca13f33970bc70d44a6159db5e14f65841abcf5a","title":"Dependency or Span, End-to-End Uniform Semantic Role Labeling"},{"paperId":"c70df347923d90e05ff19ebd724ffc2fd6232594","title":"Neural-based Pinyin-to-Character Conversion with Adaptive Vocabulary"},{"paperId":"90fe32dd2fccd4ca14713d8b0252aa34a6b910e8","title":"Subword Encoding in Lattice LSTM for Chinese Word Segmentation"},{"paperId":"226ceb666cdb2090fc3ab786129e83f3ced56e05","title":"Sentence Selection and Weighting for Neural Machine Translation Domain Adaptation"},{"paperId":"ea49cef629b6e5a87a596944ad4ea9144651bfdd","title":"Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models"},{"paperId":"8924c57128e878ec5af751ad826768c43f40f0e3","title":"NICT’s Neural and Statistical Machine Translation Systems for the WMT18 News Translation Task"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"5f0c801a72675327c80a9c2beaa971564f578818","title":"Exploring Recombination for Efficient Decoding of Neural Machine Translation"},{"paperId":"aaf17a1615db72b956e345ad4104bad08166faa9","title":"Seq2seq Dependency Parsing"},{"paperId":"b343458f1dcf23439a33313c39fb0275fcde8afa","title":"Finding Better Subword Segmentation for Neural Machine Translation"},{"paperId":"74b3f93ee47fe36ff1862ec7d52745f30ec7be49","title":"Syntax for Semantic Role Labeling, To Be, Or Not To Be"},{"paperId":"8fb5a6fe93a3d19ffee50677c0ae563e3377d2d4","title":"Chinese NER Using Lattice LSTM"},{"paperId":"65cf18f190c191ab77b43841dec97b448ac90737","title":"Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features"},{"paperId":"c8efcc854d97dfc2a42b83316a2109f9d166e43f","title":"Self-Attention with Relative Position Representations"},{"paperId":"d2074077e3cc25536360128f8fec7c403f2d9a88","title":"Syntax-Directed Attention for Neural Machine Translation"},{"paperId":"33998aff64ce51df8dee45989cdca4b6b1329ec4","title":"Graph Attention Networks"},{"paperId":"c80725ad0c0cd06416f3c01a78b7c419359d3fe2","title":"Instance Weighting for Neural Machine Translation Domain Adaptation"},{"paperId":"d145755c038599f340f8958e4a27e93ce522b5f9","title":"Neural Machine Translation with Source Dependency Representation"},{"paperId":"78e592524e336afdf586c51ac228488a71e31340","title":"Sentence Embedding for Neural Machine Translation Domain Adaptation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1e0b165d10704d0d9be4aa55ae89ab566f919758","title":"Fast and Accurate Neural Word Segmentation for Chinese"},{"paperId":"6cddfbed35c46937588bd9d6b846ca2855953cea","title":"Neural Lattice-to-Sequence Models for Uncertain Inputs"},{"paperId":"aab5002a22b9b4244a8329b140bd0a86021aa2d1","title":"OpenNMT: Open-Source Toolkit for Neural Machine Translation"},{"paperId":"6f84694963842a27ab3932473c66a0845c8b0cd7","title":"A Character-Aware Encoder for Neural Machine Translation"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"e43f713e0d2d438a4c0b03eacab58c334e869e6a","title":"Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"0a3489330820856f82e100de239aac82a3459c4a","title":"Neural Word Segmentation Learning for Chinese"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"a4ebc46452c9386a0e439e3e2e693cf0dacf7406","title":"A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing"},{"paperId":"2ac2db458c8fe8ffee9f214521bd757efbbdd05d","title":"An Empirical Study on Word Segmentation for Chinese Machine Translation"},{"paperId":"a0fb0816ddf94e57d12b51971f53a7286cfe06b3","title":"Word Lattice Reranking for Chinese Word Segmentation and Part-of-Speech Tagging"},{"paperId":"860c1880c5a98cc6a5aeb331113eeda0d84e725c","title":"Generalizing Word Lattice Translation"},{"paperId":"f5b1146b7ca79322aab124fd63825b9c175c02cf","title":"Clause Restructuring for Statistical Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"4a85665fede60ecb5ef3eb2a9352f3133152e266","title":"Integrating unsupervised and supervised word segmentation: The role of goodness measures"},{"paperId":"66e5300af0b4cef6c236f5b220157622882a751a","title":"Integrated Chinese Word Segmentation in Statistical Machine Translation"},{"paperId":"65e90d9f6754d32db464f635e7fdec672fad9ccf","title":"The Second International Chinese Word Segmentation Bakeoff"},{"paperId":"2c72257ae7a4a32dc60569f4e1fe4504b2678112","title":"The Penn Chinese TreeBank: Phrase structure annotation of a large corpus"}],"id":"0ab0fda8774c303be8f8f8c8f684a890dcf5d455","summary":"This work proposes lattice-based encoders to explore effective word or subword representation in an automatic way during training and proposes two methods: 1) lattice positional encoding and 2) lattICE-aware self-attention to further improve translation performance."},{"url":"https://www.semanticscholar.org/paper/0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":98,"citationCount":135,"influentialCitationCount":11,"publicationDate":"31/12/2020","authors":"Phillip Rust,Jonas Pfeiffer,Ivan Vulic,Sebastian Ruder,Iryna Gurevych","citations":[{"paperId":"70546742ecfeb46ff41be90d88a4d5c27c55ed2d","title":"When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages"},{"paperId":"2cb039a4dc15612970319d32754526780ee5c9fa","title":"Efficiently Adapting Pretrained Language Models To New Languages"},{"paperId":"95a6a7851def53537b1d81e546af19cf60f872cd","title":"PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications"},{"paperId":"6fdbf4f9ea26068a67de5c16ca0497c317916189","title":"PHD: Pixel-Based Language Modeling of Historical Documents"},{"paperId":"17d0cc24fd6a267e7fd59ae62054de3e5c552096","title":"Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"ed8306efe54cd507f299f3f7e8fb8b5cd9ba2cd4","title":"Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization"},{"paperId":"392a5d5629f23d6209479220c53c1e7760525af7","title":"A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems"},{"paperId":"7dd4b7e6f5f1588eae707df8334589ffd503bc54","title":"A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models"},{"paperId":"64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?"},{"paperId":"98785df4aac68fdeb8ef42ab6c0e74a3ca3eb0e1","title":"Headless Language Models: Learning without Predicting with Contrastive Weight Tying"},{"paperId":"5bd254c0775d18cdc30cb85be61e080484ee6713","title":"Multilingual Text Representation"},{"paperId":"c2cbb952e896cce72193326051c1400d82bc23a8","title":"Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability"},{"paperId":"bde98ab6970961b6a1bde2ab4c5ce83f7060b3ef","title":"Cabrita: closing the gap for foreign languages"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code"},{"paperId":"2b46cf1db16dc624c5ea877a1b3c3a16e950f731","title":"Multi3WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems"},{"paperId":"a4fc3d86b84f351e87bcdbe1376eae64750629ac","title":"Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations"},{"paperId":"45a6f7ca23944aa2050c2bc6d6a580058d032b30","title":"Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation"},{"paperId":"89b5890999ce6874507b0264fa4b468e16c03788","title":"MorphPiece : Moving away from Statistical Language Representation"},{"paperId":"ca7636c1d3426dd09a185abf25f81f7a8fa594e1","title":"How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese"},{"paperId":"ec18aef9ccec70b979d6ab3c78d3721736fd5388","title":"Where’s the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation"},{"paperId":"f4f5c747dbb09a0846b600987379148ca5e1b167","title":"Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review"},{"paperId":"17fbffb05fa14e21d1c506fd5f0f568b955fe983","title":"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models"},{"paperId":"b3cff6abe401a244c21d4706b0931e48acaeeb4e","title":"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models"},{"paperId":"aa376159d056bbdc0e343a268039dcbdd805d611","title":"FOCUS: Effective Embedding Initialization for Specializing Pretrained Multilingual Models on a Single Language"},{"paperId":"0d106c5b14ce468298ee67310edc5e389c64bcdc","title":"Cross-lingual Transfer Can Worsen Bias in Sentiment Analysis"},{"paperId":"44175d77eec72fe53cba68debfdb65fdba12dd5e","title":"Multilingual Event Extraction from Historical Newspaper Adverts"},{"paperId":"b1e67b0cc5705d6aade931e6414ce23dc0ff44b3","title":"Comparing Biases and the Impact of Multilingual Training across Multiple Languages"},{"paperId":"879a7f5abdb7ab803d48172d4f0830965f989d46","title":"Language Model Tokenizers Introduce Unfairness Between Languages"},{"paperId":"7f8ee05074a3724434e6b5dcf4aac7ebdee0c6ff","title":"Multilingual text categorization and sentiment analysis: a comparative analysis of the utilization of multilingual approaches for classifying twitter data"},{"paperId":"c304a962248b62e4efb0f4c6fece676b79dab2d6","title":"Training and Evaluation of a Multilingual Tokenizer for GPT-SW3"},{"paperId":"f08cebd0b795bc1520f1a868c729abecfb666f04","title":"L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT"},{"paperId":"e0c2a2ba6054b18f570a169790b015d333928789","title":"Romanization-based Large-scale Adaptation of Multilingual Language Models"},{"paperId":"e84c3380e18b4b1a02b9945607d2e16897106024","title":"Vietnamese Sentiment Analysis: An Overview and Comparative Study of Fine-tuning Pretrained Language Models"},{"paperId":"27facf41276ede6aa6627fb81a3161d2fbf86730","title":"SwissBERT: The Multilingual Language Model for Switzerland"},{"paperId":"62ad7ea9467bbcdbfe325b9ee561cab3908e4583","title":"MEGA: Multilingual Evaluation of Generative AI"},{"paperId":"9f105fdbe301eb23371f35f697164a19e6c45ed5","title":"OpticalBERT and OpticalTable-SQA: Text- and Table-Based Language Models for the Optical-Materials Domain"},{"paperId":"26e74a16644bdb70e66f3e7b3de40337bd270944","title":"Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension"},{"paperId":"8641c70c106c4f7485e613888b91a58e9812a5a7","title":"MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain"},{"paperId":"5d896fb2f0da16060f22ed43e582464605237f28","title":"Training-free Lexical Backdoor Attacks on Language Models"},{"paperId":"62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c","title":"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!"},{"paperId":"bfff952fb890f3eb4ba22718f1df70a030741b74","title":"Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?"},{"paperId":"66817b3f93901a86079307af230d9e626146120c","title":"Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training"},{"paperId":"6cc1d05250511e596e3607827fda4614abb6ed74","title":"Neural Transfer Learning with Transformers for Social Science Text Analysis"},{"paperId":"15a802ca1e77e678db79f61e36cf9eaf6b273c24","title":"MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource Code Completion"},{"paperId":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only"},{"paperId":"61c7e5590064b250ebb5a53c430aede492a4d8ab","title":"BERT for Natural Language Processing in Bahasa Indonesia"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"fb49e38135302a1c16d644c0f746cef7d5f10ee4","title":"Understanding BLOOM: An empirical study on diverse NLP tasks"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"296c9f9ba0139a3d7c7a1197960a1c9bce5141e7","title":"Punctuation Restoration for Singaporean Spoken Languages: English, Malay, and Mandarin"},{"paperId":"d3a01c0aeddb98b749bb48c97a48310e91e66d36","title":"Multilingual Multimodality: A Taxonomical Survey of Datasets, Techniques, Challenges and Opportunities"},{"paperId":"22fbef2bfef213a7619ee4f307e8f42d1888e638","title":"LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation"},{"paperId":"2231b5e00fa1f8f4b222089fe4bb64a95970b59a","title":"You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"3e08d84af266026e7d254729f3afc6dcd572d264","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","title":"MonoByte: A Pool of Monolingual Byte-level Language Models"},{"paperId":"f5888d776f122f53292973bd3693628ebd265bc6","title":"TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter"},{"paperId":"52b65b29a3d9a7d9e8c016a156454ef8ad80858d","title":"BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification"},{"paperId":"089fd688de463519a68bd25b11ae1c3eb57b207d","title":"Compositional Evaluation on Japanese Textual Entailment and Similarity"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"6fd36dd51da2a18b53fc7bdb9797f279ceb80462","title":"Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"20b0452359524d15a6182269be51302df850f2a7","title":"hmBERT: Historical Multilingual Language Models for Named Entity Recognition"},{"paperId":"d69ec0bbc9fc4fe898ac8cb73f629d253358bf66","title":"Stop Filtering: Multi-View Attribute-Enhanced Dialogue Learning"},{"paperId":"0232715f9089e3a2fc002cff6737bb9939805b8d","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"99be6dc17a7fd399f4af80c4c1cd7ee5247591a1","title":"Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese"},{"paperId":"036e64698594bdec9a70ef1b74e2be467a25e136","title":"Turkish abstractive text summarization using pretrained sequence-to-sequence models"},{"paperId":"210980149a6b41d3e8d95c12daa41d6aa391681f","title":"Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"04207549bf872158d117600029dbe1f1cf8e5b59","title":"Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"597cad6c7b9de94eecc153c7cdcaf824905fe915","title":"You Are What You Write: Preserving Privacy in the Era of Large Language Models"},{"paperId":"7a25155364476839b6d1fc0653cd8611327ab9ba","title":"mGPT: Few-Shot Learners Go Multilingual"},{"paperId":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation"},{"paperId":"d97867578a50f824a19168c024e138e3ca482746","title":"Deep Lexical Hypothesis: Identifying personality structure in natural language"},{"paperId":"b28e23afe988ad83ab5bd9ce7f826c140c533be8","title":"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages"},{"paperId":"63bafbf3e7cfdb576407870137b5751cbb579864","title":"Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies"},{"paperId":"f5d22b267856410695e159970271fd016c51052a","title":"Does Transliteration Help Multilingual Language Modeling?"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"eb0024439858af7cc951ce2efa5a6533c3781799","title":"WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models"},{"paperId":"070759c65bd32df9f1f57ef52a4c49a77d3057d1","title":"To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource NLP"},{"paperId":"6b2062812d2e353ea884a0cc077e9f6c73351423","title":"On Generalist and Domain-Specific Music Classification Models and Their Impacts on Brazilian Music Genre Recognition"},{"paperId":"aeb45c9bec2b3e7876621f923bc1675063994094","title":"Causal and Masked Language Modeling of Javanese Language using Transformer-based Architectures"},{"paperId":"dec42306af017bc778bbf1496776f3cd4d5bd42e","title":"Pre-trained transformer-based language models for Sundanese"},{"paperId":"ed4705fc97d35f8b02a4c11633cb6dd6bd316cc7","title":"Cross-lingual Transfer of Monolingual Models"},{"paperId":"633780929ae262e461ea35c20b36a5d7042350e7","title":"MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model"},{"paperId":"071440ccd1084d20d345fafd0bcaa5993f71fb04","title":"xGQA: Cross-Lingual Visual Question Answering"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"8dad3a12d2a652122458dd377f62025354b17a2a","title":"Subword Mapping and Anchoring across Languages"},{"paperId":"10a863a33c4dda778cd6a552ecbb02ac42510445","title":"On the ability of monolingual models to learn language-agnostic representations"},{"paperId":"657a8a8c83339dff13892b26bb989f97b2acb182","title":"Code-switched inspired losses for spoken dialog representations"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"21230f72afae4f19f59c4ce9c099075e7ecfe77c","title":"gaBERT — an Irish Language Model"},{"paperId":"9e8bad21221b88b516edd28fe902e591ac08efa5","title":"Modelling Latent Translations for Cross-Lingual Transfer"},{"paperId":"d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","title":"A Primer on Pretrained Multilingual Language Models"},{"paperId":"a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study"},{"paperId":"92f2000f29f1aed7f2cc4e3cb5f783e079ef553f","title":"MergeDistill: Merging Language Models using Pre-trained Distillation"},{"paperId":"28459083ba624020c8f1c1ed7c3a075f48b4e709","title":"KLUE: Korean Language Understanding Evaluation"},{"paperId":"9076b287843441fb749d37b69317162c1fa272e3","title":"XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond"},{"paperId":"529edafa160a77901bec123cf8858e6c08f6cd06","title":"When does pretraining help?: assessing self-supervised learning for law and the CaseHOLD dataset of 53,000+ legal holdings"},{"paperId":"3885bdef44dbbcaae85a6b4ccaf279593daadb80","title":"Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems"},{"paperId":"7b99c51d562e33309a46601c846abbe72a65c6a4","title":"What to Pre-Train on? Efficient Intermediate Task Selection"},{"paperId":"a4ab1bd1501668e932c986725b33d065e4f0a233","title":"The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models"},{"paperId":"64cc584aa9bafebbab0bc012e225563cf2873ecc","title":"Introduction to Neural Transfer Learning with Transformers for Social Science Text Analysis"},{"paperId":"13bcfb944779165983aaef22cec8a3bbd3e98e62","title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts"},{"paperId":"5b0c0d181f31acebf2442e7b7c7af27f3f6a5f4a","title":"Hints on the data for language modeling of synthetic languages with transformers"},{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"S ANTA C ODER : DON ’ T REACH FOR THE STARS !"},{"paperId":"5a6b6fbb1ab905d305223d1920205620f0580433","title":"MultiFin: A Dataset for Multilingual Financial NLP"},{"paperId":"4400303cee894ee9096f781cddf166af96ae6ec0","title":"Gradual Language Model Adaptation Using Fine-Grained Typology"},{"paperId":"ca460eb70206eaf2ae1fd0ee1289d81e061e0f6f","title":"Introducing UberText 2.0: A Corpus of Modern Ukrainian at Scale"},{"paperId":"f375613f17d7fb69b7975c279d4f1147598bb4a2","title":"Named Entity Recognition for Low-Resource Languages - Profiting from Language Families"},{"paperId":"4968f4809a197d67a75080bd43cb31c3f428b1d9","title":"DanSumT5: Automatic Abstractive Summarization for Danish"},{"paperId":"001b3fa593710271ff26919b57dbb0101c462b2a","title":"Exploring the Impact of Transliteration on NLP Performance for Low-Resource Languages: The Case of Maltese and Arabic"},{"paperId":"b767c04cd0fc659f71fa90f87af0b778a0d04886","title":"Reducing tokenizer’s tokens per word ratio in Financial domain with T-MuFin BERT Tokenizer"},{"paperId":"2b4369d50ac7310b9908a2baef89a63c68cbd893","title":"Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models"},{"paperId":"d11fc46d6a09e7508e14e43edb384f786a9b6dc7","title":"Improving Standard German Captioning of Spoken Swiss German: Evaluating Multilingual Pre-trained Models"},{"paperId":"bd0e39d075553a68e7af6fdf437c2a890ad59a58","title":"Scene-Text Aware Image and Text Retrieval with Dual-Encoder"},{"paperId":"d3f72c17404ba7f8e56d6d4d7fdf9d808f559570","title":"Fine-tuning de modèles de langues pour la veille épidémiologique multilingue avec peu de ressources (Fine-tuning Language Models for Low-resource Multilingual Epidemic Surveillance)"},{"paperId":"f1edc02e4096acb8ea491b4b47c2cbdf0ff3ec91","title":"Writing System and Speaker Metadata for 2,800+ Language Varieties"},{"paperId":"ea786f376c51dba5c3be79eed6e4814e8afc9290","title":"Faster and Cheaper Energy Demand Forecasting at Scale"},{"paperId":"2ac5442a32988f86730e460b3198f475592ae410","title":"Improving Low-Resource Languages in Pre-Trained Multilingual Language Models"},{"paperId":"8929066ce924696f960512c92a720c70bba65586","title":"On Language Spaces, Scales and Cross-Lingual Transfer of UD Parsers"},{"paperId":"02ef976bc4aedceca2a6d7577975113db1e0e26d","title":"Are Multilingual Sentiment Models Equally Right for the Right Reasons?"},{"paperId":"22c8444eb4da5ae8d43829b127d5e7ee9950b151","title":"Crossing the Conversational Chasm: A Primer on Multilingual Task-Oriented Dialogue Systems"},{"paperId":"76db3624f1278a4f4f1e69b343bfff7bba306d47","title":"XLM-T: A Multilingual Language Model Toolkit for Twitter"},{"paperId":"6adc9c231d874ea358554b8680a6aaba4bd6c963","title":"MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer"},{"paperId":"91ea7176cc41f7b97867d483075df7886aa3dc33","title":"Practical Transformer-based Multilingual Text Classification"},{"paperId":"dca4128a33ca22c02031b5c0c28548a0df022d80","title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding"},{"paperId":"8484b4da1aff6460edbd2df78bf2f5e801569248","title":"GSI-UPM at IberLEF2021: Emotion Analysis of Spanish Tweets by Fine-tuning the XLM-RoBERTa Language Model"},{"paperId":"7e5133e41d5a06b8ad4bad7f2046ac73edf43e38","title":"TUDa at WMT21: Sentence-Level Direct Assessment with Adapters"},{"paperId":"892731fd37a1d38901b0c5bc68168d96f43bd5d0","title":"Monolingual Pre-trained Language Models for Tigrinya"},{"paperId":"9d31cfe09f93a5d87657313c503b17619c2ae107","title":"LexFit: Lexical Fine-Tuning of Pretrained Language Models"},{"paperId":"cb7c0f9aa1060e237ce2d29260334538e0e04a56","title":"F OCUS : Effective Embedding Initialization for Monolingual Specialization of Multilingual Models"}],"references":[{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"13bcfb944779165983aaef22cec8a3bbd3e98e62","title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts"},{"paperId":"31392ad8722d9c66181b621936e2013199e02edc","title":"When Do You Need Billions of Words of Pretraining Data?"},{"paperId":"85dc7829455819283270eb643817bcf97133464d","title":"From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"bdeec55f95fd6b73e3e4635459b14c7248543efb","title":"AdapterDrop: On the Efficiency of Adapters in Transformers"},{"paperId":"9be1d1bf82f6ca6a7bf6a7d92f8f37b647e493d0","title":"Probing Pretrained Language Models for Lexical Semantics"},{"paperId":"33add47a46818f2e6a63cfe84e539720111f844f","title":"MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale"},{"paperId":"110c13fbf4ff87b52ee1fd9eb2d3616c839ceb41","title":"Parsing with Multilingual BERT, a Small Treebank, and a Small Corpus"},{"paperId":"03f22e693a0c00bae8a66a64a2fecb0f11a4b034","title":"IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding"},{"paperId":"575ac3f36e9fddeb258e2f639e26a6a7ec35160a","title":"Is Supervised Syntactic Parsing Beneficial for Language Understanding? An Empirical Investigation"},{"paperId":"c0ba595b2bef54f2552ec4716bb187901f52f4a3","title":"KR-BERT: A Small-Scale Korean-Specific Language Model"},{"paperId":"063f8b1ecf2394ca776ac61869734de9c1953808","title":"AdapterHub: A Framework for Adapting Transformers"},{"paperId":"528dd0da358b4939d99eeb92548deccfeac48bd6","title":"A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages"},{"paperId":"e00631018e737355f1b0b3db779641f8f26288b1","title":"WikiBERT Models: Deep Transfer Learning for Many Languages"},{"paperId":"8b8c29c0cbb6cbae26b930840396596dd5806f33","title":"Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers"},{"paperId":"14489ec7893e373a0dcc9555c52b99b2b3a429f6","title":"Are All Languages Created Equal in Multilingual BERT?"},{"paperId":"98ef0db84e62aef969629264c9de1f4d0013f3b9","title":"AdapterFusion: Non-Destructive Task Composition for Transfer Learning"},{"paperId":"d97e7561fa7710213ccd4f8128044ea6849be377","title":"XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"3b233bdb697cc43effa1eb6d2868ff14efbbab7a","title":"UDapter: Language Adaptation for Truly Universal Dependency Parsing"},{"paperId":"41a7fece6e8c47d5bff75f7701a702f351b110d6","title":"BERTurk - BERT models for Turkish"},{"paperId":"b805693c17961af2cc7f859c1a54320b26036f46","title":"Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"6551f742b825561d26242ca8a646ba0e33fb109f","title":"What the [MASK]? Making Sense of Language-Specific BERT Models"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"2bd54adb3b5588281396a4b5dae7db09496b2c61","title":"SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis"},{"paperId":"a4d5e425cac0bf84c86c0c9f720b6339d6288ffa","title":"BERTje: A Dutch BERT Model"},{"paperId":"3b2538f84812f434c740115c185be3e5e216c526","title":"Cross-Lingual Ability of Multilingual BERT: An Empirical Study"},{"paperId":"477d66dcd2c08243dcc69822d6da7ec06393773a","title":"Multilingual is not enough: BERT for Finnish"},{"paperId":"4fa37d012ad0014552a6a5a03624b29f95558bf7","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"6ffb1cc32ddde6ae01e2fc0286eafa116ade0ffb","title":"KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension"},{"paperId":"81fbf08beb80b01abaa6ad6a07b48c3034ead8a6","title":"Is Multilingual BERT Fluent in Language Generation?"},{"paperId":"26dc86404fac9d8ead6c98e8237e29c4a5f7981f","title":"Improving Bi-LSTM Performance for Indonesian Sentiment Analysis Using Paragraph Vector"},{"paperId":"498387b28d8cec0a7f9aef8411e1d94f5ca7f86b","title":"2019 International Conference of Advanced Informatics: Concepts, Theory and Applications (ICAICTA)"},{"paperId":"e471dccea2b2a05196137984847d7e2067d259a1","title":"OSIAN: Open Source International Arabic News Corpus - Preparation and Integration into the CLARIN-infrastructure"},{"paperId":"a179b6fdbaf2fbad5d7fe3dc0cc34351bc586439","title":"A Finnish news corpus for named entity recognition"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a3958ca44a736bd58a991d42fb2fdf6bfcb2029e","title":"Sentiment Analysis of Product Reviews in Russian using Convolutional Neural Networks"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"82d40215de43fbc2aa3b0f8c6ebba73f35e64c9b","title":"An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese"},{"paperId":"cc94d15ba408c260c8fe4fa4f1cb6797a996dd21","title":"Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language"},{"paperId":"2fa3f7ce620a1c7155daef6620dd6bb0e01934f3","title":"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"},{"paperId":"06979d126d8286866624f20907e57e8e8aa7df52","title":"Polyglot Contextual Representations Improve Crosslingual Transfer"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"e99e2bd4812b30e104db0feddb681f32acd88758","title":"Massively Multilingual Transfer for NER"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"3cd8d09e47b0acbd52e07d14eeb3faf6260760da","title":"Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing"},{"paperId":"c997d481606f0346164511cabe74c6d1ef3f6be5","title":"DRCD: a Chinese Machine Reading Comprehension Dataset"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"9589244bbff8c5b5e57f52f99776cda332e6ba48","title":"A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"d89ee98810039d2061ed42ee8026da49c503d16b","title":"Learning multiple visual domains with residual adapters"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"a1aaa7c75464e7ebe41cfe5c5258241ca34c6414","title":"Farasa: A Fast and Furious Segmenter for Arabic"},{"paperId":"d115eceab7153d2a1dc6fbf6b99c3bdf1b0cdd46","title":"Universal Dependencies v1: A Multilingual Treebank Collection"},{"paperId":"c5f5bb3131f5f1082bd82cce8a0ac08dea1e9366","title":"UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"66182e1ba6dca20f315cd952866a94f54d8ac820","title":"Cross-lingual polarity detection with machine translation"},{"paperId":"649d03490ef72c5274e3bccd03d7a299d2f8da91","title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"8656e5a885613f29f0f2f35589d865baeb1317a6","title":"Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation"},{"paperId":null,"title":"2020)) with batch size"},{"paperId":null,"title":"Universal dependencies 2.6. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL)"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning"},{"paperId":"e1e43d6bdb1419e08af833cf4899a460f70da26c","title":"AlBERTo: Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets"},{"paperId":null,"title":"Selection of pretrained models used in our experiments. We display the respective vocabulary sizes and the proportion of tokens"},{"paperId":null,"title":"2019) only included text passages from the articles, and used older Wikipedia dumps"},{"paperId":null,"title":"Question Answering dataset overview"},{"paperId":null,"title":"Exploring BERT's Vocabulary. Blog Post"},{"paperId":null,"title":"2020), we only use masked language modeling (MLM) as pretraining objective and omit the next sentence prediction task"},{"paperId":null,"title":"We pretrain the new monolingual models (MONOMODEL-*) from scratch for 1M steps with batch size 64"},{"paperId":"fdaa957d6c0a5c99b1cdbbf9054fe9eeed40b01c","title":"Hotel Arabic-Reviews Dataset Construction for Sentiment Analysis Applications"},{"paperId":"5308b9d6c001304a882a50891ccce9f7ccb1c3ec","title":"On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":null,"title":"Korea Maritime and Ocean University (KMOU) NER dataset29 for KO. For AR, ID, JA, RU, and TR, we use the respective portions of the WikiAnn"},{"paperId":null,"title":"2016. 1.5 billion words arabic"},{"paperId":null,"title":"Wikiextractor"},{"paperId":null,"title":"Sentiment Movie Corpus (NSMC)32 for KO, the RuReviews dataset (Smetanin and Komarov, 2019) for RU, the movie and product reviews datasets by Demirtas and Pechenizkiy"},{"paperId":null,"title":"News Articles 14041"},{"paperId":"8c8f198d582898ef06aea1edc51cbc419d922a00","title":"Early Stopping-But When?"},{"paperId":null,"title":"Universal Dependency Parsing (UDP), and Part-of-Speech Tagging (POS)"},{"paperId":null,"title":"Furkan Atmaca, Mohammed Attia"},{"paperId":null,"title":"Sentiment Analysis (SA)"}],"id":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","summary":"It is found that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language."},{"url":"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":68,"citationCount":233,"influentialCitationCount":29,"publicationDate":"28/05/2021","authors":"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel","citations":[{"paperId":"e58147980d42f845879c4231840c2857d82337eb","title":"MELA: Multilingual Evaluation of Linguistic Acceptability"},{"paperId":"cba0b336e7a2001d891d8d29275542367cd52925","title":"Lexical Normalization Using Generative Transformer Model (LN-GTM)"},{"paperId":"5f3d6c077712a2d692bb29671c084e098854c738","title":"Learning Mutually Informed Representations for Characters and Subwords"},{"paperId":"c0e62e324dfc7bd45ec0a5ce4055823d1f0c03f1","title":"Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval"},{"paperId":"ab2cd01090bebd7c52193c432e0a0fce4226982e","title":"Generating Pragmatic Examples to Train Neural Program Synthesizers"},{"paperId":"4d51e88131a7f1acc6e74a3d750f18bad527dcd7","title":"Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision"},{"paperId":"fd665a3d8c4a5fde1e1b00f78ab231d99b22abd0","title":"Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew"},{"paperId":"d921732ca5920049b6a8194f559917689f02d096","title":"Text Rendering Strategies for Pixel Language Models"},{"paperId":"d9655aad891e0dba404bc132c8419f93c098c8f8","title":"Learning to Abstract with Nonparametric Variational Information Bottleneck"},{"paperId":"0a550ee375df53c845f46084de4f0e7f16170d9a","title":"NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task"},{"paperId":"17d0cc24fd6a267e7fd59ae62054de3e5c552096","title":"Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"61644c4b76a11501d3b9b85fcfed513c56f184a7","title":"Enhancing Neural Machine Translation with Semantic Units"},{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"3448e136046d22d4f90d7a4c875890f5fb64b811","title":"PuoBERTa: Training and evaluation of a curated language model for Setswana"},{"paperId":"997cebb936d88577da59ba460a9141bdd5dcb36f","title":"To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer"},{"paperId":"64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?"},{"paperId":"c30deb8f4b0e942a36e5ef5c87bed01f575c0c6e","title":"Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention"},{"paperId":"2e9f3a96d6f6011ff1b4eab541ef1df12cde042f","title":"Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation"},{"paperId":"b4eb0ca9f019254f7d1ec6432a095e526fa3abc2","title":"Large Synthetic Data from the arXiv for OCR Post Correction of Historic Scientific Articles"},{"paperId":"10af858834ad69f1c30721e6aa732d77fa369161","title":"Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models"},{"paperId":"376f0d8bc222a33fb382933552e796bb21e6191b","title":"Abbreviation Disambiguation in Polish Press News Using Encoder-Decoder Models"},{"paperId":"e892a225c417fbac7545c3e31b45d1c42dc9c933","title":"Chain-of-Thought Reasoning is a Policy Improvement Operator"},{"paperId":"ff50383c09346e3e7d16856af3519fa09a9f8580","title":"Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning"},{"paperId":"119639d87c7ce418c57070a41505dc7ac9eb7e1d","title":"Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap"},{"paperId":"3fa61652b1a4f314590e5e8bfe8f023bbc5c3568","title":"Ngambay-French Neural Machine Translation (sba-Fr)"},{"paperId":"dc0436318a08f4df8f9653f164a830f245caca8b","title":"GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering"},{"paperId":"064af570792c61f5cc46814c648fd3969f0999e7","title":"Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction"},{"paperId":"1943144f998c2620616b3e89d376ff801b6e04c6","title":"Lightweight Adaptation of Neural Language Models via Subspace Embedding"},{"paperId":"ce5ea17bffd6333e414422d680827fef2b0ce41a","title":"Correcting Wide-Range of Arabic Spelling Mistakes Using Machine Learning and Transformers"},{"paperId":"8ce219059d777c2333ee21cb2af2aad71275c98f","title":"LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"},{"paperId":"d57cbec10dfee4e244657d3cdf9ba4cf53086744","title":"An Empirical Analysis Towards Replacing Vocabulary-Rigid Embeddings by a Vocabulary-Free Mechanism"},{"paperId":"d5241be77ff0aec9a2ac1e1b14fb69df897eb8f3","title":"Bi-Phone: Modeling Inter Language Phonetic Influences in Text"},{"paperId":"d7890d1906d95c4ae4c430b350455156d6d8aed9","title":"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"},{"paperId":"a6c9b15a529c0348fa812e25b6612492f9aedae6","title":"Estimating Post-OCR Denoising Complexity on Numerical Texts"},{"paperId":"714671a26060d289a5e12888a92e904a19034982","title":"MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems"},{"paperId":"2adc13eb55c92e026c4cefc89a47a0ee0ac95111","title":"SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding"},{"paperId":"d5d936b142eb81826002888230649c805532630e","title":"VisText: A Benchmark for Semantically Rich Chart Captioning"},{"paperId":"87875a07976c26f82705de1fc70041169e5d652b","title":"LeanDojo: Theorem Proving with Retrieval-Augmented Language Models"},{"paperId":"4e12e831bc1627545479c65d7b2296d6d2562c9a","title":"A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing"},{"paperId":"5cd4fe8174b582f27d79859e4b79e0eba2a1d5f0","title":"Is Anisotropy Inherent to Transformers?"},{"paperId":"4bd35d344c635b05f97f4d749741d196ff541bf3","title":"A Primer on Seq2Seq Models for Generative Chatbots"},{"paperId":"a903e1e0ffd04dd666f3537f6570d742d7be3486","title":"Grounded Text-to-Image Synthesis with Attention Refocusing"},{"paperId":"02dde8e9fefafae8e0b9ee2eef0ddc74c511f7cd","title":"Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models"},{"paperId":"29a54c072bcf85fa6934b6f701962a7c9aeb6489","title":"Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora"},{"paperId":"5fbe4c92791fbecb179c1ab79bba9a59b2e155ba","title":"GlyphControl: Glyph Conditional Control for Visual Text Generation"},{"paperId":"b4ee74a1f294cb5bb028a958ce43219cabb0f347","title":"TranSFormer: Slow-Fast Transformer for Machine Translation"},{"paperId":"ac36b65a6fa4a9e84de051f0d3e9d50348fa4160","title":"Lexinvariant Language Models"},{"paperId":"d1e7b054578ae0606effe63ab8402861ec54a49e","title":"Sāmayik: A Benchmark and Dataset for English-Sanskrit Translation"},{"paperId":"b3cff6abe401a244c21d4706b0931e48acaeeb4e","title":"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models"},{"paperId":"33c017b6298dd6f7d099f88f9667a6ea97131dbc","title":"mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models"},{"paperId":"2cfb1f44b34204213d789731871e599c756bdb83","title":"Exploring Large Language Models for Classical Philology"},{"paperId":"d7a379254ac2dee2e31dfdeedb440176f0285aa5","title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding"},{"paperId":"6237a49297f45ebfdeabbbf67d06de323d1fccd4","title":"Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer"},{"paperId":"7c217cc7524251f42887438834912e06129c3299","title":"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis"},{"paperId":"39bca01efce8765f0a5d3a8981bc30d56f196b96","title":"XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages"},{"paperId":"879a7f5abdb7ab803d48172d4f0830965f989d46","title":"Language Model Tokenizers Introduce Unfairness Between Languages"},{"paperId":"2183c88e9056e931b07d48f1dc44360785952073","title":"SoundStorm: Efficient Parallel Audio Generation"},{"paperId":"ea8197cb357af6f89e8b6e5548897236a24719b1","title":"What is the best recipe for character-level encoder-only modelling?"},{"paperId":"b4790ca1a967b44c3028b73c0c00d501fcd81728","title":"Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages"},{"paperId":"af6c6941acecfb23d899cd5266efdf2e27463f12","title":"Causal Language Model Aided Sequential Decoding With Natural Redundancy"},{"paperId":"37d8bc1c268773dc69c92c6f550393c93e9a1f7a","title":"Automatic document classification via transformers for regulations compliance management in large utility companies"},{"paperId":"e0d09d91784a6d426ffcd2fd12448dd9c8ceefe9","title":"Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages"},{"paperId":"ba184d335a9a08c52c5d25eabd7f4a8ea987918b","title":"UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining"},{"paperId":"15d12739b26783e2cf38bc0cbd81557fcc078eb0","title":"Sabiá: Portuguese Large Language Models"},{"paperId":"2b8d28149a43b9659a6da2c56014ec4206a912b4","title":"Ticket automation: An insight into current research with applications to multi-level classification scenarios"},{"paperId":"10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7","title":"GlyphDraw: Seamlessly Rendering Text with Intricate Spatial Structures in Text-to-Image Generation"},{"paperId":"3add55c068fe19bb2e5392cbe994602a91630ec1","title":"Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams"},{"paperId":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!"},{"paperId":"eef5b8f3c4e60d596a04101d8261c222ab739861","title":"Fine-Tashkeel: Fine-Tuning Byte-Level Models for Accurate Arabic Text Diacritization"},{"paperId":"0a438980ac42451d6d32dd2ad8ead7b55520408d","title":"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning"},{"paperId":"4e97303aeb299ee736b1b8c29cef046212690354","title":"Generation-based Code Review Automation: How Far Are Weƒ"},{"paperId":"1d405cbbfecd02598bab517a23de50d6d90c0e88","title":"DTT: An Example-Driven Tabular Transformer by Leveraging Large Language Models"},{"paperId":"2c5460afa19ad6fc2568b7e210115acacc14a40c","title":"An Overview on Language Models: Recent Developments and Outlook"},{"paperId":"f3d894cf6f7be14a545019f4621ccce41f45b088","title":"Learning the Legibility of Visual Text Perturbations"},{"paperId":"b169f2ce55e14584d2db6f64eeb5ad2702d39d40","title":"Artificial intelligence foundation and pre-trained models: Fundamentals, applications, opportunities, and social impacts"},{"paperId":"117a0c2c23a5a558cf3b39468d917a750bca720c","title":"Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation"},{"paperId":"96b005d1d92211cf053e4114a85a5e64e428d896","title":"Extending English IR methods to multi-lingual IR"},{"paperId":"67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation"},{"paperId":"1e4b6567ff66de6cdcbe58c863538241e2f62b45","title":"Aligning Text-to-Image Models using Human Feedback"},{"paperId":"a794a92e1fa516250390514eeeb3c3b3140876a3","title":"RetVec: Resilient and Efficient Text Vectorizer"},{"paperId":"0704a96e1c57c12031f1c3ca492a91dbed1f61ce","title":"Distillation of encoder-decoder transformers for sequence labelling"},{"paperId":"2da686685c543c3c244567ea6d6c6a8292f5d1e1","title":"Composer's Assistant: Interactive Transformers for Multi-Track MIDI Infilling"},{"paperId":"62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c","title":"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models"},{"paperId":"20c7d73609ba98fa27b1edc7b537ef59442e4ba2","title":"Truveta Mapper: A Zero-shot Ontology Alignment Framework"},{"paperId":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models"},{"paperId":"7cdebb73662387d9040da4f27a7dc04dbffa3c3e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"2f272ca3394656835bf32a30b80c6f8ba1c8f4c4","title":"Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks"},{"paperId":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only"},{"paperId":"4aa09cba27a489ca02471fad011ea4854fc63cc1","title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue"},{"paperId":"7c1c95f5fb7fce563171fcc0060c850390753b3c","title":"Advancing Multilingual Pre-training: TRIP Triangular Document-level Pre-training for Multilingual Language Models"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"be2df0fafa89b6355d1ff1336c10e0d4c8d27276","title":"Massively Multilingual Natural Language Understanding 2022 (MMNLU-22) Workshop and Competition"},{"paperId":"3af872a4bd314db9e7964876fee811823628f83f","title":"Text Normalization on Code-Mixed Twitter Text using Language Detection"},{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"},{"paperId":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"5e52d654fd31f04c1bd884cd5480e6af8c95ad50","title":"Efficient Transformers with Dynamic Token Pooling"},{"paperId":"af277778904463965c60626e22783d3e1740058b","title":"Introducing Semantics into Speech Encoders"},{"paperId":"d50b9db750ded246bde13e2c263e341bcbd8a335","title":"A Benchmark and Dataset for Post-OCR text correction in Sanskrit"},{"paperId":"0783c214623c18f6a8ad96b8eaf4a67a382e68ee","title":"QAmeleon: Multilingual QA with Only 5 Examples"},{"paperId":"d7e0f0cec28c34710fa631df410b717186741db5","title":"A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces"},{"paperId":"9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages"},{"paperId":"92302ab168429c7c3a8f699b35ba8302916c6e7c","title":"Bridging Speech and Textual Pre-Trained Models With Unsupervised ASR"},{"paperId":"5e786d47e3dc5ffa65ba8548ff67139a2764acf8","title":"T5lephone: Bridging Speech and Text Self-Supervised Models for Spoken Language Understanding Via Phoneme Level T5"},{"paperId":"d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6","title":"SLING: Sino Linguistic Evaluation of Large Language Models"},{"paperId":"6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc","title":"Graphemic Normalization of the Perso-Arabic Script"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"3e08d84af266026e7d254729f3afc6dcd572d264","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72","title":"Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning"},{"paperId":"7bd8859b5920c7b769e6d40dbdbcd857c1770401","title":"Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining"},{"paperId":"aa94724d1dbc9cad0ad3377903174e776175837a","title":"Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino"},{"paperId":"2a8aac78df5e1e9658a02d381662ed26c6577713","title":"Disease diagnostic method based on cascade backbone network for apple leaf disease classification"},{"paperId":"bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","title":"MonoByte: A Pool of Monolingual Byte-level Language Models"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?"},{"paperId":"e4437293a703fcf282bf1b38bf7900ed8ca711bb","title":"Training a T5 Using Lab-sized Resources"},{"paperId":"a806041621acb5cf648fc780f1ff14939aa3a721","title":"How Much Does Lookahead Matter for Disambiguation? Partial Arabic Diacritization Case Study"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"00d9a73c54053a32e2aba92b53fc3e6bc71d3238","title":"Sequence-to-sequence pretraining for a less-resourced Slovenian language"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","title":"Confident Adaptive Language Modeling"},{"paperId":"b308511ac52e1c2da915d1e55d5246dfdb4cbe28","title":"Romanian Question Answering Using Transformer Based Neural Networks"},{"paperId":"a766ef0678aade6a9798552618819cf4d0fac406","title":"TEVR: Improving Speech Recognition by Token Entropy Variance Reduction"},{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"0232715f9089e3a2fc002cff6737bb9939805b8d","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"32afdb07021fda775ceaedd231c58bfed0aa980a","title":"Automated Crossword Solving"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"b21670e8061a06ab97e7d6052c9345a326e84ff8","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"063d9fa4861356500219b7e81d5a654aa921da6f","title":"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation"},{"paperId":"7267812178393b8ae0b99648f02661ca1ff2b412","title":"Bilingual End-to-End ASR with Byte-Level Subwords"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"0de580957d23dd65e31b6c95e6bc5d15bc15c57d","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"},{"paperId":"0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"880c8973ea1376c6bff23226b3f64792657aa666","title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"943487997ecd26e871a2ab16160bd5640020369d","title":"Towards Best Practices for Training Multilingual Dense Retrieval Models"},{"paperId":"1ed66e048bb025e75aa5ea660545285212e5341f","title":"Scaling Up Models and Data with t5x and seqio"},{"paperId":"b1dad820853464b30c93b52366b32690ba4b99a6","title":"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View"},{"paperId":"a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia"},{"paperId":"8a4eec70e3d0c43abf26757252ad6210c9717f6c","title":"Towards Lithuanian grammatical error correction"},{"paperId":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation"},{"paperId":"7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"},{"paperId":"8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"},{"paperId":"f357b35e068bbfbb8468b48198ab63241713629d","title":"Correcting diacritics and typos with ByT5 transformer model"},{"paperId":"7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models"},{"paperId":"e53f455b3300d95738dac117419c88fbde58ac4e","title":"Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"0ae82f96cb8c1d2da5d8284765dd76a139ba6ff6","title":"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers"},{"paperId":"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","title":"Large Dual Encoders Are Generalizable Retrievers"},{"paperId":"6e2682eb2fbec93b329028b23764c1164e232c41","title":"ÚFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5"},{"paperId":"f3bed81c50e293c03f6b650cdb9351d14e9be347","title":"Deciphering “the language of nature”: A transformer-based language model for deleterious mutations in proteins"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer"},{"paperId":"11ccb9b509d84e845901ee097e7d0a6419fdc182","title":"EncT5: A Framework for Fine-tuning T5 as Non-autoregressive Models"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e35357ac461a669fe7e4b877ee1fad0dfda26303","title":"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings"},{"paperId":"c107835a05ca6fda6e73b64e2ed9884de4fcec0f","title":"A proposed conceptual framework for a representational approach to information retrieval"},{"paperId":"a70fc86508bd0133d5d984a4e777abef1934d76c","title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"},{"paperId":"6e0f8a47072de7e549182a5d0fc07c6d0a207325","title":"Predicting Ethnicity from Names with rethnicity: Methodology and Application"},{"paperId":"39262814fa3e47905a2e5facf13465a1f70706b9","title":"Single-Read Reconstruction for DNA Data Storage Using Transformers"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"972808b92159a782f5ca1c1ab3a8ed3867acfb2d","title":"mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset"},{"paperId":"89a4f4d1f8c93da85d829a1acfe8cafea2b50c00","title":"Transfer Learning for Multi-lingual Tasks - a Survey"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"c41819413725668fbf8e3ca1a0bcaf7fb691e582","title":"How Optimal is Greedy Decoding for Extractive Question Answering?"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer"},{"paperId":"31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview"},{"paperId":"ed7b51e4a5c4835218f6697b280afb2849211939","title":"Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation"},{"paperId":"8cc3c64e1aee320609b6b964a9a6f6f50177e20f","title":"Publish or Hold? Automatic Comment Moderation in Luxembourgish News Articles"},{"paperId":"cfe7cb66390ea0b433383d498e6eff555a198c54","title":"Murreviikko - A Dialectologically Annotated and Normalized Dataset of Finnish Tweets"},{"paperId":"d53d4ff9f3bda51534184344845a78f8c02badd9","title":"DiatopIt: A Corpus of Social Media Posts for the Study of Diatopic Language Variation in Italy"},{"paperId":"5088cd22bbd7e1b798df39eb7f3c4ae305ab7625","title":"Findings from the Bambara - French Machine Translation Competition (BFMT 2023)"},{"paperId":"380605105531d27474190451183bf6ad6126cac8","title":"GPoeT: a Language Model Trained for Rhyme Generation on Synthetic Data"},{"paperId":"82e1313f28afde442930b94bc6ed582d17e8d4b3","title":"Generating Errors: OCR Post-Processing for Icelandic"},{"paperId":"13b8060acc3db1fc555f6e55368f6d02899a1698","title":"FairPrism: Evaluating Fairness-Related Harms in Text Generation"},{"paperId":"1d578d2bdb5c920b224cfca73868566731eaeebd","title":"Towards Analysis of Biblical Entities and Names using Deep Learning"},{"paperId":"035315281c72763a3e0956775732e64f5f193d82","title":"Natural Language Generation with Pixels"},{"paperId":"e5adb9bf3f5ed9c253f38949b22e86775dca443a","title":"Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning"},{"paperId":"c3904ef47bec4fc2bfd3d390370681c33542d62d","title":"Fast Whitespace Correction with Encoder-Only Transformers"},{"paperId":"ffa4ac4a51148208cadb4084dddab954e5f57400","title":"Resolving Elliptical Compounds in German Medical Text"},{"paperId":"88108f061379045c299d62f487694cb4e6d6d4ff","title":"Findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing"},{"paperId":"659be1ff350634f50cc066d258ee6a45e697e552","title":"SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing"},{"paperId":"4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations"},{"paperId":"2bd6afb11d81fb97bf6e2114043c35bd12c96ce9","title":"P A LI: A J OINTLY -S CALED M ULTILINGUAL L ANGUAGE -I MAGE M ODEL"},{"paperId":"8e410a836174847e3fc4e5993f625e89e34dd3d1","title":"RST Discourse Parsing as Text-to-Text Generation"},{"paperId":"17d2c990dd6d25f433b02ce611ce0a57db038dc5","title":"Jetsons at the FinNLP-2023: Using Synthetic Data and Transfer Learning for Multilingual ESG Issue Classification"},{"paperId":"f83740ef449e705187a0f7d6f76b819c99340bd1","title":"Exploring the Limits of Small Language Models"},{"paperId":"e4a1e9bb360f29aceb56079f52484c4a4de1298d","title":"Intégration du raisonnement numérique dans les modèles de langue : État de l’art et direction de recherche"},{"paperId":"2b4369d50ac7310b9908a2baef89a63c68cbd893","title":"Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models"},{"paperId":"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","title":"On the Influence of Tokenizers in NLP Pipelines"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"9758782feed4b4b9bf0ec18b802462e8023a7f83","title":"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data"},{"paperId":"32290d428b50f64d1cfee6ea658dc6ba977b26e9","title":"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia"},{"paperId":"2b6b38b66fcca218cb2f381e5d4711b5c99a784f","title":"Training Text-to-Text Transformers with Privacy Guarantees"},{"paperId":"5f77157038dc7f189db7e72ea58567039222d9df","title":"Examining Single Sentence Label Leakage in Natural Language Inference Datasets"},{"paperId":"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","title":"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference"},{"paperId":"2005311276a1a90dc17cf6e2ca7725fee2e76e1e","title":"BasqueGLUE: A Natural Language Understanding Benchmark for Basque"},{"paperId":"143659fcf93f33e9139f594cf8111c6bc85c04fa","title":"Overview of the EvaLatin 2022 Evaluation Campaign"},{"paperId":"6102fe88a512290b80e83ed2fe17606b166e505a","title":"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin"},{"paperId":"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","title":"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"0ffb0b109acddf0067aec252221e0ea04d317ddc","title":"A Framework for Sexism Detection on Social Media via ByT5 and TabNet"},{"paperId":"038103632b24619818b159f5ca37b848744817db","title":"DENTRA: Denoising and Translation Pre-training for Multilingual Machine Translation"},{"paperId":"710ceab1ff91b96e8596b8400ebf2912ee6e2836","title":"Machine Translation for Multilingual Intent Detection and Slots Filling"},{"paperId":"3fa2e17332bb2888318f504cf37026001b932900","title":"Comparison of Token- and Character-Level Approaches to Restoration of Spaces, Punctuation, and Capitalization in Various Languages"},{"paperId":"c31da40e092e808f20f782eb1ee9d4dec6351708","title":"Overview of EXIST 2022: sEXism Identification in Social neTworks"},{"paperId":"de0e1f9980afa7949df64d53b8ae7a2f59c55579","title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages"},{"paperId":"7175caf7568d46c857380d0e5b64653819d5cc45","title":"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"},{"paperId":"ae7fc32df9401a86353185515f4fd5e2020ac922","title":"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations"},{"paperId":"e873ddaf58ff92ae0492983cf57221fb25837f4e","title":"Strategies in subword tokenization: humans vs. algorithms"},{"paperId":"29e593736c95fd043b7cbb0211d2f7b5cb70e5bf","title":"LLI-UAM Team at FinancES 2023: Noise, Data Augmentation and Hallucinations"}],"references":[{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"2b66a0d8a00b0ba7b585e11ab34879420ad351cb","title":"Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution"},{"paperId":"824cd8db8a68732db04f4d8b7139eb4475e59ff2","title":"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":"e46c97b2d3bea4c21eb462d5771647a0de99b81c","title":"Ensemble Self-Training for Low-Resource Languages: Grapheme-to-Phoneme Conversion and Morphological Inflection"},{"paperId":"278f4f68309fc4ffb83bfc4ba86a0ea005106682","title":"The SIGMORPHON 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion"},{"paperId":"8c122ff82dedf72fa7ef5342622667bf5848916e","title":"One-Size-Fits-All Multilingual Models"},{"paperId":"b044395ed89de33b43d304c008d3c5a7727d423d","title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection"},{"paperId":"c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c","title":"Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"b00cda1bf34cc674676bcde38a46e8fe86d8b825","title":"TWEETQA: A Social Media Focused Question Answering Dataset"},{"paperId":"be564b861e5a9bf5d1dec531807e711000027380","title":"One Size Does Not Fit All: Comparing NMT Representations of Different Granularities"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"5e93a30656ef091f55ce42374d498f2b881e0c47","title":"Bytes Are All You Need: End-to-end Multilingual Speech Recognition and Synthesis with Bytes"},{"paperId":"1125d986433735ffc63dc0a8be466043e4f5c5b4","title":"Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"305b2cf37e5dece81e95c92883d5a6e28ac93b22","title":"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"89cb259f39eb8350dd337fc1d02e2f4128c8398c","title":"Byte-based Neural Machine Translation"},{"paperId":"2397ce306e5d7f3d0492276e357fb1833536b5d8","title":"On the State of the Art of Evaluation in Neural Language Models"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"563783de03452683a9206e85fe6d661714436686","title":"HyperNetworks"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"7dba53e72c182e25e98e8f73a99d75ff69dda0c2","title":"Recurrent Highway Networks"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"4dabd6182ce2681c758f654561d351739e8df7bf","title":"Multilingual Language Processing From Bytes"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"93c20e38c85b69fc2d2eb314b3c1217913f7db11","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"d266d00cc0348bdd0692687befef68fc56433630","title":"Fifth Conference of the European Chapter of the Association for Computational Linguistics"},{"paperId":"4675f56774a2cff5c57bf7ea9800426f80601e25","title":"Louisiana"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":null,"title":"Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu"},{"paperId":null,"title":"Technologies, Volume 1 (Long and Short Papers) pages"}],"id":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","summary":"This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts."},{"url":"https://www.semanticscholar.org/paper/8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","title":"Adversarial Subword Regularization for Robust Neural Machine Translation","venue":"Findings","year":2020,"referenceCount":48,"citationCount":4,"influentialCitationCount":0,"publicationDate":"01/04/2020","authors":"Jungsoo Park,Mujeen Sung,Jinhyuk Lee,Jaewoo Kang","citations":[{"paperId":"c5e9fdb1b81edf470141843c44264a5eb1ff0cc1","title":"TransFool: An Adversarial Attack against Neural Machine Translation Models"},{"paperId":"457e73be2f876e0b838f0f8d7aa921993b7f607d","title":"Consistency Training with Virtual Adversarial Discrete Perturbation"},{"paperId":"710033d861e50f80b28c1fa68316c9ea8ab3c42c","title":"Prediction Difference Regularization against Perturbation for Neural Machine Translation"},{"paperId":"055367775f92ab03d53c470f42bc7284d4a256d8","title":"KOAS: Korean Text Offensiveness Analysis System"}],"references":[{"paperId":"32e7c6f91ab45e8db1ce6735e0d54a0e60f0d098","title":"Domain Robustness in Neural Machine Translation"},{"paperId":"229aff74294275d31ded5c8e6e80f6aa6a88fcfd","title":"A Latent Morphology Model for Open-Vocabulary Neural Machine Translation"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"b1e7ffa599c0075ab03ce7a949376ae4ccce9a05","title":"Subword Language Model for Query Auto-Completion"},{"paperId":"18a1c21f35153c45d0ef30c564bffb7d70a13ccc","title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"2ecac1099a4a49a504d1d7919c4a69897e173cdc","title":"Naver Labs Europe’s Systems for the WMT19 Machine Translation Robustness Task"},{"paperId":"4ddb924d2018f006324d930034a031b41af8c763","title":"Effective Adversarial Regularization for Neural Machine Translation"},{"paperId":"a5690b0a514a7cbc913871e41e54c9ad4f6362db","title":"Findings of the First Shared Task on Machine Translation Robustness"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"e364a32064235297e6bcf7b86aeb73679527222c","title":"Robust Neural Machine Translation with Doubly Adversarial Inputs"},{"paperId":"e84d754564c9e2ce993596370e0a1493c9c6e4b1","title":"Improving Neural Language Modeling via Adversarial Training"},{"paperId":"09342a388050fa7138116acea2a1271cf42fa95e","title":"Subword Regularization and Beam Search Decoding for End-to-end Automatic Speech Recognition"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"712cd873d7370db280f4ceaaf000dc49f76b59fe","title":"On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models"},{"paperId":"be312e930f6739a709e60547aa0dfb9c3dc44497","title":"Multilingual Neural Machine Translation With Soft Decoupled Encoding"},{"paperId":"0b9ac1035918823ffca1c6f55ec316b42d4e033f","title":"Training on Synthetic Noise Improves Robustness to Natural Noise in Machine Translation"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"ce89ee7aaeeea2c9d474707690f3ea9d948776a3","title":"MTNT: A Testbed for Machine Translation of Noisy Text"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"514e7fb769950dbe96eb519c88ca17e04dc829f6","title":"HotFlip: White-Box Adversarial Examples for Text Classification"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"ffb949d3493c3b2f3c9acf9c75cb03938933ddf0","title":"Adversarial Examples for Evaluating Reading Comprehension Systems"},{"paperId":"fe756e84676f3f4253c3063acca757f6977737b8","title":"Towards Crafting Text Adversarial Samples"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"2cd55ded95d5d13430edfa223ba591b514ebe8a5","title":"Adversarial Training Methods for Semi-Supervised Text Classification"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"93a9694b6a4149e815c30a360347593b75860761","title":"Variable-Length Word Encodings for Neural Translation Models"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"bee044c8e8903fb67523c1f8c105ab4718600cdb","title":"Explaining and Harnessing Adversarial Examples"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"226966243877f186d346be01047cf71cee1b5ec4","title":"Online EM for Unsupervised Models"},{"paperId":"96912cf06fe63e7047cbd6df3063a08f8827d398","title":"The Infinite PCFG Using Hierarchical Dirichlet Processes"},{"paperId":"cb826a3899752b796f14df1c50378c64954a6b0a","title":"Statistical Significance Tests for Machine Translation Evaluation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"d36efb9ad91e00faa334b549ce989bfae7e2907a","title":"Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"},{"paperId":"145c0b53514b02bdc3dadfb2e1cea124f2abd99b","title":"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"88edf12127a628bed608cae0bdf3700d00824df4","title":"Toward Robust Neural Machine Translation for Noisy Input Sequences"},{"paperId":"8c3cf30db12d17638b01e0e464e09d6b58a88187","title":"Variable length word encodings for neural translation models"},{"paperId":"2d6a97f83bb8207ea9d88118618ed3ab52054a88","title":"Unsupervised Morpheme Segmentation and Morphology Induction from Text Corpora Using Morfessor 1.0"}],"id":"8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","summary":"This paper presents adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for exposing diverse subword segmentations and experimentally shows that the model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the performance of N MT models in low-resource and out-domain datasets."}]}
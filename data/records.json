{"papers":[{"url":"https://www.semanticscholar.org/paper/ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning","venue":"Workshop on Representation Learning for NLP","year":2022,"referenceCount":23,"citationCount":2,"influentialCitationCount":0,"publicationDate":"22/04/2022","authors":"Md. Mofijul Islam,Gustavo Aguilar,Pragaash Ponnusamy,Clint Solomon Mathialagan,Chengyuan Ma,Chenlei Guo","citations":[{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"}],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"8ae8e5e840c5f43d4d72cbc4595690fba01aa799","title":"LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation"},{"paperId":"c9b56cb026a38e39bb0228faac57accd6f65e6f7","title":"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"7f9ca11d122957dc59088543f6cd9f907c00d0d3","title":"Neural Models of Text Normalization for Speech Applications"},{"paperId":"e07f2c383a34217a9403a4d58a4e7d61e57d9163","title":"Character Eyes: Seeing Language through Character-Level Taggers"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"5b1516c87818084dc5d195cc274e1ee8923210d2","title":"Neural Cross-Lingual Named Entity Recognition with Minimal Resources"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":null,"title":"Xnli: Evaluating crosslingual sentence representations"},{"paperId":"664ec878de4b7170712baae4a7821fc2602bba25","title":"Learning to Generate Reviews and Discovering Sentiment"},{"paperId":null,"title":"Characterlevel language modeling with hierarchical recurrent neural networks"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"}],"id":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","summary":"This work proposes a vocabulary-free neural tokenizer by distilling segmentation information from heuristic-based subword tokenization, which allows end-to-end task learning, resulting in optimal task-specific tokenization."},{"url":"https://www.semanticscholar.org/paper/9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":79,"citationCount":3,"influentialCitationCount":0,"publicationDate":"06/06/2022","authors":"Ayush Kaushal,Kyle Mahowald","citations":[{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"}],"references":[{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"3dcfa05a1c162e6cab927c5b08d0444f7b6691f4","title":"Probing Classifiers: Promises, Shortcomings, and Advances"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"911b7539e964782670e555930b291de16fa971c5","title":"Flexible Generation of Natural Language Deductions"},{"paperId":"8b723be33e62bf5bd9278769244f1c13a9510898","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP"},{"paperId":"538f8e8a36e70ca408f2c5fb6f10f303c52fc317","title":"Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"1d7f3297924a9dd90cfc0df522ebe9138c28b46f","title":"Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"cf592385909a1e3e9a428d8d6d8f427ab70b60a9","title":"Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation"},{"paperId":"bd20069f5cac3e63083ecf6479abc1799db33ce0","title":"A Primer in BERTology: What We Know About How BERT Works"},{"paperId":"1b94ebedacda0c21a4b8a40a5a40afcea4cc719a","title":"When Combating Hype, Proceed with Caution"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":null,"title":"Gpt-j-6b: A 6 billion parameter autoregressive language model"},{"paperId":null,"title":"Why don’t people use characterlevel machine translation? arXiv preprint arXiv:2110.08191"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"738c6d664aa6c3854e1aa894957bd595f621fc42","title":"Information-Theoretic Probing for Linguistic Structure"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"5034cf3e2483b9e80fa43c6d75ed309a3e028980","title":"Ré-entraîner ou entraîner soi-même ? Stratégies de pré-entraînement de BERT en domaine médical (Re-train or train from scratch ? Pre-training strategies for BERT in the medical domain )"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"9eb4cd1a4b4717c97c47e3dc4563a75779ae9390","title":"BERT is Not an Interlingua and the Bias of Tokenization"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"199ff73d2f728e997f860b62a2322823d3e3d9e8","title":"Designing and Interpreting Probes with Control Tasks"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"73cf24d4c8ca809727553925a31253a649702582","title":"Meaning to Form: Measuring Systematicity as Information"},{"paperId":"98050eeda3b79a18f555480881e1f3bc7d447882","title":"What Kind of Language Is Hard to Language-Model?"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"668f42a4d4094f0a66d402a16087e14269b31a1f","title":"Analysis Methods in Neural Language Processing: A Survey"},{"paperId":"70f7542579aeec99d684754b646e245481a71bda","title":"On the Complexity and Typology of Inflectional Morphological Systems"},{"paperId":"9852ae077c7da6a9d178acaa2b44a335289507a6","title":"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"2019. Bert is not an interlingua and"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"84aa9e1685779019fc162a697d0cdb98066f808c","title":"Subword-level Composition Functions for Learning Word Embeddings"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"97856a4c31fec7b189446a130aab4cbfa8d6a3e8","title":"Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure"},{"paperId":"31f2c1c42cb179820565839868b369c4752e735d","title":"Words cluster phonetically beyond phonotactic regularities"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":null,"title":"spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing"},{"paperId":"645283253f7e79077879ae6b7382661411e8d004","title":"Sound–meaning association biases evidenced across thousands of languages"},{"paperId":"39f22dd35bd8c00095355eab614f7d2ef9034bba","title":"What do you know about an alligator when you know the company it keeps"},{"paperId":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"7b269488183363d49e44d8b63d26d2f1be86bca3","title":"Learning novel phonological neighbors: Syntactic category matters"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"d208fa50239973289e7fab6c5af5d956c3cef3ad","title":"How arbitrary is language?"},{"paperId":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging"},{"paperId":"db734a0e1dc65fe3fe2eef474aefba6d083f54dd","title":"A New Algorithm For Data Compression"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"6ae9ec8288df868505b07bb46911b3e8619055f4","title":"Adding part-of-speech information to the SUBTLEX-US word frequencies"},{"paperId":"bbec2094640e47652234dd8fcfe218fd04886ab5","title":"The indeterminacy of word segmentation and the nature of morphology and syntax"},{"paperId":"cfdd423c8672a7b178ea85d56079328df4eea647","title":"Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit"},{"paperId":"f3054a53bb066ab229cc6f50dfdcddc5d3174060","title":"On the Origin of Language"},{"paperId":null,"title":"Probing for Character Information We use off-the-shelf APIs for lemmatization and WordNet from NLTK (Apache License 2.0"},{"paperId":"2b591d66ffa5f2e017be3c440b492ab3439ca7c5","title":"Exploring systematicity between phonological and context-cooccurrence representations of the mental lexicon"},{"paperId":"1d51de5328d6dd8336033a5b5c8e334e14000880","title":"Probabilistic Modeling in Psycholinguistics: Linguistic Comprehension and Production"},{"paperId":"978aebcea5ffb37f916fe9f1a9eeea8618b82117","title":"The differential role of phonological and distributional cues in grammatical categorisation"},{"paperId":"af25dc4cd32ef790690b10878c10d109d52d2021","title":"The Psychological Reality of Phonaesthemes"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"63470730dea262829c56722739bc53bbee99c6b8","title":"Using sound to solve syntactic problems: the role of phonology in grammatical category assignments."},{"paperId":"2461c19e8859ad54107b0bb4e11ca885cab4096f","title":"On Psychological Reality"},{"paperId":"43cd44b0c9b8bdd5d790fec110d6c0b6eab036b1","title":"Phonetic symbolism in English word-formation"},{"paperId":"1dbcdb06369d365dba0e909811825daf7dca6061","title":"Course in General Linguistics"},{"paperId":null,"title":"Fuzzy match and misspellings Fuzzy match and misspellings Fuzzy match and misspellings \"S1GNATURE"},{"paperId":null,"title":"Some examples of variations in tokenization for two example words"},{"paperId":null,"title":"Exact match and whitespaces Exact match and whitespaces \" signature"}],"id":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","summary":"The mechanisms through which PLMs acquire English-language character information during training are investigated and it is argued that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings."},{"url":"https://www.semanticscholar.org/paper/188cd686fb2200f237f688dbda7f64ffc75e67ac","title":"Subword Pooling Makes a Difference","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":17,"citationCount":13,"influentialCitationCount":1,"publicationDate":"22/02/2021","authors":"Judit Ács,'Akos K'ad'ar,András Kornai","citations":[{"paperId":"7abfcf54eeac3a5c298717a4f0fe8a5daceeaa42","title":"Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling"},{"paperId":"9518b6f3389bb85346eb7240c1b6e8ab1170e1b3","title":"AdaSL: An Unsupervised Domain Adaptation framework for Arabic multi-dialectal Sequence Labeling"},{"paperId":"62f488d3178ce448c9916caf131b83d22ff5d5ff","title":"Comparison of text preprocessing methods"},{"paperId":"00298eef5c6fcd87b572a876e1e13df9bca85bce","title":"UM6P-CS at SemEval-2022 Task 11: Enhancing Multilingual and Code-Mixed Complex Named Entity Recognition via Pseudo Labels using Multilingual Transformer"},{"paperId":"95773a105d1c14f4864c5da1023659783afe914e","title":"FiNER: Financial Numeric Entity Recognition for XBRL Tagging"},{"paperId":"90c6a48dd32f91b08fdf5a0cf9a37a54799229b1","title":"A Latent-Variable Model for Intrinsic Probing"},{"paperId":"e9cb8609d27c5e52ca7e9dfee4086c223116bb32","title":"Extended Overview of HIPE-2022: Named Entity Recognition and Linking in Multilingual Historical Documents"},{"paperId":"a42965768043c57610142d700666f59daf42d18d","title":"An ELECTRA Model for Latin Token Tagging Tasks"},{"paperId":"6da8debf6b4236454662dc3c2542ee8e6fc1e168","title":"ID10M: Idiom Identification in 10 Languages"},{"paperId":"4a3a92c876e984b564b21fdf18fd2a98bd15837d","title":"Evaluating Transferability of BERT Models on Uralic Languages"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"2ed528f2c1cb8185cd048dc37103515cb2245d44","title":"IWCLUL 2021 The Seventh International Workshop on Computational Linguistics of Uralic Languages"},{"paperId":"f7c14e79d3eb1d24b4184d106244be1672113ce2","title":"WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER"}],"references":[{"paperId":"421beeabd2b981754c2871055a8d48e08cd2a5ad","title":"Attentive Pooling with Learnable Norms for Text Representation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"455a8838cde44f288d456d01c76ede95b56dc675","title":"A Structural Probe for Finding Syntax in Word Representations"},{"paperId":"31c872514c28a172f7f0221c8596aa5bfcdb9e98","title":"75 Languages, 1 Model: Parsing Universal Dependencies Universally"},{"paperId":"526cae4863eb15b5bc39112449c2d5fdf1db85b2","title":"Multilingual Constituency Parsing with Self-Attention and Pre-Training"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"697e110df76fe33e232f019d7e44097af3572abd","title":"Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms"},{"paperId":null,"title":"Universal Dependencies 2.3. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (ÚFAL)"},{"paperId":null,"title":"Baseline needs more love: On simple wordembedding-based models and associated pooling"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"adb00206a14768c73e2255ab1a5123f17c9260cf","title":"Improving the Arabic Pronunciation Dictionary for Phone and Word Recognition with Linguistically-Based Pronunciation Rules"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"d9c71db75046473f0e3d3229950d7c84c09afd5e","title":"Text Chunking using Transformation-Based Learning"},{"paperId":null,"title":"We extract 2000 train, 200 dev and 200 test sentences for each task. We keep UD's original splits, in other words, all of our train sentences come from"}],"id":"188cd686fb2200f237f688dbda7f64ffc75e67ac","summary":"This paper investigates how the choice of subword pooling affects the downstream performance on three tasks: morphological probing, POS tagging and NER, in 9 typologically diverse languages and shows that mBERT is better than XLM-RoBERTa in all 9 languages."},{"url":"https://www.semanticscholar.org/paper/7771aa7badc3375a31bfac8dc47755ff5d5c7780","title":"From characters to words: the turning point of BPE merges","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":31,"citationCount":4,"influentialCitationCount":0,"publicationDate":2021,"authors":"Ximena Gutierrez-Vasques,C. Bentz,O. Sozinova,T. Samardžić","citations":[{"paperId":"befaa6e28da4eaaa42de5f5c1a31516de7fb26da","title":"Towards robust complexity indices in linguistic typology"},{"paperId":"8e23530814b61a0cc42233f0248a9d4e106a8ce4","title":"TeDDi Sample: Text Data Diversity Sample for Language Comparison and Multilingual NLP"},{"paperId":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}],"references":[{"paperId":"0b407aaf4a2ffff17d46d24c7b87c3dbaadc38aa","title":"Investigating the effects of i-complexity and e-complexity on the learnability of morphological systems"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"52b3692a99269307144b16db112af8490d32de07","title":"Productivity and Predictability for Measuring Morphological Complexity"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"96c75eaa732028f92065c290723de5ae4c347ab8","title":"Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche"},{"paperId":"98050eeda3b79a18f555480881e1f3bc7d447882","title":"What Kind of Language Is Hard to Language-Model?"},{"paperId":"d323d011a3214116a18d623501bca9a31d33cf4c","title":"Are All Languages Equally Hard to Language-Model?"},{"paperId":"375f5318839cdabb664f4cc00b0d6eb75a973319","title":"Word and Paradigm Morphology"},{"paperId":"236f10fa4a054e67f7a7cab334f31a80a36781a2","title":"The Entropy of Words - Learnability and Expressivity across More than 1000 Languages"},{"paperId":"a921e014249395496c22783fe58d293746af852b","title":"From Characters to Words to in Between: Do We Capture Morphology?"},{"paperId":"5cedb6606803fd849a73cd1a33160a926e9878cf","title":"A Rich Morphological Tagger for English: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax"},{"paperId":"05b359264e6fed937cb76e9e434935944c5a5b0f","title":"The statistical trade-off between word order and word structure – Large-scale evidence for the principle of least effort"},{"paperId":"339b602bd0c82c3ff6074ec2bc61221faef05ae0","title":"A Comparison Between Morphological Complexity Measures: Typological Data vs. Language Corpora"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"c4c9d7dce2d1733f98ae268ca80eb35a3cc6e71b","title":"An information-theoretic approach to assess linguistic complexity"},{"paperId":"6140f733fad17f9f12bb10e072e8babe723c1e9c","title":"Word Order Typology through Multilingual Word Alignment"},{"paperId":"dbf336a8cb911bdda6ef06bc584015a8ed2b1565","title":"Creating a massively parallel Bible corpus"},{"paperId":"a3cec34e50c0e267514b9b92f40b84b886ca570e","title":"From the extraction of continuous features in parallel texts to visual analytics of heterogeneous areal-typological datasets"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"896265da201dcc75d94faf17c995f59762e38bb6","title":"Morphological Organization: The Low Conditional Entropy Conjecture"},{"paperId":"cea92778250fd37c74d2d407f5e244ce64f6a38a","title":"Lexical typology through similarity semantics: Toward a semantic map of motion verbs"},{"paperId":"01d743a38a9759a18c220c598c42c96e6a87b4ca","title":"The type-token relationship in Slavic parallel texts"},{"paperId":"5e56afcd36262b17a36d5e3f26368ef4f9b5bce1","title":"Parallel texts: using translational equivalents in linguistic typology"},{"paperId":"a4f74ef44f176b0a2d2bdf8576388087ca4086d8","title":"Word-based morphology"},{"paperId":"6d12a1d23b21a9b170118a56386552bc5d4727de","title":"A Mathematical Theory of Communication"},{"paperId":"f193a31921ffd25b0a514dcfe71f61b11a53572d","title":"Entropy Measures, Maximum Entropy Principle and Emerging Applications"},{"paperId":"7ce5098ff4e490614157d36f3be865b90ce61bd2","title":"Cognition, quantitative linguistics, and systemic typology"},{"paperId":"384dafe7a6a3a576fd34127e0c27b00477728b6d","title":"Measuring Linguistic Complexity: The Morphological Tier"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"6713c11cd2363c24059c4dad65c68cb3e3e2b479","title":"Information Theory and Redundancy"},{"paperId":"c2dfe0f672e80b1ad1fc0fcae9345ea28e5deae2","title":"A Quantitative Approach to the Morphological Typology of Language"}],"id":"7771aa7badc3375a31bfac8dc47755ff5d5c7780","summary":"It is shown that text entropy values tend to converge at specific subword levels: relatively few BPE merges lead to the most similar distributions across languages."},{"url":"https://www.semanticscholar.org/paper/17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":2,"influentialCitationCount":1,"publicationDate":"08/04/2022","authors":"Edward Gow-Smith,Harish Tayyar Madabushi,Carolina Scarton,A. Villavicencio","citations":[{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"}],"references":[{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224","title":"Superbizarre Is Not Superb: Derivational Morphology Improves BERT’s Interpretation of Complex Words"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"7fbf9d5e1bbac56efba21021d4577620ba3b3ee5","title":"MorphyNet: a Large Multilingual Database of Derivational and Inflectional Morphology"},{"paperId":"3f02219184319aa8ca1ef182e6b091b6a7539a04","title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"44189a062182cab428e6d7ea010819f2fe978533","title":"DagoBERT: Generating Derivational Morphology with a Pretrained Language Model"},{"paperId":"8515c83e6562415021b4c536c28ed198f05a8691","title":"Emerging trends: Subwords, seriously?"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"2e808fa44bd52e7234f04f9fb8819ff840608586","title":"Morfessor EM+Prune: Improved Subword Segmentation with Expectation Maximization and Pruning"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"d56c1fc337fb07ec004dc846f80582c327af717c","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},{"paperId":"a9c3a009d754a110379574b069b48c0b4c75db40","title":"Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"},{"paperId":"2d505eb3d7cba1b2513e2879ac8f025fcb481b77","title":"UniMorph 3.0: Universal Morphology"},{"paperId":null,"title":"BART: Denoising sequence-to-sequence pretraining for natural language"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"6c1beae31b92c70b42ebeb99e5598d73bff6eea5","title":"NEZHA: Neural Contextualized Representation for Chinese Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"7da138d704ef0fc055825fa132f5c452ed3fb52a","title":"LADEC: The Large Database of English Compounds"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"3cea3a3eb5b83612a7f8da49fde0d7244058ee06","title":"MorphoLex: A derivational morphological database for 70,000 English words"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"5c71a835e13a834cbe8f1241159a7237b820dc92","title":"Morfessor 2.0: Python Implementation and Extensions for Morfessor Baseline"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":null,"title":"Japanese 693 and korean voice search Attention is all 707 you need"},{"paperId":"56010a55d49ac1f42355538f494427fd22402be1","title":"Exploring the Limits"},{"paperId":"aaa8f3b7504476727e72806e6f9ded01e13cd494","title":"The English Lexicon Project"},{"paperId":"19b8d1def48288a22cc4256a9a620cdf7f294d2f","title":"Morpheme Segmentation Gold Standards for Finnish and English"},{"paperId":null,"title":"On the role of derivational affixes in recognizing complex words"},{"paperId":"0c5043108eda7d2fa467fe91e3c47d4ba08e0b48","title":"Unsupervised Discovery of Morphemes"}],"id":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","summary":"An alterna- 013 tive tokenisation approach where spaces are treated as individual tokens are experiments, which show that the modi- 022 ﬁed algorithms give improved performance on downstream NLP tasks that involve handling 024 complex words, whilst having no detrimental effect on performance in general natural lan- 026 guage understanding tasks."},{"url":"https://www.semanticscholar.org/paper/373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation","venue":"AMTA","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Salvador Carrión-Ponz,F. Casacuberta","citations":[],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":null,"title":"Autonmt: A framework to streamline the research of seq2seq models"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed","title":"Sparse is Enough in Scaling Transformers"},{"paperId":"9436806f98d0c71245135d5d45025427bb36cd33","title":"Optimizing Transformer for Low-Resource Neural Machine Translation"},{"paperId":"3fd45fc420a882ab2fba3166ef08f376cc758ad0","title":"On Long-Tailed Phenomena in Neural Machine Translation"},{"paperId":"0156fb2096c39839384c0cae0194e123e01cc575","title":"Character-Level Transformer-Based Neural Machine Translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"295065d942abca0711300b2b4c39829551060578","title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"6dc710b46bc510a42af487a3ac220e4fabf4d518","title":"VOLT: Improving Vocabularization via Optimal Transport for Machine Translation"},{"paperId":"10b9eee99b2632359d4d26f991e765bff8d91dee","title":"Revisiting Low-Resource Neural Machine Translation: A Case Study"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"7e77d37e7fbfe90f58e4e96e8198903f364d6402","title":"FearNet: Brain-Inspired Model for Incremental Learning"},{"paperId":"8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a","title":"Learning without Forgetting"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"5151d6cb3a4eaec14a56944d58338251fca344ab","title":"Overcoming catastrophic forgetting in neural networks"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"22bcad7bed8b82f98d5144b7b57fbfc01d46f64a","title":"Neurogenesis Deep Learning"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"53c9443e4e667170acc60ca1b31a0ec7151fe753","title":"Progressive Neural Networks"},{"paperId":"84ca430856a92000e90cd728445ca2241c10ddc3","title":"Very Deep Convolutional Networks for Natural Language Processing"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"1eb09fecd75eb27825dce4f964b97f4f5cc399d7","title":"On the Properties of Neural Machine Translation: Encoder–Decoder Approaches"},{"paperId":"105146a7872835a52c8c5c55a3aae62c5d8852a1","title":"Substring-based machine translation"},{"paperId":"e9ad27106dd487893bcc1cc12bbf645168c60f87","title":"Can We Translate Letters?"},{"paperId":null,"title":"Bleu : A method for automatic eval - 662 uation of machine translation A call for clarity in reporting BLEU 666 scores"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"}],"id":"373588ff1fb9f7590db000a04de8d838b1516e5a","summary":"This work suggests that quasi-character-level models have practically the same generalization capabilities as character-based models but at lower computational costs and appear to help achieve greater consistency between domains than standard subword- level models, although the catastrophic forgetting problem is not mitigated."},{"url":"https://www.semanticscholar.org/paper/15db055ac7cb49bac96cc1be1270d9bf2b0b1770","title":"Beyond Characters: Subword-level Morpheme Segmentation","venue":"SIGMORPHON","year":2022,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Ben Peters,André F. T. Martins","citations":[{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"73cb4755f15502b9cb34797d487b6fb77c7ba4b4","title":"Word-level Morpheme segmentation using Transformer neural network"}],"references":[{"paperId":"b522b0ece0cc38d7a3a5b25c5c19ce626556a897","title":"CLUZH at SIGMORPHON 2022 Shared Tasks on Morpheme Segmentation and Inflection Generation"},{"paperId":"238d45d79209f9f28f01a02dc337888942499e2f","title":"Morfessor-enriched features and multilingual training for canonical morphological segmentation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"1e6171d9937a36b2dba6b622bd5857f2407d1b2e","title":"Smoothing and Shrinking the Sparse Seq2Seq Search Space"},{"paperId":"28d7562a716099a5147509bfd84f76de08b1192c","title":"Searching for Search Errors in Neural Morphological Inflection"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"05cfd96eed27ceb2aa35285991a745a5cd119abc","title":"If Beam Search Is the Answer, What Was the Question?"},{"paperId":"8c122ff82dedf72fa7ef5342622667bf5848916e","title":"One-Size-Fits-All Multilingual Models"},{"paperId":"1e0a14db59c5a1a18c83dfbeb17eb5be9e67623d","title":"Modeling Word Formation in English–German Neural Machine Translation"},{"paperId":"82665af3fa153ef6def31cc42a1c0aac39afa61b","title":"Neural Polysynthetic Language Modelling"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"d3231772937a2182b2377d028417245c49868dd1","title":"On NMT Search Errors and Model Errors: Cat Got Your Tongue?"},{"paperId":"3cee801d10f410f0feb1a2390776a01ba2765001","title":"Sparse Sequence-to-Sequence Models"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"e6272f72f780fa4a14cc83569abb7389bcf592d1","title":"IT–IST at the SIGMORPHON 2019 Shared Task: Sparse Two-headed Models for Inflection"},{"paperId":null,"title":"Exploring bert’s vocabulary"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"834724753187ba86709f9d859740f23861d42191","title":"Meaningless yet meaningful: Morphology grounded subword-level NMT"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"69ffe3277011087b55afaeaef0a3933160beee9a","title":"Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":"c75ecddc5eda6b71b54bf8ef40f6fa9da1d8f108","title":"Neural Morphological Analysis: Encoding-Decoding Canonical Segments"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"1f462943c8d0af69c12a09058251848324135e5a","title":"Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition"},{"paperId":"b20c0758a38bd5a4083f64eff53af924499a8e29","title":"Possible generalization of Boltzmann-Gibbs statistics"},{"paperId":"145c0b53514b02bdc3dadfb2e1cea124f2abd99b","title":"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"},{"paperId":null,"title":"Mathias Creutz, and Mikko Kurimo. 2022. Morfessorenriched features and multilingual training for canonical morphological segmentation"}],"id":"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","summary":"This paper presents DeepSPIN’s submissions to the SIGMORPHON 2022 Shared Task on Morpheme Segmentation, and challenges the assumption that models for morphological tasks should be trained at the character level by building a transformer that generates morphemes as sequences of unigram language model-induced subwords."},{"url":"https://www.semanticscholar.org/paper/d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP","venue":"ArXiv","year":2021,"referenceCount":186,"citationCount":19,"influentialCitationCount":1,"publicationDate":"20/12/2021","authors":"Sabrina J. Mielke,Zaid Alyafeai,Elizabeth Salesky,Colin Raffel,Manan Dey,Matthias Gallé,Arun Raja,Chenglei Si,Wilson Y. Lee,Benoît Sagot,Samson Tan","citations":[{"paperId":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models"},{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"092312cb2a59502a2db7ffea1e31c19ad2eecc7c","title":"H2-Golden-Retriever: Methodology and Tool for an Evidence-Based Hydrogen Research Grantsmanship"},{"paperId":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"8a37503b60898afee407162f27c8376d3137a2fd","title":"Evolving Label Usage within Generation Z when Self-Describing Sexual Orientation"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"fe35bf433202b4e130709d042aae09dbf4d76232","title":"Text Generation with Text-Editing Models"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"cd48780a4bb7513e6b81cf3bb993b4f4fb962e13","title":"Word-order Typology in Multilingual BERT: A Case Study in Subordinate-Clause Detection"},{"paperId":"61d9162eea5aceb5b78c2d1230f18d8dfe10a208","title":"The Risks of Machine Learning Systems"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","title":"Beyond Characters: Subword-level Morpheme Segmentation"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE"}],"references":[{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"d7aa3c65eae71776ccc3cddafb1d4b288deae2cb","title":"A Masked Segmental Language Model for Unsupervised Natural Language Segmentation"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"13b4d9a1096113db63158590e9d2e58744766e3b","title":"Unsupervised Word Segmentation with Bi-directional Neural Language Model"},{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80","title":"How BPE Affects Memorization in Transformers"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"58e13e1fed9b28e50efcaff611772801d7980c80","title":"Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation"},{"paperId":"fd400801136261ca329aa84b11307be0e28571f5","title":"Recurrent Neural Networks with Mixed Hierarchical Structures for Natural Language Processing"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"38e99b3b56d03ad77cc058381bcc90f6cae5cb75","title":"The Effectiveness of Morphology-aware Segmentation in Low-Resource Neural Machine Translation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"5e8da0a332906a2cc5262483ddc27eb9b792dbaf","title":"Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine Translation: The Case of Fon Language"},{"paperId":"b73b3b086465c629170fd7be5a25164652655f26","title":"One Size Does Not Fit All: Finding the Optimal N-gram Sizes for FastText Models across Languages"},{"paperId":"fcfeaba7f8aacc4198d3028d0c7ab6d6b7679224","title":"Superbizarre Is Not Superb: Derivational Morphology Improves BERT’s Interpretation of Complex Words"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"72eed6570807c1d3c9a60289c7fc6813277e1e98","title":"Fast WordPiece Tokenization"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"1c59de25af45cef20d846ec7454251e8237d45d1","title":"A Statistical Extension of Byte-Pair Encoding"},{"paperId":"7771aa7badc3375a31bfac8dc47755ff5d5c7780","title":"From characters to words: the turning point of BPE merges"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"d7a1c8da1b8163f5bbd4d12e58cad5002947105e","title":"Word Shape Matters: Robust Machine Translation with Visual Embedding"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"110c13fbf4ff87b52ee1fd9eb2d3616c839ceb41","title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank"},{"paperId":"9ae116139ae77ca6d78e162e9639681adef5761c","title":"The Unstoppable Rise of Computational Linguistics in Deep Learning"},{"paperId":"82665af3fa153ef6def31cc42a1c0aac39afa61b","title":"Neural Polysynthetic Language Modelling"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"9d92905edc1a5d027b0827d1309bc6ac03dd94a0","title":"Character-Level Translation with Self-attention"},{"paperId":"651c01b897f18e31bab3ad482154a980ac92c638","title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"2e808fa44bd52e7234f04f9fb8819ff840608586","title":"Morfessor EM+Prune: Improved Subword Segmentation with Expectation Maximization and Pruning"},{"paperId":"76a211ec3b4b96f37038c3993c81597cb1ea7a4a","title":"Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation"},{"paperId":"3b2538f84812f434c740115c185be3e5e216c526","title":"Cross-Lingual Ability of Multilingual BERT: An Empirical Study"},{"paperId":"229aff74294275d31ded5c8e6e80f6aa6a88fcfd","title":"A Latent Morphology Model for Open-Vocabulary Neural Machine Translation"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":null,"title":"Unsupervised cross-lingual representation"},{"paperId":null,"title":"Linguist vs"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"7c848b37605cdfe6d2d4778b06e66c0710c3ec34","title":"On the Importance of Word Boundaries in Character-level Neural Machine Translation"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"3504dd566b25f24f44cf647731370760a537b483","title":"Stochastic Tokenization with a Language Model for Neural Text Classification"},{"paperId":"8a7e9f91d949764d6159c114d4feb961ec07b32d","title":"Training Hybrid Language Models by Marginalizing over Segmentations"},{"paperId":"98050eeda3b79a18f555480881e1f3bc7d447882","title":"What Kind of Language Is Hard to Language-Model?"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"2fa3f7ce620a1c7155daef6620dd6bb0e01934f3","title":"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"},{"paperId":"b401aca53c04cec865e1a733090a2c60ca7ebe97","title":"Glyce: Glyph-vectors for Chinese Character Representations"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"20ff6bd5fe67a2d73fb3423216ec2b7369d5a091","title":"Comparing neural‐ and N‐gram‐based language models for word segmentation"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"9852ae077c7da6a9d178acaa2b44a335289507a6","title":"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Characterbased nmt with transformer"},{"paperId":null,"title":"Exploring BERT's vocabulary"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"a98c340f316fc041dd071efcec1fbddf176282d9","title":"Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"64b8361a133ad545cbf9c035e109ef8461b31f67","title":"Super Characters: A Conversion from Sentiment Classification to Image Classification"},{"paperId":"1e6d69f8cfd698b8139cde557252753b22c7d712","title":"BPE and CharCNNs for Translation of Morphology: A Cross-Lingual Comparison and Analysis"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"589e237c9b15c49f15b4989a4684792d9e705cce","title":"Revisiting the Hierarchical Multiscale LSTM"},{"paperId":"2e638ac785703b7332393e5f2d8064b03a3fccf8","title":"Universal Word Segmentation: Implementation and Interpretation"},{"paperId":"19885d5b288e6043eb989d296cc12bc8dbead8e3","title":"Morphological and Language-Agnostic Word Segmentation for NMT"},{"paperId":"d323d011a3214116a18d623501bca9a31d33cf4c","title":"Are All Languages Equally Hard to Language-Model?"},{"paperId":"771c1cb5fb161231e9aaa0a189caba672256a880","title":"Using Morphological Knowledge in Open-Vocabulary Neural Language Models"},{"paperId":"c79ce664a42d7867846ee468180bf9ad8c9e17b2","title":"CoNLL-UL: Universal Morphological Lattices for Universal Dependency Parsing"},{"paperId":"57faf160097049d14399ac6d317bbe4d1b8aa2de","title":"Compositional Representation of Morphologically-Rich Input for Neural Machine Translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"287efcd4f62c84c45818a71cf5bdd70364ff10eb","title":"An Evaluation of Two Vocabulary Reduction Methods for Neural Machine Translation"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"5616064812996ab1fae525f9679f300c7c307895","title":"Towards Neural Phrase-based Machine Translation"},{"paperId":"834724753187ba86709f9d859740f23861d42191","title":"Meaningless yet meaningful: Morphology grounded subword-level NMT"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"8e39bc4e1711e941881f64935b203a93259acb50","title":"Glyph-aware Embedding of Chinese Characters"},{"paperId":"a3a39cebbe65cc21009182808300f8f83af2213c","title":"A Dataset for Sanskrit Word Segmentation"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"45e3f381d16e6d6db5449b7a44b7bdf294a8a822","title":"Multiscale sequence modeling with a learned dictionary"},{"paperId":"ed5003e6a2b97dd4b65c5190ccb88b9c45b032b8","title":"Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"},{"paperId":"db253b17043b6a86e02173b6aa597664b0c7f256","title":"Learning Character-level Compositionality with Visual Features"},{"paperId":"8da6b21fb29fcbe3d6596a8b87242ccb3b06a894","title":"From Segmentation to Analyses: a Probabilistic Model for Unsupervised Morphology Induction"},{"paperId":"5f09d0cbd59e7c84b912692b6ded42107e8f9733","title":"Chinese–Spanish neural machine translation enhanced with character and word bitmap fonts"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"a486e2839291111bb44fa1f07731ada123539f75","title":"Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"},{"paperId":"2d7782c225e0fc123d6e227f2cb253e58279ac73","title":"Improving Neural Language Models with a Continuous Cache"},{"paperId":"105788dd22393d5a4333c167814ec3d38c7d6612","title":"Latent Sequence Decompositions"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"f797fd44b9ddd5845611eb7a705ca9464a8819d1","title":"Very Deep Convolutional Networks for Text Classification"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":"acd87e4f672f0b92ea4164414c213560c23bee52","title":"Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"6b904d6e84c98c6ce22ce6923224b205a2a24ee1","title":"Segmental Recurrent Neural Networks"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"c2e1e362e27ff5ced52bfc9bde3ca165e7eafe76","title":"Neural machine translation using bitmap fonts"},{"paperId":"6dab1c6491929d396e9e5463bc2e87af88602aa2","title":"Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"},{"paperId":"f22e78a284df04b57d8fc8e622acf1942f288c61","title":"An Unsupervised Method for Uncovering Morphological Chains"},{"paperId":"746bd8b886be53b4f3bdd27d64de5e6181ba8195","title":"Morfessor FlatCat: An HMM-Based Method for Unsupervised and Semi-Supervised Learning of Morphology"},{"paperId":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"5522764282c85aea422f1c4dc92ff7e0ca6987bc","title":"A Clockwork RNN"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"1e6c802a7663e309d47ee45487c03333fd388014","title":"A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability"},{"paperId":"876e51a39ae8bf9b1d48f4e219bbc4e0705ee1cd","title":"Text segmentation with character-level text embeddings"},{"paperId":"62c76ca0b2790c34e85ba1cce09d47be317c7235","title":"Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"ae9768ea90a257f710756fcf19a852d3545fb5c1","title":"Language-independent compound splitting with morphological operations"},{"paperId":"ea9a3a0aaee44cf95d61cd7134c26b8d5e5f4b0e","title":"The sequence memoizer"},{"paperId":"aa5b0f501670a76eac7deaf3027ded218971b633","title":"Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models"},{"paperId":"3df68bed55021596d09b44769d4d74f7495379d2","title":"Empirical Comparison of Evaluation Methods for Unsupervised Learning of Morphology"},{"paperId":"1fd7fc06653723b05abe5f3d1de393ddcf6bdddb","title":"SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"},{"paperId":"dd79821ce068f263ab7d6c2037a83c20e562dff3","title":"Morpho Challenge 2005-2010: Evaluations and Results"},{"paperId":"9e022fa8effeaaff77d86be0a3d1bf50d899b5b8","title":"Painless Unsupervised Learning with Features"},{"paperId":"0dda3342416a221eb840f1499d01a47b05d7c7f4","title":"Learning from Unseen Data"},{"paperId":"51c88cf10b75ed1a5445660cc64cee3d1c6fd8c5","title":"Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling"},{"paperId":"46f31f9069bb934498a288126053bcab01ff34aa","title":"A Bayesian framework for word segmentation: Exploring the effects of context"},{"paperId":"41fbb2cbace32055d26616fa8c00da5e4782c776","title":"Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars"},{"paperId":"07943f471d65614d3f545f1d37be4ec25739d218","title":"Probabilistic ParaMor"},{"paperId":"57458bc1cffe5caa45a885af986d70f723f406b4","title":"A unified architecture for natural language processing: deep neural networks with multitask learning"},{"paperId":"b773bc1bed0976c0e3f8ebe4ad883bf582e5902c","title":"SxPipe 2: architecture pour le traitement pré-syntaxique de corpus bruts"},{"paperId":"1375dca2b57bdadbfc263e641dd95d7826a06073","title":"ParaMor: Minimally Supervised Induction of Paradigm Structure and Morphological Analysis"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"02711b7e8c73779db802cb00b6cbf407d0872b66","title":"Unsupervised models for morpheme segmentation and morphology learning"},{"paperId":"e04fa3599e22b92336a06eca79417f4d6af040f3","title":"Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models"},{"paperId":"a99d6ebe6e583b752d1639a9002dd60581983dc1","title":"Contextual Dependencies in Unsupervised Word Segmentation"},{"paperId":"6bf6c77b895069239ef7a180aee5332ed7b40c79","title":"A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes"},{"paperId":"24b20f7b118588055346f4ac5cdb1fe22e886dda","title":"Interpolating between types and tokens by estimating power-law generators"},{"paperId":"fcc5e6ab7e7d88835732e58661d264d0b6e9381c","title":"MAF: a Morphosyntactic Annotation Framework"},{"paperId":"a218eb044d91fce34126a2e672ac0a0935a8a7f1","title":"INDUCING THE MORPHOLOGICAL LEXICON OF A NATURAL LANGUAGE FROM UNANNOTATED TEXT"},{"paperId":"cdaae7a8f0db8b280266606004f1c6f164a13f6d","title":"Empirical Methods for Compound Splitting"},{"paperId":"af4c762a7a4f4803dfca6223955aba43895aa606","title":"Lexicon-directed segmentation and tagging in Sanskrit"},{"paperId":null,"title":"Finite-state morphology: Xerox tools and techniques"},{"paperId":"0c5043108eda7d2fa467fe91e3c47d4ba08e0b48","title":"Unsupervised Discovery of Morphemes"},{"paperId":"0649db23e4d672d3122435e48030880ad6b0b0db","title":"A Probabilistic Model for Learning Concatenative Morphology"},{"paperId":"f7a8bd1a62d22de3a3db5edfcc364b462cd4558e","title":"A Bayesian Model for Morpheme and Paradigm Identification"},{"paperId":"30545fe538a773b57e06b4217cd495ef84230bc8","title":"Knowledge-Free Induction of Inflectional Morphologies"},{"paperId":"9f834ee11902ada79b874e7fe5072159d72a0f9f","title":"Unsupervised Learning of the Morphology of a Natural Language"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","title":"A Neural Probabilistic Language Model"},{"paperId":"4d1dac08bb3d960baa88e4ff3477ec834446d056","title":"Minimally Supervised Morphological Analysis by Multimodal Alignment"},{"paperId":"efada5827fd7eedecb4a7dc101caa4509a9f770f","title":"Empirical Estimates of Adaptation: The chance of Two Noriegas is closer to p/2 than p2"},{"paperId":"63d165e916b851a0aeaafaa528f6aad99a78d04f","title":"Distributional cues in morpheme discovery: A computational model and empirical evidence"},{"paperId":"d4e8bed3b50a035e1eabad614fe4218a34b3b178","title":"An empirical study of smoothing techniques for language modeling"},{"paperId":"72661eb1c6b3c94128c6edc394e22d2c9ffde8d0","title":"Offline dictionary-based compression"},{"paperId":"7ec7bc1d7f35558640ac05e94f9b854ec17d7bc0","title":"Linguistic Structure as Composition and Perturbation"},{"paperId":"b13813b49f160e1a2010c44bd4fb3d09a28446e3","title":"Hierarchical Recurrent Neural Networks for Long-Term Dependencies"},{"paperId":"01fa57bd91f731522c861404d29e4604ba6ac6d3","title":"A hierarchical Dirichlet language model"},{"paperId":null,"title":"CELEX2"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"0b44fcbeea9415d400c5f5789d6b892b6f98daff","title":"Building a Large Annotated Corpus of English: The Penn Treebank"},{"paperId":"db6ae486a695efc02910b1dc08eeba13b50d5ca8","title":"Tokenization as the Initial Phase in NLP"},{"paperId":"50c770b425a5bb25c77387f687a9910a9d130722","title":"Learning Complex, Extended Sequences Using the Principle of History Compression"},{"paperId":"c69201d091dd92699fd90a17b9e3407319726791","title":"Neural Sequence Chunkers"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","title":"Finding Structure in Time"},{"paperId":"66e5551231b018f32004b53b559f8144f38e3c4e","title":"Stochastic Complexity in Statistical Inquiry Theory"},{"paperId":"72d5278244aa183a580fb4df47f87949de7f1184","title":"On the syntactic structure of protein sequences and the concept of grammar complexity"},{"paperId":"3d951ad1164c17217a24e46b57f9b31cea1b2b96","title":"Data compression via textual substitution"},{"paperId":"a651bb7cc7fc68ece0cc66ab921486d163373385","title":"An algorithm for suffix stripping"},{"paperId":"5e37f888248c44a8d8098ccd07e91cc06e8b4d42","title":"AN ALGORITHM FOR THE SEGMENTATION OF AN ARTIFICIAL LANGUAGE ANALOGUE"},{"paperId":null,"title":"Robbie Jimmerson, Vasilisa Andriyanets"}],"id":"d617f51833860dc50d202af7f80be71304b2e994","summary":"This survey connects several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subwordbased approaches based on learned segmentation have been proposed and evaluated."},{"url":"https://www.semanticscholar.org/paper/d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation","venue":"ArXiv","year":2021,"referenceCount":96,"citationCount":4,"influentialCitationCount":0,"publicationDate":"10/09/2021","authors":"Yuval Pinter","citations":[{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"d7a793e7ce1e42e1feaadb026633e481131a8692","title":"Char2Subword: Extending the Subword Embedding Space from Pre-trained Models Using Robust Character Compositionality"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"03826718c8c47d8f1f25e437fd6ff15165162e8c","title":"Will it Unblend?"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"b08545e1281c1eb748e4474687eb61fd3b25d1a6","title":"NYTWIT: A Dataset of Novel Words in the New York Times"},{"paperId":null,"title":"Will it unblend? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1525–1535, Online"},{"paperId":"2af39b0698b2872c13c2d207e7937e4f1e317cae","title":"Attending Form and Context to Generate Specialized Out-of-VocabularyWords Representations"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"fddd3dab90c243ab7fc038bc6449ef62c0e06037","title":"Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"5fc7b4dbc154bbbf26d8cee2f18f31ecbf286bcf","title":"Generalizing Word Embeddings using Bag of Subwords"},{"paperId":"fac1c95993e86f92c7adeec7f72e06503e4190d5","title":"Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"4aa508296cf1addc47db2fc23820b20eb5c714ed","title":"Predicting Semantic Relations using Global Graph Properties"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"ce2d5b5856bb6c9ab5c2390eb8b180c75a162055","title":"Recent Trends in Deep Learning Based Natural Language Processing [Review Article]"},{"paperId":"9697d32ed0a16da167f2bdba05ef96d0da066eb5","title":"Convolutional 2D Knowledge Graph Embeddings"},{"paperId":"4190a06408f029ae46d8a9656df20f9e17ce7e03","title":"How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?"},{"paperId":"cd8a9914d50b0ac63315872530274d158d6aff09","title":"Modeling Relational Data with Graph Convolutional Networks"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"Recent trends in deep learning based natural language processing. ieee Computational intelligenCe magazine"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"d77312a62ac174b93e9ff980876369ca841b40e5","title":"Context encoders as a simple but powerful extension of word2vec"},{"paperId":"1590bd1bca945fc6ff50b8cdf2da14ea2061c79a","title":"Poincaré Embeddings for Learning Hierarchical Representations"},{"paperId":"90d32d6ecfc83f88d7685508993dca537ce3b91c","title":"Sound Symbolism in English: Weighing the Evidence"},{"paperId":"89f985dda7bbbae161d19e318257dbe13fd92d21","title":"The Interplay of Semantics and Morphology in Word Embeddings"},{"paperId":"495e83b32d2306c683ecf0f3a2f8eda1c30af3ed","title":"Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection"},{"paperId":"e3274206b36a603abc4a335af91273ecba5e73cc","title":"ProjE: Embedding Projection for Knowledge Graph Completion"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"87f714f3534c7a3ca2bf41ce5825139ddc8247bf","title":"What to do about non-standard (or non-canonical) language in NLP"},{"paperId":"f690c45d7cb80ce46a438c4dd1f9b50e1a45a84e","title":"Morphological Priors for Probabilistic Neural Word Embeddings"},{"paperId":"59761abc736397539bdd01ad7f9d91c8607c0457","title":"context2vec: Learning Generic Context Embedding with Bidirectional LSTM"},{"paperId":"36ee2c8bd605afd48035d15fdc6b8c8842363376","title":"node2vec: Scalable Feature Learning for Networks"},{"paperId":"12e9d005c77f76e344361f79c4b008034ae547eb","title":"Charagram: Embedding Words and Sentences via Character n-grams"},{"paperId":"acd87e4f672f0b92ea4164414c213560c23bee52","title":"Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss"},{"paperId":"24158c9fc293c8a998ac552b1188404a877da292","title":"Neural Architectures for Named Entity Recognition"},{"paperId":"45c84b1d25cfdfad6fcbebec230539aef308926e","title":"Mapping Unseen Words to Task-Trained Embedding Spaces"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"6dab1c6491929d396e9e5463bc2e87af88602aa2","title":"Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"},{"paperId":"b5c29457a90ee9af7c3b2985e9f665ce4b5b97d6","title":"Observed versus latent features for knowledge base and text inference"},{"paperId":"18bd7cd489874ed9976b4f87a6a558f9533316e0","title":"Knowledge Graph Embedding via Dynamic Mapping Matrix"},{"paperId":"e745b0506f4133263633eb05e5006a8cff4129f0","title":"Traversing Knowledge Graphs in Vector Space"},{"paperId":"86412306b777ee35aba71d4795b02915cb8a04c3","title":"Embedding Entities and Relations for Learning and Inference in Knowledge Bases"},{"paperId":"7a96765c147c9c814803c8c9de28a1dd069271da","title":"SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation"},{"paperId":"7e928ef936c2815d7522c5176163d6ab7309a8b7","title":"Representing Text for Joint Embedding of Text and Knowledge Bases"},{"paperId":"8c68094a59dd2f24415082c53464abf45387f0bb","title":"Compositional Vector Space Models for Knowledge Base Inference"},{"paperId":null,"title":"Retrofitting word vectors to semantic lexicons"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"dab7e605237ad4f4fe56dcba2861b8f0a57112be","title":"Wikidata: a free collaborative knowledgebase"},{"paperId":"f70391f9a6aeec75bc52b9fe38588b9d0b4d40c6","title":"Learning Character-level Representations for Part-of-Speech Tagging"},{"paperId":"46f418bf6fab132f193661226c5c27d67f870ea5","title":"Compositional Morphology for Word Representations and Language Modelling"},{"paperId":"2582ab7c70c9e7fcb84545944eba8f3a7f253248","title":"Translating Embeddings for Modeling Multi-relational Data"},{"paperId":"50d53cc562225549457cbc782546bfbe1ac6f0cf","title":"Reasoning With Neural Tensor Networks for Knowledge Base Completion"},{"paperId":"fdb813d8b927bdd21ae1858cafa6c34b66a36268","title":"Learning deep structured semantic models for web search using clickthrough data"},{"paperId":"53ab89807caead278d3deb7b6a4180b277d3cb77","title":"Better Word Representations with Recursive Neural Networks for Morphology"},{"paperId":"c4fd9c86b2b41df51a6fe212406dda81b1997fd4","title":"Linguistic Regularities in Continuous Space Word Representations"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"5fac0ca1b3ea3b6f234dd0821e1f3678f0b6096d","title":"Relation Extraction with Matrix Factorization and Universal Schemas"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"f6764d853a14b0c34df1d2283e76277aead40fde","title":"A Three-Way Model for Collective Learning on Multi-Relational Data"},{"paperId":"bc1022b031dc6c7019696492e8116598097a8c12","title":"Natural Language Processing (Almost) from Scratch"},{"paperId":"31d33747d8fff0b7a0c40dcf9944015af9a15b1a","title":"BabelNet: Building a Very Large Multilingual Semantic Network"},{"paperId":"569b09c261c232d38be9a5beabf950aa680bba1e","title":"English Word Formation Processes"},{"paperId":"57458bc1cffe5caa45a885af986d70f723f406b4","title":"A unified architecture for natural language processing: deep neural networks with multitask learning"},{"paperId":"1976c9eeccc7115d18a04f1e7fb5145db6b96002","title":"Freebase: a collaboratively created graph database for structuring human knowledge"},{"paperId":"02711b7e8c73779db802cb00b6cbf407d0872b66","title":"Unsupervised models for morpheme segmentation and morphology learning"},{"paperId":"54bf9a296ccdf7344d58993bc33c69d51026e320","title":"Word-Formation in English"},{"paperId":null,"title":"A neural probabilistic language model. The journal of machine learning research"},{"paperId":"0c5043108eda7d2fa467fe91e3c47d4ba08e0b48","title":"Unsupervised Discovery of Morphemes"},{"paperId":"be26417ca5af69535be71fb0aeed1f6c38d0ba2e","title":"Global organization of the Wordnet lexicon"},{"paperId":"e0c01df98a6b633b25c96c1a99b713ac96f1c5be","title":"Placing search in context: the concept revisited"},{"paperId":null,"title":"How many words are there? Glottometrics"},{"paperId":null,"title":"Word: a typological framework"},{"paperId":null,"title":"How many words are there? Glottometrics, 4:61–86"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","title":"A Neural Probabilistic Language Model"},{"paperId":"d87ceda3042f781c341ac17109d1e94a717f5f60","title":"WordNet : an electronic lexical database"},{"paperId":"d560a8d279075a529e9cadb0d664b27957aac5a2","title":"TnT - A Statistical Part-of-Speech Tagger"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"2b2eb4a9bb146e3ffaa0b025fba0ed14240c683f","title":"Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging"},{"paperId":"3de5d40b60742e3dfa86b19e7f660962298492af","title":"Class-Based n-gram Models of Natural Language"},{"paperId":"20a80a7356859daa4170fb4da6b87b84adbb547f","title":"Indexing by Latent Semantic Analysis"},{"paperId":"48658035de019d1dc062810a9c4db0ef7cb728ee","title":"Sound Advice on Brand Names"},{"paperId":"7ef3ac14cdb484aaa2b039850093febd5cf73a21","title":"Contextual correlates of synonymy"},{"paperId":"decd9bc0385612bdf936928206d83730718e737e","title":"Distributional Structure"},{"paperId":"ca12a908e86a87db152c0991ae9c5a40f1a5d2a3","title":"The nature and measurement of meaning."},{"paperId":null,"title":"Gestalt psychology, 2nd edn new york"},{"paperId":null,"title":"Cours de linguistique générale (roy harris, trans.)"}],"id":"d87647784c12517d31964cc508d5b8423cc24f50","summary":"A survey of the distributional, compositional, and relational approaches to addressing the problem of representing the atomic elements of language in modern neural learning systems is presented, with special emphasis on the word level and the out-of-vocabulary phenomenon."},{"url":"https://www.semanticscholar.org/paper/f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models","venue":"","year":2021,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/06/2021","authors":"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Yasheng Wang,Qun Liu,Maosong Sun","citations":[{"paperId":"c991f7f99dd35eadb66b383b4d711f9cc25af7e3","title":"Pronunciation-aware unique character encoding for RNN Transducer-based Mandarin speech recognition"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"59ca487aca988c50dac2f1605756cc04c36603b4","title":"The effect of second-language orthographic input on the phonological encoding of Mandarin words"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"cc50f846ed7222698d130cddbc58ed4d547914ed","title":"CPM: A Large-scale Generative Chinese Pre-trained Language Model"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training With Whole Word Masking for Chinese BERT"},{"paperId":"582649c87e590101617a21524f0ec203ea935937","title":"MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"6dd1a0f657ff547b1736c2d8b30451f49a9a48f5","title":"OCNLI: Original Chinese Natural Language Inference"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"6afe0fb12ceacadbbfed7202d430770a3f344731","title":"Revisiting Pre-Trained Models for Chinese Natural Language Processing"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"e0697594749cca8e1a272033d35e6102bfd39c00","title":"CLUENER2020: Fine-grained Name Entity Recognition for Chinese"},{"paperId":"6007bd2a34385132a7885b934d90b519a1f65bba","title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"1b07a24b81834116f6ad1d0232485ba81b9445f3","title":"Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"},{"paperId":"6c1beae31b92c70b42ebeb99e5598d73bff6eea5","title":"NEZHA: Neural Contextualized Representation for Chinese Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e278e072774f23675266881750e20bca74804cb9","title":"ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"},{"paperId":"ae43556f2cc1cecb8b48762b4e09df319fbaa4d9","title":"Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"b401aca53c04cec865e1a733090a2c60ca7ebe97","title":"Glyce: Glyph-vectors for Chinese Character Representations"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":null,"title":"The #BenderRule: On Naming the Languages We Study and Why It Matters. The Gradient"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"7afb83134d5b7914131e10b229d30dc2593266f6","title":"The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification"},{"paperId":"57b57e88edcc9a20c78388e847b42e088b451c55","title":"cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":null,"title":"THULAC: An Efficient Lexical Analyzer for Chinese"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"7f147cd5d509e4feb628972922297f48b86ebd3e","title":"Punctuation as Implicit Annotations for Chinese Word Segmentation"},{"paperId":"5b2beddeb063bd5988da70ae58f3e0a6564e647a","title":"Optimizing Chinese Word Segmentation for Machine Translation Performance"},{"paperId":"885476e9d1b2ea67405f7f9b46c2c8536657a204","title":"Scalable Term Selection for Text Categorization"},{"paperId":"a6c2773efba0c69e4d0ca5043cbafef9b5fc3d26","title":"Writing Systems of the World"}],"id":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","summary":"Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency, and 2) Pronunciation-based SubChartokenization can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to all homophone typos."},{"url":"https://www.semanticscholar.org/paper/dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?","venue":"WNUT","year":2021,"referenceCount":59,"citationCount":7,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Arij Riabi,Benoît Sagot,Djamé Seddah","citations":[{"paperId":"0eeac73a3cdc7d272a3a451afa7d6ec72aa877d7","title":"MicroBERT: Effective Training of Low-resource Monolingual BERTs through Parameter Reduction and Multitask Learning"},{"paperId":"e92b881ec7b762fee1c2e6df47693aecfc3aafb1","title":"Part-of-Speech and Morphological Tagging of Algerian Judeo-Arabic"},{"paperId":"b162638cd42b65e6add199b0b34f1b375070fc7c","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"c2d2d98a1dd0ba591224b729a37673b899146708","title":"Text normalization for endangered languages: the case of Ligurian"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"3979141ef17f9bd2b8a5c290e33775f90a14154c","title":"DziriBERT: a Pre-trained Language Model for the Algerian Dialect"}],"references":[{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"fff1aaa53a249da73ab5f71a023427c23ea7d006","title":"Treebanking User-Generated Content: a UD Based Overview of Guidelines, Corpora and Unified Recommendations"},{"paperId":"18a4af8bb50f7fea3a6c9207f729470e07167113","title":"Noisy UGC Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models"},{"paperId":"d33d05a558cea37cfe06be6163754f974f784aaf","title":"DziriBERT: a Pre-trained Language Model for the Algerian Dialect"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"1cfd0f08bb7ab449bbccc3d69a62ffbf43a7eb7f","title":"First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT"},{"paperId":"29599829d26b4acaaf0ad88698ed4362d33b2ae7","title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models"},{"paperId":"1b86df622f1cc3e2e666318f427ffb7a03aa34e6","title":"Analyse en dépendances du français avec des plongements contextualisés (French dependency parsing with contextualized embeddings)"},{"paperId":null,"title":"Byt5: Towards a token-free"},{"paperId":"b29ebfe83ccd5aae614ca9986fb847d4c739aff4","title":"Arabizi Language Models for Sentiment Analysis"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"5259de991f875b2c614c4bd7bf7bda97b81b9264","title":"On the Importance of Pre-training Data Volume for Compact Language Models"},{"paperId":"bc3c662d2b6f5159202d703d201a9283687d43b9","title":"Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell"},{"paperId":"14489ec7893e373a0dcc9555c52b99b2b3a429f6","title":"Are All Languages Created Equal in Multilingual BERT?"},{"paperId":"370648eabfcde7e0d6ef2c9e2a332672b556e8a6","title":"Can Multilingual Language Models Transfer to an Unseen Dialect? A Case Study on North African Arabizi"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"b805693c17961af2cc7f859c1a54320b26036f46","title":"Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"4fa37d012ad0014552a6a5a03624b29f95558bf7","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"a9c3a009d754a110379574b069b48c0b4c75db40","title":"Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"},{"paperId":"95236a88cc959656958d7a49422f4004015d594d","title":"Comparison between NMT and PBSMT Performance for Translating Noisy User-Generated Content"},{"paperId":"3f9df96b26c42dea6dd6cad64557a3b7d698ea90","title":"MultiFiT: Efficient Multi-lingual Language Model Fine-tuning"},{"paperId":"55ace73cb8f7c3c4bd6d8ead45c0ba6193d1afda","title":"A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"92343cecdc990380de362b969eec60081959f507","title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Linguistic variation [online]. 2019"},{"paperId":null,"title":"How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996– 5001, Florence, Italy"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"c935bcc748c4e9b87420deaf23074535592950d0","title":"Addressing Code-Switching in French/Algerian Arabic Speech"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":"60e328b861d43ae94cf00bd2e8e8c82e22f17921","title":"Massively Multilingual Word Embeddings"},{"paperId":null,"title":"Fasttext"},{"paperId":"e0945081b5b87187a53d4329cf77cd8bff635795","title":"Highway Networks"},{"paperId":"a72325ced3a56f00114c4947cb0e702a95df074b","title":"Rhapsodie: a Prosodic-Syntactic Treebank for Spoken French"},{"paperId":"6fc879a6f264ecb791ffcfac686562230c428b89","title":"An Algerian Arabic-French Code-Switched Corpus"},{"paperId":"e95ad36302f92f45abe169fbc5185e55407bcc34","title":"Universal Dependency Annotation for Multilingual Parsing"},{"paperId":"c928e453122fa7c0658e02a7aa07a623d4e5b679","title":"What to do about bad language on the internet"},{"paperId":"2a04cb0103c3e94bb98a341bca195f821d58d0c1","title":"The French Social Media Bank: a Treebank of Noisy User Generated Content"},{"paperId":"4af5ce5ca404054aada0ff421f50c541078a2c3d","title":"Le corpus Sequoia : annotation syntaxique et exploitation pour l’adaptation d’analyseur par pont lexical (The Sequoia Corpus : Syntactic Annotation and Use for a Parser Lexical Domain Adaptation Method) [in French]"},{"paperId":"99fd177dfddeb77de290e337e690552bc4c8be7d","title":"Introduction to Arabic Natural Language Processing"},{"paperId":"ca9c140636a7e9545e0071d0af5337fee8a349f6","title":"“cba to check the spelling”: Investigating Parser Performance on Discussion Forum Posts"},{"paperId":"b741a9b06c4253c8fc89ce6196d58fbf544a9baa","title":"Statistical French Dependency Parsing: Treebank Conversion and First Results"},{"paperId":"fa5ea3e99d5846ba05dbdc35f67e9f95244d432c","title":"Linguistic Variation"},{"paperId":null,"title":"Djamé Seddah, and Amir Zeldes. 2020. Treebanking user-generated content: a UD based overview of guidelines"}],"id":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","summary":"This work shows that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models."},{"url":"https://www.semanticscholar.org/paper/62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Makesh Narsimhan Sreedhar,Xiangpeng Wan,Yu-Jie Cheng,Junjie Hu","citations":[],"references":[{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"51d62830c1112ea7443398990b850a988ed7c86c","title":"Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":"0bfa9275561f45bc772c4d544fd75f5d65143c5e","title":"When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"651c01b897f18e31bab3ad482154a980ac92c638","title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"95856e0789481eedc2cedc413581a0a819ef8fc8","title":"Unsupervised Domain Clusters in Pretrained Language Models"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"1c84438eff3d10cfb97fb64fc9f35d734209a465","title":"Character-based NMT with Transformer"},{"paperId":"9237d6efc603465765e80eb5ca1268c2bd7b5c24","title":"compare-mt: A Tool for Holistic Comparison of Language Generation Systems"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4","title":"Rapid Adaptation of Neural Machine Translation to New Languages"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"f0b6c1ffed9984317050d0c1dfb005cb65582f13","title":"On the State of the Art of Evaluation in Neural Language Models"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"7dba53e72c182e25e98e8f73a99d75ff69dda0c2","title":"Recurrent Highway Networks"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"81aace0e90c6a962059b117c24db0d856f340f41","title":"Report on the 11th IWSLT evaluation campaign"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"25ca4a36df2955b345634b5f8a6b6bb66a774b3c","title":"Parallel Data, Tools and Interfaces in OPUS"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"}],"id":"62ece609555bf833a2afd25ef796d72b5f59e767","summary":"This paper proposes a Lo cal B yt e F usion (LOBEF) method for byte-based machine translation—utilizing byte n -gram and word boundaries—to aggregate local semantic information and indicates that the byte- based models are parameter-efﬁcient and can be trained faster than subword models while showcasing competitive performance."},{"url":"https://www.semanticscholar.org/paper/12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Hui-li Xue,Nikolaos Aletras","citations":[],"references":[{"paperId":"5ca0a54fa0f76ae0e1881899c61b36d35d3bd166","title":"How does the pre-training objective affect what large language models learn about linguistic properties?"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"cddf40e579a596d0110b260313adf43470617c4c","title":"Datasets: A Community Library for Natural Language Processing"},{"paperId":"acac699d02a972f58b091bfbef7518f0e61c8225","title":"Frustratingly Simple Pretraining Alternatives to Masked Language Modeling"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"1531ffc206bdb310ae08adc02e3df44d1d125d99","title":"On-Device Text Representations Robust To Misspellings via Projections"},{"paperId":"05a6fa3bb47bd283c95b3d86ba4cbbc471cbd6ac","title":"ProFormer: Towards On-Device LSH Projection Based Transformers"},{"paperId":"738215a396f6eee1709c6b521a6199769f0ce674","title":"Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"},{"paperId":"090be9851f8ceea2acaebce4763c780287c85693","title":"Extremely Small BERT Models from Mixed-Vocabulary Training"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"82ba6cc2fd424b7374cb8c2a056496925b4c2948","title":"Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"fb73b93de3734a996829caf31e4310e0054e9c6b","title":"Green AI"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"e26c03768664a1b2a2821659b1e234f59c738c43","title":"Efficient On-Device Models using Neural Projections"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6","title":"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"4bf6ce4a9366cdba069a45651606538f2febd8e6","title":"Compressing Word Embeddings via Deep Compositional Code Learning"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"8215fb083cb4b0ed2b6858b81dcc30fbd0afb6e1","title":"Mining of Massive Datasets"},{"paperId":"511b0bb924d109767feb11855acd059125ff0164","title":"Hash Kernels"},{"paperId":"3fb1c64b763d27fba2a18c923637c5ea0e048e3b","title":"Small Statistical Models by Random Feature Mixing"},{"paperId":null,"title":"The MD5 messagedigest algorithm"},{"paperId":null,"title":"Albert Villanova del Moral"}],"id":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","summary":"It is empirically demonstrated that H ASH F ORMERS are more memory efﬁcient compared to standard pre-trained transformers while achieving comparable predictive performance when ﬁne-tuned on multiple text classiﬂcation tasks."},{"url":"https://www.semanticscholar.org/paper/023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Shaked Yehezkel,Yuval Pinter","citations":[{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"}],"references":[{"paperId":"8c82d3d758897ef9f166924683831ecf6085f21a","title":"MaxMatch-Dropout: Subword Regularization for WordPiece"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"a0ea1ee10daeb27d421a66de0075323e038b78ac","title":"Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"7694aae9766d5f1fe74d900cd82aee898cb6e8e9","title":"How to Train BERT with an Academic Budget"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"0438f16147dc44669863722f81efaa13420885f4","title":"Subword Sampling for Low Resource Word Alignment"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"9127c3873cac9979433a208e69d426782747910b","title":"Evaluating Sub-word Embeddings in Cross-lingual Models"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"997855e1f17d34dd3922d953a587742d198844e6","title":"CrossWeigh: Training Named Entity Tagger from Imperfect Annotations"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"c39e409d6b7744200c4fd12a6b81e51f6145cfae","title":"Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"834724753187ba86709f9d859740f23861d42191","title":"Meaningless yet meaningful: Morphology grounded subword-level NMT"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"531bc31aa70981cc989180f6e3f7d3c6442e7d14","title":"POLYGLOT-NER: Massive Multilingual Named Entity Recognition"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":null,"title":"Inflectional synthesis of the verb"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":null,"title":"Gensim–python framework for vector space modelling"},{"paperId":"57458bc1cffe5caa45a885af986d70f723f406b4","title":"A unified architecture for natural language processing: deep neural networks with multitask learning"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":null,"title":"ponence of selected inflectional formatives"}],"id":"023fd42f0867a88a2206f906c7f127701058feb6","summary":"SAGE is presented, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase, and does a better job than current widespread tokenizers in keeping token contexts cohesive, while not incurring a large price in terms of encoding efficiency or domain robustness."},{"url":"https://www.semanticscholar.org/paper/2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers","venue":"ACL","year":2022,"referenceCount":54,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Valentin Hofmann,Hinrich Schütze,J. Pierrehumbert","citations":[{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"1606793daaa20d4a4a78e859c2fd6b4f7535680c","title":"Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment"}],"references":[{"paperId":"6cd9c0630c422f70ceced4dc400fcbc6001d15b5","title":"ABC: Attention with Bounded-memory Control"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"03aa38229a6c853d96430edfc8f1542598f2116e","title":"AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain"},{"paperId":"3c477f0e43660ce1ba39c111df312929da37f81f","title":"Efficient Domain Adaptation of Language Models via Adaptive Tokenization"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"3376118362db3751cfbd88acd0c090b8a3897733","title":"Superbizarre Is Not Superb: Improving BERT's Interpretations of Complex Words with Derivational Morphology"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"bb0b0c9853f658db9d6a7f4aa90c24906ba4b1dc","title":"A Pointer Network Architecture for Joint Morphological Segmentation and Tagging"},{"paperId":"fd2dc47c38945c0364641ac1085239b076454d40","title":"Tackling the Low-resource Challenge for Canonical Segmentation"},{"paperId":"110c13fbf4ff87b52ee1fd9eb2d3616c839ceb41","title":"Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank"},{"paperId":"dcd72571e380eacf3ddccf225f3324eb8c51ece0","title":"A Graph Auto-encoder Model of Derivational Morphology"},{"paperId":"df8511a57b5d72e3f7e7f3b83bb271d58ef66a39","title":"Predicting the Growth of Morphological Families from Social and Linguistic Factors"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"44189a062182cab428e6d7ea010819f2fe978533","title":"DagoBERT: Generating Derivational Morphology with a Pretrained Language Model"},{"paperId":"8515c83e6562415021b4c536c28ed198f05a8691","title":"Emerging trends: Subwords, seriously?"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3dbc85c3af12736a4c7f89a2204370b061e1d192","title":"Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"fb73b93de3734a996829caf31e4310e0054e9c6b","title":"Green AI"},{"paperId":"21079df28af037f93620a0bd63f04fe49cf5df7d","title":"On the Importance of Tokenization in Arabic Embedding Models"},{"paperId":"7da138d704ef0fc055825fa132f5c452ed3fb52a","title":"LADEC: The Large Database of English Compounds"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"a6cd2c524740aff36d79468ae99f1f484de01e1b","title":"Correcting Whitespace Errors in Digitized Historical Texts"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"5839b0a5f7fce57941620e4e9e2e7168a335a43c","title":"Subword-Level Language Identification for Intra-Word Code-Switching"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"25687348fd01908c56664b40e249a611fb4b6dd8","title":"Gendered associations of English morphology"},{"paperId":"c7ec1b98fce650503168a51131a889942c6f0aa8","title":"A Distributional and Orthographic Aggregation Model for English Derivational Morphology"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"38afef35de217bdce44e2d02f366ca42bedfb60e","title":"Joint Semantic Synthesis and Morphological Analysis of the Derived Word"},{"paperId":"44a590466bf95545eb47daf8ce2a468fa8818751","title":"On Hapax Legomena and Morphological Productivity"},{"paperId":null,"title":"Journal of the Association for Laboratory Phonology"},{"paperId":"90cbdddcf9e8490b42b08ee17602d20bc214354a","title":"Paradigm Completion for Derivational Morphology"},{"paperId":"5bcc16402c812c236adee3449a5e9ba5659b5796","title":"Neural Sequence-to-sequence Learning of Internal Word Structure"},{"paperId":"3cda98ecef94392d8ce4492ee3515b2ebe75899b","title":"Context-Aware Prediction of Derivational Word-forms"},{"paperId":"c75ecddc5eda6b71b54bf8ef40f6fa9da1d8f108","title":"Neural Morphological Analysis: Encoding-Decoding Canonical Segments"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"0a275c52c61ecf2429189387426d6173e40d695a","title":"A Joint Model of Orthography and Morphological Segmentation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"9c73d98785cc143db64da987659dd5656c08db23","title":"Singulars and plurals in Dutch: Evidence for a parallel dual-route model"},{"paperId":"66e73e696414e9fde9201b176c447bf1ae91b00b","title":"The Cambridge encyclopedia of the english language"},{"paperId":"60258897d250a41f11cfee27de828a0130110b5e","title":"The CELEX Lexical Database (CD-ROM)"},{"paperId":"061e742d1d63631ed28d5dcc8b8e3c884519e924","title":"Distributional properties of derivational affixes: Implications for processing"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"9ba4b5ac12496b14e9d2e571515fad89904d19fa","title":"Lexical access and inflectional morphology"},{"paperId":null,"title":". Needle and Janet B . Pierrehumbert . 2018 . Gendered associations of english morphology"}],"id":"2f07f97563a73d9b691ec6144e4bba25a347ab87","summary":"FLOTA (Few Longest Token Approximation) leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise."},{"url":"https://www.semanticscholar.org/paper/6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"C HARFORMER : F AST C HARACTER T RANSFORMERS VIA G RADIENT - BASED S UBWORD T OKENIZATION","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","citations":[],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"2365410a710b421b2295cdca0074946cb50bb1d4","title":"Are Pre-trained Convolutions Better than Pre-trained Transformers?"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"7e9ff94476f41041c75e253e84f487db00e9c861","title":"Long Range Arena: A Benchmark for Efficient Transformers"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"26b277f2d7cd86b67bc3572257557edc4640b8e9","title":"Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","title":"GLU Variants Improve Transformer"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"b611a8095630557229dc5fb6b07c272f1cd614da","title":"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"2270b8628fd8ca67ae39d277f45bc3c38ac63d5f","title":"Mesh-TensorFlow: Deep Learning for Supercomputers"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"1db9bd18681b96473f3c82b21edc9240b44dc329","title":"Image Transformer"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":null,"title":"FRAGE: FrequencyAgnostic Word Representation"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"deff3b0635e141297238797d924d7a9aba3a132a","title":"Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"6f35b070c250507dbec8a7365cf01b25eb66d792","title":"Ex Machina: Personal Attacks Seen at Scale"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"649d03490ef72c5274e3bccd03d7a299d2f8da91","title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"3bf2e6941dbb87ac0d2c771c159e1e27366a26e3","title":"Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics"},{"paperId":"0c09f282d78f42f77d73a52b08cb425e1cabb9b2","title":"Are Vowels and Consonants Processed Differently? Event-related Potential Evidence with a Delayed Letter Paradigm"},{"paperId":"475354f10798f110d34792b6d88f31d6d5cb099e","title":"Automatically Constructing a Corpus of Sentential Paraphrases"},{"paperId":"084c55d6432265785e3ff86a2e900a49d501c00a","title":"Foundations of statistical natural language processing"},{"paperId":"98b405fd0153f24e8359448d79acc49c3ffc8f4c","title":"The relative contribution of consonants and vowels to word identification during reading"},{"paperId":null,"title":"Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee"}],"id":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","summary":"This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion that paves the way for highly performant token-free models that are trained completely end-to-end."},{"url":"https://www.semanticscholar.org/paper/b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA","venue":"","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Peng Chen","citations":[],"references":[{"paperId":"ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51","title":"Efficiently Modeling Long Sequences with Structured State Spaces"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"7e9ff94476f41041c75e253e84f487db00e9c861","title":"Long Range Arena: A Benchmark for Efficient Transformers"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"0964490205fdc38c2f0980c9d778069089ca92e3","title":"HiPPO: Recurrent Memory with Optimal Polynomial Projections"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"22655979df781d222eaf812b0d325fa9adf11594","title":"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"8c1b00128e74f1cd92aede3959690615695d5101","title":"QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"3a7b63b50c64f4ec3358477790e84cbd6be2a0b4","title":"Bidirectional Attention Flow for Machine Comprehension"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"698d675ba7134ac701de810c9ca4a6de72cb414b","title":"Character-Level Question Answering with Attention"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":null,"title":"Longformer: The long-document transformer. CoRR, abs"}],"id":"b639124771f9c62cd656a24e8e685a456918e0ff","summary":"A question answering model that encodes text inputs at character level to utilize subword structures and mitigate the out-of-vocabulary problem is proposed, and both S4 and character-level encoding improve the model performance on the question answering task."},{"url":"https://www.semanticscholar.org/paper/f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?","venue":"Findings","year":2021,"referenceCount":78,"citationCount":4,"influentialCitationCount":0,"publicationDate":"15/10/2021","authors":"Jindřich Libovický,Helmut Schmid,Alexander Fraser","citations":[{"paperId":"11ddb0953eae196dab339bfdc117221594cf945e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"559bfba3bee31f6061a5d5c7061f22794de47e39","title":"State-of-the-art generalisation research in NLP: a taxonomy and review"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}],"references":[{"paperId":"00191ddacdbef6f5819e8682ddfe184d5b415d27","title":"What Do You Get When You Cross Beam Search with Nucleus Sampling?"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"af59aeeb46fb5d8412f550f6dd5c5dc99afc9c1a","title":"Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation"},{"paperId":"37c4ec8d4fcaaf7f2c3f01a2fb30e9e41d8865a2","title":"TextFlint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":"0bfa9275561f45bc772c4d544fd75f5d65143c5e","title":"When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation"},{"paperId":null,"title":"What do you get when you cross beam search with nucleus sampling? CoRR, abs/2107.09729"},{"paperId":null,"title":"What do you get when you cross beam search with nucleus sampling? CoRR"},{"paperId":"f3aaec8d24e07d6370b78aefc757e5d5fe642f60","title":"The University of Edinburgh’s English-German and English-Hausa Submissions to the WMT21 News Translation Task"},{"paperId":null,"title":"CUNI system for WMT 17 automatic post - editing task Attention is all you need"},{"paperId":null,"title":"mT5: A massively"},{"paperId":null,"title":"CUNI system for WMT 17 automatic post - editing task Attention is all you need"},{"paperId":"648bea1c1e11936ecae023593a43d848e3942c0e","title":"Why Neural Machine Translation Prefers Empty Outputs"},{"paperId":"c204d40384d39c59cd7249bde4cd8615972acaac","title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness"},{"paperId":"f85040f8aecee4cdb63137bbad5bbab3ba838c33","title":"Gender Coreference and Bias Evaluation at WMT 2020"},{"paperId":"05cfd96eed27ceb2aa35285991a745a5cd119abc","title":"If Beam Search Is the Answer, What Was the Question?"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"dd7fdaa997a074dbbc4849d0330a42985e7b3c3a","title":"Announcing CzEng 2.0 Parallel Corpus with over 2 Gigawords"},{"paperId":"0156fb2096c39839384c0cae0194e123e01cc575","title":"Character-Level Transformer-Based Neural Machine Translation"},{"paperId":"8b6c9adf85a9d6391e3ccd503c2e5af929a36735","title":"Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation"},{"paperId":"9d92905edc1a5d027b0827d1309bc6ac03dd94a0","title":"Character-Level Translation with Self-attention"},{"paperId":"651c01b897f18e31bab3ad482154a980ac92c638","title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems"},{"paperId":"027946f80f3cb276ea38bc2cf19903052f59cd0e","title":"How Decoding Strategies Affect the Verifiability of Generated Text"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"671cd646d61c670fffc2fea4cf7b7a1f6f80dcf7","title":"NRC Systems for the 2020 Inuktitut-English News Translation Task"},{"paperId":"e8f297e161f57e461ede2d4e0c26573981cad077","title":"Findings of the 2020 Conference on Machine Translation (WMT20)"},{"paperId":"1c84438eff3d10cfb97fb64fc9f35d734209a465","title":"Character-based NMT with Transformer"},{"paperId":"7c848b37605cdfe6d2d4778b06e66c0710c3ec34","title":"On the Importance of Word Boundaries in Character-level Neural Machine Translation"},{"paperId":"d3231772937a2182b2377d028417245c49868dd1","title":"On NMT Search Errors and Model Errors: Cat Got Your Tongue?"},{"paperId":"b33a9df8e39966217786855ef8fcf87d11294774","title":"The University of Helsinki Submissions to the WMT19 Similar Language Translation Task"},{"paperId":"a5690b0a514a7cbc913871e41e54c9ad4f6362db","title":"Findings of the First Shared Task on Machine Translation Robustness"},{"paperId":"6e722ac4d386489aa47703887881835ec0e1331d","title":"Tagged Back-Translation"},{"paperId":"1670a07b70f90cc4ddba71343e6a7ee4b5198595","title":"Evaluating Gender Bias in Machine Translation"},{"paperId":"10b9eee99b2632359d4d26f991e765bff8d91dee","title":"Revisiting Low-Resource Neural Machine Translation: A Case Study"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"90e531d15be3c19f3d83085b906d5c9b5aae540d","title":"JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English"},{"paperId":"48fbdf1be70221ac8a6b22079245030ab6158760","title":"Findings of the 2018 Conference on Machine Translation (WMT18)"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"4236cf737c8825e751db16689b6025d44a75ef97","title":"The WMT’18 Morpheval test suites for English-Czech, English-German, English-Finnish and Turkish-English"},{"paperId":"b699fce01693dcec868a6905488fcd3a652dd412","title":"Neural Machine Translation of Logographic Language Using Sub-character Level Information"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"8fec5d6ac57e90f459e7330775165f2671abc445","title":"Training Deeper Neural Machine Translation Models with Transparent Attention"},{"paperId":"622d324604a2e219dddc1d474ba9c38d9e05ac6c","title":"Character-level Chinese-English Translation through ASCII Encoding"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"d3707cf521e3596313af1f53acba6413d0d528a6","title":"Training Tips for the Transformer Model"},{"paperId":"aeeb2e5ecb5b926bd5673d1fef3e3e61589fabb2","title":"Findings of the WMT 2017 Biomedical Translation Shared Task"},{"paperId":"51b8a5d5264b0e891595a1e111b863efb86c8c72","title":"Evaluating the morphological competence of Machine Translation Systems"},{"paperId":"346e012f79ee607f1b563bc66fd8243f1b1ae4e9","title":"Extending hybrid word-character neural machine translation with multi-task learning of morphological analysis"},{"paperId":"89cb259f39eb8350dd337fc1d02e2f4128c8398c","title":"Byte-based Neural Machine Translation"},{"paperId":"d4cc6fb1b81dd6cffde542f60b001129e3175b93","title":"The Helsinki Neural Machine Translation System"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"63c4114bd373dd0fcfe0d25a605b353c62be2995","title":"How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"d2b2d748208ff79fdaa7064cf9e7bf84a408f2ac","title":"University of Rochester WMT 2017 NMT System Submission"},{"paperId":"cb0ab255c4079e2082ba6e3a807529527d96687c","title":"Overview of the IWSLT 2017 Evaluation Campaign"},{"paperId":"6dc33afdf5a90e44e3fa20086fc270ddc88681a2","title":"CUNI System for WMT17 Automatic Post-Editing Task"},{"paperId":"b561b860e6931436c66f378386a4aec5c778f9d4","title":"The TALP-UPC Neural Machine Translation System for German/Finnish-English Using the Inverse Direction Model in Rescoring"},{"paperId":"ea40fba2dc52bde32dedd45b05f83d27dc2ea6c7","title":"Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe"},{"paperId":null,"title":"The TALP-UPC neural machine"},{"paperId":"1a327709cc53ff9e52454e50a643abf4a0ac92af","title":"Findings of the 2016 Conference on Machine Translation"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"e0945081b5b87187a53d4329cf77cd8bff635795","title":"Highway Networks"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"0f8992ee6418d367d8e50ecbb59b08ea15e8431f","title":"Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems"},{"paperId":"cb826a3899752b796f14df1c50378c64954a6b0a","title":"Statistical Significance Tests for Machine Translation Evaluation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"cd5a169879504ea91660a443b9151753cc29c42f","title":"Minimum Bayes-risk automatic speech recognition"}],"id":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","summary":"It is shown that even with recent modeling innovations in character-level natural language processing, character- level MT systems still struggle to match their subword-based counterparts, and show neither better domain robustness, nor better morphological generalization."},{"url":"https://www.semanticscholar.org/paper/2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":2,"influentialCitationCount":1,"publicationDate":"19/04/2022","authors":"Cagri Toraman,E. Yilmaz,Furkan Sahinuc,Oguzhan Ozcelik","citations":[{"paperId":"7aef8e81883e9ded80076b8d2002e29ec5555564","title":"Bioberturk: Exploring Turkish Biomedical Language Model Development Strategies in Low Resource Setting"},{"paperId":"a611ecc0b48b1302abbd33712b23f025f35be0b4","title":"ARC-NLP at CheckThat!-2022: Contradiction for Harmful Tweet Detection"}],"references":[{"paperId":"bef55aedf15ebcf9b21b0e3f5df4dde8def512bd","title":"Large-Scale Hate Speech Detection with Cross-Domain Transfer"},{"paperId":"72d8a122c62abdc223280dfd56b34f7634e27e92","title":"Ethical Challenges in AI"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":null,"title":"Climate Impact by Area"},{"paperId":"c7d50a6b33d1fc9b460e5bb0c141a00203c82645","title":"Towards Tokenization and Part-of-Speech Tagging for Khmer: Data and Discussion"},{"paperId":"019e6093a5439468abc82038dc9e6d72c49e23b7","title":"The social cost of carbon dioxide under climate-economy feedbacks and temperature variability"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5e8da0a332906a2cc5262483ddc27eb9b792dbaf","title":"Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine Translation: The Case of Fon Language"},{"paperId":"1c15d9f928d2ff13ba857e3eafc1e784a0100838","title":"Finding Better Subwords for Tibetan Neural Machine Translation"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"86b369759bff13ec23b45ca23cc5461292a75415","title":"WangchanBERTa: Pretraining transformer-based Thai Language Models"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"2c953a3c378b40dadf2e3fb486713c8608b8e282","title":"Pretrained Transformers for Text Ranking: BERT and Beyond"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"7b3f9d6343aeb5659b2d6c39e8a4004b00b0695e","title":"Semantic Similarity Based Evaluation for Abstractive News Summarization"},{"paperId":"3b6bab26e32a297b15999160c5cf5727e2635210","title":"Should we find another model?: Improving Neural Machine Translation Performance with ONE-Piece Tokenization Method without Model Modification"},{"paperId":"e53c82abd656858f6c4715e545737aa3c9889f36","title":"Pre-Training on Mixed Data for Low-Resource Neural Machine Translation"},{"paperId":null,"title":"Oscar Dataset Huggingface"},{"paperId":"4d1367141eb8210a981428f3a7fdbdbc9db3e248","title":"A Tokenization System for the Kurdish Language"},{"paperId":"3f02219184319aa8ca1ef182e6b091b6a7539a04","title":"Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"2f3f885a52b92b2ab6bddcc3ce54b7eff541bd02","title":"Enhancing Tokenization by Embedding Romanian Language Specific Morphology"},{"paperId":"1cbcc8a05fcd76254211b0d499063b268f6a1a9a","title":"Data and Representation for Turkish Natural Language Inference"},{"paperId":"41a7fece6e8c47d5bff75f7701a702f351b110d6","title":"BERTurk - BERT models for Turkish"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"13f25c69973373e616c48688d06a6b6ae2736ef0","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":null,"title":"Primary Energy and GHG Emissions Coefficients of Electricity"},{"paperId":null,"title":"Turkish Language Models"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"92343cecdc990380de362b969eec60081959f507","title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"690edf44e8739fd80bdfb76f40c9a4a222f3bba8","title":"BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"},{"paperId":"3d4e09b30ede9ad6790fe830d39e1333fb6e5ee4","title":"NOVA: A Feasible and Flexible Annotation System for Joint Tokenization and Part-of-Speech Tagging"},{"paperId":"7365f887c938ca21a6adbef08b5a520ebbd4638f","title":"Model Cards for Model Reporting"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"2019. BERT: Pre-training"},{"paperId":"a9a5d671271fff45429084e184a788f611b6f194","title":"FRAGE: Frequency-Agnostic Word Representation"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"052418ebf497a3a4eaa76cd350987932253dee2b","title":"Morphological Disambiguation for Turkish"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"66182e1ba6dca20f315cd952866a94f54d8ac820","title":"Cross-lingual polarity detection with machine translation"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"b8417c267678a66919e3a85f7ffb05de8e71e4ff","title":"Information extraction from web services: a comparison of Tokenisation algorithms"},{"paperId":"61031f5b863de2d2b235c964d28de468d02bd2d5","title":"Developing a text categorization template for Turkish news portals"},{"paperId":"22dc2148ab8bd58c6b4491fe7b59257f5739d2e7","title":"Exploiting Separation of Closed-Class Categories for Arabic Tokenization and Part-of-Speech Tagging"},{"paperId":null,"title":"Language Detection Library for Java"},{"paperId":null,"title":"Zemberek, An Open Source NLP Framework for Turkic Languages"},{"paperId":"28c80bce6fe1b21dc52ad165c3a329eeddab8c3d","title":"A statistical information extraction system for Turkish"},{"paperId":"d9c71db75046473f0e3d3229950d7c84c09afd5e","title":"Text Chunking using Transformation-Based Learning"}],"id":"2ffacbeeebd3d9e7467610057b4308635a165b6b","summary":"Morphological and word-level tokenizers outperform de-facto tok- 017 enizers in particular cases and mini models can be competitive to larger state-of-the-art models, such that a 14-times smaller model can recover 94% of the performance of a larger model."},{"url":"https://www.semanticscholar.org/paper/17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?","venue":"Conference of the Association for Machine Translation in the Americas","year":2022,"referenceCount":37,"citationCount":2,"influentialCitationCount":0,"publicationDate":"29/04/2022","authors":"Shiyue Zhang,Vishrav Chaudhary,Naman Goyal,James Cross,Guillaume Wenzek,Mohit Bansal,Francisco Guzmán","citations":[{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"c464352b7ab5f74bc201b5cf94bb4b9a14f5f487","title":"Masked Part-Of-Speech Model: Does Modeling Long Context Help Unsupervised POS-tagging?"}],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"4d8a5338042da99819746ff835b6f299135e2023","title":"Statistical Power Analysis for the Behavioral Sciences"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c17985a669522e7e85ae3d34754c7df49c7187d1","title":"Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Exploring bert’s vocabulary"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"5308b9d6c001304a882a50891ccce9f7ccb1c3ec","title":"On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627","title":"Six Challenges for Neural Machine Translation"},{"paperId":"a486e2839291111bb44fa1f07731ada123539f75","title":"Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"9ed9bff37ec952134564b3b2a022b7aba9479ff2","title":"Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"17e2977b907aad2532c45185947539e83ac639cd","summary":"This work analyzes how translation performance changes as the data ratios among languages vary in the tokenizer training corpus and finds that while relatively better performance is often observed when languages are more equally sampled, the downstream performance is more robust to language imbalance than the authors usually expected."},{"url":"https://www.semanticscholar.org/paper/062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/08/2022","authors":"Borun Chen,Hongyin Tang,Jingang Wang,Qifan Wang,Haitao Zheng,Wei Yu Wu,Liqian Yu","citations":[],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","title":"LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization"},{"paperId":"8b954e1654c6b759a957fd11e66c111e6105fb3f","title":"CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding"},{"paperId":"077c713bccd9d2c7fde68d4cbde06ab0f07a6855","title":"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"},{"paperId":"19537be34dbadbcaa4fffcf028a8ada5095b1b5c","title":"COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"32d281a1e7a0a2d4e2b3f34e0f71780c987e1374","title":"DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training With Whole Word Masking for Chinese BERT"},{"paperId":"b9478e237b58160c65acd2c41894493d27e2c277","title":"WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"},{"paperId":"83ed88e4f745cc9aecd1fbd479612b11beddcb86","title":"CLEAR: Contrastive Learning for Sentence Representation"},{"paperId":"6dd1a0f657ff547b1736c2d8b30451f49a9a48f5","title":"OCNLI: Original Chinese Natural Language Inference"},{"paperId":"574c7c7260a795d2906f03b5229ace3f54ee0ab3","title":"An Unsupervised Sentence Embedding Method by Mutual Information Maximization"},{"paperId":"38643c2926b10f6f74f122a7037e2cd20d77c0f1","title":"Supervised Contrastive Learning"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"17d5884215b5afa53545cd7cb6135de5478da4ec","title":"CERT: Contrastive Self-supervised Learning for Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"ae43556f2cc1cecb8b48762b4e09df319fbaa4d9","title":"Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"549c1a581b61f9ea47afc6f6871845392eaebbc4","title":"LCQMC:A Large-scale Chinese Question Matching Corpus"},{"paperId":"c997d481606f0346164511cabe74c6d1ef3f6be5","title":"DRCD: a Chinese Machine Reading Comprehension Dataset"},{"paperId":"2e10560579f2bdeae0143141f26bd9f0a195b4b7","title":"Mixed Precision Training"},{"paperId":"7afb83134d5b7914131e10b229d30dc2593266f6","title":"The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"346d900f2f73b706ec539caecb4800d131fc9f6a","title":"An empirical study of sentiment analysis for chinese documents"},{"paperId":"885476e9d1b2ea67405f7f9b46c2c8536657a204","title":"Scalable Term Selection for Text Categorization"}],"id":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","summary":"This work proposes a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations, and implicitly encodes the coarse-grained information into the fine-Grained representations through contrastive learning on multi-grains information."},{"url":"https://www.semanticscholar.org/paper/035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences","venue":"EMNLP","year":2019,"referenceCount":37,"citationCount":13,"influentialCitationCount":0,"publicationDate":"01/11/2019","authors":"Matthias Gallé","citations":[{"paperId":"2e53fe6ff7d6d74da962957a5c0b282eaabd4edf","title":"How Effective is Byte Pair Encoding for Out-Of-Vocabulary Words in Neural Machine Translation?"},{"paperId":"1c98fcd62a5889c70ea9da4cd168b36253c7101c","title":"No Features Needed: Using BPE Sequence Embeddings for Web Log Anomaly Detection"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey"},{"paperId":"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","title":"Beyond Characters: Subword-level Morpheme Segmentation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"617c0f6fc2d6807a9cfa72f487b0542b25342fd0","title":"Optimizing Word Alignments with Better Subword Tokenization"},{"paperId":"d36d6abf8f9f1e80124d8a72dc5203802a6fdb26","title":"IIITT at CASE 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection"},{"paperId":"308e3090cc3c77712733a5552d10d25f210cd905","title":"Practical Random Access to SLP-Compressed Texts"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"25fc1d08e98172afe9ac2e4a64fb8fe359045547","title":"Morphological segmentation method for Turkic language neural machine translation"},{"paperId":"ec69e191e25ff4ab9b59178fdc5bd171f9f147b6","title":"Using Context to Help Predict Speaker's Emotions in Social Dialogue"},{"paperId":"d458218398f84b6ce259f3ef684582eea9e9158e","title":"String Processing and Information Retrieval: 27th International Symposium, SPIRE 2020, Orlando, FL, USA, October 13–15, 2020, Proceedings"}],"references":[{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"4ab8662b0004305470c234acebe9b0381234107d","title":"Rpair: Rescaling RePair with Rsync"},{"paperId":"0ce95955ef06804aace7f3a03f8e882e8826e6eb","title":"How Much Does Tokenization Affect Neural Machine Translation?"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features"},{"paperId":"6e205e973af09673028918658b99196f31fb8684","title":"Neural Machine Translation for Morphologically Rich Languages with Improved Sub-word Units and Synthetic Data"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6cddfbed35c46937588bd9d6b846ca2855953cea","title":"Neural Lattice-to-Sequence Models for Uncertain Inputs"},{"paperId":"f958d4921951e394057a1c4ec33bad9a34e5dad1","title":"A Convolutional Encoder Model for Neural Machine Translation"},{"paperId":"e43f713e0d2d438a4c0b03eacab58c334e869e6a","title":"Lattice-Based Recurrent Neural Network Encoders for Neural Machine Translation"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":"5a14f3fe1b555afe30a62e780965bff414e1444c","title":"Lexis: An Optimization Framework for Discovering the Hierarchical Structure of Sequential Data"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"9aebe7899e9de235254dd78ec0bdc4cdef234f72","title":"An effective heuristic for the smallest grammar problem"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"611a498be099f8bd97b0104ea28f79bcac895221","title":"Searching for smallest grammars on large sequences and application to DNA"},{"paperId":"021002a852fdc590fbfbad6027cf8342ff2ea975","title":"The Smallest Grammar Problem as Constituents Choice and Minimal Grammar Parsing"},{"paperId":null,"title":"The Kyoto free translation task"},{"paperId":"fda0ba4dc485df29f1bcb11fc6badff356594489","title":"Linear-Time Text Compression by Longest-First Substitution"},{"paperId":"5aada25e3d3122f02a84d5d393c2ef2a78bf7e8b","title":"Text Compression"},{"paperId":"35dc54a582a06007c53e00943e51f2ecfa28966d","title":"Re-pair Achieves High-Order Entropy"},{"paperId":"54d7a8d91c2a2f09ea12d3a789bcb5bb33c3fe51","title":"The smallest grammar problem"},{"paperId":"32bdcb13b099314dcab80fea0e7416ea8a97d41b","title":"Off-line dictionary-based compression"},{"paperId":"e0f0f1f1872264e7769ccbd9edaaf435bb400b81","title":"Data compression using long common strings"},{"paperId":"7c72b917a38b09e6d3ab19d28a4344ba54edb6ae","title":"Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology"},{"paperId":"062f5fd99da136c01d6dff356073c54a91838412","title":"Compression and Explanation Using Hierarchical Grammars"},{"paperId":"7ec7bc1d7f35558640ac05e94f9b854ec17d7bc0","title":"Linguistic Structure as Composition and Perturbation"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":"1de77281d812c67b605e160d2db841415ec19c17","title":"An analysis of the longest match and the greedy heuristics in text encoding"},{"paperId":"39cc07b9bd6ac368c46544d0c2805b78f68bb923","title":"On the syntactic structure of protein sequences and the concept of grammar complexity"},{"paperId":"3d951ad1164c17217a24e46b57f9b31cea1b2b96","title":"Data compression via textual substitution"},{"paperId":"d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5","title":"Modeling By Shortest Data Description*"},{"paperId":"5e37f888248c44a8d8098ccd07e91cc06e8b4d42","title":"AN ALGORITHM FOR THE SEGMENTATION OF AN ARTIFICIAL LANGUAGE ANALOGUE"},{"paperId":"91e56dee713c103aa8a86ab12383391d292cca45","title":"A comparison of algorithms for data base compression by use of fragments as language elements"},{"paperId":"d435f1a0d3c4325d02332f5b39b581b3f0c34488","title":"Common phrases and minimum-space text storage"}],"id":"035df9ecf84da7ae475175f326095ab16b97dd47","summary":"The experiments show that - given a fixed vocabulary size budget - the fewer tokens an algorithm needs to cover the test set, the better the translation (as measured by BLEU)."},{"url":"https://www.semanticscholar.org/paper/5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation","venue":"Findings","year":2020,"referenceCount":36,"citationCount":32,"influentialCitationCount":1,"publicationDate":"05/04/2020","authors":"Thamme Gowda,Jonathan May","citations":[{"paperId":"caec2c201aa002ee23d0c7fea7dff6e7ffa7b267","title":"Towards a general purpose machine translation system for Sranantongo"},{"paperId":"9d83941a205e31d394f614424cdc553cea9e35f9","title":"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"a30318cc5281187bd315915f0d0cccd7007f75b9","title":"The VolcTrans System for WMT22 Multilingual Machine Translation Task"},{"paperId":"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","title":"Checks and Strategies for Enabling Code-Switched Machine Translation"},{"paperId":"bb459f27d173b3ffa293a0213ec29e31e44c404d","title":"Reinforcement Learning with Large Action Spaces for Neural Machine Translation"},{"paperId":"3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class"},{"paperId":"4f68042a0aa40f34027a49ceec64ad2bbe2211aa","title":"When does Parameter-Efficient Transfer Learning Work for Machine Translation?"},{"paperId":"88aeac6f5efe99bf24aae982a9c2d00d02685bf7","title":"IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"a821453809f36a23402ce93b6042c76d85a720a6","title":"Frequency-Aware Contrastive Learning for Neural Machine Translation"},{"paperId":"48d5e267ab336a0b4caf2e1909d7860b02c6a736","title":"Direct Speech-to-Speech Translation With Discrete Units"},{"paperId":"3442f4bd36ee6b23196b8db40a0b8f84c55a960d","title":"SeqTrans: Automatic Vulnerability Fix via Sequence to Sequence Learning"},{"paperId":"4ddd463c22fab24e6d89105b6aa887e149832dac","title":"HFT: High Frequency Tokens for Low-Resource NMT"},{"paperId":"9e3f3789ef3bfbf8f2ab3174c4d9a49b92085787","title":"Limitations and Challenges of Unsupervised Cross-lingual Pre-training"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"b9bba726584babe18ed249c3ff55e9b851a2876c","title":"Automatic Gloss-level Data Augmentation for Sign Language Translation"},{"paperId":"52d4e170bc9a34797c521fa7c35c650538b5f909","title":"IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations"},{"paperId":"674265c672777b6d10d5455adc58a6cacb0d0cfe","title":"BPE beyond Word Boundary: How NOT to use Multi Word Expressions in Neural Machine Translation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"318853f0448a9b95235dbb99338af0b19d138eaa","title":"Quasi Character-Level Transformers to Improve Neural Machine Translation on Small Datasets"},{"paperId":"2607dce6dcb9043ca9cae67e25e6a24411f08c0b","title":"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation"},{"paperId":"94772377a9c08ad81e506240f844534b6669b8e9","title":"Diversifying Dialog Generation via Adaptive Label Smoothing"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"49a5a1a550d0ce757befbad7715f373d8a5b3b87","title":"On the Strengths of Cross-Attention in Pretrained Transformers for Machine Translation"},{"paperId":"3e32139deb17761a25075f8839daa61ad5992fc9","title":"Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation"},{"paperId":"3b34e79610e5acaba352def4323a59f6d531fac7","title":"Macro-Average: Rare Types Are Important Too"},{"paperId":"a49b0997efaec2db61109a6deed1512672c3cd0c","title":"Many-to-English Machine Translation Tools, Data, and Pretrained Models"},{"paperId":"7cb4b8406255d0f8115afa23d8efea1bb780cfb8","title":"On the Compatibility of Tokenizations Across Languages"},{"paperId":"7d4e38cc1c26ef513b3e9764d1778917e5420187","title":"Transformers for Low-Resource Languages: Is Féidir Linn!"},{"paperId":"f742e7a7582d647ffe9b8037ba286c4932eb84dd","title":"Boosting Neural Machine Translation from Finnish to Northern Sámi with Rule-Based Backtranslation"},{"paperId":"7771aa7badc3375a31bfac8dc47755ff5d5c7780","title":"From characters to words: the turning point of BPE merges"}],"references":[{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"ea3e18c7b10a137d495054682c055a80b5be768c","title":"Findings of the 2019 Conference on Machine Translation (WMT19)"},{"paperId":"f9163156eeba67762a7441db48fe6720106137cd","title":"Survey on deep learning with class imbalance"},{"paperId":"5dc1a37bf05fddcdb77efafec3bc50c8ded7cef0","title":"End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features"},{"paperId":"d3707cf521e3596313af1f53acba6413d0d528a6","title":"Training Tips for the Transformer Model"},{"paperId":"15e81c8d1c21f9e928c72721ac46d458f3341454","title":"Non-Autoregressive Neural Machine Translation"},{"paperId":"9757ef31095227cb289af22b0a4010eda754d100","title":"A systematic study of the class imbalance problem in convolutional neural networks"},{"paperId":"25c0e5ad89b77ae9c7ff91765ebd4e1e21abdcb3","title":"The IIT Bombay English-Hindi Parallel Corpus"},{"paperId":null,"title":"Nonautoregressive neural machine translation"},{"paperId":"2d8edc4e38bf9907170238726ec902cb3739393b","title":"Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627","title":"Six Challenges for Neural Machine Translation"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"63e39cdf1ad884da6bc69096bb3413b5b1100559","title":"Using the Output Embedding to Improve Language Models"},{"paperId":null,"title":"Tjong Kim Sang and Fien De Meulder . 2003 . Introduction to the CoNLL - 2003 shared task : Language - independent named entity recognition"},{"paperId":"9fbeebb98f479405dadd912e95796ba0256b74ac","title":"CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies"},{"paperId":null,"title":"Mark Steedman . 2008 . On becoming a discipline"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":null,"title":", Matt Post , and Marcos Zampieri . 2019 . Findings of the 2019 conference on machine translation ( WMT 19 )"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"1eb09fecd75eb27825dce4f964b97f4f5cc399d7","title":"On the Properties of Neural Machine Translation: Encoder–Decoder Approaches"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"649d03490ef72c5274e3bccd03d7a299d2f8da91","title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"fa0324d3fddbbac02d24887b8a644e4db111d574","title":"Training neural network classifiers for medical decision making: The effects of imbalanced datasets on classification performance"},{"paperId":"5e379191996d63afed4daee23b0a4568d1715b90","title":"Last Words: On Becoming a Discipline"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746","title":"The class imbalance problem: A systematic study"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"8dcaf96f66340c453e775ab217a1b1bd9ba63449","title":"Time series analysis, forecasting and control"}],"id":"5e788c833321b12671206b96a438c0e5b1202027","summary":"This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and reveals an explanation for why certain vocabulary sizes are better than others."},{"url":"https://www.semanticscholar.org/paper/5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":52,"citationCount":13,"influentialCitationCount":1,"publicationDate":"16/04/2021","authors":"Elizabeth Salesky,David Etter,Matt Post","citations":[{"paperId":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only"},{"paperId":"a94e2ce7f94d189e5f788cfa431c504b3fb49402","title":"A Major Obstacle for NLP Research: Let's Talk about Time Allocation!"},{"paperId":"c9b98d7dd15acfddf8de8448263bfff0feb6c382","title":"Logographic Information Aids Learning Better Representations for Natural Language Inference"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"4743ee49af83d3010549ee105e0c193b36fa239a","title":"Machine Translation Robustness to Natural Asemantic Variation"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e","title":"vTTS: visual-text to speech"},{"paperId":"96fd89de07a69dd2dc94d71f884e64174c5974e2","title":"Interpreting the Robustness of Neural NLP Models to Textual Perturbations"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"a76c98c6814ce8de07707b81c18520af508b7184","title":"BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks"},{"paperId":"38375a9961778355f073bf974e643e4e00d6c10e","title":"Visual Cues and Error Correction for Translation Robustness"}],"references":[{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":null,"title":"Costa ­ jussà , David Aldón , and José A . R . Fonollosa"},{"paperId":"c204d40384d39c59cd7249bde4cd8615972acaac","title":"Findings of the WMT 2020 Shared Task on Machine Translation Robustness"},{"paperId":"da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71","title":"OCR Post-Correction for Endangered Language Texts"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"d7a1c8da1b8163f5bbd4d12e58cad5002947105e","title":"Word Shape Matters: Robust Machine Translation with Visual Embedding"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"5dd34d2781ca702d0e3cd1224517ff60d6c3e2ee","title":"Phonetic and Visual Priors for Decipherment of Informal Romanization"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"e8f297e161f57e461ede2d4e0c26573981cad077","title":"Findings of the 2020 Conference on Machine Translation (WMT20)"},{"paperId":"a5690b0a514a7cbc913871e41e54c9ad4f6362db","title":"Findings of the First Shared Task on Machine Translation Robustness"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"4dda68faa3ea2c888711ce5ced009afcb612e05b","title":"Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems"},{"paperId":"bdbf635476477eec5be5a292b494e20b8902cc35","title":"Improving Robustness of Machine Translation with Synthetic Noise"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":", andM . Carreiras . 2008 . R 34 d 1 ng w 0 rd 5 w 1 th numb 3 r 5"},{"paperId":null,"title":"Glyce: Glyph­vectors for chinese"},{"paperId":"64b8361a133ad545cbf9c035e109ef8461b31f67","title":"Super Characters: A Conversion from Sentiment Classification to Image Classification"},{"paperId":"1e6d69f8cfd698b8139cde557252753b22c7d712","title":"BPE and CharCNNs for Translation of Morphology: A Cross-Lingual Comparison and Analysis"},{"paperId":"ce89ee7aaeeea2c9d474707690f3ea9d948776a3","title":"MTNT: A Testbed for Machine Translation of Noisy Text"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"d09e0187879c6dbaacb16c23a2dddb31d74b8b0b","title":"On the Impact of Various Types of Noise on Neural Machine Translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"6ae164b36a8b712ca5b51659e7d77aa124151b4e","title":"COPPA V2. 0: Corpus Of Parallel Patent Applications Building Large Parallel Corpora with GNU Make"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":null,"title":"The multitarget TED talks task"},{"paperId":"4b5d9e8225d9caa2643217ad033b32ba3fe97b05","title":"Combining Convolutional Neural Networks and LSTMs for Segmentation-Free OCR"},{"paperId":"8e39bc4e1711e941881f64935b203a93259acb50","title":"Glyph-aware Embedding of Chinese Characters"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"db253b17043b6a86e02173b6aa597664b0c7f256","title":"Learning Character-level Compositionality with Visual Features"},{"paperId":"5f09d0cbd59e7c84b912692b6ded42107e8f9733","title":"Chinese–Spanish neural machine translation enhanced with character and word bitmap fonts"},{"paperId":"711e6e9e2fb1f4ab7238cdd556002ae36deeece7","title":"Robsut Wrod Reocginiton via Semi-Character Recurrent Neural Network"},{"paperId":"892e53fe5cd39f037cb2a961499f42f3002595dd","title":"Bag of Tricks for Efficient Text Classification"},{"paperId":"8e9149ab00236d04db23394774e716c4f1d89231","title":"An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"c2e1e362e27ff5ced52bfc9bde3ca165e7eafe76","title":"Neural machine translation using bitmap fonts"},{"paperId":null,"title":"2016a. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651"},{"paperId":null,"title":"Robsut wrod reocgini ­ ton via semi ­ character recurrent neural network Optimizing segmentation granularity for neural machine trans ­ lation"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","title":"Show and tell: A neural image caption generator"},{"paperId":"fba7c0a51a6301ca4086a5ce59b1f13af9acad7f","title":"Pointwise Prediction for Robust, Adaptable Japanese Morphological Analysis"},{"paperId":"a7bfdac75a0f0dfbd5faa375d90132c3115f9725","title":"R34D1NG W0RD5 W1TH NUMB3R5."},{"paperId":null,"title":"Journal of experi mental psychology. Human perception and perfor mance"},{"paperId":"3169b65a8769302ccb3ef41d98617bd2d9826293","title":"Raeding Wrods With Jubmled Lettres"},{"paperId":"cf61a21a58d3c762e0dc868ce09114e18304ed9d","title":"Word recognition inside out and outside in"}],"id":"5c3005e22e6fb218aa76fea49971f3f991993b32","summary":"This work proposes the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows, and demonstrates significant robustness to varied types of noise."},{"url":"https://www.semanticscholar.org/paper/2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?","venue":"EMNLP","year":2021,"referenceCount":50,"citationCount":8,"influentialCitationCount":1,"publicationDate":"02/09/2021","authors":"Chantal Amrhein,Rico Sennrich","citations":[{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"457987f1e7c22e193a8d0dd3704781b3a51b977f","title":"DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages"},{"paperId":"1d587db607e610fa40172f6557f74395170531ce","title":"Quantifying Synthesis and Fusion and their Impact on Machine Translation"},{"paperId":"5e87127e2795ae727c1e3d596fe8cc328da9c54b","title":"How can NLP Help Revitalize Endangered Languages? A Case Study and Roadmap for the Cherokee Language"},{"paperId":"258e9773d9746ccb57e223614831fa6da6369781","title":"Modeling Target-Side Morphology in Neural Machine Translation: A Comparison of Strategies"},{"paperId":"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","title":"Beyond Characters: Subword-level Morpheme Segmentation"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}],"references":[{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"109b5c0bade0dc285153d4c7f90d42a8af06126b","title":"On Biasing Transformer Attention Towards Monotonicity"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"f886218ddd2b8de54950e5fd46c7d39f85eea0e9","title":"Multi-Adversarial Learning for Cross-Lingual Word Embeddings"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"b044395ed89de33b43d304c008d3c5a7727d423d","title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection"},{"paperId":"651c01b897f18e31bab3ad482154a980ac92c638","title":"Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems"},{"paperId":"5c27f7106d2f0c95fc51fb98ece672e00185ffa1","title":"Scheduled DropHead: A Regularization Method for Transformer Models"},{"paperId":"b56e2e7b93be127c953b6ad18230d5905051d23b","title":"BLiMP: A Benchmark of Linguistic Minimal Pairs for English"},{"paperId":"904d305e232f3590189f6108441bdce4584027de","title":"On the Linguistic Representational Power of Neural Machine Translation Models"},{"paperId":"4099c4d272c12081b562392606e6d567e4ae7031","title":"Masked Language Model Scoring"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":null,"title":"spaCy: Industrial-strength Natural Language Processing in Python"},{"paperId":"ec070add234e7ef8f7b80b91702e54a37d3be483","title":"Exact Hard Monotonic Attention for Character-Level Transduction"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"dd2aac5fe114ad98dbc15c21d66f5fc042393204","title":"Reduplication"},{"paperId":"4d00097433a538002b36cfd7a621daddde3e4c0d","title":"Targeted Syntactic Evaluation of Language Models"},{"paperId":"e82d5e7210ac4a5a9ccb07af86a38ddd3c1c1933","title":"Neural Transition-based String Transduction for Limited-Resource Setting in Morphology"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"57faf160097049d14399ac6d317bbe4d1b8aa2de","title":"Compositional Representation of Morphologically-Rich Input for Neural Machine Translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"5da4567918c9d47a85575008edecb78fdf9dd391","title":"Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder"},{"paperId":"51b8a5d5264b0e891595a1e111b863efb86c8c72","title":"Evaluating the morphological competence of Machine Translation Systems"},{"paperId":"1f080959faf9bf2a282c0aadbd8584d8b32f6e24","title":"Modeling Target-Side Inflection in Neural Machine Translation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a921e014249395496c22783fe58d293746af852b","title":"From Characters to Words to in Between: Do We Capture Morphology?"},{"paperId":"100104b980d56a40cadfbd7034fa7807ce49b3fb","title":"Nematus: a Toolkit for Neural Machine Translation"},{"paperId":"63c4114bd373dd0fcfe0d25a605b353c62be2995","title":"How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs"},{"paperId":"a32763adef1ef22cc27d4d67ef7ac1490d23ce0b","title":"Morphological Inflection Generation with Hard Monotonic Attention"},{"paperId":"63e39cdf1ad884da6bc69096bb3413b5b1100559","title":"Using the Output Embedding to Improve Language Models"},{"paperId":"6e617ba8be5fe80f2091232cb685ca33ebadcfd7","title":"Word Representation Models for Morphologically Rich Languages in Neural Machine Translation"},{"paperId":null,"title":"A Frequency Dictio"},{"paperId":"72fde8260622e13866e9096741145a7ab960f474","title":"Efficient Word Alignment with Markov Chain Monte Carlo"},{"paperId":"85232a180301d270953db9173bcd3f2cae17bb98","title":"The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages"},{"paperId":"1a327709cc53ff9e52454e50a643abf4a0ac92af","title":"Findings of the 2016 Conference on Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"14ab2bc257ba3c4c7bbc501dfcdc4c22dd137ab7","title":"Introduction To Language"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"d6dab7eb4f3667045264c54c2c6a617da58e53f6","title":"The Oxford Handbook of Derivational Morphology"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":"105ed573024e9a31eddc766b6018297ab4383bb9","title":"On Achieving and Evaluating Language-Independence in NLP"},{"paperId":"dd79821ce068f263ab7d6c2037a83c20e562dff3","title":"Morpho Challenge 2005-2010: Evaluations and Results"},{"paperId":"5b2a639ce92b4e5c446a48bfac4a433217191460","title":"Turkish: A Comprehensive Grammar"},{"paperId":"cf5fd46fbdae661484b71287f2820ea0077cfe73","title":"Itzaj Maya Grammar"},{"paperId":null,"title":"Side Train Freq. bpe32k bpe-d32k bpe-d500 char bpe32k"}],"id":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","summary":"It is found that learning to analyse and generate morphologically complex surface representations is still challenging, especially for nonconcatenative morphological phenomena like reduplication or vowel harmony and for rare word stems."},{"url":"https://www.semanticscholar.org/paper/379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"F. Meyer,Jan Buys","citations":[],"references":[{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"d7aa3c65eae71776ccc3cddafb1d4b288deae2cb","title":"A Masked Segmental Language Model for Unsupervised Natural Language Segmentation"},{"paperId":"5a50d90c7ad715c57b5f0cd9d8473b3dff705d40","title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"13b4d9a1096113db63158590e9d2e58744766e3b","title":"Unsupervised Word Segmentation with Bi-directional Neural Language Model"},{"paperId":"7b8d0413ee38d09421e0243dbb63c1e51fd1b571","title":"Low-Resource Language Modelling of South African Languages"},{"paperId":"bf607962d2edc1f52f06622853c93ec65efc6d03","title":"Canonical and Surface Morphological Segmentation for Nguni Languages"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"b2474a00d7de3373bab934c09acef1994fa82207","title":"Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"550871a80614dd481d153a43f94ef880cc4c461b","title":"Adaptation of Deep Bidirectional Transformers for Afrikaans Language"},{"paperId":"485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"b079db3fe7cc3037fc15a5a1071254a0046774ac","title":"On the Importance of Subword Information for Morphological Tasks in Truly Low-Resource Languages"},{"paperId":"40c531fb0267437b0f76fd8f0080fb4de9ffe146","title":"A Systematic Study of Leveraging Subword Information for Learning Word Representations"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"7e55cb75b085ea144597710e9fb75be7479b2a64","title":"Towards an unsupervised morphological segmenter for isiXhosa"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"c46fd08184f91144dbc56bb2e41dd800669ac5f8","title":"Evaluation of combined bi-directional branching entropy language models for morphological segmentation of isiXhosa"},{"paperId":"e6ffe957179541a4cad7b6eba888c79c4aad8f91","title":"Unsupervised Morphological Segmentation for Low-Resource Polysynthetic Languages"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Exploring BERT's vocabulary"},{"paperId":"a98c340f316fc041dd071efcec1fbddf176282d9","title":"Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"fce6bacacd59de6ad7b486261d7f955374164c7f","title":"Unsupervised Morphological Segmentation Using Neural Word Embeddings"},{"paperId":"0a275c52c61ecf2429189387426d6173e40d695a","title":"A Joint Model of Orthography and Morphological Segmentation"},{"paperId":"6b904d6e84c98c6ce22ce6923224b205a2a24ee1","title":"Segmental Recurrent Neural Networks"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"1a7715e8d41a228f8fc35f7e3f2580988eea9b24","title":"Developing Text Resources for Ten South African Languages"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"5f4df9ebb4d4dddc4e84bcf1fb53d053f69105c3","title":"Unsupervised Morphological Segmentation with Log-Linear Models"},{"paperId":"02711b7e8c73779db802cb00b6cbf407d0872b66","title":"Unsupervised models for morpheme segmentation and morphology learning"},{"paperId":"fb3670b1b33adc6d90b6cf2326b48e9869c68081","title":"A Comparison of Approaches to Word Class Tagging: Disjunctively vs. Conjunctively Written Bantu Languages"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"668087f0ae7ce1de6e0bd0965dbb480c08103260","title":"Finding Structure in Time"}],"id":"379722c04fb3b54215f82512eb86398cb02d42dd","summary":"A subword segmental language model (SSLM) that learns how to segment words while being trained for autoregressive language modelling, enabling the model to discover morpheme-like subwords that improve its LM capabilities."},{"url":"https://www.semanticscholar.org/paper/70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Jimin Sun,Patrick Fernandes,Xinyi Wang,Graham Neubig","citations":[],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"58e13e1fed9b28e50efcaff611772801d7980c80","title":"Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"4ceff7472c04ee6d76bce89d61ba4b445d8dbf74","title":"InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training"},{"paperId":"8741a0768d643c1593f8fea75dfdd0e5e90e1147","title":"Vocabulary Adaptation for Domain Adaptation in Neural Machine Translation"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"e99e2bd4812b30e104db0feddb681f32acd88758","title":"Massively Multilingual Transfer for NER"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"}],"id":"70dd68c07b322b68836eded1fb4f78c0efcad685","summary":"Surprisingly, it is found that subword-based models might still be the most practical choice in many settings, achieving better performance for lower inference latency and memory usage."},{"url":"https://www.semanticscholar.org/paper/e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE","venue":"","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"N. Chirkova,Sergey Troshin","citations":[],"references":[{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"31bfcbb1cbd74e8f38d97911f987108bd9517c6b","title":"Efficient Inference for Multilingual Neural Machine Translation"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c2b98ea55333895f736b9267414b4c9b63b9d04b","title":"AVATAR: A Parallel Corpus for Java-Python Program Translation"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1c84438eff3d10cfb97fb64fc9f35d734209a465","title":"Character-based NMT with Transformer"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"735ce0447e459e13f89ae751b19323d76a2af786","title":"Program Synthesis and Semantic Parsing with Learned Code Idioms"},{"paperId":"f9160d669a65b1283636d2eae524144ef4f0f658","title":"A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation"},{"paperId":"6faf62ac9e69e66cb92c923b749fa071554d23ca","title":"Learning Programmatic Idioms for Scalable Semantic Parsing"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"Tensor2Tensor for neural machine translation. In Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)"},{"paperId":null,"title":"URL https://d4mucfpksywv"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"3a29aa4eff48624752c07059a44d3288a678c8ab","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"de7293a0137fefe92412d61d3db93e22c0988136","title":"Evaluating clone detection tools with BigCloneBench"},{"paperId":null,"title":"[ From : Machine translation : from real users to research : 6 th conference of the Association for Machine Translation in the Americas"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"Graphcode { bert } : Pre - training code representations with data flow Character - based nmt with transformer"},{"paperId":null,"title":"Unsupervised translation of programming languages Dobf : A deob - fuscation pre - training objective for programming languages"}],"id":"e6b73466bab5e52ce0db19dd06d9353c26557dae","summary":"This work proposes subtokenziation that reduces average length by 17–40% without downstream performance drop, and shows that a carefully chosen subtokenization may significantly improve quality by 0.5-2%, possibly with some length increase."},{"url":"https://www.semanticscholar.org/paper/4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification","venue":"Neural Processing Letters","year":2021,"referenceCount":52,"citationCount":5,"influentialCitationCount":1,"publicationDate":"14/06/2021","authors":"Zaid Alyafeai,Maged S. Al-shaibani,Mustafa Ghaleb,Irfan Ahmad","citations":[{"paperId":"8c8aa03f8940b95336d0d70a9c37e024eff9e7bf","title":"Review on Recent Arabic Information Retrieval Techniques"},{"paperId":"a6fad9b1b32ace586b735e1764d26f1d67f2f7c7","title":"Arabic Document Classification: Performance Investigation of Preprocessing and Representation Techniques"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"d1d3dde91e3e73ccfbeb176f3af565a4507be077","title":"AI-Based Misogyny Detection from Arabic Levantine Twitter Tweets"},{"paperId":"b2ed3ed8d8b5717644be0ae705c13e2c2121a058","title":"Cloud computing architecture for Tagging Arabic Text Using Hybrid Model"}],"references":[{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"2e4089ca4e1c4bda989e9e73fc92223e0ec96c99","title":"Classifying and diacritizing Arabic poems using deep recurrent neural networks"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"f3460673df92ec46de780f8e7813e1c35c3bb41f","title":"A comparative study of effective approaches for Arabic sentiment analysis"},{"paperId":"1e4cda8be54999ced1324777fa462a85e2c9746c","title":"ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic"},{"paperId":"45aabb7249efb8f6deed0e14aa635bb03a9e530f","title":"The Korean Morphologically Tight-Fitting Tokenizer for Noisy User-Generated Texts"},{"paperId":"22e7c2cac5f6d810f3777a0d966f79caabdd3910","title":"MetRec: A dataset for meter classification of arabic poetry"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"22b9b4ca72a7dff11a60f5f43d96b0555014ad76","title":"Meter classification of Arabic poems using deep bidirectional recurrent neural networks"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"4fa37d012ad0014552a6a5a03624b29f95558bf7","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"21079df28af037f93620a0bd63f04fe49cf5df7d","title":"On the Importance of Tokenization in Arabic Embedding Models"},{"paperId":null,"title":"Arabic optical characters recognition by neural network based arabic unicode"},{"paperId":null,"title":"Arabic optical characters recognition by neural network based arabic unicode"},{"paperId":"d6271bbe0b76e5e248767f09ae669177f554ba78","title":"Arabic sentiment analysis: studies, resources, and tools"},{"paperId":"91663fbb9257b1ee5d22d7b9e796e03f374a7a6a","title":"hULMonA: The Universal Language Model in Arabic"},{"paperId":"f89df2ef9cd3a4e5f7becaf8123c4d66fcbddd35","title":"The Impact of Preprocessing on Arabic-English Statistical and Neural Machine Translation"},{"paperId":"ad3ce37737fdbab5e4834a161d755249b0c2ee24","title":"A Survey of Opinion Mining in Arabic"},{"paperId":"60d00c7c09135d3643fd3efbd59d922bb60a5904","title":"Learning meters of Arabic and English poems with Recurrent Neural Networks: a step forward for language understanding and synthesis"},{"paperId":"a1f1dd2e73f6ce765a19787849b6e5b23735ab9c","title":"A comprehensive survey of arabic sentiment analysis"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"1e6d69f8cfd698b8139cde557252753b22c7d712","title":"BPE and CharCNNs for Translation of Morphology: A Cross-Lingual Comparison and Analysis"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"20e5dd2fe186cae77e9cca9be5dc66e18c596ffb","title":"Morphological Word Embeddings for Arabic Neural Machine Translation in Low-Resource Settings"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"4f94c40285c52128329d29b053976734349a0371","title":"DataSet for Arabic Classification"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"f94f20ee56489dff949d368e0a17d02dee7bad83","title":"Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging"},{"paperId":"2ebc9c842376e2f8cd56686e50d1daf19cbf080d","title":"Arabic Online Handwriting Recognition (AOHR)"},{"paperId":"39c3073181f2330804696a489f32d9c189febec1","title":"Arabic Tweets Sentimental Analysis Using Machine Learning"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"3924aa213ff891812c66a6909ab902684d3eb107","title":"AraVec: A set of Arabic Word Embedding Models for use in Arabic NLP"},{"paperId":null,"title":"Arabic online handwriting recognition (aohr) a survey"},{"paperId":"f3eeef4afb81223df96575adadf808fe7fe440b4","title":"1.5 billion words Arabic Corpus"},{"paperId":"8da1c2acdb8174f16566606d8d8b7bf3870d649a","title":"Orthographic Syllable as basic unit for SMT between Related Languages"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"a1aaa7c75464e7ebe41cfe5c5258241ca34c6414","title":"Farasa: A Fast and Furious Segmenter for Arabic"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"93a9694b6a4149e815c30a360347593b75860761","title":"Variable-Length Word Encodings for Neural Translation Models"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"f5886c433084d1baaa24152cd3e4c555c79bfe4f","title":"MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"a9c3a94b3e620e479ef52e2dc312202ec22af6bf","title":"LABR: A Large Scale Arabic Book Reviews Dataset"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"bc5097c5821d7fefd776d93e238247e76dba7554","title":"A comparative study between methods of Arabic baseline detection"},{"paperId":null,"title":"A comparative study between methods of arabic baseline detection Dataset for arabic classification"}],"id":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","summary":"This paper introduces three new tokenization algorithms for Arabic and compares them to three other baselines using unsupervised evaluations and shows that the performance of suchtokenization algorithms depends on the size of the dataset, type of the task, and the amount of morphology that exists in the dataset."},{"url":"https://www.semanticscholar.org/paper/bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation","venue":"ArXiv","year":2022,"referenceCount":13,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/05/2022","authors":"Lukas Edman,Antonio Toral,Gertjan van Noord","citations":[{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":null,"title":"Why don’t people use characterlevel machine translation? arXiv preprint arXiv:2110.08191"},{"paperId":null,"title":"Why don't people use characterlevel machine translation?"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","summary":"The GBST method from Charformer groups (aka downsamples) characters is used, thereby enabling character grouping in the decoder, and promising performance on English– Turkish translation indicate the potential of character-level models for morphologically rich languages."},{"url":"https://www.semanticscholar.org/paper/1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","title":"Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings","venue":"EACL","year":2021,"referenceCount":18,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"O-Yeon Kwon,Dohyun Kim,Soobee Lee,Junyoung Choi,SangKeun Lee","citations":[{"paperId":"f22ba8b56b89b2f19e84fbe3439bf78e8fc926b2","title":"KLASIFIKASI EMOSI PADA CUITAN DI TWITTER DENGAN PRINCIPAL COMPONENT ANALYSIS DAN SUPPORT VECTOR MACHINE"}],"references":[{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"8fa7d1f3f82526935ba122c20c8d0648506301b3","title":"Misspelling Oblivious Word Embeddings"},{"paperId":"9348186c18bbd35d77a9011474fdd76ef98c86c5","title":"Robust Word Vectors: Context-Informed Embeddings for Noisy Texts"},{"paperId":"6c145caf5da0e2f078c34d0b65b399d7124e2fd8","title":"Learning to Generate Word Representations using Subword Information"},{"paperId":"1dff26dc9be229daae61231a340f57efa8126e0d","title":"Subword-level Word Vector Representations for Korean"},{"paperId":"c0fbe0378150bb35e94bdd800bf02a4e57d9f2c2","title":"Morpheme-based Efficient Korean Word Embedding"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"},{"paperId":"6fe682de00baba4d50bb855ae243f55a4a2a4e14","title":"A Word Embedding and a Josa Vector for Korean Unsupervised Semantic Role Induction"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":null,"title":"Analyzing of hangul search query spelling error patterns and developing query spelling correction system based on user logs"},{"paperId":"b92a80c848d51bb009758c117ab4f05a99913106","title":"THE KOREAN LANGUAGE: Structure, use and context"},{"paperId":"b0130277677e5b915d5cd86b3afafd77fd08eb2e","title":"Estimation of probabilities from sparse data for the language model component of a speech recognizer"}],"id":"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","summary":"A robust Hangeul word embedding model against typos is proposed that utilizes a Convolutional Neural Network architecture with a channel attention mechanism that learns to infer the original word embeddings."},{"url":"https://www.semanticscholar.org/paper/b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels","venue":"ArXiv","year":2022,"referenceCount":136,"citationCount":5,"influentialCitationCount":1,"publicationDate":"14/07/2022","authors":"Phillip Rust,Jonas F. Lotz,Emanuele Bugliarello,Elizabeth Salesky,Miryam de Lhoneux,Desmond Elliott","citations":[{"paperId":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"fb41147776fd3ef8c3370ce2574efc15486c9a0f","title":"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding"},{"paperId":"527603f0d3c120bbe56753ce4cd7e5a41e0d5e6a","title":"Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems"},{"paperId":"134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?"}],"references":[{"paperId":"33ebe01c8705a19def511040686d198871145409","title":"Story Beyond the Eye: Glyph Positions Break PDF Text Redaction"},{"paperId":"18c92da1b7f7f8ea6877e5b3a0d5a6df14a09e00","title":"SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners"},{"paperId":"8f26262437bde0ff8fe5e14d5a6cb4cc05a495ef","title":"Multimodal Masked Autoencoders Learn Transferable Representations"},{"paperId":"0d273fe1504a6b4251a773796853c75c2514df03","title":"Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models"},{"paperId":"88b15679a7e60a99d5da1f1fde8aa332b368e1b8","title":"Masked Autoencoders As Spatiotemporal Learners"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"b05f5006a879fab3668365918a15dd35acd77204","title":"Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Models"},{"paperId":"ac6e85cc3f105567e20e019f5752ed38a0a34a25","title":"Breaking Character: Are Subwords Good Enough for MRLs After All?"},{"paperId":"17a4477765f27fe3ad037eff9e663ffc6799d7fe","title":"Improving Tokenisation by Alternative Treatment of Spaces"},{"paperId":"3682a8716817743c6341003b2a121fbd183d6bbc","title":"MAE-AST: Masked Autoencoding Audio Spectrogram Transformer"},{"paperId":"a41196670d894ac8304d06f9bfeb0dd84fa5cb5d","title":"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training"},{"paperId":"a8fffb507d1790851550af5e4ebdd06e5bae1cee","title":"Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"b28e23afe988ad83ab5bd9ce7f826c140c533be8","title":"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages"},{"paperId":"2d439ec2c301d058bd4a8b4743328e3d9939625e","title":"Should You Mask 15% in Masked Language Modeling?"},{"paperId":"2ec3fb471f059fe401c7e1d7d7a199ba4feae814","title":"JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension"},{"paperId":"44f75282915d468413e38ac0e1f59b3ee6860485","title":"Does Transliteration Help Multilingual Language Modeling?"},{"paperId":"d0336e5be72e97e0493b1ba77ef8ec3c349d496a","title":"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"5b540745f4b51f95bf90fb3420e51edb037fc51a","title":"The MultiBERTs: BERT Reproductions for Robustness Analysis"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"dab46dd3985d1de5cd6549319797ab3705b6a801","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"7a670e9c4cf6655cfbcacf169565d4d645c0d475","title":"Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":null,"title":"Story beyond the eye"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"7f631586a368f1762866b01ff9f43c265851d52e","title":"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"03aa38229a6c853d96430edfc8f1542598f2116e","title":"AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain"},{"paperId":"dbd9fab59832369040661bb050daef6de405230c","title":"Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training"},{"paperId":"445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages"},{"paperId":"59c0c6b62e33850cda08663d4c9ecabcf5d21596","title":"IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization"},{"paperId":"8dad3a12d2a652122458dd377f62025354b17a2a","title":"Subword Mapping and Anchoring across Languages"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"964f079b80e6f3ce3164dd883dd1836299f0dba3","title":"GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph"},{"paperId":"06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer"},{"paperId":"a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study"},{"paperId":"c89ff2a0416a6d0870e724953a61a798deee238f","title":"How to Adapt Your Pretrained Multilingual Model to 1600 Languages"},{"paperId":"a76c98c6814ce8de07707b81c18520af508b7184","title":"BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"13bcfb944779165983aaef22cec8a3bbd3e98e62","title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"3fefe4d9ec77cd020463e5828408c9ced650611d","title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality"},{"paperId":"29599829d26b4acaaf0ad88698ed4362d33b2ae7","title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"575ac3f36e9fddeb258e2f639e26a6a7ec35160a","title":"Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"3ba96d6f6f4a828b6b32812d7867d86897b88df1","title":"Morphologically-Guided Segmentation For Translation of Agglutinative Low-Resource Languages"},{"paperId":"b62a56f629f77acce9ea69456d67dd77de1c7dfb","title":"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"f5dfed82b0c8747e41a1206f52a6d0ea3dce4a5c","title":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"85dc7829455819283270eb643817bcf97133464d","title":"From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"0a0bb62e56929d2c55ad6b9f18386f4fd7ba520a","title":"Towards End-to-End In-Image Neural Machine Translation"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"335a05c16fcbc7a2c2bf5221299de608b08030f0","title":"From Hero to Zéroe: A Benchmark of Low-Level Adversarial Attacks"},{"paperId":"68f2d74f82ec544433f3936dbbf6b6f5255fee10","title":"An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"},{"paperId":"e4ad5c24985127e015be86a53d57e1bb43876b4c","title":"On Romanization for Model Transfer Between Scripts in Neural Machine Translation"},{"paperId":"df29486c04eafd004f2f0816e84c798783802cdf","title":"Transliteration for Cross-Lingual Morphological Inflection"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"14489ec7893e373a0dcc9555c52b99b2b3a429f6","title":"Are All Languages Created Equal in Multilingual BERT?"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"b805693c17961af2cc7f859c1a54320b26036f46","title":"Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"0e141942fa265142f41a2a26eb17b6005d3af29e","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"0495d9df8eb84dcdab4e5536179823cd26279949","title":"Big Transfer (BiT): General Visual Representation Learning"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"21079df28af037f93620a0bd63f04fe49cf5df7d","title":"On the Importance of Tokenization in Arabic Embedding Models"},{"paperId":null,"title":"BERT: Pre-training of deep"},{"paperId":"e4f19271b9491b0297a03318a96b8a5157873acd","title":"Writing Across the World's Languages: Deep Internationalization for Gboard, the Google Keyboard"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"6ffb1cc32ddde6ae01e2fc0286eafa116ade0ffb","title":"KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"c0aaee2337e5af680e5dca1bfc349a737dfec573","title":"Fixing the train-test resolution discrepancy"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"541263935bfde368af9a5c4537fd1578e75d36d7","title":"Squared English Word: A Method of Generating Glyph to Use Super Characters for Sentiment Analysis"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"e547f4c0728552c1c41df44b7c6469198cf44924","title":"MorphoBERT: a Persian NER System with BERT and Morphological Analysis"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Exploring BERT’s Vocabulary"},{"paperId":null,"title":"Exploring BERT's Vocabulary. Blog Post"},{"paperId":"4fcec7ea62825953ac8d483cfa5c748b4daa4e7e","title":"The Coptic Universal Dependency Treebank"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e74ad670a6cbd693e225b0150f3a33fc1931faf7","title":"Universal Dependencies Version 2 for Japanese"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"2c4574c7f42ac3e887670876a37ea6708a77966e","title":"AROMA: A Recursive Deep Learning Model for Opinion Mining in Arabic as a Low Resource Language"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2","title":"Deep Biaffine Attention for Neural Dependency Parsing"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"b022f2a277a4bf5f42382e86e4380b96340b9e86","title":"SGDR: Stochastic Gradient Descent with Warm Restarts"},{"paperId":"3a29aa4eff48624752c07059a44d3288a678c8ab","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"4361e64f2d12d63476fdc88faf72a0f70d9a2ffb","title":"Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":null,"title":"Ud_chinese-gsd. GitHub repository, 2016"},{"paperId":null,"title":"Ud_chinese-gsd. GitHub repository"},{"paperId":"f04df4e20a18358ea2f689b4c129781628ef7fc1","title":"A large annotated corpus for learning natural language inference"},{"paperId":"753e30826f1908a62a8d251fc6b1b598f86d2bb2","title":"Shared Tasks of the 2015 Workshop on Noisy User-generated Text: Twitter Lexical Normalization and Named Entity Recognition"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"ab140a276968908ca3827d95389b9a1e143227ae","title":"Morfessor 2.0: Toolkit for statistical morphological segmentation"},{"paperId":"82cf69e48ede65b9d1f419da786c0349342d449d","title":"A Gold Standard Dependency Corpus for English"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"77ef6c449d3b7f5f5c55a06564b79eb4438c85b9","title":"Prague Dependency Style Treebank for Tamil"},{"paperId":"2fa38d8b67c7f59ac2a2bf1b53173a973422d8e2","title":"Prague Arabic Dependency Treebank 1.0"},{"paperId":"a7d1c7979b2d8205e9382e9f4fc39eb999fcd955","title":"Building a Large Syntactically-Annotated Corpus of Vietnamese"},{"paperId":"6f59b8056fd7367f5b5088d9c566d6849ea8a663","title":"Hindi Syntax: Annotating Dependency, Lexical Predicate-Argument Structure, and Phrase Structure"},{"paperId":null,"title":"Hassanová. Prague arabic dependency treebank"},{"paperId":"bb7d7dfad94e6a94bf0926fd28e6a0dd7d13b278","title":"Advances on natural language processing"},{"paperId":"944e7729088c5f36cbe0b0801ca0fea432dc5326","title":"Pango, an open-source Unicode text layout engine"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"for an overview of the rendering pipeline"},{"paperId":null,"title":"Transformers: State-of-the-art natural Preprint. Work in progress. language processing"},{"paperId":null,"title":"European Language Resources Association (ELRA"}],"id":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","summary":"PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels, and is more robust to noisy text inputs than BERT, further confirming the benefits of modelling language with pixels."},{"url":"https://www.semanticscholar.org/paper/e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization","venue":"International Conference on Learning Representations","year":2021,"referenceCount":69,"citationCount":55,"influentialCitationCount":11,"publicationDate":"23/06/2021","authors":"Yi Tay,V. Tran,Sebastian Ruder,Jai Gupta,Hyung Won Chung,Dara Bahri,Zhen Qin,Simon Baumgartner,Cong Yu,Donald Metzler","citations":[{"paperId":"b1dad820853464b30c93b52366b32690ba4b99a6","title":"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View"},{"paperId":"52136f813243ac3de8e277906112a41590a376d4","title":"What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis"},{"paperId":"0351167c875b8931366e85eb5e517819d7db80cc","title":"What Makes for Good Tokenizers in Vision Transformer?"},{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"11ddb0953eae196dab339bfdc117221594cf945e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"00675f1591392622b0db2d9cd37a8a1f32e37aa8","title":"Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"071fb0b7af9b3d390c7aa64a26068e23a81c52d7","title":"Paraphrase Identification with Deep Learning: A Review of Datasets and Methods"},{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"},{"paperId":"111dadc41f7b0337235fb526bf0cd3a4ac23b98d","title":"Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"7abfcf54eeac3a5c298717a4f0fe8a5daceeaa42","title":"Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling"},{"paperId":"9f9196beee29c02b8c5837c6edbc69967189b735","title":"Efficient Transformers with Dynamic Token Pooling"},{"paperId":"9769992ca9ff1c3894d19f4bd2b6dac27ed1befd","title":"A review on abusive content automatic detection: approaches, challenges and opportunities"},{"paperId":"dd568e6838903ad7c381f13c1268c94c5db08b02","title":"Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"ecb788b406e4e605526c5127b20bdcc5f8e55d39","title":"Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"487f9e74fff5e29b8aa37ba551edab71cfcbd256","title":"L2-GEN: A Neural Phoneme Paraphrasing Approach to L2 Speech Synthesis for Mispronunciation Diagnosis"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation"},{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"c13b69672ebee01914b0be69d052161350f653e8","title":"CONSENT: Context Sensitive Transformer for Bold Words Classification"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"645b10b7802a035e034488e3640fc0bc415de34c","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction"},{"paperId":"a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia"},{"paperId":"7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"},{"paperId":"8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"2d4f66046bb436864cd6bf589e3a931c405f9f44","title":"Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color"},{"paperId":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE"},{"paperId":"e34a59a7391a28200ff9052c13bd4498a8eaa4af","title":"S CALE E FFICIENTLY : I NSIGHTS FROM P RE - TRAINING AND F INE - TUNING T RANSFORMERS"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"2365410a710b421b2295cdca0074946cb50bb1d4","title":"Are Pre-trained Convolutions Better than Pre-trained Transformers?"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"7e9ff94476f41041c75e253e84f487db00e9c861","title":"Long Range Arena: A Benchmark for Efficient Transformers"},{"paperId":"bc87279d4b32a425377ff18ab63f7ecf95ff228c","title":"Rethinking embedding coupling in pre-trained language models"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"26b277f2d7cd86b67bc3572257557edc4640b8e9","title":"Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87","title":"Linformer: Self-Attention with Linear Complexity"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"64adba957aadb96dfa202bd6d1fa179c6134c5df","title":"Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","title":"GLU Variants Improve Transformer"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"b611a8095630557229dc5fb6b07c272f1cd614da","title":"Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"2270b8628fd8ca67ae39d277f45bc3c38ac63d5f","title":"Mesh-TensorFlow: Deep Learning for Supercomputers"},{"paperId":"f0149c69dffbbbfa94e9869360291bb1fb6b245e","title":"Learning to Segment Inputs for NMT Favors Character-Level Processing"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"1db9bd18681b96473f3c82b21edc9240b44dc329","title":"Image Transformer"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":null,"title":"FRAGE: FrequencyAgnostic Word Representation"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"deff3b0635e141297238797d924d7a9aba3a132a","title":"Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU"},{"paperId":"6db294e9309349d82df47a912ccc47eab1dc1358","title":"Sequence Modeling via Segmentations"},{"paperId":"6f35b070c250507dbec8a7365cf01b25eb66d792","title":"Ex Machina: Personal Attacks Seen at Scale"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"649d03490ef72c5274e3bccd03d7a299d2f8da91","title":"Learning Word Vectors for Sentiment Analysis"},{"paperId":"3bf2e6941dbb87ac0d2c771c159e1e27366a26e3","title":"Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics"},{"paperId":"0c09f282d78f42f77d73a52b08cb425e1cabb9b2","title":"Are Vowels and Consonants Processed Differently? Event-related Potential Evidence with a Delayed Letter Paradigm"},{"paperId":"475354f10798f110d34792b6d88f31d6d5cb099e","title":"Automatically Constructing a Corpus of Sentential Paraphrases"},{"paperId":"084c55d6432265785e3ff86a2e900a49d501c00a","title":"Foundations of statistical natural language processing"},{"paperId":"98b405fd0153f24e8359448d79acc49c3ffc8f4c","title":"The relative contribution of consonants and vowels to word identification during reading"},{"paperId":null,"title":"Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee"}],"id":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","summary":"This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion and paves the way for highly performant token-free models that are trained completely end-to-end."},{"url":"https://www.semanticscholar.org/paper/9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information","venue":"ArXiv","year":2021,"referenceCount":32,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/08/2021","authors":"Yuval Pinter,A. Stent,Mark Dredze,Jacob Eisenstein","citations":[{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"}],"references":[{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"188cd686fb2200f237f688dbda7f64ffc75e67ac","title":"Subword Pooling Makes a Difference"},{"paperId":"e7a2ffd26cd76e5b662ecb8624ecb3e177dbb8da","title":"Pitfalls of Static Language Modelling"},{"paperId":null,"title":"Recent Advances in Language Model Fine-tuning"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"641250e235d81e5b5c0023c32e71731aa0b0027c","title":"Multi-resolution Annotations for Emoji Prediction"},{"paperId":"d7a793e7ce1e42e1feaadb026633e481131a8692","title":"Char2Subword: Extending the Subword Embedding Space from Pre-trained Models Using Robust Character Compositionality"},{"paperId":"9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2","title":"TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"00476bffaaea3919b432e20681aca5f97e6a1638","title":"Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?"},{"paperId":"b08545e1281c1eb748e4474687eb61fd3b25d1a6","title":"NYTWIT: A Dataset of Novel Words in the New York Times"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a022bda79947d1f656a1164003c1b3ae9a843df9","title":"How to Fine-Tune BERT for Text Classification?"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3e9674a344db5e4e7aa02222d659e58e4307b084","title":"SemEval 2018 Task 2: Multilingual Emoji Prediction"},{"paperId":"a235999a41cfcc5868ddacb94b47cda933f00a8c","title":"Peperomia at SemEval-2018 Task 2: Vector Similarity Based Approach for Emoji Prediction"},{"paperId":"8245fc60669776991dcb8ecd53a107f37bec29d2","title":"Hatching Chick at SemEval-2018 Task 2: Multilingual Emoji Prediction"},{"paperId":"321f91528af535cefa1b6971df31c609673f463f","title":"Backpropagating through Structured Argmax using a SPIGOT"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"72e87913632d0f1295fbcfa46795c5bb26d0a422","title":"Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"28ac5c728449c19be229e8839f5b5d6acd896f15","title":"Results of the WNUT16 Named Entity Recognition Shared Task"},{"paperId":"a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee","title":"MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"d895647b4a80861703851ef55930a2627fe19492","title":"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"}],"id":"9c2e4e5ee224c20a45c37244924138b50f3fe603","summary":"It is shown that incorporating XRAYEMB’s learned vectors into sequences of pre- trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on nonstandard English text."},{"url":"https://www.semanticscholar.org/paper/937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":16,"citationCount":7,"influentialCitationCount":1,"publicationDate":"25/08/2021","authors":"I. Itzhak,Omer Levy","citations":[{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"b162638cd42b65e6add199b0b34f1b375070fc7c","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"8b723be33e62bf5bd9278769244f1c13a9510898","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP"}],"references":[{"paperId":"7694aae9766d5f1fe74d900cd82aee898cb6e8e9","title":"How to Train BERT with an Academic Budget"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"e9d59dd33698adcdafabeb0f9ea873b2ed36f20b","title":"Farasa: A New Fast and Accurate Arabic Word Segmenter"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"01a660ec8aa995a88a00bfb41cb86c022047a9db","title":"NLTK: The Natural Language Toolkit"},{"paperId":"b2f8876482c97e804bb50a5e2433881ae31d0cdd","title":"Binary codes capable of correcting deletions, insertions, and reversals"}],"id":"937b177d2ed7cee27ee45300c690f2f60c81bae5","summary":"The results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell doesn’t appear to enhance its performance on such tasks."},{"url":"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":75,"citationCount":114,"influentialCitationCount":28,"publicationDate":"28/05/2021","authors":"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel","citations":[{"paperId":"b1dad820853464b30c93b52366b32690ba4b99a6","title":"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View"},{"paperId":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models"},{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"11ddb0953eae196dab339bfdc117221594cf945e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only"},{"paperId":"d4c23dc85ea1eba2f863ff6ffbbb5e473e491ff4","title":"TRIP: Triangular Document-level Pre-training for Multilingual Language Models"},{"paperId":"4aa09cba27a489ca02471fad011ea4854fc63cc1","title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"be2df0fafa89b6355d1ff1336c10e0d4c8d27276","title":"The Massively Multilingual Natural Language Understanding 2022 (MMNLU-22) Workshop and Competition"},{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"},{"paperId":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"9f9196beee29c02b8c5837c6edbc69967189b735","title":"Efficient Transformers with Dynamic Token Pooling"},{"paperId":"d50b9db750ded246bde13e2c263e341bcbd8a335","title":"A Benchmark and Dataset for Post-OCR text correction in Sanskrit"},{"paperId":"9dab4c20648cd4c7c6830e6274a95294b014aac9","title":"QAmeleon: Multilingual QA with Only 5 Examples"},{"paperId":"9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages"},{"paperId":"92302ab168429c7c3a8f699b35ba8302916c6e7c","title":"Bridging Speech and Textual Pre-trained Models with Unsupervised ASR"},{"paperId":"5e786d47e3dc5ffa65ba8548ff67139a2764acf8","title":"T5lephone: Bridging Speech and Text Self-supervised Models for Spoken Language Understanding via Phoneme level T5"},{"paperId":"d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6","title":"SLING: Sino Linguistic Evaluation of Large Language Models"},{"paperId":"6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc","title":"Graphemic Normalization of the Perso-Arabic Script"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72","title":"Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning"},{"paperId":"265ebfc1074c73917233dbecad802c0c64921a1c","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"595cc15b9db4e985b30a2e175399a38c021d4ce7","title":"Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino"},{"paperId":"2a8aac78df5e1e9658a02d381662ed26c6577713","title":"Disease diagnostic method based on cascade backbone network for apple leaf disease classification"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"9fc5878d49c41beb12b62032b0afe9a8501fe9db","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?"},{"paperId":"e4437293a703fcf282bf1b38bf7900ed8ca711bb","title":"Training a T5 Using Lab-sized Resources"},{"paperId":"a806041621acb5cf648fc780f1ff14939aa3a721","title":"How Much Does Lookahead Matter for Disambiguation? Partial Arabic Diacritization Case Study"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"d6df842f67fedd63acdf43df8fe449c8c4c66def","title":"Sequence to sequence pretraining for a less-resourced Slovenian language"},{"paperId":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","title":"Confident Adaptive Language Modeling"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"b308511ac52e1c2da915d1e55d5246dfdb4cbe28","title":"Romanian Question Answering Using Transformer Based Neural Networks"},{"paperId":"a766ef0678aade6a9798552618819cf4d0fac406","title":"TEVR: Improving Speech Recognition by Token Entropy Variance Reduction"},{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"32afdb07021fda775ceaedd231c58bfed0aa980a","title":"Automated Crossword Solving"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"645b10b7802a035e034488e3640fc0bc415de34c","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"063d9fa4861356500219b7e81d5a654aa921da6f","title":"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation"},{"paperId":"7267812178393b8ae0b99648f02661ca1ff2b412","title":"Bilingual End-to-End ASR with Byte-Level Subwords"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"},{"paperId":"0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"880c8973ea1376c6bff23226b3f64792657aa666","title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"1ed66e048bb025e75aa5ea660545285212e5341f","title":"Scaling Up Models and Data with t5x and seqio"},{"paperId":"a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia"},{"paperId":"8a4eec70e3d0c43abf26757252ad6210c9717f6c","title":"Towards Lithuanian grammatical error correction"},{"paperId":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation"},{"paperId":"7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"},{"paperId":"8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"},{"paperId":"f357b35e068bbfbb8468b48198ab63241713629d","title":"Correcting diacritics and typos with ByT5 transformer model"},{"paperId":"7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models"},{"paperId":"e53f455b3300d95738dac117419c88fbde58ac4e","title":"Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e35357ac461a669fe7e4b877ee1fad0dfda26303","title":"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings"},{"paperId":"a70fc86508bd0133d5d984a4e777abef1934d76c","title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview"},{"paperId":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"CHARFORMER: FAST CHARACTER TRANSFORMERS"},{"paperId":"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","title":"On the Influence of Tokenizers in NLP Pipelines"},{"paperId":"9758782feed4b4b9bf0ec18b802462e8023a7f83","title":"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"0ffb0b109acddf0067aec252221e0ea04d317ddc","title":"A Framework for Sexism Detection on Social Media via ByT5 and TabNet"},{"paperId":"6102fe88a512290b80e83ed2fe17606b166e505a","title":"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin"},{"paperId":"143659fcf93f33e9139f594cf8111c6bc85c04fa","title":"Overview of the EvaLatin 2022 Evaluation Campaign"},{"paperId":"2005311276a1a90dc17cf6e2ca7725fee2e76e1e","title":"BasqueGLUE: A Natural Language Understanding Benchmark for Basque"},{"paperId":"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","title":"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference"},{"paperId":"32290d428b50f64d1cfee6ea658dc6ba977b26e9","title":"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia"},{"paperId":"2b6b38b66fcca218cb2f381e5d4711b5c99a784f","title":"Training Text-to-Text Transformers with Privacy Guarantees"},{"paperId":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"5f77157038dc7f189db7e72ea58567039222d9df","title":"Examining Single Sentence Label Leakage in Natural Language Inference Datasets"},{"paperId":"db3690379953f2ea4f2a015615de02e643d437ea","title":"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers"},{"paperId":"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","title":"Large Dual Encoders Are Generalizable Retrievers"},{"paperId":"6e2682eb2fbec93b329028b23764c1164e232c41","title":"ÚFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5"},{"paperId":"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","title":"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome"},{"paperId":"de0e1f9980afa7949df64d53b8ae7a2f59c55579","title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages"},{"paperId":"c107835a05ca6fda6e73b64e2ed9884de4fcec0f","title":"A proposed conceptual framework for a representational approach to information retrieval"},{"paperId":"4318c9f87221b9f504f286540c9a6d5c1cc4e4c8","title":"Rethnicity: Predicting Ethnicity from Names"},{"paperId":"9d13bde760d8e77f059436d60160881becd2d2e0","title":"Predicting Ethnicity from Names with rethnicity: Methodology and Application"},{"paperId":"39262814fa3e47905a2e5facf13465a1f70706b9","title":"Single-Read Reconstruction for DNA Data Storage Using Transformers"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"972808b92159a782f5ca1c1ab3a8ed3867acfb2d","title":"mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset"},{"paperId":"89a4f4d1f8c93da85d829a1acfe8cafea2b50c00","title":"Transfer Learning for Multi-lingual Tasks - a Survey"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"c41819413725668fbf8e3ca1a0bcaf7fb691e582","title":"How Optimal is Greedy Decoding for Extractive Question Answering?"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning"},{"paperId":"06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"7175caf7568d46c857380d0e5b64653819d5cc45","title":"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization"},{"paperId":"e873ddaf58ff92ae0492983cf57221fb25837f4e","title":"Strategies in subword tokenization: humans vs. algorithms"},{"paperId":"ae7fc32df9401a86353185515f4fd5e2020ac922","title":"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"}],"references":[{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"2b66a0d8a00b0ba7b585e11ab34879420ad351cb","title":"Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution"},{"paperId":"824cd8db8a68732db04f4d8b7139eb4475e59ff2","title":"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"3fd0f34117cf9395130e08c3f02ac2dadcca7206","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":null,"title":"The GEM benchmark: Natural language"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"e46c97b2d3bea4c21eb462d5771647a0de99b81c","title":"Ensemble Self-Training for Low-Resource Languages: Grapheme-to-Phoneme Conversion and Morphological Inflection"},{"paperId":"8c122ff82dedf72fa7ef5342622667bf5848916e","title":"One-Size-Fits-All Multilingual Models"},{"paperId":"278f4f68309fc4ffb83bfc4ba86a0ea005106682","title":"The SIGMORPHON 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion"},{"paperId":"b044395ed89de33b43d304c008d3c5a7727d423d","title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection"},{"paperId":"c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c","title":"Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":null,"title":"Language ID in the wild"},{"paperId":null,"title":"Neural machine translation with bytelevel subwords"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"5b78c2476984452c372e7fce6ac8246d15c97efa","title":"TWEETQA: A Social Media Focused Question Answering Dataset"},{"paperId":"be564b861e5a9bf5d1dec531807e711000027380","title":"One Size Does Not Fit All: Comparing NMT Representations of Different Granularities"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"5e93a30656ef091f55ce42374d498f2b881e0c47","title":"Bytes Are All You Need: End-to-end Multilingual Speech Recognition and Synthesis with Bytes"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":null,"title":"Characterlevel language modeling with deeper selfattention"},{"paperId":null,"title":"Characterlevel language modeling with deeper selfattention"},{"paperId":"1125d986433735ffc63dc0a8be466043e4f5c5b4","title":"Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"305b2cf37e5dece81e95c92883d5a6e28ac93b22","title":"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"f0b6c1ffed9984317050d0c1dfb005cb65582f13","title":"On the State of the Art of Evaluation in Neural Language Models"},{"paperId":"89cb259f39eb8350dd337fc1d02e2f4128c8398c","title":"Byte-based Neural Machine Translation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"563783de03452683a9206e85fe6d661714436686","title":"HyperNetworks"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"7dba53e72c182e25e98e8f73a99d75ff69dda0c2","title":"Recurrent Highway Networks"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"4dabd6182ce2681c758f654561d351739e8df7bf","title":"Multilingual Language Processing From Bytes"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":null,"title":"Neural machine"},{"paperId":null,"title":"Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":null,"title":"GLUE : A multitask benchmark and analysis platform for natural language understanding"},{"paperId":null,"title":"Bowman . 2019 b . GLUE : A multitask benchmark and analysis platform for natural language understanding"},{"paperId":null,"title":"Le . 2017 . Hypernetworks"},{"paperId":null,"title":"Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-totext transformer"}],"id":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","summary":"This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts."},{"url":"https://www.semanticscholar.org/paper/3b34e79610e5acaba352def4323a59f6d531fac7","title":"Macro-Average: Rare Types Are Important Too","venue":"NAACL","year":2021,"referenceCount":42,"citationCount":4,"influentialCitationCount":1,"publicationDate":"12/04/2021","authors":"Thamme Gowda,Weiqiu You,Constantine Lignos,Jonathan May","citations":[{"paperId":"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","title":"Checks and Strategies for Enabling Code-Switched Machine Translation"},{"paperId":"cb48e3bcc185ae94899007e1ad3cdb49ff39428b","title":"Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video"},{"paperId":"0a82ffe5c889d4defbd6300fffab4f3fa3dade99","title":"Deep convolution neural network with weighted loss to detect rice seeds vigor based on hyperspectral imaging under the sample-imbalanced condition"},{"paperId":"04cd4c224f61e8f25a405103d4210f161d091d1c","title":"Look It Up: Bilingual Dictionaries Improve Neural Machine Translation"}],"references":[{"paperId":"868207797e69df5055f5c3fd4aa78a33e5a7ca45","title":"Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics"},{"paperId":"de02a7e9c5e0eeb2cab4d7dd810e5e573327a2f5","title":"Corpora for Cross-Language Information Retrieval in Six Less-Resourced Languages"},{"paperId":"4ae52766028e69186052ea8f33a137fbbbdb986a","title":"BLEURT: Learning Robust Metrics for Text Generation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":null,"title":"XLM-UNMT-Models"},{"paperId":null,"title":"Multilingual denoising"},{"paperId":"dce91eb862d19a646b8f5171ec66e61a987f3b3c","title":"Putting Evaluation in Context: Contextual Embeddings Improve Machine Translation Evaluation"},{"paperId":"145b8b5d99a2beba6029418ca043585b90138d12","title":"MASS: Masked Sequence to Sequence Pre-training for Language Generation"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"9237d6efc603465765e80eb5ca1268c2bd7b5c24","title":"compare-mt: A Tool for Holistic Comparison of Language Generation Systems"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"c590d2c8c2fb6ce5d32ee9165ab24171165f2b70","title":"Assessing gender bias in machine translation: a case study with Google Translate"},{"paperId":"03853267d98af5ec5fdc3dc5cb85cc0681e435e4","title":"Human vs Automatic Metrics: on the Importance of Correlation Design"},{"paperId":"c2a7afbb5609a723f8eea91bfde4b02579b048d6","title":"Unsupervised Neural Machine Translation"},{"paperId":null,"title":"2020), which is fine-tuned on WMT Metrics ratings data from 2015-2018. The BLEURT model is retrieved from https://storage.googleapis.com/ bleurt-oss/bleurt-base-128.zip"},{"paperId":"dc4b3112000e151583da2532b63e0b1e59cbff8a","title":"Results of the WMT17 Metrics Shared Task"},{"paperId":"19a632b17b2ee5f64df41bdd23755316a02fb939","title":"Creating Training Corpora for NLG Micro-Planners"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"1a327709cc53ff9e52454e50a643abf4a0ac92af","title":"Findings of the 2016 Conference on Machine Translation"},{"paperId":"6c35070b6824ae5a2af4c667860e87f2c7ef6a9a","title":"Complementarity, F-score, and NLP Evaluation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"41ae4078cc698edd8dbb510dbfbb0f14b998d132","title":"BEER: BEtter Evaluation as Ranking"},{"paperId":null,"title":"Findings of the 2014 workshop"},{"paperId":"6836eadca4e506b580fc3f7c3374bff363fe0664","title":"The TAO of ATWV: Probing the mysteries of keyword search performance"},{"paperId":"0f8992ee6418d367d8e50ecbb59b08ea15e8431f","title":"Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems"},{"paperId":"b82756edb00e92f7314f1a3e036bf292664ad3a5","title":"A similarity measure for indefinite rankings"},{"paperId":"7bb21d4d4505db402d70c383962d19c42d9a7cbe","title":"Influence functions of the Spearman and Kendall correlation measures"},{"paperId":"20c11546a035d2fa2fa1121a7b31e890d20d6b6b","title":"(Meta-) Evaluation of Machine Translation"},{"paperId":"51951073580f6995e55be873db9a7f6a9736ca86","title":"A Study of Translation Edit Rate with Targeted Human Annotation"},{"paperId":"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7","title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":null,"title":"Europarl: A parallel corpus"},{"paperId":"4f09e6ec1b7d4390d23881852fd7240994abeb58","title":"A statistical interpretation of term specificity and its application in retrieval"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f","title":"Automatic evaluation of machine translation quality using n-gram co-occurrence statistics"},{"paperId":null,"title":"Les irréductibles du M23, soit quelques centaines de combattants, étaient retranchés à près de 2000 mètres d’altitude"},{"paperId":"1f7e50d220f41f4fac985a991c8d5187323aab4c","title":"Applications and Explanations of Zipf’s Law"},{"paperId":"3dcda1bec36586b46b1dc67a477beca2c5a105be","title":"MUC-4 evaluation metrics"},{"paperId":null,"title":"Information Retrieval, 2nd edition"},{"paperId":"2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac","title":"Human behavior and the principle of least effort"},{"paperId":null,"title":"2-3 octombrie), Iasi (Palas, 6-7 octombrie), Brasov (Casa Armatei, 14-15 octombrie), Cluj Global (Sala Polivalenta, 20-21 octombrie), Cluj IT (Sala Polivalenta, 22-23 octombrie)"}],"id":"3b34e79610e5acaba352def4323a59f6d531fac7","summary":"It is found that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance, and can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods’ outputs."},{"url":"https://www.semanticscholar.org/paper/59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization","venue":"Findings","year":2020,"referenceCount":46,"citationCount":25,"influentialCitationCount":7,"publicationDate":"27/08/2020","authors":"Xinsong Zhang,Hang Li","citations":[{"paperId":"1d08b41de815c79d3fb874ddb159fbe21828c024","title":"VSCA: A Sentence Matching Model Incorporating Visual Perception"},{"paperId":"beb40cc99a6ab931aa8ea1758c1a0397ecd32847","title":"Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling"},{"paperId":"31d6e59dc275f33da71a72b6b86b38daeed9ffaa","title":"mmLayout: Multi-grained MultiModal Transformer for Document Understanding"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"f8a2428bee464ae69cdb25ce5c2259e88ce576b4","title":"Clickbait Detection of Indonesian News Headlines using Fine-Tune Bidirectional Encoder Representations from Transformers (BERT)"},{"paperId":"05ea28584e5db18c0c31d1aac40e9c1905327557","title":"Effectiveness of Fine-tuned BERT Model in Classification of Helpful and Unhelpful Online Customer Reviews"},{"paperId":"2ffacbeeebd3d9e7467610057b4308635a165b6b","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"8e1227b0d58b769a0919f8debdfd093ff6f6a65a","title":"MarkBERT: Marking Word Boundaries Improves Chinese BERT"},{"paperId":"18dc24da603896db2264e9174116407a95c593d5","title":"“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"35eeeefcbea6fa8edae4c310ee5ee717861c7e60","title":"Improving Constituent Representation with Hypertree Neural Networks"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"976f47bade21fd787f029142b39631cb17f16ec2","title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation"},{"paperId":"5722d101859846a6a023b8fa00830742203cd0c1","title":"Span Fine-tuning for Pre-trained Language Models"},{"paperId":"27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","title":"LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization"},{"paperId":"4690eb050572a279f94560b6bbdccaae577b45f5","title":"MVP-BERT: Multi-Vocab Pre-training for Chinese BERT"},{"paperId":"7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"964da4ed9ac61b12bc2a7adc1e94e3964cc63861","title":"Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question Answering Data"},{"paperId":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"},{"paperId":"518cb6d4247bdebf21e2811f296b0c7372602a0a","title":"PMI-Masking: Principled masking of correlated spans"},{"paperId":"923d2376103dbc9e9b2af4518b56299d7630b46b","title":"J URASSIC -1: T ECHNICAL D ETAILS AND E VALUATION"}],"references":[{"paperId":"7c5c149699a0ba54b52cd5b9e291077f4a1f9d13","title":"Synthesizer: Rethinking Self-Attention in Transformer Models"},{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training With Whole Word Masking for Chinese BERT"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"913e9384a1440dbf860f6d8597953187237fa1b4","title":"Disagreement-Regularized Imitation Learning"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"6007bd2a34385132a7885b934d90b519a1f65bba","title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"d56c1fc337fb07ec004dc846f80582c327af717c","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"1b07a24b81834116f6ad1d0232485ba81b9445f3","title":"Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"},{"paperId":null,"title":"2e-5 1.0 C DATA AUGMENTATION To enhance the performance, we conduct data augmentation for the three Chinese classification tasks of TNEWS, CSL, and CLUEWSC2020"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"335613303ebc5eac98de757ed02a56377d99e03a","title":"What Does BERT Learn about the Structure of Language?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"0de0a44b859a3719d11834479112314b4caba669","title":"A Multiscale Visualization of Attention in the Transformer Model"},{"paperId":"e278e072774f23675266881750e20bca74804cb9","title":"ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"ae43556f2cc1cecb8b48762b4e09df319fbaa4d9","title":"Is Word Segmentation Necessary for Deep Learning of Chinese Representations?"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"c34b7796a666f5e205693f6bd1e25e993db19075","title":"Probing Prior Knowledge Needed in Challenging Chinese Machine Reading Comprehension"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","title":"Neural Network Acceptability Judgments"},{"paperId":"4ae2960d3c6ac489b3b072666fb0b91d0480a170","title":"A Span-Extraction Dataset for Chinese Machine Reading Comprehension"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Openwebtext corpus"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"2019) is a large-scale Chinese IDiom cloze test. C3 (Sun et al., 2019a) is a free-form multiple-choice machine reading comprehension for Chinese"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"687bac2d3320083eb4530bf18bb8f8f721477600","title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"},{"paperId":"883d1d06d857a85a0e64bb19f0b17d56f2cc9d7b","title":"KenLM: Faster and Smaller Language Model Queries"},{"paperId":"0f8468de03ee9f12d693237bec87916311bf1c24","title":"The Seventh PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"db8885a0037fe47d973ade79d696586453710233","title":"The Sixth PASCAL Recognizing Textual Entailment Challenge"},{"paperId":"475354f10798f110d34792b6d88f31d6d5cb099e","title":"Automatically Constructing a Corpus of Sentential Paraphrases"},{"paperId":null,"title":"We adopt the standard hyper-parameters of BERT in pre-training of the models. Table 8 shows the hyper-parameters in our"}],"id":"59c0076b3d814588e320820b95563965733d1875","summary":"This paper proposes a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine- grained and coarse-grains tokenizations, which outperforms the existing best performing models in almost all cases."},{"url":"https://www.semanticscholar.org/paper/969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":82,"citationCount":68,"influentialCitationCount":14,"publicationDate":"11/03/2021","authors":"J. Clark,Dan Garrette,Iulia Turc,J. Wieting","citations":[{"paperId":"0351167c875b8931366e85eb5e517819d7db80cc","title":"What Makes for Good Tokenizers in Vision Transformer?"},{"paperId":"353d6a18a222aec0be66de0b8be111fbbe67012d","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"11ddb0953eae196dab339bfdc117221594cf945e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy"},{"paperId":"9f9196beee29c02b8c5837c6edbc69967189b735","title":"Efficient Transformers with Dynamic Token Pooling"},{"paperId":"9769992ca9ff1c3894d19f4bd2b6dac27ed1befd","title":"A review on abusive content automatic detection: approaches, challenges and opportunities"},{"paperId":"fc78d652c4d395ba2737ab0406bc53fd025d4aad","title":"Detecting Languages Unintelligible to Multilingual Models through Local Structure Probes"},{"paperId":"9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages"},{"paperId":"4061a9941fa0ff106e884272d9ed753650417ec4","title":"Collateral facilitation in humans and language models"},{"paperId":"dd568e6838903ad7c381f13c1268c94c5db08b02","title":"Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing"},{"paperId":"10672baf790962195677c7581a2fe984032e7f98","title":"Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages"},{"paperId":"265ebfc1074c73917233dbecad802c0c64921a1c","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward"},{"paperId":"695ea88d22e7d39fa3d0b9f53e75d41e82be81d8","title":"Transformers with Learnable Activation Functions"},{"paperId":"ba9a592438447c2a35afc203be40f7f0a2d3fb5a","title":"A Compact Pretraining Approach for Neural Language Models"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"},{"paperId":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model"},{"paperId":"0376c01a9320027694f2dca57236c27f0c5b36eb","title":"Multilingual Abusiveness Identification on Code-Mixed Social Media Text"},{"paperId":"7ed63a4f928fc126f6780d27e75fa6447a00a8c4","title":"Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference"},{"paperId":"12809bcb734beafeb47876f42e7b438e27fe99fe","title":"General-purpose, long-context autoregressive modeling with Perceiver AR"},{"paperId":"7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models"},{"paperId":"2ec281d654f622da7267d7da8145e96bb77a9ede","title":"An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"CHARFORMER: FAST CHARACTER TRANSFORMERS"},{"paperId":"610088e1d7a8e44f921b2c4894fe1a7b204ce5a9","title":"Vicomtech at LivingNER2022"},{"paperId":"e11d8663ccf2cc412f408853fa5f19ebac75df54","title":"How to encode arbitrarily complex morphology in word embeddings, no corpus needed"},{"paperId":"b1be10b76314ea8569249f2310f1e6eead8a5adc","title":"Building Domain-specific Corpora from the Web: the Case of European Digital Service Infrastructures"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"12fec03c538c7aaeca1a4e1ab8d66aed5f793fc0","title":"reamtchka at SemEval-2022 Task 6: Investigating the effect of different loss functions for Sarcasm detection for unbalanced datasets"},{"paperId":"e6b73466bab5e52ce0db19dd06d9353c26557dae","title":"C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"e43a360728e34e43934707665db0f7be02c8e5a7","title":"Interpreting BERT-based stance classification: a case study about the Brazilian COVID vaccination"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5","title":"Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study"},{"paperId":"f332a615c33c69f54dfcb9a8b14f96e8b5725def","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"dca4128a33ca22c02031b5c0c28548a0df022d80","title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"b62a56f629f77acce9ea69456d67dd77de1c7dfb","title":"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization"},{"paperId":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"},{"paperId":"58403c6bc061670f7ce6267a3a0cd01ba1bb8e50","title":"Intérêt des modèles de caractères pour la détection d’événements (The interest of character-level models for event detection)"}],"references":[{"paperId":"b57da3ccf214e8dad49116c8db9590c2c89629f5","title":"MasakhaNER: Named Entity Recognition for African Languages"},{"paperId":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization"},{"paperId":"9ed25f101f19ea735ca300848948ed64064b97ca","title":"Random Feature Attention"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"466f7cfe7442738ee974b12939b4c2e32ee098bf","title":"Rethinking Attention with Performers"},{"paperId":"59c0076b3d814588e320820b95563965733d1875","title":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"},{"paperId":"88340786475bf1649b83b7ac0ad7f57e60a20b52","title":"Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT"},{"paperId":"4d3f6673009589971973a81a097441e7c78a265e","title":"Improving Multilingual Models with Language-Clustered Vocabularies"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"2a218786f4615b82389f78472e7ff22e6ce57490","title":"ConvBERT: Improving BERT with Span-based Dynamic Convolution"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"4ca3b0ea12f02e2dea01a4aa505956bae5500a09","title":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b1a71677a13299755a12375f0c982484088aa9ef","title":"English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too"},{"paperId":"26299d5fdc5137291dc6a091573b3d18aba1d1c2","title":"MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"},{"paperId":"56676aef356ebb13cba77fc9e4d70760fbc151f5","title":"ETC: Encoding Long and Structured Inputs in Transformers"},{"paperId":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b","title":"RobBERT: a Dutch RoBERTa-based Language Model"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"d023e4c652dc21b2068a8527203a70d9eaf195d9","title":"BPE-Dropout: Simple and Effective Subword Regularization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"bc789aef715498e79a74f857fa090ece9e383bf1","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"},{"paperId":"d619570b03e629653d69b5157764be183e8521bf","title":"UnihanLM: Coarse-to-Fine Chinese-Japanese Language Model Pretraining with the Unihan Database"},{"paperId":"8912eaad7d9c779bd698c73409e410fdf8c5e3c2","title":"PRADO: Projection Attention Networks for Document Classification On-Device"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"8a7e9f91d949764d6159c114d4feb961ec07b32d","title":"Training Hybrid Language Models by Marginalizing over Segmentations"},{"paperId":"809cc93921e4698bde891475254ad6dfba33d03b","title":"How Multilingual is Multilingual BERT?"},{"paperId":"1d51b59fcf0297e8df931c8b614bd55165b24172","title":"Better Character Language Modeling through Morphology"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"e07f2c383a34217a9403a4d58a4e7d61e57d9163","title":"Character Eyes: Seeing Language through Character-Level Taggers"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"a4501dc10d78d4d81e545bf93fbaacdbf45e8d4f","title":"Learning to Discover, Ground and Use Words with Segmental Neural Language Models"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"9852ae077c7da6a9d178acaa2b44a335289507a6","title":"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"bae055edb1d552c24bbea56556bafdde9ef61c50","title":"On the Strength of Character Language Models for Multilingual Named Entity Recognition"},{"paperId":"b23300ff8ba9fcd82e349da03a6f13386bff0077","title":"What do character-level models learn about morphology? The case of dependency parsing"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"5b1516c87818084dc5d195cc274e1ee8923210d2","title":"Neural Cross-Lingual Named Entity Recognition with Minimal Resources"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"4dd9d67a0259eef54ca32770db24fab5e42d362a","title":"Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction"},{"paperId":"771c1cb5fb161231e9aaa0a189caba672256a880","title":"Using Morphological Knowledge in Open-Vocabulary Neural Language Models"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"fc7e6502dace26305e3e062e426034f61a18095e","title":"Byte-Level Machine Reading Across Morphologically Varied Languages"},{"paperId":"0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b","title":"Neural Lattice Language Models"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"b959e905c093e1aa4a27eda0c0f1b4f727b93e63","title":"Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification"},{"paperId":"647979987ebfdf4f566add387bf3318fa8190305","title":"Hash Embeddings for Efficient Word Representations"},{"paperId":"07466fb914982f55051cc0b236fd524bdcd8bdc7","title":"Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"45e3f381d16e6d6db5449b7a44b7bdf294a8a822","title":"Multiscale sequence modeling with a learned dictionary"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a921e014249395496c22783fe58d293746af852b","title":"From Characters to Words to in Between: Do We Capture Morphology?"},{"paperId":"ed5003e6a2b97dd4b65c5190ccb88b9c45b032b8","title":"Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"},{"paperId":"664ec878de4b7170712baae4a7821fc2602bba25","title":"Learning to Generate Reviews and Discovering Sentiment"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"f4819c08515a03f086ada34f5babbe3395b3ffe9","title":"Character-level language modeling with hierarchical recurrent neural networks"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"12e9d005c77f76e344361f79c4b008034ae547eb","title":"Charagram: Embedding Words and Sentences via Character n-grams"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"4dabd6182ce2681c758f654561d351739e8df7bf","title":"Multilingual Language Processing From Bytes"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"fdb813d8b927bdd21ae1858cafa6c34b66a36268","title":"Learning deep structured semantic models for web search using clickthrough data"},{"paperId":"31100d7f8fdd3d263238624a888dab0f7aba3d5a","title":"Adaptor Grammars for Learning Non-Concatenative Morphology"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"727209a57c643472c8fc166ba3cc373936acc8d0","title":"Learning a Part-of-Speech Tagger from Two Hours of Annotation"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"9fae0f18e23db076e27b23f17a417f3149f63e2e","title":"TweetMotif: Exploratory Search and Topic Summarization for Twitter"},{"paperId":"e9d100404934e025a2df61882faf37ae2031af03","title":"Bloom Maps"},{"paperId":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":"502858989368adae91e0f4a643ebc2aa6efd5738","title":"Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition"},{"paperId":null,"title":"Language-wise breakdown for TYDI QA primary tasks"}],"id":"969287b8a96e242793b11f0dbb99ec341228106f","summary":"Canine is presented, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias."},{"url":"https://www.semanticscholar.org/paper/485b3f77b9913e151e7ca897d99497e70e7f30d1","title":"Optimizing segmentation granularity for neural machine translation","venue":"Machine Translation","year":2018,"referenceCount":26,"citationCount":24,"influentialCitationCount":1,"publicationDate":"19/10/2018","authors":"Elizabeth Salesky,Andrew Runge,Alex Coda,J. Niehues,Graham Neubig","citations":[{"paperId":"9d83941a205e31d394f614424cdc553cea9e35f9","title":"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22"},{"paperId":"023fd42f0867a88a2206f906c7f127701058feb6","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"379722c04fb3b54215f82512eb86398cb02d42dd","title":"Subword Segmental Language Modelling for Nguni Languages"},{"paperId":"3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class"},{"paperId":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"e6b252ad22486c10b1b288e0a5e1ad468690be70","title":"Joint Optimization of Tokenization and Downstream Model"},{"paperId":"5c3005e22e6fb218aa76fea49971f3f991993b32","title":"Robust Open-Vocabulary Translation from Visual Text Representations"},{"paperId":"24fd022750ea88e44e6790c7c7e1923635885c71","title":"Translation Mechanism of Neural Machine Algorithm for Online English Resources"},{"paperId":"b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation"},{"paperId":"6dc710b46bc510a42af487a3ac220e4fabf4d518","title":"VOLT: Improving Vocabularization via Optimal Transport for Machine Translation"},{"paperId":"8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task"},{"paperId":"38b40ae531ddca434de07015637b78413370d15a","title":"The Roles of Language Models and Hierarchical Models in Neural Sequence-to-Sequence Prediction"},{"paperId":"405cd5cd6e056675d4545eba12742174cc75d7e7","title":"Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes"},{"paperId":"8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","title":"Adversarial Subword Regularization for Robust Neural Machine Translation"},{"paperId":"4d08dcd2cc1e9691defe664a10f021424a896a1e","title":"Neural Machine Translation: A Review"},{"paperId":"201fae97e51fb6aea7ed8120147e806e43834de6","title":"Neural Machine Translation: A Review and Survey"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation"},{"paperId":"95236a88cc959656958d7a49422f4004015d594d","title":"Comparison between NMT and PBSMT Performance for Translating Noisy User-Generated Content"},{"paperId":"fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a","title":"Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation"},{"paperId":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019"}],"references":[{"paperId":"ab77f32382d63bceaa3ff9b8267e4e086945fa82","title":"Optimizing segmentation granularity for neural machine translation"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"491d29cb0018578b47e9abe19b93d5b65dc59113","title":"Improving Character-Based Decoding Using Target-Side Morphological Information for Neural Machine Translation"},{"paperId":"ac7c04a668bdb0e3eff168e65cb85689b4f7ef57","title":"Lifelong Learning with Dynamically Expandable Networks"},{"paperId":"b36a5bb1707bb9c70025294b3a310138aae8327a","title":"Automatic differentiation in PyTorch"},{"paperId":"69a037af886a6235d9af3cdeef4c7233d34c86ce","title":"Growing a Brain: Fine-Tuning by Increasing Model Capacity"},{"paperId":"1f080959faf9bf2a282c0aadbd8584d8b32f6e24","title":"Modeling Target-Side Inflection in Neural Machine Translation"},{"paperId":"69ffe3277011087b55afaeaef0a3933160beee9a","title":"Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English"},{"paperId":"100104b980d56a40cadfbd7034fa7807ce49b3fb","title":"Nematus: a Toolkit for Neural Machine Translation"},{"paperId":"f1d5904484d9b3f5e2a395ebe88f2ab90e32fd5e","title":"Target-side Word Segmentation Strategies for Neural Machine Translation"},{"paperId":null,"title":"Autodiff Workshop: the future of gradient-based machine learning software and techniques"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"16cb6876666f3a7b56a636c1d85ad00bd0d98bf3","title":"Net2Net: Accelerating Learning via Knowledge Transfer"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"6dab1c6491929d396e9e5463bc2e87af88602aa2","title":"Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"88b66f705a329da8292e7b8aa4bfe26de4759cfa","title":"Machine Translation without Words through Substring Alignment"},{"paperId":"791c0df5557890e7a4d8c81b12cd966531e7b42c","title":"Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability"},{"paperId":"8f4987d648d0daf3e36172e1abcda796fe8cf865","title":"An exponential translation model for target language morphology"},{"paperId":"42fc4c6580bfa54d57b5d6c56b5dfde58c6f6abb","title":"English-to-Czech Factored Machine Translation"},{"paperId":"4ee2eab4c298c1824a9fb8799ad8eed21be38d21","title":"Moses: Open Source Toolkit for Statistical Machine Translation"},{"paperId":"319af0958e268b3243975b8628262ebc1980ce40","title":"of the European Chapter of the Association for Computational Linguistics"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":null,"title":"Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations"}],"id":"485b3f77b9913e151e7ca897d99497e70e7f30d1","summary":"This paper incrementally introduces new BPE vocabulary online based on the held-out validation loss, and matches the results found with grid search, optimizing segmentation granularity while significantly reducing overall training time."},{"url":"https://www.semanticscholar.org/paper/ab139e341c005929848a7326f3d44f8a6aa9863c","title":"Improving Neural Machine Translation by Incorporating Hierarchical Subword Features","venue":"International Conference on Computational Linguistics","year":2018,"referenceCount":17,"citationCount":16,"influentialCitationCount":0,"publicationDate":"12/04/2018","authors":"Makoto Morishita,Jun Suzuki,Masaaki Nagata","citations":[{"paperId":"2dcdc4ef997507630f2c9b1b6682646ae31bdb7f","title":"SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models"},{"paperId":"2e6028f8b156c8b344e1a68d15d88403a978c71d","title":"Learning Multiscale Transformer Models for Sequence Generation"},{"paperId":"c68959f4da94fd35da5e8649465177b24d0dd351","title":"Byte-based Multilingual NMT for Endangered Languages"},{"paperId":"bfbfeb0613beae296d76077b2adc9e38a6b678a4","title":"Sub-Subword N-Gram Features for Subword-Level Neural Machine Translation"},{"paperId":"188f913be48a218b44b0bd964662b4926fd0b0b8","title":"Neural Machine Translation: A Review of Methods, Resources, and Tools"},{"paperId":"ca83469977ab49baae02ef1e12f419120927ecdc","title":"Mixed-Level Neural Machine Translation"},{"paperId":"fac7f9b04fba4889b445e309d384644d15ee7e88","title":"Trends and Advances in Neural Machine Translation"},{"paperId":"5e788c833321b12671206b96a438c0e5b1202027","title":"Finding the Optimal Vocabulary Size for Neural Machine Translation"},{"paperId":"debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes"},{"paperId":"035df9ecf84da7ae475175f326095ab16b97dd47","title":"Investigating the Effectiveness of BPE: The Power of Shorter Sequences"},{"paperId":"fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation"},{"paperId":"9f3f6deeb1f03ebd52f9ab44275ad382fe60d073","title":"Latent Part-of-Speech Sequences for Neural Machine Translation"},{"paperId":"0aef56962035a79101821480f51897fdc4443945","title":"Character n-gram Embeddings to Improve RNN Language Models"},{"paperId":"0ab0fda8774c303be8f8f8c8f684a890dcf5d455","title":"Lattice-Based Transformer Encoder for Neural Machine Translation"},{"paperId":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019"},{"paperId":"247328a082d86199ed5a98e1d726aa205c1da9df","title":"Neural Machine Translation"}],"references":[{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"56cf69bbe6598471d1655a4d0ccd4a61e946a532","title":"NTT Neural Machine Translation Systems at WAT 2017"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"106d5e0cf44ea08500adc91c4d5bb3e6c8a4d627","title":"Six Challenges for Neural Machine Translation"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"503981515e1a209f57f8119f616cc0f0c6bf168d","title":"Kyoto University Participation to WAT 2017"},{"paperId":"3bc6bcb60c7c00efcb471191fb62fd3ebd231d66","title":"Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions"},{"paperId":"46d0aa6b357c5427f46c7f8ff7053617c4309649","title":"Linguistic Input Features Improve Neural Machine Translation"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"ed6262b569c0a62c51d941228c54f34e563af022","title":"Japanese and Korean voice search"},{"paperId":"09cd7876b72d6105c83db59052572433a0a2b36c","title":"WIT3: Web Inventory of Transcribed and Translated Talks"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"ab139e341c005929848a7326f3d44f8a6aa9863c","summary":"It is confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets and the assumption that in the NMT model, the appropriate subword units for the following three modules can differ is confirmed."},{"url":"https://www.semanticscholar.org/paper/473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters","venue":"International Conference on Computational Linguistics","year":2020,"referenceCount":36,"citationCount":77,"influentialCitationCount":8,"publicationDate":"20/10/2020","authors":"Hicham El Boukkouri,Olivier Ferret,T. Lavergne,Hiroshi Noji,Pierre Zweigenbaum,Junichi Tsujii","citations":[{"paperId":"0351167c875b8931366e85eb5e517819d7db80cc","title":"What Makes for Good Tokenizers in Vision Transformer?"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"9769992ca9ff1c3894d19f4bd2b6dac27ed1befd","title":"A review on abusive content automatic detection: approaches, challenges and opportunities"},{"paperId":"5d239bb9d64ba63ff1390342706300f67ea7e8f2","title":"ConBERT: A Concatenation of Bidirectional Transformers for Standardization of Operative Reports from Electronic Medical Records"},{"paperId":"dd568e6838903ad7c381f13c1268c94c5db08b02","title":"Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing"},{"paperId":"04a13f0223e3c8b2a6b0b863e689a8cdf9befadb","title":"BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"1e2a5dca6f310241dd8a9b56873edf8ca211c781","title":"Enriching Biomedical Knowledge for Low-resource Language Through Translation"},{"paperId":"b82f58985bfb939750aa5756ede69feaf453d86f","title":"Enriching Biomedical Knowledge for Vietnamese Low-resource Language Through Large-Scale Translation"},{"paperId":"8297a3b52eea4e8c3ab8e97910839b4a3b917891","title":"On the State of the Art in Authorship Attribution and Authorship Verification"},{"paperId":"8f541cf5b193654d262dbdaea2b8a0d7d913e8dc","title":"Survey of NLP in Pharmacology: Methodology, Tasks, Resources, Knowledge, and Tools"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"7c206d541552a57b4c0cfe5b5113d3cc8e31df01","title":"IMSE: interaction information attention and molecular structure based drug drug interaction extraction"},{"paperId":"c75b1fb2348c0f54259319b3595ececaf1d98430","title":"Medical terminology-based computing system: a lightweight post-processing solution for out-of-vocabulary multi-word terms"},{"paperId":"305062aa036ffeafdd6300303898dff8ed59a7dc","title":"Cross-lingual Approaches for the Detection of Adverse Drug Reactions in German from a Patient’s Perspective"},{"paperId":"b82f43182aa7a188c12eb9cb6992b6f82cca2120","title":"Language Modelling with Pixels"},{"paperId":"f1d8dbe4e217d2221c5276f1e0c616ba5f82d1d3","title":"A review on Natural Language Processing Models for COVID-19 research"},{"paperId":"91141410b0bd141bc3b1ac0c190c3867181d699c","title":"Multi-task learning in under-resourced Dravidian languages"},{"paperId":"62ece609555bf833a2afd25ef796d72b5f59e767","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"9727fa4acdc5312ff86745875ef3db8578f153ac","title":"Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction"},{"paperId":"6058ce3819d72c3e429bea58d78d80c719cb4bdb","title":"Data Augmentation for Biomedical Factoid Question Answering"},{"paperId":"bff16ea31d5bc6b25b6e48327a2d7026e92a788d","title":"CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos"},{"paperId":"b3b1659c992cbbd233038522ddd170887c033fe7","title":"Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech"},{"paperId":"9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e","title":"vTTS: visual-text to speech"},{"paperId":"e68d1aa5e78226bb868a43842a87a5532b813b0b","title":"CancerBERT: a cancer domain-specific language model for extracting breast cancer phenotypes from electronic health records"},{"paperId":"16f0acab166e8ac756927fc8f2e8f68be641605b","title":"Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models"},{"paperId":"e4645f7d08b5bc878bd38c3db19c3b1a9978bd43","title":"Imputing Out-of-Vocabulary Embeddings with LOVE Makes LanguageModels Robust with Little Cost"},{"paperId":"7746ac2e1e84c2f1d82a4593e2cdfdd4c93a04d8","title":"An open-source natural language processing toolkit to support software development: addressing automatic bug detection, code summarisation and code search"},{"paperId":"1b012686cf42baed47c11f0454b20a4d820a1db1","title":"The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms"},{"paperId":"e923c415140a095711857420976b41e7a07cfe9c","title":"Towards improving the robustness of sequential labeling models against typographical adversarial examples using triplet loss"},{"paperId":"ec72909b75b2389efd588aa38d9c664e654d90d3","title":"Automatic Classification of Cancer Pathology Reports: A Systematic Review"},{"paperId":"2ec281d654f622da7267d7da8145e96bb77a9ede","title":"An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification"},{"paperId":"937b177d2ed7cee27ee45300c690f2f60c81bae5","title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"aeeb168feb05da1b3d31c0389e7a9196bc8970f6","title":"DravidianCodeMix: sentiment analysis and offensive language identification dataset for Dravidian languages in code-mixed text"},{"paperId":"420c897bc67e6f438db522d919d925df1a10aa8c","title":"AMMU - A Survey of Transformer-based Biomedical Pretrained Language Models"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"edc17c862b074fb986f9b9a144f10decb843f5fa","title":"Décontextualiser des plongements contextuels pour construire des thésaurus distributionnels (Decontextualizing contextual embeddings for building distributional thesauri )"},{"paperId":"82004b8e8488d2ebb413427ef4221b7e0d6e9f1e","title":"BERTSeg: BERT Based Unsupervised Subword Segmentation for Neural Machine Translation"},{"paperId":"abb47fb115a420fc363ed3acbd8abb1a37f0dcd5","title":"DeepREF: A Framework for Optimized Deep Learning-based Relation Classification"},{"paperId":"d5c2119aec38b8e8e154ec2bf0106dfac1ecc80f","title":"Building Static Embeddings from Contextual Ones: Is It Useful for Building Distributional Thesauri?"},{"paperId":"04bc7d5ac048133a1aaead7ab1e8021b055359c4","title":"Design principles of an open-source language modeling microservice package for AAC text-entry applications"},{"paperId":"0826af7c0f221878a142dd1681a63adb3f7e4569","title":"RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining"},{"paperId":"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","title":"CHARFORMER: FAST CHARACTER TRANSFORMERS"},{"paperId":"b42e3a759348f27cca2f918a6bd0b139a5312e44","title":"A Survey of Pretrained Language Models Based Text Generation"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"cdc71d86bb0b822d73a8359338db0492a2c89b89","title":"Gated Character-aware Convolutional Neural Network for Effective Automated Essay Scoring"},{"paperId":"a3d24d9d8ca41a0b87957819a7a9095d1cdb8c15","title":"Transferring BERT-like Transformers' Knowledge for Authorship Verification"},{"paperId":"9424d98d7805574661da1adf4f8bc682be5714da","title":"Rethinking the Authorship Verification Experimental Setups"},{"paperId":"597b1387f721fa8f7524b257aca920c7d373ed48","title":"Using Distributional Principles for the Semantic Study of Contextual Language Models"},{"paperId":"11a583c33887e8fc16c0ee8c458fad92d5641dd5","title":"Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching"},{"paperId":"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","title":"Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?"},{"paperId":"705554c3532b7be9c1eb7c993132ffb940282e1b","title":"Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models"},{"paperId":"395000b02848b666ebd2433c93127fac5113df9f","title":"BERT Cannot Align Characters"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?"},{"paperId":"c0cd91d975df23ae3d7c088c4ddd375271dfe17c","title":"Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning"},{"paperId":"d85d0c5d218b22366abd2bd4ee35df179dda1d85","title":"CancerBERT: a BERT model for Extracting Breast Cancer Phenotypes from Electronic Health Records"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"e6a6c7c1e231e745021d2312f7f88f3ea24bcba8","title":"Benchmarking Multi-Task Learning for Sentiment Analysis and Offensive Language Identification in Under-Resourced Dravidian Languages"},{"paperId":"82d9823db8d3ab364f04f51914919e0a7c6b9b44","title":"Character-Level Syntax Infusion in Pre-Trained Models for Chinese Semantic Role Labeling"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"972793927b9b1dd0805ee36b24f37c7f2c3ba691","title":"Combining formal and machine learning techniques for the generation of JML specifications"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"87bca71f182bdb942adbe3f6898894cadef6776e","title":"CodemixedNLP: An Extensible and Open NLP Toolkit for Code-Mixing"},{"paperId":"3ca6eb7b69da71899ff2d20d6a65f3e668b97312","title":"IIITT@LT-EDI-EACL2021-Hope Speech Detection: There is always hope in Transformers"},{"paperId":"d33713b55f6e79270a529cdcce4843c70a051f83","title":"UniParma at SemEval-2021 Task 5: Toxic Spans Detection Using CharacterBERT and Bag-of-Words Model"},{"paperId":"7dcb6a4b7b1a1d2317c3058f529c5dbcceebbcae","title":"Easy-to-use Combination of POS and BERT Model for Domain-Specific and Misspelled Terms"},{"paperId":"c3152c57f2c3960f9423f2b7278a5f98d149b3d8","title":"Differential Evaluation: a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing"},{"paperId":"71ea59f202562410f7342764265a29a27778da69","title":"Type- and Token-based Word Embeddings in the Digital Humanities"},{"paperId":"9b6e51039f57cabdb77ca70cbbdd6ff80bdf1c3a","title":"FETD2: A Framework for Enabling Textual Data Denoising via Robust Contextual Embeddings"},{"paperId":"58403c6bc061670f7ce6267a3a0cd01ba1bb8e50","title":"Intérêt des modèles de caractères pour la détection d’événements (The interest of character-level models for event detection)"},{"paperId":"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","title":"Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings"},{"paperId":"8ce62f8d83f02e8ce9d7a9e1b5b7affd5c13bb7d","title":"Sensitive Data Detection with High-Throughput Neural Network Models for Financial Institutions"}],"references":[{"paperId":"2ff41a463a374b138bb5a012e5a32bc4beefec20","title":"Pre-Training With Whole Word Masking for Chinese BERT"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef","title":"ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"},{"paperId":"bc789aef715498e79a74f857fa090ece9e383bf1","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"7304afb491930d520e194f4ca8b91a9652f0e658","title":"MedSTS: a resource for clinical semantic textual similarity"},{"paperId":"2b8f3c930478053444a12a889bc1ee26f8f0bff2","title":"BERT Goes to Law School: Quantifying the Competitive Advantage of Access to Large Legal Corpora in Contract Understanding"},{"paperId":"d8d7e338e528bdf89c589ab5926fc9ec4339c78b","title":"Embedding Strategies for Specialized Domains: Application to Clinical Entity Recognition"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"6a7769116c6733dffa347444b2835e50129e0143","title":"Deep Dominance - How to Properly Compare Deep Neural Models"},{"paperId":"347bac45298f37cd83c3e79d99b826dc65a70c46","title":"Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"43c844c30765f3fa25bfabd83490ef826b9ceca1","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"06b36e744dca445863c9f9aefe76aea95ba95999","title":"Enhancing Clinical Concept Extraction with Contextual Embedding"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"OpenWebText corpus"},{"paperId":null,"title":"2019), which is the original model using the “base-uncased” architecture, and BlueBERT (Peng et al., 2019), a medical BERT pre-trained from the former using MIMIC-III and the abstracts of PMC OA"},{"paperId":null,"title":"2019) using the “base-uncased” architecture, and BlueBERT (Peng et al., 2019) a medical BERT that is the result of re-training the former model on MIMIC-III and PubMed abstracts"},{"paperId":"f2588de5173fb047192dbb93d62ce6636bdf46bd","title":"Lessons from Natural Language Inference in the Clinical Domain"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"eed781f498b563df5a9e8a241c67d63dd1d92ad5","title":"Overview of the BioCreative VI chemical-protein interaction Track"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"95cd83603a0d2b6918a8e34a5637a8f382da96f5","title":"MIMIC-III, a freely accessible critical care database"},{"paperId":"733b821faeebe49b6efcf5369e3b9902b476529e","title":"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"b92aa7024b87f50737b372e5df31ef091ab54e62","title":"Training Very Deep Networks"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"53ca064b9f1b92951c1997e90b776e95b0880e52","title":"Learning word embeddings efficiently with noise-contrastive estimation"},{"paperId":"6ea353ada2b89763f58d8068a74b2e6def526948","title":"The DDI corpus: An annotated corpus with pharmacological substances and drug-drug interactions"},{"paperId":"5e095981ebf4d389e9356bd56e59e0ade1b42e88","title":"2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text"},{"paperId":"a8e8f3c8d4418c8d62e306538c9c1292635e9d27","title":"Backpropagation Applied to Handwritten Zip Code Recognition"}],"id":"473921de1b52f98f34f37afd507e57366ff7d1ca","summary":"This work proposes CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters, and shows that this new model improves the performance of Bert on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations."}]}
{"papers":[{"url":"https://www.semanticscholar.org/paper/1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","title":"Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings","venue":"EACL","year":2021,"referenceCount":18,"citationCount":7,"influentialCitationCount":0,"publicationDate":2021,"authors":"O-Yeon Kwon,Dohyun Kim,Soobee Lee,Junyoung Choi,SangKeun Lee","citations":[{"paperId":"f8ef95fe83ee5d5a88220797f44bf59a2e95c477","title":"GlareShell: Graph learning-based PHP webshell detection for web server of industrial internet"},{"paperId":"7a139679ef9b310008e4ad799d8a27fb7c0747a8","title":"Computational Models to Study Language Processing in the Human Brain: A Survey"},{"paperId":"02942af51e5c80fd6c6fa8097ad389e697d53677","title":"DPMS: Data-Driven Promotional Management System of Universities Using Deep Learning on Social Media"},{"paperId":"ce26a9458de83c7394ebfe6f62d26ad73869b0cb","title":"From task to evaluation: an automatic text summarization review"},{"paperId":"e985d34a29a1988659e7971d520496eca1a7a548","title":"BejaGNN: behavior-based Java malware detection via graph neural network"},{"paperId":"96e77dfdd50411fe404e6b6bbde5224df9eee802","title":"Tweets Emotions Analysis of Community Activities Restriction as COVID-19 Policy in Indonesia Using Support Vector Machine"},{"paperId":"f22ba8b56b89b2f19e84fbe3439bf78e8fc926b2","title":"KLASIFIKASI EMOSI PADA CUITAN DI TWITTER DENGAN PRINCIPAL COMPONENT ANALYSIS DAN SUPPORT VECTOR MACHINE"}],"references":[{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"8fa7d1f3f82526935ba122c20c8d0648506301b3","title":"Misspelling Oblivious Word Embeddings"},{"paperId":"9348186c18bbd35d77a9011474fdd76ef98c86c5","title":"Robust Word Vectors: Context-Informed Embeddings for Noisy Texts"},{"paperId":"6c145caf5da0e2f078c34d0b65b399d7124e2fd8","title":"Learning to Generate Word Representations using Subword Information"},{"paperId":"1dff26dc9be229daae61231a340f57efa8126e0d","title":"Subword-level Word Vector Representations for Korean"},{"paperId":"c0fbe0378150bb35e94bdd800bf02a4e57d9f2c2","title":"Morpheme-based Efficient Korean Word Embedding"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"},{"paperId":"6fe682de00baba4d50bb855ae243f55a4a2a4e14","title":"A Word Embedding and a Josa Vector for Korean Unsupervised Semantic Role Induction"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"b0130277677e5b915d5cd86b3afafd77fd08eb2e","title":"Estimation of probabilities from sparse data for the language model component of a speech recognizer"},{"paperId":null,"title":"Analyzing of hangul search query spelling error patterns and developing query spelling correction system based on user logs"},{"paperId":"b92a80c848d51bb009758c117ab4f05a99913106","title":"THE KOREAN LANGUAGE: Structure, use and context"}],"id":"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","summary":"A robust Hangeul word embedding model against typos is proposed that utilizes a Convolutional Neural Network architecture with a channel attention mechanism that learns to infer the original word embeddings."},{"url":"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":68,"citationCount":304,"influentialCitationCount":29,"publicationDate":"28/05/2021","authors":"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel","citations":[{"paperId":"915b10732f264bebd952c806db93692bf2cf8b08","title":"SpaceByte: Towards Deleting Tokenization from Large Language Modeling"},{"paperId":"fe1d3ef1c85c1c067bea900a7ba9a75f2e992a6c","title":"Neural Semantic Parsing with Extremely Rich Symbolic Meaning Representations"},{"paperId":"91ca6535fc8fb03efe0ecbe424ce5354ed129b0c","title":"Towards Large Language Models as Copilots for Theorem Proving in Lean"},{"paperId":"9a4167592c9aaedecf74fee36354601462b45740","title":"LLMs4OM: Matching Ontologies with Large Language Models"},{"paperId":"6a4501fefaf73261dc180ff86b52208679f3fb9c","title":"A Survey on Deep Learning for Theorem Proving"},{"paperId":"1ef95907484526088c4370c5a60164fe56399551","title":"Resilience of Large Language Models for Noisy Instructions"},{"paperId":"0168d210ef1cf15c165582625a8d1e3d2a7c2b84","title":"Measuring Cross-lingual Transfer in Bytes"},{"paperId":"dfcdcb1c7da0792ab6cc79f7543411f2d380a722","title":"Gaining More Insight into Neural Semantic Parsing with Challenging Benchmarks"},{"paperId":"30f4be3eb28ce58058f6e4ba7bebf4e57a7e93e3","title":"On the Effect of (Near) Duplicate Subwords in Language Modelling"},{"paperId":"be445964370b5ea8c15aa1bf6cf10f951fa90b9a","title":"Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers"},{"paperId":"e27c9c39ecb67e8e7708d8d53bec2f891cfa7a40","title":"Training LLMs over Neurally Compressed Text"},{"paperId":"cd77f734f0c54bccbea1b75c9458cbde121a38dc","title":"CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models"},{"paperId":"5b838e3be86f9f86828f9892aca4f937ec882da8","title":"CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech"},{"paperId":"f914e6e4cd65a4f04a78d4f6af2fd8aa093b79cd","title":"An Analysis of BPE Vocabulary Trimming in Neural Machine Translation"},{"paperId":"0169ee108d2cf9e388d9c6c3773402b84e95ab7b","title":"Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens"},{"paperId":"8f8e46b7d454406ec35358f525ce9ce17ff3f2b0","title":"LeanReasoner: Boosting Complex Logical Reasoning with Lean"},{"paperId":"01574fb3abdf10867850b0970bc47b178027ee6b","title":"Wav2Gloss: Generating Interlinear Glossed Text from Speech"},{"paperId":"c20942cedd92dcc4e6270f85af780464da655c4c","title":"Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models"},{"paperId":"8efaa8206874c2f7a79bba2a9bcba542e4cabf31","title":"MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling"},{"paperId":"9b13450581dc2a04366ad95fb60e30cd69453be2","title":"Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering"},{"paperId":"cef7c8825b59713324b62110a626c3b0589150a5","title":"Embedded Translations for Low-resource Automated Glossing"},{"paperId":"55c15c1bc0660fd6a532d4ad9a1175064166991e","title":"GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing"},{"paperId":"02a694bc78a8178f2409387b709039849f36bd83","title":"Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance"},{"paperId":"93a71c09e8ea389750c7e3aa6ec5577a8ba227f7","title":"LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition"},{"paperId":"4d31aae7d8555279fda45a0fc00e6aca2904772b","title":"A Bit of a Problem: Measurement Disparities in Dataset Sizes Across Languages"},{"paperId":"99ed8dd210fb84de7417424adf8f10ae463d2b99","title":"Advancing Generative AI for Portuguese with Open Decoder Gerv√°sio PT"},{"paperId":"25cee6fe99fc51b60760c31de4b28195edca1b6c","title":"Beyond Language Models: Byte Models are Digital World Simulators"},{"paperId":"7861d04b41c9848905cb73268040d10e23409c77","title":"Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning"},{"paperId":"441d9c239ba5fee0be1ac122330052c7b6bf822e","title":"An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference"},{"paperId":"9dee70e17a06fa067cf408d1a4a862f2ec77baa3","title":"Knowledge of Pretrained Language Models on Surface Information of Tokens"},{"paperId":"f7f2d9e56aed2aef09a11e98fab14940af726c97","title":"Pixel Sentence Representation Learning"},{"paperId":"a640215755e23e2649f4b3d3246a47b14fea93f7","title":"Getting the most out of your tokenizer for pre-training and domain adaptation"},{"paperId":"a6e2dca754f3dc625a9da5f10f9b7a57079bfd27","title":"MambaByte: Token-free Selective State Space Model"},{"paperId":"de643836277d108e1f8410ea85b0c3ed0e686163","title":"MaLA-500: Massive Language Adaptation of Large Language Models"},{"paperId":"16daf858baef5ff2a698596bf26999a618fa521e","title":"Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion"},{"paperId":"96c88b196e3e432710debab39f49ee72f2b96a10","title":"Anisotropy Is Inherent to Self-Attention in Transformers"},{"paperId":"ece1c04e9dcf89cbc6170fcccdec8412d11503d8","title":"A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation"},{"paperId":"9b782ec0e038287227c8ff28f1bbcae548cdd721","title":"Pheme: Efficient and Conversational Speech Generation"},{"paperId":"0eef63fe71efba4404a9a883bf89ec634dbaad82","title":"Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method"},{"paperId":"7845058565eb71e3e423ec72641abe9d7ba2ed0a","title":"UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models"},{"paperId":"7354a57261d27a281e56dc428b6ec146b9992afd","title":"Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text"},{"paperId":"1c6e2a4da1ead685a95c079751bf4d7a727d8180","title":"TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering"},{"paperId":"e58147980d42f845879c4231840c2857d82337eb","title":"MELA: Multilingual Evaluation of Linguistic Acceptability"},{"paperId":"c4ae033f14a89db8b6b6b4da598375f177a1fac4","title":"Memory Augmented Language Models through Mixture of Word Experts"},{"paperId":"cba0b336e7a2001d891d8d29275542367cd52925","title":"Lexical Normalization Using Generative Transformer Model (LN-GTM)"},{"paperId":"5f3d6c077712a2d692bb29671c084e098854c738","title":"Learning Mutually Informed Representations for Characters and Subwords"},{"paperId":"c0e62e324dfc7bd45ec0a5ce4055823d1f0c03f1","title":"Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval"},{"paperId":"ab2cd01090bebd7c52193c432e0a0fce4226982e","title":"Generating Pragmatic Examples to Train Neural Program Synthesizers"},{"paperId":"4d51e88131a7f1acc6e74a3d750f18bad527dcd7","title":"Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision"},{"paperId":"9cb33adc68daa6f27d8a362532e6c54205e0e22b","title":"Too Much Information: Keeping Training Simple for BabyLMs"},{"paperId":"fd665a3d8c4a5fde1e1b00f78ab231d99b22abd0","title":"Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew"},{"paperId":"d921732ca5920049b6a8194f559917689f02d096","title":"Text Rendering Strategies for Pixel Language Models"},{"paperId":"d9655aad891e0dba404bc132c8419f93c098c8f8","title":"Learning to Abstract with Nonparametric Variational Information Bottleneck"},{"paperId":"0a550ee375df53c845f46084de4f0e7f16170d9a","title":"NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task"},{"paperId":"17d0cc24fd6a267e7fd59ae62054de3e5c552096","title":"Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"61644c4b76a11501d3b9b85fcfed513c56f184a7","title":"Enhancing Neural Machine Translation with Semantic Units"},{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"3448e136046d22d4f90d7a4c875890f5fb64b811","title":"PuoBERTa: Training and evaluation of a curated language model for Setswana"},{"paperId":"997cebb936d88577da59ba460a9141bdd5dcb36f","title":"To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer"},{"paperId":"64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?"},{"paperId":"c30deb8f4b0e942a36e5ef5c87bed01f575c0c6e","title":"Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention"},{"paperId":"2e9f3a96d6f6011ff1b4eab541ef1df12cde042f","title":"Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation"},{"paperId":"b4eb0ca9f019254f7d1ec6432a095e526fa3abc2","title":"Large Synthetic Data from the arXiv for OCR Post Correction of Historic Scientific Articles"},{"paperId":"10af858834ad69f1c30721e6aa732d77fa369161","title":"Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models"},{"paperId":"376f0d8bc222a33fb382933552e796bb21e6191b","title":"Abbreviation Disambiguation in Polish Press News Using Encoder-Decoder Models"},{"paperId":"e892a225c417fbac7545c3e31b45d1c42dc9c933","title":"Chain-of-Thought Reasoning is a Policy Improvement Operator"},{"paperId":"ff50383c09346e3e7d16856af3519fa09a9f8580","title":"Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning"},{"paperId":"119639d87c7ce418c57070a41505dc7ac9eb7e1d","title":"Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap"},{"paperId":"3fa61652b1a4f314590e5e8bfe8f023bbc5c3568","title":"Ngambay-French Neural Machine Translation (sba-Fr)"},{"paperId":"dc0436318a08f4df8f9653f164a830f245caca8b","title":"GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering"},{"paperId":"064af570792c61f5cc46814c648fd3969f0999e7","title":"Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction"},{"paperId":"1943144f998c2620616b3e89d376ff801b6e04c6","title":"Lightweight Adaptation of Neural Language Models via Subspace Embedding"},{"paperId":"ce5ea17bffd6333e414422d680827fef2b0ce41a","title":"Correcting Wide-Range of Arabic Spelling Mistakes Using Machine Learning and Transformers"},{"paperId":"8ce219059d777c2333ee21cb2af2aad71275c98f","title":"LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"},{"paperId":"519692ea4c588754bc9e3e69c48f98a6e508fc84","title":"Aligning Word Embeddings from BERT to Vocabulary-Free Representations"},{"paperId":"d57cbec10dfee4e244657d3cdf9ba4cf53086744","title":"An Empirical Analysis Towards Replacing Vocabulary-Rigid Embeddings by a Vocabulary-Free Mechanism"},{"paperId":"d5241be77ff0aec9a2ac1e1b14fb69df897eb8f3","title":"Bi-Phone: Modeling Inter Language Phonetic Influences in Text"},{"paperId":"d7890d1906d95c4ae4c430b350455156d6d8aed9","title":"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"},{"paperId":"e83c3f46e036df811d50aca56e53199f79f1a415","title":"Estimating Post-OCR Denoising Complexity on Numerical Texts"},{"paperId":"714671a26060d289a5e12888a92e904a19034982","title":"MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems"},{"paperId":"2adc13eb55c92e026c4cefc89a47a0ee0ac95111","title":"SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding"},{"paperId":"d5d936b142eb81826002888230649c805532630e","title":"VisText: A Benchmark for Semantically Rich Chart Captioning"},{"paperId":"87875a07976c26f82705de1fc70041169e5d652b","title":"LeanDojo: Theorem Proving with Retrieval-Augmented Language Models"},{"paperId":"4e12e831bc1627545479c65d7b2296d6d2562c9a","title":"A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing"},{"paperId":"5cd4fe8174b582f27d79859e4b79e0eba2a1d5f0","title":"Is Anisotropy Inherent to Transformers?"},{"paperId":"4bd35d344c635b05f97f4d749741d196ff541bf3","title":"A Primer on Seq2Seq Models for Generative Chatbots"},{"paperId":"a903e1e0ffd04dd666f3537f6570d742d7be3486","title":"Grounded Text-to-Image Synthesis with Attention Refocusing"},{"paperId":"02dde8e9fefafae8e0b9ee2eef0ddc74c511f7cd","title":"Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models"},{"paperId":"29a54c072bcf85fa6934b6f701962a7c9aeb6489","title":"Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora"},{"paperId":"5fbe4c92791fbecb179c1ab79bba9a59b2e155ba","title":"GlyphControl: Glyph Conditional Control for Visual Text Generation"},{"paperId":"b4ee74a1f294cb5bb028a958ce43219cabb0f347","title":"TranSFormer: Slow-Fast Transformer for Machine Translation"},{"paperId":"ac36b65a6fa4a9e84de051f0d3e9d50348fa4160","title":"Lexinvariant Language Models"},{"paperId":"36dfcdc43664f03b15e5e03373a9d46728672e28","title":"mPLM-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models"},{"paperId":"b3cff6abe401a244c21d4706b0931e48acaeeb4e","title":"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models"},{"paperId":"d1e7b054578ae0606effe63ab8402861ec54a49e","title":"SƒÅmayik: A Benchmark and Dataset for English-Sanskrit Translation"},{"paperId":"2cfb1f44b34204213d789731871e599c756bdb83","title":"Exploring Large Language Models for Classical Philology"},{"paperId":"d7a379254ac2dee2e31dfdeedb440176f0285aa5","title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding"},{"paperId":"c60736e61f8961ec535ecfdc6f0398925d34d0b8","title":"Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer"},{"paperId":"7c217cc7524251f42887438834912e06129c3299","title":"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis"},{"paperId":"39bca01efce8765f0a5d3a8981bc30d56f196b96","title":"XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages"},{"paperId":"879a7f5abdb7ab803d48172d4f0830965f989d46","title":"Language Model Tokenizers Introduce Unfairness Between Languages"},{"paperId":"2183c88e9056e931b07d48f1dc44360785952073","title":"SoundStorm: Efficient Parallel Audio Generation"},{"paperId":"ea8197cb357af6f89e8b6e5548897236a24719b1","title":"What is the best recipe for character-level encoder-only modelling?"},{"paperId":"b4790ca1a967b44c3028b73c0c00d501fcd81728","title":"Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages"},{"paperId":"af6c6941acecfb23d899cd5266efdf2e27463f12","title":"Causal Language Model Aided Sequential Decoding With Natural Redundancy"},{"paperId":"37d8bc1c268773dc69c92c6f550393c93e9a1f7a","title":"Automatic document classification via transformers for regulations compliance management in large utility companies"},{"paperId":"e0d09d91784a6d426ffcd2fd12448dd9c8ceefe9","title":"Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages"},{"paperId":"ba184d335a9a08c52c5d25eabd7f4a8ea987918b","title":"UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining"},{"paperId":"e0c2a2ba6054b18f570a169790b015d333928789","title":"Romanization-based Large-scale Adaptation of Multilingual Language Models"},{"paperId":"2b8d28149a43b9659a6da2c56014ec4206a912b4","title":"Ticket automation: An insight into current research with applications to multi-level classification scenarios"},{"paperId":"10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7","title":"GlyphDraw: Seamlessly Rendering Text with Intricate Spatial Structures in Text-to-Image Generation"},{"paperId":"3add55c068fe19bb2e5392cbe994602a91630ec1","title":"Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams"},{"paperId":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!"},{"paperId":"eef5b8f3c4e60d596a04101d8261c222ab739861","title":"Fine-Tashkeel: Fine-Tuning Byte-Level Models for Accurate Arabic Text Diacritization"},{"paperId":"0a438980ac42451d6d32dd2ad8ead7b55520408d","title":"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning"},{"paperId":"4e97303aeb299ee736b1b8c29cef046212690354","title":"Generation-based Code Review Automation: How Far Are We∆í"},{"paperId":"0dfc7eecc5f12b8613152f8c62f9ccfadc4f92b7","title":"DTT: An Example-Driven Tabular Transformer for Joinability by Leveraging Large Language Models"},{"paperId":"2c5460afa19ad6fc2568b7e210115acacc14a40c","title":"An Overview on Language Models: Recent Developments and Outlook"},{"paperId":"f3d894cf6f7be14a545019f4621ccce41f45b088","title":"Learning the Legibility of Visual Text Perturbations"},{"paperId":"b169f2ce55e14584d2db6f64eeb5ad2702d39d40","title":"Artificial intelligence foundation and pre-trained models: Fundamentals, applications, opportunities, and social impacts"},{"paperId":"2c8935ec872eca14636a090e5f6b49bc1c90c30d","title":"Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation"},{"paperId":"96b005d1d92211cf053e4114a85a5e64e428d896","title":"Extending English IR methods to multi-lingual IR"},{"paperId":"67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation"},{"paperId":"1e4b6567ff66de6cdcbe58c863538241e2f62b45","title":"Aligning Text-to-Image Models using Human Feedback"},{"paperId":"a794a92e1fa516250390514eeeb3c3b3140876a3","title":"RetVec: Resilient and Efficient Text Vectorizer"},{"paperId":"0704a96e1c57c12031f1c3ca492a91dbed1f61ce","title":"Distillation of encoder-decoder transformers for sequence labelling"},{"paperId":"62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c","title":"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models"},{"paperId":"20c7d73609ba98fa27b1edc7b537ef59442e4ba2","title":"Truveta Mapper: A Zero-shot Ontology Alignment Framework"},{"paperId":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models"},{"paperId":"7cdebb73662387d9040da4f27a7dc04dbffa3c3e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"2f272ca3394656835bf32a30b80c6f8ba1c8f4c4","title":"Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks"},{"paperId":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only"},{"paperId":"4aa09cba27a489ca02471fad011ea4854fc63cc1","title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue"},{"paperId":"7c1c95f5fb7fce563171fcc0060c850390753b3c","title":"Advancing Multilingual Pre-training: TRIP Triangular Document-level Pre-training for Multilingual Language Models"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"be2df0fafa89b6355d1ff1336c10e0d4c8d27276","title":"Massively Multilingual Natural Language Understanding 2022 (MMNLU-22) Workshop and Competition"},{"paperId":"3af872a4bd314db9e7964876fee811823628f83f","title":"Text Normalization on Code-Mixed Twitter Text using Language Detection"},{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"},{"paperId":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"5e52d654fd31f04c1bd884cd5480e6af8c95ad50","title":"Efficient Transformers with Dynamic Token Pooling"},{"paperId":"0783c214623c18f6a8ad96b8eaf4a67a382e68ee","title":"QAmeleon: Multilingual QA with Only 5 Examples"},{"paperId":"af277778904463965c60626e22783d3e1740058b","title":"Introducing Semantics into Speech Encoders"},{"paperId":"d50b9db750ded246bde13e2c263e341bcbd8a335","title":"A Benchmark and Dataset for Post-OCR text correction in Sanskrit"},{"paperId":"d7e0f0cec28c34710fa631df410b717186741db5","title":"A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces"},{"paperId":"9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages"},{"paperId":"92302ab168429c7c3a8f699b35ba8302916c6e7c","title":"Bridging Speech and Textual Pre-Trained Models With Unsupervised ASR"},{"paperId":"5e786d47e3dc5ffa65ba8548ff67139a2764acf8","title":"T5lephone: Bridging Speech and Text Self-Supervised Models for Spoken Language Understanding Via Phoneme Level T5"},{"paperId":"d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6","title":"SLING: Sino Linguistic Evaluation of Large Language Models"},{"paperId":"6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc","title":"Graphemic Normalization of the Perso-Arabic Script"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"3e08d84af266026e7d254729f3afc6dcd572d264","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72","title":"Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning"},{"paperId":"cd9fdf84f1cbb482e3952ca63de70d03f9030644","title":"Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining"},{"paperId":"aa94724d1dbc9cad0ad3377903174e776175837a","title":"Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino"},{"paperId":"2a8aac78df5e1e9658a02d381662ed26c6577713","title":"Disease diagnostic method based on cascade backbone network for apple leaf disease classification"},{"paperId":"bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","title":"MonoByte: A Pool of Monolingual Byte-level Language Models"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?"},{"paperId":"e4437293a703fcf282bf1b38bf7900ed8ca711bb","title":"Training a T5 Using Lab-sized Resources"},{"paperId":"a806041621acb5cf648fc780f1ff14939aa3a721","title":"How Much Does Lookahead Matter for Disambiguation? Partial Arabic Diacritization Case Study"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"00d9a73c54053a32e2aba92b53fc3e6bc71d3238","title":"Sequence-to-sequence pretraining for a less-resourced Slovenian language"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","title":"Confident Adaptive Language Modeling"},{"paperId":"b308511ac52e1c2da915d1e55d5246dfdb4cbe28","title":"Romanian Question Answering Using Transformer Based Neural Networks"},{"paperId":"a766ef0678aade6a9798552618819cf4d0fac406","title":"TEVR: Improving Speech Recognition by Token Entropy Variance Reduction"},{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"0232715f9089e3a2fc002cff6737bb9939805b8d","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"32afdb07021fda775ceaedd231c58bfed0aa980a","title":"Automated Crossword Solving"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"b21670e8061a06ab97e7d6052c9345a326e84ff8","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"063d9fa4861356500219b7e81d5a654aa921da6f","title":"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation"},{"paperId":"7267812178393b8ae0b99648f02661ca1ff2b412","title":"Bilingual End-to-End ASR with Byte-Level Subwords"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"0de580957d23dd65e31b6c95e6bc5d15bc15c57d","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"},{"paperId":"0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"880c8973ea1376c6bff23226b3f64792657aa666","title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion"},{"paperId":"943487997ecd26e871a2ab16160bd5640020369d","title":"Toward Best Practices for Training Multilingual Dense Retrieval Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"1ed66e048bb025e75aa5ea660545285212e5341f","title":"Scaling Up Models and Data with t5x and seqio"},{"paperId":"b1dad820853464b30c93b52366b32690ba4b99a6","title":"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View"},{"paperId":"a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia"},{"paperId":"8a4eec70e3d0c43abf26757252ad6210c9717f6c","title":"Towards Lithuanian grammatical error correction"},{"paperId":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation"},{"paperId":"7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"},{"paperId":"8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"},{"paperId":"f357b35e068bbfbb8468b48198ab63241713629d","title":"Correcting diacritics and typos with ByT5 transformer model"},{"paperId":"7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models"},{"paperId":"e53f455b3300d95738dac117419c88fbde58ac4e","title":"Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"0ae82f96cb8c1d2da5d8284765dd76a139ba6ff6","title":"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers"},{"paperId":"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","title":"Large Dual Encoders Are Generalizable Retrievers"},{"paperId":"6e2682eb2fbec93b329028b23764c1164e232c41","title":"√öFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5"},{"paperId":"f3bed81c50e293c03f6b650cdb9351d14e9be347","title":"Deciphering ‚Äúthe language of nature‚Äù: A transformer-based language model for deleterious mutations in proteins"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer"},{"paperId":"11ccb9b509d84e845901ee097e7d0a6419fdc182","title":"EncT5: A Framework for Fine-tuning T5 as Non-autoregressive Models"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don‚Äôt people use character-level machine translation?"},{"paperId":"e35357ac461a669fe7e4b877ee1fad0dfda26303","title":"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings"},{"paperId":"c107835a05ca6fda6e73b64e2ed9884de4fcec0f","title":"A proposed conceptual framework for a representational approach to information retrieval"},{"paperId":"a70fc86508bd0133d5d984a4e777abef1934d76c","title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"},{"paperId":"6e0f8a47072de7e549182a5d0fc07c6d0a207325","title":"Predicting Ethnicity from Names with rethnicity: Methodology and Application"},{"paperId":"39262814fa3e47905a2e5facf13465a1f70706b9","title":"Single-Read Reconstruction for DNA Data Storage Using Transformers"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"972808b92159a782f5ca1c1ab3a8ed3867acfb2d","title":"mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset"},{"paperId":"89a4f4d1f8c93da85d829a1acfe8cafea2b50c00","title":"Transfer Learning for Multi-lingual Tasks - a Survey"},{"paperId":"c41819413725668fbf8e3ca1a0bcaf7fb691e582","title":"How Optimal is Greedy Decoding for Extractive Question Answering?"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer"},{"paperId":"31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview"},{"paperId":"0f16ba2afde6db0a71ea93f733ff393590eeb471","title":"MaintNorm: A corpus and benchmark model for lexical normalisation and masking of industrial maintenance short text"},{"paperId":"13be1e6782439e599ce3416c3bac29b13ec58ca4","title":"Post-OCR Correction of Digitized Swedish Newspapers with ByT5"},{"paperId":"15f0a97a0857ed8ac8df201bc9c1fe92d0bfd1e2","title":"Embible: Reconstruction of Ancient Hebrew and Aramaic Texts Using Transformers"},{"paperId":"0a152bd2b474e1ece1e2f0ea5fd27651cbcab77c","title":"Investigating the Potential of Task Arithmetic for Cross-Lingual Transfer"},{"paperId":"dbf84a9e3c75f6790f4ba8f25ac5c70282a6e427","title":"Where are we Still Split on Tokenization?"},{"paperId":"164575bf818b53d125c7eaf3b77f6ca62abb76e1","title":"Exploring Data Augmentation in Neural DRS-to-Text Generation"},{"paperId":"ed7b51e4a5c4835218f6697b280afb2849211939","title":"Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation"},{"paperId":"117a0c2c23a5a558cf3b39468d917a750bca720c","title":"Are Character-level Translations Worth the Wait? Comparing Character-and Subword-level Models for Machine Translation"},{"paperId":"8cc3c64e1aee320609b6b964a9a6f6f50177e20f","title":"Publish or Hold? Automatic Comment Moderation in Luxembourgish News Articles"},{"paperId":"2efefcb2d48eecf308f856cab48f23621e4a88f1","title":"Scaling Neural ITN for Numbers and Temporal Expressions in Tamil: Findings for an Agglutinative Low-resource Language"},{"paperId":"a92b837f2fd0ba023d5a927fde41d83b73191cf5","title":"Good Meta-tasks Make A Better Cross-lingual Meta-transfer Learning for Low-resource Languages"},{"paperId":"cfe7cb66390ea0b433383d498e6eff555a198c54","title":"Murreviikko - A Dialectologically Annotated and Normalized Dataset of Finnish Tweets"},{"paperId":"d53d4ff9f3bda51534184344845a78f8c02badd9","title":"DiatopIt: A Corpus of Social Media Posts for the Study of Diatopic Language Variation in Italy"},{"paperId":"5088cd22bbd7e1b798df39eb7f3c4ae305ab7625","title":"Findings from the Bambara - French Machine Translation Competition (BFMT 2023)"},{"paperId":"380605105531d27474190451183bf6ad6126cac8","title":"GPoeT: a Language Model Trained for Rhyme Generation on Synthetic Data"},{"paperId":"82e1313f28afde442930b94bc6ed582d17e8d4b3","title":"Generating Errors: OCR Post-Processing for Icelandic"},{"paperId":"13b8060acc3db1fc555f6e55368f6d02899a1698","title":"FairPrism: Evaluating Fairness-Related Harms in Text Generation"},{"paperId":"1d578d2bdb5c920b224cfca73868566731eaeebd","title":"Towards Analysis of Biblical Entities and Names using Deep Learning"},{"paperId":"035315281c72763a3e0956775732e64f5f193d82","title":"Natural Language Generation with Pixels"},{"paperId":"e5adb9bf3f5ed9c253f38949b22e86775dca443a","title":"Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning"},{"paperId":"c3904ef47bec4fc2bfd3d390370681c33542d62d","title":"Fast Whitespace Correction with Encoder-Only Transformers"},{"paperId":"ffa4ac4a51148208cadb4084dddab954e5f57400","title":"Resolving Elliptical Compounds in German Medical Text"},{"paperId":"88108f061379045c299d62f487694cb4e6d6d4ff","title":"Findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing"},{"paperId":"659be1ff350634f50cc066d258ee6a45e697e552","title":"SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing"},{"paperId":"4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations"},{"paperId":"8e410a836174847e3fc4e5993f625e89e34dd3d1","title":"RST Discourse Parsing as Text-to-Text Generation"},{"paperId":"17d2c990dd6d25f433b02ce611ce0a57db038dc5","title":"Jetsons at the FinNLP-2023: Using Synthetic Data and Transfer Learning for Multilingual ESG Issue Classification"},{"paperId":"f83740ef449e705187a0f7d6f76b819c99340bd1","title":"Exploring the Limits of Small Language Models"},{"paperId":"e4a1e9bb360f29aceb56079f52484c4a4de1298d","title":"Int√©gration du raisonnement num√©rique dans les mod√®les de langue : √âtat de l‚Äôart et direction de recherche"},{"paperId":"29e593736c95fd043b7cbb0211d2f7b5cb70e5bf","title":"LLI-UAM Team at FinancES 2023: Noise, Data Augmentation and Hallucinations"},{"paperId":"2b4369d50ac7310b9908a2baef89a63c68cbd893","title":"Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models"},{"paperId":"30869d285642a8b981702d2a54be0ac54f01aa01","title":"Composer's Assistant: Interactive Transformers for Multi-Track MIDI Infilling"},{"paperId":"ee26bb846f04240fe6e82170a4b15f3b8bd3a09e","title":"Designing Human-Centric Foundation Models"},{"paperId":"c6410d9791f390aa43ed26bf5567fdde6fc89f25","title":"Type Enhanced BERT for Correcting NER Errors"},{"paperId":"ce093816af5d8c1a7aab74808245048b7f3669a5","title":"The Helsinki-NLP Submissions at NADI 2023 Shared Task: Walking the Baseline"},{"paperId":"104974b36654acbc613226b9f70c4ad503655aa1","title":"The Linearity of the Effect of Surprisal on Reading Times across Languages"},{"paperId":"8a930572177545e7394ba5cd03e9342142da564e","title":"Better Quality Pre-training Data and T5 Models for African Languages"},{"paperId":"7ef91d8896c406865e91d90f60a8592a03ed6ded","title":"TRIP: Accelerating Document-level Multilingual Pre-training via Triangular Document-level Pre-training on Parallel Data Triplets"},{"paperId":"cbeb7f5050790d5abc8c617f6bfd70706c52ae0e","title":"Automatic Translation of Span-Prediction Datasets"},{"paperId":"64668e46d52302851d62afbc9779ce8c4d46c5eb","title":"Dialect-to-Standard Normalization: A Large-Scale Multilingual Evaluation"},{"paperId":"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","title":"On the Influence of Tokenizers in NLP Pipelines"},{"paperId":"9758782feed4b4b9bf0ec18b802462e8023a7f83","title":"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data"},{"paperId":"32290d428b50f64d1cfee6ea658dc6ba977b26e9","title":"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"e75037b638aca1dfc8a9b013bb7dcb8d19633986","title":"Examining Single Sentence Label Leakage in Natural Language Inference Datasets"},{"paperId":"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","title":"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference"},{"paperId":"2005311276a1a90dc17cf6e2ca7725fee2e76e1e","title":"BasqueGLUE: A Natural Language Understanding Benchmark for Basque"},{"paperId":"143659fcf93f33e9139f594cf8111c6bc85c04fa","title":"Overview of the EvaLatin 2022 Evaluation Campaign"},{"paperId":"6102fe88a512290b80e83ed2fe17606b166e505a","title":"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin"},{"paperId":"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","title":"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"4da5c8acd25a7dc7357d6070bb0a89bad187ebd8","title":"A Framework for Sexism Detection on Social Media via ByT5 and TabNet"},{"paperId":"038103632b24619818b159f5ca37b848744817db","title":"DENTRA: Denoising and Translation Pre-training for Multilingual Machine Translation"},{"paperId":"710ceab1ff91b96e8596b8400ebf2912ee6e2836","title":"Machine Translation for Multilingual Intent Detection and Slots Filling"},{"paperId":"3fa2e17332bb2888318f504cf37026001b932900","title":"Comparison of Token- and Character-Level Approaches to Restoration of Spaces, Punctuation, and Capitalization in Various Languages"},{"paperId":"c31da40e092e808f20f782eb1ee9d4dec6351708","title":"Overview of EXIST 2022: sEXism Identification in Social neTworks"},{"paperId":"2b6b38b66fcca218cb2f381e5d4711b5c99a784f","title":"Training Text-to-Text Transformers with Privacy Guarantees"},{"paperId":"5a6f6f44a2e05709d81245526786f8dc8f8ab263","title":"Relation Leakage in Elicited Natural Language Inference Datasets"},{"paperId":"de0e1f9980afa7949df64d53b8ae7a2f59c55579","title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages"},{"paperId":"7175caf7568d46c857380d0e5b64653819d5cc45","title":"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization"},{"paperId":"921c1216edbf6b2931b15874f24847ff1007ad8c","title":"EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"},{"paperId":"e873ddaf58ff92ae0492983cf57221fb25837f4e","title":"Strategies in subword tokenization: humans vs. algorithms"},{"paperId":"b8146800f8fd63c29e5f00b1e1b441da55f28c05","title":"Sentence-Level Discourse Parsing as Text-to-Text Generation"},{"paperId":"ae7fc32df9401a86353185515f4fd5e2020ac922","title":"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations"},{"paperId":"8496c62bb8ecc67dfb0959ef9321f875d5ff067b","title":"Phone-ing it in: Towards Flexible, Multi-Modal Language Model Training using Phonetic Representations of Data"},{"paperId":"103dc8d8572eadafa984e4b4c15855f9cdc754fb","title":"Few-shot Controllable Style Transfer for Low-Resource Multilinugal Settings"},{"paperId":"4c70347a3b9b5a27f4964c8703ae22d015bb49c1","title":"Machine Reading Comprehension: Generative or Extractive Reader?"},{"paperId":"f31a11257aad37847893b5495024865ca5f41ef9","title":"End-to-end style-conditioned poetry generation: What does it take to learn from examples alone?"},{"paperId":"28f68458f8c73da7ba1493e1a2c50862580fe0b4","title":"EncT5: Fine-tuning T5 Encoder for Discriminative Tasks"},{"paperId":"3c55d5e88c301237ecfb362cae5adcd87a1bc4af","title":"VU Research Portal Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"afbcc8ba3959f55dd5cc3b4d34c4238cc1622382","title":"Diacritization for the World‚Äôs Scripts"},{"paperId":"f8665a1a5dcf4c771c146edc67c353f007355911","title":"Leveraging Pre-training Models for Speech Processing"}],"references":[{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"2b66a0d8a00b0ba7b585e11ab34879420ad351cb","title":"Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution"},{"paperId":"824cd8db8a68732db04f4d8b7139eb4475e59ff2","title":"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":"e46c97b2d3bea4c21eb462d5771647a0de99b81c","title":"Ensemble Self-Training for Low-Resource Languages: Grapheme-to-Phoneme Conversion and Morphological Inflection"},{"paperId":"278f4f68309fc4ffb83bfc4ba86a0ea005106682","title":"The SIGMORPHON 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion"},{"paperId":"8c122ff82dedf72fa7ef5342622667bf5848916e","title":"One-Size-Fits-All Multilingual Models"},{"paperId":"b044395ed89de33b43d304c008d3c5a7727d423d","title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection"},{"paperId":"c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c","title":"Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"b00cda1bf34cc674676bcde38a46e8fe86d8b825","title":"TWEETQA: A Social Media Focused Question Answering Dataset"},{"paperId":"be564b861e5a9bf5d1dec531807e711000027380","title":"One Size Does Not Fit All: Comparing NMT Representations of Different Granularities"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"5e93a30656ef091f55ce42374d498f2b881e0c47","title":"Bytes Are All You Need: End-to-end Multilingual Speech Recognition and Synthesis with Bytes"},{"paperId":"1125d986433735ffc63dc0a8be466043e4f5c5b4","title":"Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"305b2cf37e5dece81e95c92883d5a6e28ac93b22","title":"Don‚Äôt Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"89cb259f39eb8350dd337fc1d02e2f4128c8398c","title":"Byte-based Neural Machine Translation"},{"paperId":"2397ce306e5d7f3d0492276e357fb1833536b5d8","title":"On the State of the Art of Evaluation in Neural Language Models"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"563783de03452683a9206e85fe6d661714436686","title":"HyperNetworks"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"7dba53e72c182e25e98e8f73a99d75ff69dda0c2","title":"Recurrent Highway Networks"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"4dabd6182ce2681c758f654561d351739e8df7bf","title":"Multilingual Language Processing From Bytes"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"93c20e38c85b69fc2d2eb314b3c1217913f7db11","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"d266d00cc0348bdd0692687befef68fc56433630","title":"Fifth Conference of the European Chapter of the Association for Computational Linguistics"},{"paperId":"b9f25555d550219b979fe12d409070a4798a02f9","title":"Louisiana"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":null,"title":"A√§ron van den Oord, Alex Graves, and Koray Kavukcuoglu"},{"paperId":null,"title":"Technologies, Volume 1 (Long and Short Papers) pages"}],"id":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","summary":"This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts."}]}
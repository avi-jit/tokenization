{"papers":[{"url":"https://www.semanticscholar.org/paper/23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels","venue":"International Conference on Learning Representations","year":2022,"referenceCount":144,"citationCount":24,"influentialCitationCount":3,"publicationDate":"14/07/2022","authors":"Phillip Rust,Jonas F. Lotz,Emanuele Bugliarello,Elizabeth Salesky,Miryam de Lhoneux,Desmond Elliott","id":"23f4b6432b74e5db05da04e354341807f5044f7e","summary":"PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels, and is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.","score":12},{"url":"https://www.semanticscholar.org/paper/a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":49,"citationCount":4,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Shaked Yehezkel,Yuval Pinter","id":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","summary":"SaGe, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase, is presented, showing its robustness to language properties such as morphological exponence and agglutination.","score":10},{"url":"https://www.semanticscholar.org/paper/a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/10/2023","authors":"Avijit Thawani,Saurabh Ghanekar,Xiaoyuan Zhu,Jay Pujara","id":"a401510c434b2274b299e9444085df0b18808aaa","summary":"This paper considers an alternative 'learn your tokens' scheme which utilizes the word boundary to pool bytes/characters into word representations, which are fed to the primary language model, before again decoding individual characters/bytes per word in parallel.","score":8},{"url":"https://www.semanticscholar.org/paper/64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2023","authors":"Mehdi Ali,Michael Fromm,Klaudia Thellmann,Richard Rutmann,Max LÃ¼bbering,Johannes Leveling,Katrin Klug,Jan Ebert,Niclas Doll,Jasper Schulze Buschhoff,Charvi Jain,Alexander Arno Weber,Lena Jurkschat,Hammam Abdelwahab,Chelsea John,Pedro Ortiz Suarez,Malte Ostendorff,Samuel Weinbach,R. Sifa,Stefan Kesselheim,Nicolas Flores-Herr","id":"64384525318f934fa085de86badc24403487d38a","summary":"A comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale finds that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model's downstream performance.","score":7},{"url":"https://www.semanticscholar.org/paper/3c55d5e88c301237ecfb362cae5adcd87a1bc4af","title":"VU Research Portal Analyzing Cognitive Plausibility of Subword Tokenization","venue":"","year":null,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Lisa Beinborn,Yuval Pinter","id":"3c55d5e88c301237ecfb362cae5adcd87a1bc4af","summary":"This work presents a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization, and analyzes the correlation of the tokenizer output with the response time and accuracy of human performance on a lexical decision task.","score":7},{"url":"https://www.semanticscholar.org/paper/17d0cc24fd6a267e7fd59ae62054de3e5c552096","title":"Analyzing Cognitive Plausibility of Subword Tokenization","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2023","authors":"Lisa Beinborn,Yuval Pinter","id":"17d0cc24fd6a267e7fd59ae62054de3e5c552096","summary":"This work presents a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization, and analyzes the correlation of the tokenizer output with the response time and accuracy of human performance on a lexical decision task.","score":7},{"url":"https://www.semanticscholar.org/paper/67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/02/2023","authors":"Du-Yeong Kim,Jeeeun Kim","id":"67c28d697460b684a0ba97d989719b4ed3c9cffc","summary":"Using this novel approach, the standard transformer architecture can be reused for all levels of language representations and be able to process much longer sequences at the same time-complexity without\"any\"architectural modification and additional overhead.","score":6},{"url":"https://www.semanticscholar.org/paper/997cebb936d88577da59ba460a9141bdd5dcb36f","title":"To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer","venue":"MRL","year":2023,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/10/2023","authors":"Md Mushfiqur Rahman,Fardin Ahsan Sakib,FAHIM FAISAL,Antonios Anastasopoulos","id":"997cebb936d88577da59ba460a9141bdd5dcb36f","summary":"A comparative analysis on language models having diverse text representation modalities reveals that image-based models excel in cross-lingual transfer when languages are closely related and share visually similar scripts, however, for tasks biased toward word meaning (POS, NER), segmentation- based models prove to be superior.","score":6},{"url":"https://www.semanticscholar.org/paper/4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations","venue":"CAWL","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Manex Agirrezabal,Sidsel Boldsen,Nora Hollenstein","id":"4044c062683533d92f598715afe2508be29c739c","summary":"The multilingual contextual CANINE model is probed, including Faroese as an additional zero-shot instance, and it is observed that some phonetic information is indeed encoded in the character representations, as consonants and vowels can be well distinguished using a linear classifier.","score":6},{"url":"https://www.semanticscholar.org/paper/2c8935ec872eca14636a090e5f6b49bc1c90c30d","title":"Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation","venue":"","year":2023,"referenceCount":47,"citationCount":2,"influentialCitationCount":0,"publicationDate":"28/02/2023","authors":"Lukas Edman,Gabriele Sarti,Antonio Toral,Gertjan van Noord,Arianna Bisazza","id":"2c8935ec872eca14636a090e5f6b49bc1c90c30d","summary":"This work performs an extensive comparison across multiple languages and experimental conditions of character- and subword-level pretrained models (ByT5 and mT5, respectively) on NMT, and shows the effectiveness of character-level modeling in translation, particularly in cases where fine-tuning data is limited.","score":6},{"url":"https://www.semanticscholar.org/paper/2b4369d50ac7310b9908a2baef89a63c68cbd893","title":"Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models","venue":"Recent Advances in Natural Language Processing","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Shun Kiyono,Sho Takase,Shengzhe Li,Toshinori Sato","id":"2b4369d50ac7310b9908a2baef89a63c68cbd893","summary":"The principle of the method is to apply the subword regularization technique to generate a mixture of subword- and character-level segmentation, which can halve the computational cost of pretraining.","score":6},{"url":"https://www.semanticscholar.org/paper/9a1a9ae2fc2911f1f275702bef38aa8f79c86986","title":"What do tokens know about their characters and how do they know it?","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":83,"citationCount":12,"influentialCitationCount":1,"publicationDate":"06/06/2022","authors":"Ayush Kaushal,Kyle Mahowald","id":"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","summary":"The mechanisms through which PLMs acquire English-language character information during training are investigated and it is argued that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.","score":6},{"url":"https://www.semanticscholar.org/paper/879a7f5abdb7ab803d48172d4f0830965f989d46","title":"Language Model Tokenizers Introduce Unfairness Between Languages","venue":"arXiv.org","year":2023,"referenceCount":118,"citationCount":14,"influentialCitationCount":1,"publicationDate":"17/05/2023","authors":"Aleksandar Petrov,Emanuele La Malfa,Philip H. S. Torr,Adel Bibi","id":"879a7f5abdb7ab803d48172d4f0830965f989d46","summary":"It is shown how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked, and the case is made that future language models should be trained using multilingually fair subword tokenizers.","score":6},{"url":"https://www.semanticscholar.org/paper/b3cff6abe401a244c21d4706b0931e48acaeeb4e","title":"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":69,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Benjamin Minixhofer,Jonas Pfeiffer,Ivan Vulic","id":"b3cff6abe401a244c21d4706b0931e48acaeeb4e","summary":"This work systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale and uses self-supervised models to leverage decompounding during the creation of a subword tokenizer, which it refers to as CompoundPiece.","score":6},{"url":"https://www.semanticscholar.org/paper/17fbffb05fa14e21d1c506fd5f0f568b955fe983","title":"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":82,"citationCount":9,"influentialCitationCount":1,"publicationDate":"23/05/2023","authors":"Orevaoghene Ahia,Sachin Kumar,Hila Gonen,Jungo Kasai,David R. Mortensen,Noah A. Smith,Yulia Tsvetkov","id":"17fbffb05fa14e21d1c506fd5f0f568b955fe983","summary":"This work conducts a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages and shows evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.","score":6},{"url":"https://www.semanticscholar.org/paper/d57cbec10dfee4e244657d3cdf9ba4cf53086744","title":"An Empirical Analysis Towards Replacing Vocabulary-Rigid Embeddings by a Vocabulary-Free Mechanism","venue":"LatinX in AI at International Conference on Machine Learning 2023","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/07/2023","authors":"Alejandro Rodriguez,Korn Sooksatra,Pablo Rivas,Ernesto Quevedo,Javier Turek,Gisela Bichler,Tomas Cerny,Laurie Giddens,S. Petter","id":"d57cbec10dfee4e244657d3cdf9ba4cf53086744","summary":"The research contributes techniques for re-training transformer embedding layers and provides insights into loss function selection and the findings have implications for developing flexible and robust NLPmodels.","score":6},{"url":"https://www.semanticscholar.org/paper/0232715f9089e3a2fc002cff6737bb9939805b8d","title":"Local Byte Fusion for Neural Machine Translation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Makesh Narsimhan Sreedhar,Xiangpeng Wan,Yu-Jie Cheng,Junjie Hu","id":"0232715f9089e3a2fc002cff6737bb9939805b8d","summary":"A Local Byte Fusion (LOBEF) method for byte-based machine translationâutilizing byte n-gram and word boundariesâto aggregate local semantic information to perform competitive to subword models.","score":5},{"url":"https://www.semanticscholar.org/paper/5079e188ae86d8330414edb65e898ed306ac450d","title":"Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/05/2023","authors":"Francois Meyer,Jan Buys","id":"5079e188ae86d8330414edb65e898ed306ac450d","summary":"SSMT unifies subword segmentation and MT in a single trainable model that learns to segment target sentence words while jointly learning to generate target sentences and proves more robust on a test set constructed for evaluating morphological compositional generalisation.","score":5},{"url":"https://www.semanticscholar.org/paper/29a54c072bcf85fa6934b6f701962a7c9aeb6489","title":"Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/05/2023","authors":"Svanhv'it Lilja Ing'olfsd'ottir,PÃ©tur Orri Ragnarsson,H. JÃ³nsson,Haukur Barri S'imonarson,VilhjÃ¡lmur Ãorsteinsson,VÃ©steinn SnÃ¦bjarnarson","id":"29a54c072bcf85fa6934b6f701962a7c9aeb6489","summary":"It is shown that a byte-level model enables higher correction quality than a subword approach, not only for simple spelling errors, but also for more complex semantic, stylistic and grammatical issues.","score":5},{"url":"https://www.semanticscholar.org/paper/117a0c2c23a5a558cf3b39468d917a750bca720c","title":"Are Character-level Translations Worth the Wait? Comparing Character-and Subword-level Models for Machine Translation","venue":"","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Lukas Edman,Gabriele Sarti,Antonio Toral,Gertjan van Noord,Arianna Bisazza","id":"117a0c2c23a5a558cf3b39468d917a750bca720c","summary":"This work performs an extensive comparison across multiple languages and experimental conditions of state-of-the-art character-and subword-level pre-trained models (ByT5 and mT5, respectively) on NMT, showing the effectiveness of character-level modeling in translation, particularly in cases where training data is limited.","score":5},{"url":"https://www.semanticscholar.org/paper/ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only","venue":"Computer Vision and Pattern Recognition","year":2022,"referenceCount":87,"citationCount":21,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"M. Tschannen,Basil Mustafa,N. Houlsby","id":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","summary":"The fact that CLIPPO does not require a tokenizer is exploited to show that it can achieve strong performance on multilingual multimodal retrieval without modifications, and it can obtain good accuracy in visual question answering, simply by rendering the question and image together.","score":5},{"url":"https://www.semanticscholar.org/paper/f8ef90a8cac1a8edf9477688aac24864c547d6cd","title":"Character-Aware Models Improve Visual Text Rendering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":182,"citationCount":25,"influentialCitationCount":6,"publicationDate":"20/12/2022","authors":"Rosanne Liu,Daniel H Garrette,Chitwan Saharia,William Chan,Adam Roberts,Sharan Narang,Irina Blok,R. Mical,Mohammad Norouzi,Noah Constant","id":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","summary":"This paper trains a suite of image generation models, and shows that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (the authors' DrawText benchmark), and sets a much higher state-of-the-art on visual spelling.","score":5},{"url":"https://www.semanticscholar.org/paper/5bd254c0775d18cdc30cb85be61e080484ee6713","title":"Multilingual Text Representation","venue":"arXiv.org","year":2023,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/09/2023","authors":"FAHIM FAISAL","id":"5bd254c0775d18cdc30cb85be61e080484ee6713","summary":"Light is shed on this iterative progression of multilingual text representation and the driving factors that ultimately led to the current state-of-the-art, and how the full potential of language democratization could be obtained is discussed.","score":5},{"url":"https://www.semanticscholar.org/paper/d921732ca5920049b6a8194f559917689f02d096","title":"Text Rendering Strategies for Pixel Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/11/2023","authors":"Jonas F. Lotz,Elizabeth Salesky,Phillip Rust,Desmond Elliott","id":"d921732ca5920049b6a8194f559917689f02d096","summary":"It is found that simple character bigram rendering brings improved performance on sentence-level tasks without compromising performance on token-level or multilingual tasks, and makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model.","score":5},{"url":"https://www.semanticscholar.org/paper/9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation","venue":"Special Interest Group on Computational Morphology and Phonology Workshop","year":2022,"referenceCount":81,"citationCount":19,"influentialCitationCount":2,"publicationDate":"15/06/2022","authors":"Khuyagbaatar Batsuren,GÃ¡bor Bella,Aryaman Arora,Viktor Martinovi'c,Kyle Gorman,Zdenvek vZabokrtsk'y,A. Ganbold,vS'arka Dohnalov'a,Magda vSevvc'ikov'a,Katevrina Pelegrinov'a,Fausto Giunchiglia,Ryan Cotterell,Ekaterina Vylomova","id":"9ccad0208b50042d8378b77700146a98bd22ea3f","summary":"To facilitate error analysis and support any type of future studies, the SIGMORPHON 2022 shared task on morpheme segmentation released all system predictions, the evaluation script, and all gold standard datasets.","score":5},{"url":"https://www.semanticscholar.org/paper/0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!","venue":"EUSFLAT/AGOP","year":2023,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/03/2023","authors":"Christos Theodoropoulos,Marie-Francine Moens","id":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","summary":"The main outcome is twofold: tokenization patterns can introduce inductive bias that results in state-of-the-art performance, and the character-based models produce promising results; thus, transitioning to token-free IE models is feasible.","score":5},{"url":"https://www.semanticscholar.org/paper/d7a379254ac2dee2e31dfdeedb440176f0285aa5","title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":54,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Li Sun,F. Luisier,K. Batmanghelich,D. FlorÃªncio,Changrong Zhang","id":"d7a379254ac2dee2e31dfdeedb440176f0285aa5","summary":"This work introduces a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level, and demonstrates that this hierarchical model is robust to textual corruption and domain shift.","score":5},{"url":"https://www.semanticscholar.org/paper/035315281c72763a3e0956775732e64f5f193d82","title":"Natural Language Generation with Pixels","venue":"","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"","id":"035315281c72763a3e0956775732e64f5f193d82","summary":"A conditional diffusion-based decoder for modeling rendered natural language that is capable of generating coherent, plausible natural language rendered as images and compared with strong transformer-based autoregressive baselines in both the unconditional and sequence-to-sequence setting.","score":4},{"url":"https://www.semanticscholar.org/paper/afbcc8ba3959f55dd5cc3b4d34c4238cc1622382","title":"Diacritization for the Worldâs Scripts","venue":"","year":null,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Kyle Gorman,Yuval Pinter,Michael Elhadad","id":"afbcc8ba3959f55dd5cc3b4d34c4238cc1622382","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad","title":"Sub-Character Tokenization for Chinese Pretrained Language Models","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":61,"citationCount":3,"influentialCitationCount":0,"publicationDate":"01/06/2021","authors":"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Yasheng Wang,Qun Liu,Maosong Sun","id":"5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad","summary":"Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency, and 2) Pronunciation-based SubChartokenization can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to homophone typos.","score":4},{"url":"https://www.semanticscholar.org/paper/0de580957d23dd65e31b6c95e6bc5d15bc15c57d","title":"Impact of Tokenization on Language Models: An Analysis for Turkish","venue":"ACM Trans. Asian Low Resour. Lang. Inf. Process.","year":2022,"referenceCount":68,"citationCount":23,"influentialCitationCount":3,"publicationDate":"19/04/2022","authors":"Cagri Toraman,E. Yilmaz,Furkan ÅahinuÃ§,Oguzhan Ozcelik","id":"0de580957d23dd65e31b6c95e6bc5d15bc15c57d","summary":"This work compares five tokenizers at different granularity levels, that is, their outputs vary from the smallest pieces of characters to the surface form of words, and finds that increasing the vocabulary size improves the performance of Morphological- and Word-level tokenizers more than that of de facto tokenizers.","score":4},{"url":"https://www.semanticscholar.org/paper/c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":80,"citationCount":76,"influentialCitationCount":5,"publicationDate":"12/05/2022","authors":"Jonas Pfeiffer,Naman Goyal,Xi Victoria Lin,Xian Li,James Cross,Sebastian Riedel,Mikel Artetxe","id":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","summary":"This work introduces language-specific modules of their Cross-lingual Modular models from the start, which allows them to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant.","score":4},{"url":"https://www.semanticscholar.org/paper/7cdebb73662387d9040da4f27a7dc04dbffa3c3e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":79,"citationCount":8,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Jonas Belouadi,Steffen Eger","id":"7cdebb73662387d9040da4f27a7dc04dbffa3c3e","summary":"This work successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with the authors' styles, and shows that ByG PT5 outperforms other models such as mT5, ByT4, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans.","score":4},{"url":"https://www.semanticscholar.org/paper/5f3d6c077712a2d692bb29671c084e098854c738","title":"Learning Mutually Informed Representations for Characters and Subwords","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2023","authors":"Yilin Wang,Xinyi Hu,Matthew R. Gormley","id":"5f3d6c077712a2d692bb29671c084e098854c738","summary":"The entanglement model is introduced, aiming to combine character and subword language models, Inspired by vision-language models, and it generates mutually informed representations for both granularities as output.","score":4},{"url":"https://www.semanticscholar.org/paper/d9655aad891e0dba404bc132c8419f93c098c8f8","title":"Learning to Abstract with Nonparametric Variational Information Bottleneck","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/10/2023","authors":"Melika Behjati,Fabio Fehr,James Henderson","id":"d9655aad891e0dba404bc132c8419f93c098c8f8","summary":"This work applies Nonparametric Variational Information Bottleneck (NVIB) to stacked Transformer self-attention layers in the encoder, which encourages an information-theoretic compression of the representations through the model and finds that the layers within the model correspond to increasing levels of abstraction and that their representations are more linguistically informed.","score":4},{"url":"https://www.semanticscholar.org/paper/ed7b51e4a5c4835218f6697b280afb2849211939","title":"Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation","venue":"arXiv.org","year":2023,"referenceCount":28,"citationCount":2,"influentialCitationCount":0,"publicationDate":2023,"authors":"Lukas Edman,Antonio Toral,Gertjan van Noord","id":"ed7b51e4a5c4835218f6697b280afb2849211939","summary":"An extensive comparison across multiple languages and experimental conditions of state-of-the-art character- and subword-level pre-trained models on NMT shows that the former not only are effective in translation, but frequently outperform subword models, particularly in cases where training data is limited.","score":4},{"url":"https://www.semanticscholar.org/paper/7e15af5f6b3a794167bc4dc194768b348becc5d3","title":"Improving Tokenisation by Alternative Treatment of Spaces","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":45,"citationCount":6,"influentialCitationCount":2,"publicationDate":"08/04/2022","authors":"Edward Gow-Smith,Harish Tayyar Madabushi,Carolina Scarton,Aline Villavicencio","id":"7e15af5f6b3a794167bc4dc194768b348becc5d3","summary":"This work experiments with an alternative tokenisation approach where spaces are always treated as individual tokens, and finds this modification to the BPE and Unigram algorithms lead to improved performance on downstream NLP tasks that involve handling complex words, whilst having no detrimental effect on performance in general natural language understanding tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/ea8197cb357af6f89e8b6e5548897236a24719b1","title":"What is the best recipe for character-level encoder-only modelling?","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/05/2023","authors":"Kris Cao","id":"ea8197cb357af6f89e8b6e5548897236a24719b1","summary":"This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level, and finds that the best performing character-level model exceeds the performance of a token-based model trained with the same settings on the same data.","score":4},{"url":"https://www.semanticscholar.org/paper/c60736e61f8961ec535ecfdc6f0398925d34d0b8","title":"Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Elizabeth Salesky,Neha Verma,Philipp Koehn,Matt Post","id":"c60736e61f8961ec535ecfdc6f0398925d34d0b8","summary":"This work introduces and demonstrates how to effectively train multilingual machine translation models with pixel representations and explores various properties of pixel representations such as parameter sharing within and across scripts to better understand where they lead to positive transfer.","score":4},{"url":"https://www.semanticscholar.org/paper/5b0c0d181f31acebf2442e7b7c7af27f3f6a5f4a","title":"Hints on the data for language modeling of synthetic languages with transformers","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Rodolfo Zevallos,NÃºria Bel","id":"5b0c0d181f31acebf2442e7b7c7af27f3f6a5f4a","summary":"To what extent the critical amount of data varies for languages of different morphological typology, in particular those that have a rich inflectional morphology, and whether the tokenization method to preprocess the data can make a difference, is addressed.","score":4},{"url":"https://www.semanticscholar.org/paper/d77885e0478c318df7a271674dce04349601e80f","title":"Morpheme-Based Neural Machine Translation Models for Low-Resource Fusion Languages","venue":"ACM Trans. Asian Low Resour. Lang. Inf. Process.","year":2023,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/07/2023","authors":"A. Gezmu,A. NÃ¼rnberger","id":"d77885e0478c318df7a271674dce04349601e80f","summary":"A word segmentation method is resorted to that segments words by restoring the altered morphemes in fusion languages, and it is proved that morpheme-based models outperform conventional subword models on a benchmark dataset.","score":4},{"url":"https://www.semanticscholar.org/paper/e4d2ba4cb6f364079ce3731e86b860264677ae7d","title":"Downstream Task-Oriented Neural Tokenizer Optimization with Vocabulary Restriction as Post Processing","venue":"arXiv.org","year":2023,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/04/2023","authors":"Tatsuya Hiraoka,Tomoya Iwakura","id":"e4d2ba4cb6f364079ce3731e86b860264677ae7d","summary":"An example of the BiLSTM-based tokenizer with vocabulary restriction is proposed, which can capture wider contextual information for the tokenization process than non-neural-basedtokenization methods used in existing work.","score":3},{"url":"https://www.semanticscholar.org/paper/e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization","venue":"International Conference on Learning Representations","year":2021,"referenceCount":71,"citationCount":103,"influentialCitationCount":17,"publicationDate":"23/06/2021","authors":"Yi Tay,Vinh Q. Tran,Sebastian Ruder,Jai Gupta,Hyung Won Chung,Dara Bahri,Zhen Qin,Simon Baumgartner,Cong Yu,Donald Metzler","id":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","summary":"A soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion is introduced that paves the way for highly performant token-free models that are trained completely end-to-end.","score":3},{"url":"https://www.semanticscholar.org/paper/3e08d84af266026e7d254729f3afc6dcd572d264","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks","venue":"Workshop on Representation Learning for NLP","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Gregor Geigle,Chen Cecilia Liu,Jonas Pfeiffer,Iryna Gurevych","id":"3e08d84af266026e7d254729f3afc6dcd572d264","summary":"This work exhaustively experiments with three popular VEs on six downstream V+L tasks and suggests that diverse VEs complement each other, resulting in improved downstream V-L task performance, where the improvements are not due to simple ensemble effects.","score":3},{"url":"https://www.semanticscholar.org/paper/62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c","title":"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":37,"citationCount":25,"influentialCitationCount":6,"publicationDate":"25/01/2023","authors":"Davis Liang,Hila Gonen,Yuning Mao,Rui Hou,Naman Goyal,Marjan Ghazvininejad,Luke Zettlemoyer,Madian Khabsa","id":"62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c","summary":"A new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language is introduced.","score":3},{"url":"https://www.semanticscholar.org/paper/3d3f8399d625238fddb366697acb73446129d65c","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":90,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Abbas Ghaddar,Yimeng Wu,Sunyam Bagga,Ahmad Rashid,Khalil Bibi,Mehdi Rezagholizadeh,Chao Xing,Yasheng Wang,Xinyu Duan,Zhefeng Wang,Baoxing Huai,Xin Jiang,Qun Liu,P. Langlais","id":"3d3f8399d625238fddb366697acb73446129d65c","summary":"This work addresses two major problems in existing Arabic PLMs that limit the progress of the Arabic NLU and NLG fields and releases three new Arabic BERT-style models, and achieves a new state-of-the-art performance on discriminative and generative ArabicNLU andNLG tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation","venue":"arXiv.org","year":2021,"referenceCount":93,"citationCount":5,"influentialCitationCount":0,"publicationDate":"10/09/2021","authors":"Yuval Pinter","id":"d87647784c12517d31964cc508d5b8423cc24f50","summary":"A survey of the distributional, compositional, and relational approaches to addressing the problem of representing the atomic elements of language in modern neural learning systems is presented, with special emphasis on the word level and the out-of-vocabulary phenomenon.","score":3},{"url":"https://www.semanticscholar.org/paper/24fbe3af030f393e55bda3dc7dae0f57bd270d04","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding","venue":"arXiv.org","year":2022,"referenceCount":83,"citationCount":3,"influentialCitationCount":1,"publicationDate":"21/05/2022","authors":"Abbas Ghaddar,Yimeng Wu,Sunyam Bagga,Ahmad Rashid,Khalil Bibi,Mehdi Rezagholizadeh,Chao Xing,Yasheng Wang,Duan Xinyu,Zhefeng Wang,Baoxing Huai,Xin Jiang,Qun Liu,P. Langlais","id":"24fbe3af030f393e55bda3dc7dae0f57bd270d04","summary":"This work revisits both the pre-training and evaluation of existing Arabic PLMs and releases three new Arabic BERT-style models, and achieves a new state-of-the-art performance on discriminative and generative Arabic NLU and NLG tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/36e1b3973c673d3c323014a168e05c5762f84262","title":"OneCAD: One Classifier for All image Datasets using multimodal learning","venue":"arXiv.org","year":2023,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/05/2023","authors":"S. Wadekar,E. Culurciello","id":"36e1b3973c673d3c323014a168e05c5762f84262","summary":"This work addresses the question: Is it possible to create a number-of-class-agnostic model architecture and proposes a training and inference framework OneCAD (One Classifier for All image Datasets) to achieve close-to number- of-class,agnostic transformer model.","score":3},{"url":"https://www.semanticscholar.org/paper/491eaab2ff93772d52be8c014bf97466d8c6fe73","title":"Tokenization Tractability for Human and Machine Learning Model: An Annotation Study","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/04/2023","authors":"Tatsuya Hiraoka,Tomoya Iwakura","id":"491eaab2ff93772d52be8c014bf97466d8c6fe73","summary":"A quantitative investigation result is provided that shows the tractable tokenizations for humans and machine learning models are not necessarily the same as each other.","score":3},{"url":"https://www.semanticscholar.org/paper/7dd4b7e6f5f1588eae707df8334589ffd503bc54","title":"A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/10/2023","authors":"Yi Zhou,JosÃ© Camacho-Collados,D. Bollegala","id":"7dd4b7e6f5f1588eae707df8334589ffd503bc54","summary":"A comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages sheds light on important factors often neglected in prior literature, such as tokenization or model objectives.","score":3},{"url":"https://www.semanticscholar.org/paper/e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order","venue":"arXiv.org","year":2021,"referenceCount":70,"citationCount":10,"influentialCitationCount":1,"publicationDate":2021,"authors":"Louis ClouÃ¢tre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","summary":"Investigating the insensitivity of natural language models to word-order by quantifying perturbations and analysing their effect on neural modelsâ performance on language understanding tasks in GLUE benchmark finds that neural language models â pretrained and non-pretrained Transformers, LSTMs, and Convolutional architectures â require local ordering more so than the global ordering of tokens.","score":3},{"url":"https://www.semanticscholar.org/paper/d53d4ff9f3bda51534184344845a78f8c02badd9","title":"DiatopIt: A Corpus of Social Media Posts for the Study of Diatopic Language Variation in Italy","venue":"Workshop on NLP for Similar Languages, Varieties and Dialects","year":2023,"referenceCount":31,"citationCount":11,"influentialCitationCount":2,"publicationDate":2023,"authors":"Alan Ramponi,Camilla Casula","id":"d53d4ff9f3bda51534184344845a78f8c02badd9","summary":"The first corpus specifically focused on diatopic language variation in Italy for language varieties other than Standard Italian is introduced, and the representativeness of DiatopIt is assessed, and it is shown that the density of non-Standard Italian content across areas correlates with actual language use.","score":3},{"url":"https://www.semanticscholar.org/paper/91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU","venue":"Findings","year":2021,"referenceCount":74,"citationCount":8,"influentialCitationCount":0,"publicationDate":"29/07/2021","authors":"Louis ClouÃ¢tre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","summary":"It is empirically shown that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.","score":3},{"url":"https://www.semanticscholar.org/paper/9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs","venue":"International Conference on Learning Representations","year":2021,"referenceCount":103,"citationCount":363,"influentialCitationCount":45,"publicationDate":"30/07/2021","authors":"Andrew Jaegle,Sebastian Borgeaud,Jean-Baptiste Alayrac,Carl Doersch,Catalin Ionescu,David Ding,Skanda Koppula,Andrew Brock,Evan Shelhamer,Olivier J. H'enaff,M. Botvinick,Andrew Zisserman,O. Vinyals,JoÃ£o Carreira","id":"9933a5af7895354087baf6c96b64dc8a8973eaed","summary":"This work proposes Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs and augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering.","score":3},{"url":"https://www.semanticscholar.org/paper/231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models","venue":"NAACL-HLT","year":2021,"referenceCount":39,"citationCount":29,"influentialCitationCount":2,"publicationDate":"26/10/2021","authors":"Piotr Nawrot,Szymon Tworkowski,MichaÅ Tyrolski,Lukasz Kaiser,Yuhuai Wu,Christian Szegedy,H. Michalewski","id":"231e768f0cd280faa0f725bb353262cb4fed08d1","summary":"Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.","score":3},{"url":"https://www.semanticscholar.org/paper/54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/08/2022","authors":"Jan Jezabek,A. Singh","id":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","summary":"A novel method of retroactively adding resilience to misspellings to transformer-based NLP models without the need for re-training of the original NLP model is proposed, which significantly reduces the cost needed to evaluate a modelâs resilience to adversarial attacks.","score":3},{"url":"https://www.semanticscholar.org/paper/7bd8859b5920c7b769e6d40dbdbcd857c1770401","title":"Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":42,"citationCount":3,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Asa Cooper Stickland,Sailik Sengupta,Jason Krone,Saab Mansour,He He","id":"7bd8859b5920c7b769e6d40dbdbcd857c1770401","summary":"After investigating several ways to boost the robustness of multilingual models in this setting, this work proposes Robust Contrastive Pretraining (RCP), which combines data augmentation with a contrastive loss term at the pretraining stage and achieves large improvements on noisy data.","score":3},{"url":"https://www.semanticscholar.org/paper/5e52d654fd31f04c1bd884cd5480e6af8c95ad50","title":"Efficient Transformers with Dynamic Token Pooling","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":55,"citationCount":12,"influentialCitationCount":1,"publicationDate":"17/11/2022","authors":"Piotr Nawrot,J. Chorowski,Adrian La'ncucki,E. Ponti","id":"5e52d654fd31f04c1bd884cd5480e6af8c95ad50","summary":"The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget.","score":3},{"url":"https://www.semanticscholar.org/paper/0a438980ac42451d6d32dd2ad8ead7b55520408d","title":"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning","venue":"Inf.","year":2023,"referenceCount":129,"citationCount":6,"influentialCitationCount":0,"publicationDate":"16/03/2023","authors":"Evans Kotei,Ramkumar Thirunavukarasu","id":"0a438980ac42451d6d32dd2ad8ead7b55520408d","summary":"A comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks is given and future directions to further improvement in pretrained transformer-based language models are presented.","score":3},{"url":"https://www.semanticscholar.org/paper/36dfcdc43664f03b15e5e03373a9d46728672e28","title":"mPLM-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models","venue":"","year":2023,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Peiqin Lin,Chengzhi Hu,Zheyu Zhang,AndrÃ© F. T. Martins,Hinrich SchÃ¼tze","id":"36dfcdc43664f03b15e5e03373a9d46728672e28","summary":"This study proposes mPLMSim, a language similarity measure that induces the similarities across languages from mPLMs using multi-parallel corpora and shows that mPLM-Sim exhibits moderately high correlations with linguistic similarity measures, such as lexicostatistics, genealogical language family, and geographical sprachbund.","score":3},{"url":"https://www.semanticscholar.org/paper/ec18aef9ccec70b979d6ab3c78d3721736fd5388","title":"Whereâs the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":62,"citationCount":2,"influentialCitationCount":0,"publicationDate":"30/05/2023","authors":"Benjamin Minixhofer,Jonas Pfeiffer,Ivan Vulic","id":"ec18aef9ccec70b979d6ab3c78d3721736fd5388","summary":"This work introduces a multilingual punctuation-agnostic sentence segmentation method, currently covering 85 languages, trained in a self-supervised fashion on unsegmented text, by making use of newline characters which implicitly perform segmentation into paragraphs.","score":3},{"url":"https://www.semanticscholar.org/paper/1943144f998c2620616b3e89d376ff801b6e04c6","title":"Lightweight Adaptation of Neural Language Models via Subspace Embedding","venue":"International Conference on Information and Knowledge Management","year":2023,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/08/2023","authors":"Amit Kumar Jaiswal,Haiming Liu","id":"1943144f998c2620616b3e89d376ff801b6e04c6","summary":"A new compact embedding structure is presented to reduce the memory footprint of the pre-trained language models with a sacrifice of up to 4% absolute accuracy and is evaluated on similarity and textual entailment tasks, sentence and paraphrase tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/fd665a3d8c4a5fde1e1b00f78ab231d99b22abd0","title":"Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew","venue":"arXiv.org","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2023","authors":"Eylon Gueta,Omer Goldman,Reut Tsarfaty","id":"fd665a3d8c4a5fde1e1b00f78ab231d99b22abd0","summary":"This work investigates the hypothesis that incorporating explicit morphological knowledge in the pre-training phase can improve the performance of PLMs for MRLs, and proposes various morphologically driven tokenization methods enabling the model to leverage morphological cues beyond raw text.","score":3},{"url":"https://www.semanticscholar.org/paper/5adc06cf3fc8ece1c937117ebf5c13d384ecb8a3","title":"FREED++: Improving RL Agents for Fragment-Based Molecule Generation by Thorough Reproduction","venue":"","year":2024,"referenceCount":99,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/01/2024","authors":"Alexander Telepov,Artem Tsypin,Kuzma Khrabrov,Sergey Yakukhnov,Pavel Strashnov,Petr Zhilyaev,Egor Rumiantsev,Daniel Ezhov,Manvel Avetisian,Olga Popova,Artur Kadurin","id":"5adc06cf3fc8ece1c937117ebf5c13d384ecb8a3","summary":"It is shown that the resulting fixed model is capable of producing molecules with superior docking scores compared to alternative approaches, and conducted an accurate comparison with current state-of-the-art methods for protein-conditioned molecule generation.","score":3},{"url":"https://www.semanticscholar.org/paper/ff50383c09346e3e7d16856af3519fa09a9f8580","title":"Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/09/2023","authors":"Miles Williams,Nikolaos Aletras","id":"ff50383c09346e3e7d16856af3519fa09a9f8580","summary":"This work demonstrates that a significant proportion of the vocabulary remains unused in pre-trained language models deployed in memory-constrained settings, and proposes a simple yet effective approach that leverages this finding to minimize the memory footprint of the embedding matrix.","score":3},{"url":"https://www.semanticscholar.org/paper/c30deb8f4b0e942a36e5ef5c87bed01f575c0c6e","title":"Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":89,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2023","authors":"Huiyin Xue,Nikolaos Aletras","id":"c30deb8f4b0e942a36e5ef5c87bed01f575c0c6e","summary":"This work proposes an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head, and empirically demonstrates that this MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/2dce73e4a3e19d71249fc7a53c2a9531daaff839","title":"Inducing Meaningful Units from Character Sequences with Dynamic Capacity Slot Attention","venue":"","year":2021,"referenceCount":71,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/02/2021","authors":"Melika Behjati,James Henderson","id":"2dce73e4a3e19d71249fc7a53c2a9531daaff839","summary":"This work proposes an unsupervised distributional method to learn the abstract meaningful units in a sequence of characters and discovers continuous representations of the objects in the sequence, extending an architecture for object discovery in images.","score":3},{"url":"https://www.semanticscholar.org/paper/0842aba60d8fc1fc61ad95df34872c17bcfcc123","title":"Effects of sub-word segmentation on performance of transformer language models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/05/2023","authors":"Jue Hou,Anisia Katinskaia,Anh Vu,R. Yangarber","id":"0842aba60d8fc1fc61ad95df34872c17bcfcc123","summary":"This paper compares GPT and BERT models trained with the statistical segmentation algorithm BPE vs. two unsupervised algorithms for morphological segmentation -- Morfessor and StateMorph and shows that LMs of smaller size using morphological segmentsation can perform comparably to models of larger size trained with BPE.","score":3},{"url":"https://www.semanticscholar.org/paper/7a25155364476839b6d1fc0653cd8611327ab9ba","title":"mGPT: Few-Shot Learners Go Multilingual","venue":"Transactions of the Association for Computational Linguistics","year":2022,"referenceCount":114,"citationCount":62,"influentialCitationCount":9,"publicationDate":"15/04/2022","authors":"Oleh Shliazhko,Alena Fenogenova,M. Tikhonova,V. Mikhailov,A. Kozlova,Tatiana Shavrina","id":"7a25155364476839b6d1fc0653cd8611327ab9ba","summary":null,"score":3},{"url":"https://www.semanticscholar.org/paper/5917ba80a905aeba41487f7dcdb8b885f18689f5","title":"Tokenization and the Noiseless Channel","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":46,"citationCount":5,"influentialCitationCount":0,"publicationDate":"29/06/2023","authors":"VilÃ©m Zouhar,Clara Meister,Juan Luis Gastaldi,Li Du,Mrinmaya Sachan,Ryan Cotterell","id":"5917ba80a905aeba41487f7dcdb8b885f18689f5","summary":"It is proposed that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution.","score":3},{"url":"https://www.semanticscholar.org/paper/ca7636c1d3426dd09a185abf25f81f7a8fa594e1","title":"How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/06/2023","authors":"T. Fujii,Koki Shibata,Atsuki Yamaguchi,Terufumi Morishita,Yasuhiro Sogawa","id":"ca7636c1d3426dd09a185abf25f81f7a8fa594e1","summary":null,"score":3},{"url":"https://www.semanticscholar.org/paper/cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":76,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sidsel Boldsen,Manex Agirrezabal,Nora Hollenstein","id":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","summary":"This cross-lingual analysis shows that textual character representations correlate strongly with sound representations for languages using an alphabetic script, while shape correlates with featural scripts.","score":3},{"url":"https://www.semanticscholar.org/paper/14efaa989b8db89ad907e6a15253d482cbba77d9","title":"Linguistically inspired roadmap for building biologically reliable protein language models","venue":"Nature Machine Intelligence","year":2022,"referenceCount":134,"citationCount":14,"influentialCitationCount":0,"publicationDate":"03/07/2022","authors":"Mai Ha Vu,R. Akbar,Philippe A. Robert,B. Swiatczak,G. K. Sandve,V. Greiff,Dag Trygve Tryslew Haug","id":"14efaa989b8db89ad907e6a15253d482cbba77d9","summary":"It is argued that guidance drawn from linguistics, a field specialized in analytical rule extraction from natural language data, can aid with building more interpretable protein LMs that are more likely to learn relevant domain-specific rules.","score":3},{"url":"https://www.semanticscholar.org/paper/77221764ce0eb68d29e1fc35ae61a21179e7dbe5","title":"What changes when you randomly choose BPE merge operations? Not much.","venue":"First Workshop on Insights from Negative Results in NLP","year":2023,"referenceCount":22,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/05/2023","authors":"Jonne Saleva,Constantine Lignos","id":"77221764ce0eb68d29e1fc35ae61a21179e7dbe5","summary":"Two simple randomized variants of byte pair encoding are introduced and whether randomizing the selection of merge operations substantially affects a downstream machine translation task is explored, hypothesizing that this task may show sensitivity to the method of choosing subwords.","score":3},{"url":"https://www.semanticscholar.org/paper/3a766d97276f394c455a51c5a5f7f25c7ff7f338","title":"Are you talking to ['xem'] or ['x', 'em']? On Tokenization and Addressing Misgendering in LLMs with Pronoun Tokenization Parity","venue":"arXiv.org","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2023","authors":"Anaelia Ovalle,Ninareh Mehrabi,Palash Goyal,J. Dhamala,Kai-Wei Chang,Richard Zemel,A. Galstyan,Yuval Pinter,Rahul Gupta","id":"3a766d97276f394c455a51c5a5f7f25c7ff7f338","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation","venue":"arXiv.org","year":2022,"referenceCount":43,"citationCount":27,"influentialCitationCount":3,"publicationDate":"07/03/2022","authors":"Gabriele Sarti,M. Nissim","id":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","summary":"IT5 is introduced, the first family of encoder-decoder transformer models pretrained specifically on Italian, with the monolingual IT5 models consistently outperforming their multilingual counterparts and setting a new state-of-the-art for most Italian conditional language generation tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","title":"MonoByte: A Pool of Monolingual Byte-level Language Models","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/09/2022","authors":"Hugo Abonizio,Leandro Rodrigues de Souza,R. Lotufo,Rodrigo Nogueira","id":"bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","summary":"Experiments on QA and NLI tasks show that monolingual models achieve competitive performance to the multilingual one, and hence can be served to strengthen the understanding of cross-lingual transferability in language models.","score":2},{"url":"https://www.semanticscholar.org/paper/ece1c04e9dcf89cbc6170fcccdec8412d11503d8","title":"A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation","venue":"arXiv.org","year":2024,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/01/2024","authors":"Jimin Hong,Gibbeum Lee,Jaewoong Cho","id":"ece1c04e9dcf89cbc6170fcccdec8412d11503d8","summary":"A novel framework is introduced designed to expedite text generation in non-roman alphabetic languages and predicts larger linguistic units than those of conventional multilingual tokenizers and is specifically tailored to the target language, thereby reducing the number of decoding steps required.","score":2},{"url":"https://www.semanticscholar.org/paper/d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","title":"A Primer on Pretrained Multilingual Language Models","venue":"arXiv.org","year":2021,"referenceCount":141,"citationCount":44,"influentialCitationCount":3,"publicationDate":"01/07/2021","authors":"Sumanth Doddapaneni,Gowtham Ramesh,Anoop Kunchukuttan,Pratyush Kumar,Mitesh M. Khapra","id":"d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","summary":"A review of the existing literature covering the above broad areas of research pertaining toMultilingual Language Models and some promising directions of future research are recommended.","score":2},{"url":"https://www.semanticscholar.org/paper/61c7e5590064b250ebb5a53c430aede492a4d8ab","title":"BERT for Natural Language Processing in Bahasa Indonesia","venue":"2022 2nd International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA)","year":2022,"referenceCount":65,"citationCount":3,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Danny Sebastian,H. Purnomo,I. Sembiring","id":"61c7e5590064b250ebb5a53c430aede492a4d8ab","summary":"The findings in this article can be used as ideas for developing NLP using the BERT model in Indonesian and the downstream task of the Indonesian BERT models are sentiment analysis, classification, and text summarization.","score":2},{"url":"https://www.semanticscholar.org/paper/15d12739b26783e2cf38bc0cbd81557fcc078eb0","title":"SabiÃ¡: Portuguese Large Language Models","venue":"Brazilian Conference on Intelligent Systems","year":2023,"referenceCount":88,"citationCount":7,"influentialCitationCount":1,"publicationDate":"16/04/2023","authors":"Ramon Pires,Hugo Abonizio,Thales Rog'erio,Rodrigo Nogueira","id":"15d12739b26783e2cf38bc0cbd81557fcc078eb0","summary":"It is demonstrated that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora, and that the majority of the benefits stem from the domain-specific knowledge acquired through monolingUAL pretraining.","score":2},{"url":"https://www.semanticscholar.org/paper/d7e0f0cec28c34710fa631df410b717186741db5","title":"A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces","venue":"","year":2022,"referenceCount":35,"citationCount":3,"influentialCitationCount":1,"publicationDate":"14/11/2022","authors":"Dominic Rampas,Pablo Pernias,M. Aubreville","id":"d7e0f0cec28c34710fa631df410b717186741db5","summary":"Despite its remarkable simplicity, the proposed streamlined approach for text-to-image generation yields aesthetically pleasing images with few sampling iterations, allows for intriguing ways for conditioning the model, and imparts advantages absent in state-of-the-art techniques.","score":2},{"url":"https://www.semanticscholar.org/paper/1e4b6567ff66de6cdcbe58c863538241e2f62b45","title":"Aligning Text-to-Image Models using Human Feedback","venue":"arXiv.org","year":2023,"referenceCount":50,"citationCount":78,"influentialCitationCount":6,"publicationDate":"23/02/2023","authors":"Kimin Lee,Hao Liu,M. Ryu,Olivia Watkins,Yuqing Du,Craig Boutilier,P. Abbeel,M. Ghavamzadeh,S. Gu","id":"1e4b6567ff66de6cdcbe58c863538241e2f62b45","summary":"A fine-tuning method for aligning text-to-image models using human feedback, comprising three stages, that generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model.","score":2},{"url":"https://www.semanticscholar.org/paper/10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7","title":"GlyphDraw: Seamlessly Rendering Text with Intricate Spatial Structures in Text-to-Image Generation","venue":"","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/03/2023","authors":"Jiancang Ma,Mingjun Zhao,Chen Chen,Ruichen Wang,Di Niu,H. Lu,Xiaodong Lin","id":"10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7","summary":"GlyphDraw is introduced, a general learning framework aiming to endow image generation models with the capacity to generate images coherently embedded with text for any specific language.","score":2},{"url":"https://www.semanticscholar.org/paper/5fbe4c92791fbecb179c1ab79bba9a59b2e155ba","title":"GlyphControl: Glyph Conditional Control for Visual Text Generation","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":8,"influentialCitationCount":2,"publicationDate":"29/05/2023","authors":"Yukang Yang,Dongnan Gui,Yuhui Yuan,Haisong Ding,Hang-Rui Hu,Kai Chen","id":"5fbe4c92791fbecb179c1ab79bba9a59b2e155ba","summary":"Empirical evaluations demonstrate that GlyphControl outperforms the recent DeepFloyd IF approach in terms of OCR accuracy, CLIP score, and FID, highlighting the efficacy of the method.","score":2},{"url":"https://www.semanticscholar.org/paper/02dde8e9fefafae8e0b9ee2eef0ddc74c511f7cd","title":"Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models","venue":"","year":2023,"referenceCount":48,"citationCount":4,"influentialCitationCount":0,"publicationDate":"01/06/2023","authors":"Pablo Pernias,Dominic Rampas,Mats L. Richter,Christopher J. Pal,M. Aubreville","id":"02dde8e9fefafae8e0b9ee2eef0ddc74c511f7cd","summary":"Wurstchen is introduced, a novel architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text- to-image diffusion models and significantly reduces the computational requirements to achieve state-of-the-art results.","score":2},{"url":"https://www.semanticscholar.org/paper/a903e1e0ffd04dd666f3537f6570d742d7be3486","title":"Grounded Text-to-Image Synthesis with Attention Refocusing","venue":"arXiv.org","year":2023,"referenceCount":61,"citationCount":21,"influentialCitationCount":4,"publicationDate":"08/06/2023","authors":"Quynh Phung,Songwei Ge,Jia-Bin Huang","id":"a903e1e0ffd04dd666f3537f6570d742d7be3486","summary":"The potential causes in the diffusion model's cross-att attention and self-attention layers are revealed and two novel losses to refocus attention maps according to a given spatial layout during sampling are proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/d7890d1906d95c4ae4c430b350455156d6d8aed9","title":"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis","venue":"arXiv.org","year":2023,"referenceCount":56,"citationCount":218,"influentialCitationCount":65,"publicationDate":"04/07/2023","authors":"Dustin Podell,Zion English,Kyle Lacey,A. Blattmann,Tim Dockhorn,Jonas Muller,Joe Penna,Robin Rombach","id":"d7890d1906d95c4ae4c430b350455156d6d8aed9","summary":"It is demonstrated that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators.","score":2},{"url":"https://www.semanticscholar.org/paper/1c6e2a4da1ead685a95c079751bf4d7a727d8180","title":"TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2023","authors":"Jingye Chen,Yupan Huang,Tengchao Lv,Lei Cui,Qifeng Chen,Furu Wei","id":"1c6e2a4da1ead685a95c079751bf4d7a727d8180","summary":"TextDiffuser-2 is presented, aiming to unleash the power of language models for text rendering by fine-tune a large language model for layout planning and utilizing the language model within the diffusion model to encode the position and texts at the line level.","score":2},{"url":"https://www.semanticscholar.org/paper/7845058565eb71e3e423ec72641abe9d7ba2ed0a","title":"UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models","venue":"arXiv.org","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2023","authors":"Yiming Zhao,Zhouhui Lian","id":"7845058565eb71e3e423ec72641abe9d7ba2ed0a","summary":"This paper proposes a novel approach for text image generation, utilizing a pre-trained diffusion model, Stable Diffusion, which replaces the original CLIP encoder and provides more robust text embeddings as conditional guidance and achieves notably high sequence accuracy when synthesizing text in arbitrarily given images.","score":2},{"url":"https://www.semanticscholar.org/paper/bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMUâs Machine Translation System for IWSLT 2019","venue":"International Workshop on Spoken Language Translation","year":2019,"referenceCount":31,"citationCount":3,"influentialCitationCount":0,"publicationDate":2019,"authors":"Tejas Srinivasan,Ramon Sanabria,Florian Metze","id":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","summary":"Block Multitask Learning (BMTL) is presented, a novel NMT architecture that predicts multiple targets of different granularities simulta- neously, removing the need to search for the optimal subword segmentation strategy.","score":2},{"url":"https://www.semanticscholar.org/paper/fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation","venue":"International Workshop on Spoken Language Translation","year":2019,"referenceCount":31,"citationCount":5,"influentialCitationCount":0,"publicationDate":"27/10/2019","authors":"Tejas Srinivasan,Ramon Sanabria,Florian Metze","id":"fec6def294027a2ce9094267ce7b7d57f78daf74","summary":"Block Multitask Learning (BMTL), a novel NMT architecture that predicts multiple targets of different granularities simultaneously, removing the need to search for the optimal segmentation strategy, is presented.","score":2},{"url":"https://www.semanticscholar.org/paper/debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes","venue":"arXiv.org","year":2020,"referenceCount":32,"citationCount":3,"influentialCitationCount":0,"publicationDate":"05/04/2020","authors":"Thamme Gowda,Jonathan May","id":"debb3877b778eeb8689729d37e2b90f9f000d877","summary":"This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and investigates the effect of class imbalance on NMT.","score":2},{"url":"https://www.semanticscholar.org/paper/dca4128a33ca22c02031b5c0c28548a0df022d80","title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding","venue":"","year":2021,"referenceCount":56,"citationCount":8,"influentialCitationCount":2,"publicationDate":2021,"authors":"Abhik Bhattacharjee,Tahmid Hasan,Kazi Samin Mubasshir,Md. Saiful Islam,M. Rahman,Anindya Iqbal,Rifat Shahriyar","id":"dca4128a33ca22c02031b5c0c28548a0df022d80","summary":"The Embedding Barrier is introduced, a phenomenon that limits the monolingual performance of multilingual models on low-resource languages having unique typologies and a straightforward solution by transcribing languages to a common script is proposed, which can effectively improve the performance of a multilingual model for the Bangla language.","score":2},{"url":"https://www.semanticscholar.org/paper/a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study","venue":"MRL","year":2021,"referenceCount":72,"citationCount":19,"influentialCitationCount":2,"publicationDate":"16/06/2021","authors":"Ethan C. Chau,Noah A. Smith","id":"a20a802839d72bee1c85f4a1cb77addadacb2179","summary":"These evaluations on part-of-speech tagging, universal dependency parsing, and named entity recognition in nine diverse low-resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low- resource settings.","score":2},{"url":"https://www.semanticscholar.org/paper/31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning","venue":"Journal of Big Data","year":2021,"referenceCount":144,"citationCount":206,"influentialCitationCount":8,"publicationDate":"29/06/2021","authors":"Connor Shorten,T. Khoshgoftaar,B. Furht","id":"31852f9fc732c0868af12d631c72693702d80521","summary":"The major motifs of Data Augmentation are summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form.","score":2},{"url":"https://www.semanticscholar.org/paper/7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models","venue":"International Conference on Agents and Artificial Intelligence","year":2022,"referenceCount":55,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/01/2022","authors":"Konstantin Todorov,Giovanni Colavizza","id":"7e3081b0d698f8abf16dee626d782f3339482fe7","summary":"It is found that OCR noise poses a significant obstacle to language modelling, with language models increasingly diverging from their noiseless targets as OCR quality lowers, and simpler models including PPMI and Word2Vec consistently outperform transformer-based models in this respect.","score":2},{"url":"https://www.semanticscholar.org/paper/9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages","venue":"AACL","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"Louis ClouÃ¢tre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"9a5b2dc77bda19759df8481aaf283da353ac7e77","summary":"This work replicates a study on the importance of local structure, and the relative unimportance of global structure, in a multilingual setting and finds that the phenomenon observed on the English language broadly translates to over 120 languages, with a few caveats.","score":2},{"url":"https://www.semanticscholar.org/paper/e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy","venue":"IEEE Access","year":2022,"referenceCount":107,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/11/2022","authors":"Philipp Siebers,Christian Janiesch,Patrick Zschech","id":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","summary":"This work contributes threefold to this lack of compilation, composition, and systematization by providing a survey of current approaches, arranging them in a genealogy, and conceptualizing a taxonomy of text representation methods to examine and explain the state-of-the-art.","score":2},{"url":"https://www.semanticscholar.org/paper/412e266cddfd87c79087a88ba1e4d11b89a45a13","title":"MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers","venue":"arXiv.org","year":2023,"referenceCount":47,"citationCount":24,"influentialCitationCount":2,"publicationDate":"12/05/2023","authors":"L. Yu,Daniel Simig,Colin Flaherty,Armen Aghajanyan,Luke Zettlemoyer,M. Lewis","id":"412e266cddfd87c79087a88ba1e4d11b89a45a13","summary":"Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes, is proposed, establishing the viability of tokenization-free autoregressive sequence modeling at scale.","score":2},{"url":"https://www.semanticscholar.org/paper/e4a95f595b5d60a0858725996b9355f7275492cf","title":"Hierarchical Attention Encoder Decoder","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/06/2023","authors":"Asier Mujika","id":"e4a95f595b5d60a0858725996b9355f7275492cf","summary":"A model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture that independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/5cd4fe8174b582f27d79859e4b79e0eba2a1d5f0","title":"Is Anisotropy Inherent to Transformers?","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/06/2023","authors":"Nathan Godey,Eric Villemonte de la Clergerie,BenoÃ®t Sagot","id":"5cd4fe8174b582f27d79859e4b79e0eba2a1d5f0","summary":"This paper shows that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences of cross-entropy loss on long-tailed distributions of tokens.","score":2},{"url":"https://www.semanticscholar.org/paper/96c88b196e3e432710debab39f49ee72f2b96a10","title":"Anisotropy Is Inherent to Self-Attention in Transformers","venue":"","year":2024,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/01/2024","authors":"Nathan Godey,Eric Villemonte de la Clergerie,Benoit Sagot","id":"96c88b196e3e432710debab39f49ee72f2b96a10","summary":"It is shown in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences of cross-entropy loss on long-tailed distributions of tokens.","score":2},{"url":"https://www.semanticscholar.org/paper/a6e2dca754f3dc625a9da5f10f9b7a57079bfd27","title":"MambaByte: Token-free Selective State Space Model","venue":"","year":2024,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2024","authors":"Junxiong Wang,Tushaar Gangavarapu,Jing Nathan Yan,Alexander M. Rush","id":"a6e2dca754f3dc625a9da5f10f9b7a57079bfd27","summary":"This work experiments with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences, and finds MambaByte to be competitive with and even outperform state-of-the-art subword Transformers.","score":2},{"url":"https://www.semanticscholar.org/paper/134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":28,"citationCount":7,"influentialCitationCount":0,"publicationDate":"06/09/2022","authors":"Doan Nam Long Vu,N. Moosavi,Steffen Eger","id":"134e4d72e23bca51e290db171d063989883020f4","summary":"It is shown that an embedding-based metric that has the highest correlation with human evaluations on a standard benchmark can have the lowest correlation if the amount of input noise or unknown tokens increases, and the highest robustness is achieved when using character-level embeddings, instead of token-based embeddments, from the first layer of the pretrained model.","score":2},{"url":"https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359","title":"Self-Supervised Multimodal Learning: A Survey","venue":"arXiv.org","year":2023,"referenceCount":231,"citationCount":14,"influentialCitationCount":0,"publicationDate":"31/03/2023","authors":"Yongshuo Zong,Oisin Mac Aodha,Timothy M. Hospedales","id":"84b6fecf016d74512869c698c66c83729abdf359","summary":"A comprehensive review of the state-of-the-art in SSML, in which three major challenges intrinsic to self-supervised learning with multimodal data are elucidated and existing solutions to these challenges are detailed.","score":2},{"url":"https://www.semanticscholar.org/paper/fd53fd29c919a2dd40d54649c5930a2ba5e1ee71","title":"Mixed Orthographic/Phonemic Language Modeling: Beyond Orthographically Restricted Transformers (BORT)","venue":"Workshop on Representation Learning for NLP","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Robert C. Gale,A. Salem,G. Fergadiotis,S. Bedrick","id":"fd53fd29c919a2dd40d54649c5930a2ba5e1ee71","summary":"BORT is introduced, an LLM for mixed orthography/IPA meant to overcome limitations in LLM support for the international phonetic alphabet, and fine-tuning from the models is more accurate and improved on character error rates than those starting from the original model.","score":2},{"url":"https://www.semanticscholar.org/paper/3448e136046d22d4f90d7a4c875890f5fb64b811","title":"PuoBERTa: Training and evaluation of a curated language model for Setswana","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2023","authors":"V. Marivate,Moseli Mots'oehli,Valencia Wagner,Richard Lastrucci,Isheanesu Dzingirai","id":"3448e136046d22d4f90d7a4c875890f5fb64b811","summary":"This paper evaluated PuoBERTa across several NLP tasks, including part-of-speech tagging, named entity recognition (NER), and news categorisation, and introduced a new SetswanaNews categorisation dataset and provided the initial benchmarks using PuoberTa.","score":2},{"url":"https://www.semanticscholar.org/paper/2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":59,"citationCount":10,"influentialCitationCount":1,"publicationDate":"02/09/2021","authors":"Chantal Amrhein,Rico Sennrich","id":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","summary":"It is found that learning to analyse and generate morphologically complex surface representations is still challenging, especially for non-concatenative morphological phenomena like reduplication or vowel harmony and for rare word stems.","score":2},{"url":"https://www.semanticscholar.org/paper/940e3ee32edfe425e958a672644a0bab2fa30ddc","title":"When Vision Fails: Text Attacks Against ViT and OCR","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":2,"influentialCitationCount":0,"publicationDate":"12/06/2023","authors":"Nicholas Boucher,Jenny Blessing,Ilia Shumailov,Ross Anderson,Nicolas Papernot","id":"940e3ee32edfe425e958a672644a0bab2fa30ddc","summary":"It is shown how a genetic algorithm can be used to generate visual adversarial examples in a black-box setting, and a user study is conducted to establish that the model-fooling adversarialExamples do not affect human comprehension.","score":2},{"url":"https://www.semanticscholar.org/paper/8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey","venue":"Journal of Artificial Intelligence Research","year":2021,"referenceCount":366,"citationCount":53,"influentialCitationCount":1,"publicationDate":"14/04/2021","authors":"Danielle Saunders","id":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","summary":"This work surveys approaches to domain adaptation for NMT, particularly where a system may need to translate across multiple domains, and divides techniques into those revolving around data selection or generation, model architecture, parameter adaptation procedure, and inference procedure.","score":2},{"url":"https://www.semanticscholar.org/paper/d680dd4b0e528b6c575fb210db6971c124210b20","title":"Finding the Optimal Vocabulary Size for Turkish Named Entity Recognition","venue":"ALTNLP","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Y. Kaya,A. C. Tantug","id":"d680dd4b0e528b6c575fb210db6971c124210b20","summary":"Novel BERT models are presented pretrained with various vocabulary sizes from scratch on BERTurk corpus and fine-tuned for named entity recognition (NER) downstream task in the Turkish language, achieving state-of-the-art performance on the WikiANN dataset.","score":2},{"url":"https://www.semanticscholar.org/paper/445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":40,"citationCount":11,"influentialCitationCount":2,"publicationDate":"13/09/2021","authors":"Antonis Maronikolakis,Philipp Dufter,Hinrich SchÃ¼tze","id":"445f82be81e37b064730285cf8de70edce80d6a5","summary":"It is shown that the compatibility measure proposed allows the system designer to create vocabularies across languages that are compatible -- a desideratum that so far has been neglected in multilingual models.","score":2},{"url":"https://www.semanticscholar.org/paper/19b7c860128e5461d451b3d15f3836a9e1680ffc","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class","venue":"ACM Transactions on Software Engineering and Methodology","year":2022,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/07/2022","authors":"Luke Dramko,Jeremy Lacomis,Pengcheng Yin,Edward J. Schwartz,Miltiadis Allamanis,Graham Neubig,Bogdan Vasilescu,Claire Le Goues","id":"19b7c860128e5461d451b3d15f3836a9e1680ffc","summary":"This work investigates how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains, and evaluates DIREâs overall performance without respect to data quality.","score":2},{"url":"https://www.semanticscholar.org/paper/9d83941a205e31d394f614424cdc553cea9e35f9","title":"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22","venue":"Conference on Machine Translation","year":2022,"referenceCount":33,"citationCount":4,"influentialCitationCount":1,"publicationDate":"02/12/2022","authors":"Laia TarrÃ©s,Gerard I. GÃ¡llego,Xavier GirÃ³-i-Nieto,Jordi Torres","id":"9d83941a205e31d394f614424cdc553cea9e35f9","summary":"The system developed at the Universitat PolitÃ¨cnica de Catalunya for the Workshop on Machine Translation 2022 Sign Language Translation Task, in particular, for the sign-to-text direction is described, with poor results for both the baseline and the system, and thus, the unreliability of the findings.","score":2},{"url":"https://www.semanticscholar.org/paper/cd70bbd80f4e07b18ee2cc339e13feb15b87e934","title":"Optimized Tokenization for Transcribed Error Correction","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/10/2023","authors":"Tomer Wullach,Shlomo E. Chazan","id":"cd70bbd80f4e07b18ee2cc339e13feb15b87e934","summary":"It is demonstrated that the performance of correction models can be significantly increased by training solely using synthetic data and applying language-specific adjustments to the vocabulary of a BPE tokenizer strikes a balance between adapting to unseen distributions and retaining knowledge of transcribed errors.","score":2},{"url":"https://www.semanticscholar.org/paper/62ad7ea9467bbcdbfe325b9ee561cab3908e4583","title":"MEGA: Multilingual Evaluation of Generative AI","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":100,"citationCount":53,"influentialCitationCount":6,"publicationDate":"22/03/2023","authors":"Kabir Ahuja,Rishav Hada,Millicent Ochieng,Prachi Jain,Harshita Diddee,Krithika Ramesh,Samuel C. Maina,T. Ganu,Sameer Segal,Maxamed Axmed,Kalika Bali,Sunayana Sitaram","id":"62ad7ea9467bbcdbfe325b9ee561cab3908e4583","summary":"This work presents the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages and presents a thorough analysis of the performance of models across languages and tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/fb49e38135302a1c16d644c0f746cef7d5f10ee4","title":"Understanding BLOOM: An empirical study on diverse NLP tasks","venue":"arXiv.org","year":2022,"referenceCount":74,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/11/2022","authors":"Parag Dakle,Sai Krishna Rallabandi,Preethi Raghavan","id":"fb49e38135302a1c16d644c0f746cef7d5f10ee4","summary":"Evaluating the smaller BLOOM model variants on several NLP benchmark datasets and popular leaderboards shows that the 560m variant performs similarly to or better than the 1b7 variant, and toxicity analysis of prompt-based text generation using the RealToxicityPrompts dataset shows that it is at least 17\\% less toxic than GPT-2 and G PT-3 models.","score":2},{"url":"https://www.semanticscholar.org/paper/9bedd67004ef344b801365ae28c09dce28410517","title":"Stolen Subwords: Importance of Vocabularies for Machine Translation Model Stealing","venue":"","year":2024,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/01/2024","authors":"VilÃ©m Zouhar","id":"9bedd67004ef344b801365ae28c09dce28410517","summary":"It is found that the vocabulary itself does not have a large effect on the local model's performance, and the results of the minimum effect of vocabulary choice are important more broadly for black-box knowledge distillation.","score":2},{"url":"https://www.semanticscholar.org/paper/a9cd3b1ad7ea09dfddf1a22124c27070c6f910e1","title":"Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT","venue":"European Association for Machine Translation Conferences/Workshops","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/06/2023","authors":"Benoist Wolleb,Romain Silvestri,Giorgos Vernikos,Ljiljana Dolamic Andrei Popescu-Belis","id":"a9cd3b1ad7ea09dfddf1a22124c27070c6f910e1","summary":"A tokenization approach is proposed that enables us to separate frequency (the first advantage) from compositionality, thanks to the use of Huffman coding, which tokenizes words using a fixed amount of symbols.","score":2},{"url":"https://www.semanticscholar.org/paper/b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Colin Leong,Daniel Whitenack","id":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","summary":"Initial experiments using Swahili and Kinyarwanda data suggest the viability of the multi-modal approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch.","score":2},{"url":"https://www.semanticscholar.org/paper/f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms","venue":"arXiv.org","year":2022,"referenceCount":113,"citationCount":111,"influentialCitationCount":21,"publicationDate":2022,"authors":"Yi Tay,Mostafa Dehghani,Vinh Q. Tran,Xavier GarcÃ­a,Dara Bahri,Tal Schuster,H. Zheng,N. Houlsby,Donald Metzler","id":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","summary":"This paper presents a uniï¬ed framework for pre-training models that are universally effective across datasets and setups, and proposes Mixture-of-Denoisers (MoD), a pre- Training objective that combines diverse pre- training paradigms together.","score":2},{"url":"https://www.semanticscholar.org/paper/66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer","venue":"International Conference on Learning Representations","year":2021,"referenceCount":88,"citationCount":69,"influentialCitationCount":6,"publicationDate":"25/10/2021","authors":"Mostafa Dehghani,Anurag Arnab,Lucas Beyer,Ashish Vaswani,Yi Tay","id":"66d735987a31d666a6459566ae026c40ab9a1c3a","summary":"It is demonstrated how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models, and suggestions to improve reporting of efficiency metrics are presented.","score":2},{"url":"https://www.semanticscholar.org/paper/8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language","venue":"International Conference on Machine Learning","year":2022,"referenceCount":87,"citationCount":507,"influentialCitationCount":86,"publicationDate":"07/02/2022","authors":"Alexei Baevski,Wei-Ning Hsu,Qiantong Xu,Arun Babu,Jiatao Gu,Michael Auli","id":"8f2bca9d684005675e294b33c26481e36f528cdb","summary":"Data2vec is a framework that uses the same learning method for either speech, NLP or computer vision to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture.","score":2},{"url":"https://www.semanticscholar.org/paper/7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers","venue":"Knowledge Discovery and Data Mining","year":2022,"referenceCount":38,"citationCount":59,"influentialCitationCount":13,"publicationDate":"22/02/2022","authors":"Alyssa Lees,Vinh Q. Tran,Yi Tay,Jeffrey Scott Sorensen,Jai Gupta,Donald Metzler,Lucy Vasserman","id":"7016eb4f34611f97fe8c99176246e314678e03f4","summary":"This paper presents the fundamentals behind the next version of the Perspective API from Google Jigsaw, and presents a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":213,"citationCount":47,"influentialCitationCount":2,"publicationDate":"24/03/2022","authors":"Alham Fikri Aji,Genta Indra Winata,Fajri Koto,Samuel Cahyawijaya,Ade Romadhony,Rahmad Mahendra,Kemal Kurniawan,David Moeljadi,Radityo Eko Prasojo,Timothy Baldwin,Jey Han Lau,Sebastian Ruder","id":"a747e8f2659df479c0092301b9658fc582423df1","summary":"An overview of the current state of NLP research for Indonesia's 700+ languages is provided and general recommendations are provided to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.","score":2},{"url":"https://www.semanticscholar.org/paper/b1dad820853464b30c93b52366b32690ba4b99a6","title":"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View","venue":"ACM Computing Surveys","year":2022,"referenceCount":162,"citationCount":12,"influentialCitationCount":1,"publicationDate":"26/03/2022","authors":"Ruiqi Li,Xiang Zhao,M. Moens","id":"b1dad820853464b30c93b52366b32690ba4b99a6","summary":"This survey summarizes the current universal sentence-embedding methods, categorizes them into four groups from a linguistic view, and ultimately analyzes their reported performance.","score":2},{"url":"https://www.semanticscholar.org/paper/0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":27,"citationCount":7,"influentialCitationCount":1,"publicationDate":"16/04/2022","authors":"Mingchen Li,J. Chen,Samuel Mensah,Nikolaos Aletras,Xiulong Yang,Yang Ye","id":"0b9e130c6305de7766697ba7655f56010aaffd61","summary":"A Hierarchical N-Gram framework for Zero-Shot Link Prediction (HNZSLP), which considers the dependencies among character n-grams of the relation surface name for ZSLP.","score":2},{"url":"https://www.semanticscholar.org/paper/b21670e8061a06ab97e7d6052c9345a326e84ff8","title":"UL2: Unifying Language Learning Paradigms","venue":"International Conference on Learning Representations","year":2022,"referenceCount":144,"citationCount":148,"influentialCitationCount":24,"publicationDate":"10/05/2022","authors":"Yi Tay,Mostafa Dehghani,Vinh Q. Tran,Xavier GarcÃ­a,Jason Wei,Xuezhi Wang,Hyung Won Chung,Dara Bahri,Tal Schuster,H. Zheng,Denny Zhou,N. Houlsby,Donald Metzler","id":"b21670e8061a06ab97e7d6052c9345a326e84ff8","summary":"A unified framework for pre-training models that are universally effective across datasets and setups is presented and Mixture-of-Denoisers (MoD) is proposed, a pre- Training objective that combines diverse pre- training paradigms together.","score":2},{"url":"https://www.semanticscholar.org/paper/bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation","venue":"arXiv.org","year":2022,"referenceCount":20,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/05/2022","authors":"Lukas Edman,Antonio Toral,Gertjan van Noord","id":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","summary":"The GBST method from Charformer groups (aka downsamples) characters is used, thereby enabling character grouping in the decoder, and promising performance on English--Turkish translation indicate the potential of character-level models for morphologically-rich languages.","score":2},{"url":"https://www.semanticscholar.org/paper/dd568e6838903ad7c381f13c1268c94c5db08b02","title":"Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing","venue":"2022 IEEE International Conference on Big Data (Big Data)","year":2022,"referenceCount":40,"citationCount":5,"influentialCitationCount":0,"publicationDate":"04/11/2022","authors":"Yibo Wang,Congying Xia,Guan Wang,Philip S. Yu","id":"dd568e6838903ad7c381f13c1268c94c5db08b02","summary":"This work reformulates entity typing into a textual entailment problem to handle new entities that are not present during training and designs a model to automatically generate textual entailments hypotheses using a continuous prompt tuning method, which can generate better textual entailMENT hypotheses without manual design.","score":2},{"url":"https://www.semanticscholar.org/paper/9769992ca9ff1c3894d19f4bd2b6dac27ed1befd","title":"A review on abusive content automatic detection: approaches, challenges and opportunities","venue":"PeerJ Computer Science","year":2022,"referenceCount":135,"citationCount":3,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"Bedour Alrashidi,Amani Jamal,Imtiaz Khan,Ali Alkhathlan","id":"9769992ca9ff1c3894d19f4bd2b6dac27ed1befd","summary":"A review of abusive content automatic detection approaches focusing on the recent contributions that were using natural language processing (NLP) technologies to detect the abusive content in social media and proposes a new taxonomy of abusivecontent automatic detection by covering five different aspects and tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/2f272ca3394656835bf32a30b80c6f8ba1c8f4c4","title":"Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Kaiser Sun,Peng Qi,Yuhao Zhang,Lan Liu,William Yang Wang,Zhiheng Huang","id":"2f272ca3394656835bf32a30b80c6f8ba1c8f4c4","summary":"It is shown that, with consistent tokenization, the model performs better in both in-domain and out-of-domain datasets, with a notable average of +1.7 F2 gain when a BART model is trained on SQuAD and evaluated on 8 QA datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/5fce7d9442b06cab91174fb68ba52ff6bdaa29cc","title":"A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks","venue":"Expert systems with applications","year":2023,"referenceCount":239,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/06/2023","authors":"Saidul Islam,Hanae Elmekki,Ahmed Elsebai,J. Bentahar,Najat Drawel,Gaith Rjoub,W. Pedrycz","id":"5fce7d9442b06cab91174fb68ba52ff6bdaa29cc","summary":"An extensive survey of proposed transformer models from 2017 to 2022 encompasses the identification of the top five application domains for transformer-based models, namely: NLP, Computer Vision, Multi-Modality, Audio and Speech Processing, and Signal Processing and analyzes the impact of highly influential transformer- based models in these domains.","score":2},{"url":"https://www.semanticscholar.org/paper/2adc13eb55c92e026c4cefc89a47a0ee0ac95111","title":"SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding","venue":"Knowledge Discovery and Data Mining","year":2023,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/06/2023","authors":"Vasilisa Bashlovkina,Riley Matthews,Zhaobin Kuang,Simon Baumgartner,Michael Bendersky","id":"2adc13eb55c92e026c4cefc89a47a0ee0ac95111","summary":"It is shown that learning a tokenizer and pretraining on a mix of social media and conventional language yields an LM that outperforms the best similar-sized alternative by 4.2 points on the overall SMILE score.","score":2},{"url":"https://www.semanticscholar.org/paper/28c75ab7f6951f1c2bb349b34abc3204ba8b9498","title":"Toucan: Token-Aware Character Level Language Modeling","venue":"arXiv.org","year":2023,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/11/2023","authors":"William Fleshman,Benjamin Van Durme","id":"28c75ab7f6951f1c2bb349b34abc3204ba8b9498","summary":"Toucan is proposed, an augmentation to character-level models to make them \"token-aware\", demonstrating significant speed-ups in character generation without a loss in language modeling performance.","score":2},{"url":"https://www.semanticscholar.org/paper/b69d4a00fb01212a8970c698c5a4e8d2d64b81e9","title":"Modular Adaptation of Multilingual Encoders to Written Swiss German Dialect","venue":"","year":2024,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/01/2024","authors":"Jannis Vamvas,NoÃ«mi Aepli,Rico Sennrich","id":"b69d4a00fb01212a8970c698c5a4e8d2d64b81e9","summary":"This paper builds on several existing multilingual encoders and adapt them to Swiss German using continued pre-training and finds that for the task of retrieving Swiss German sentences given Standard German queries, adapting a character-level model is more effective than the other adaptation strategies.","score":2},{"url":"https://www.semanticscholar.org/paper/89b5890999ce6874507b0264fa4b468e16c03788","title":"MorphPiece : Moving away from Statistical Language Representation","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/07/2023","authors":"Haris Jabbar","id":"89b5890999ce6874507b0264fa4b468e16c03788","summary":"A linguistically motivated tokenization scheme, MorphPiece, is proposed, which is based partly on morphological segmentation of the underlying text, which shows superior convergence compared to the same architecture trained on a standard BPE tokenizer.","score":2},{"url":"https://www.semanticscholar.org/paper/b162638cd42b65e6add199b0b34f1b375070fc7c","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models","venue":"AACL/IJCNLP","year":2022,"referenceCount":59,"citationCount":3,"influentialCitationCount":1,"publicationDate":"24/10/2022","authors":"Syrielle Montariol,Arij Riabi,DjamÃ© Seddah","id":"b162638cd42b65e6add199b0b34f1b375070fc7c","summary":"It is shown how hate speech detection models benefit from a cross-lingual knowledge proxy brought by auxiliary tasks fine-tuning and highlight these tasks' positive impact on bridging the hate speech linguistic and cultural gap between languages.","score":2},{"url":"https://www.semanticscholar.org/paper/e0d09d91784a6d426ffcd2fd12448dd9c8ceefe9","title":"Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages","venue":"Workshop on NLP for Similar Languages, Varieties and Dialects","year":2023,"referenceCount":54,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/04/2023","authors":"Verena Blaschke,Hinrich SchÃ¼tze,Barbara Plank","id":"e0d09d91784a6d426ffcd2fd12448dd9c8ceefe9","summary":"Overall, it is found that the similarity between the percentage of words that get split into subwords in the source and target data (the isplit word ratio difference/i) is the strongest predictor for model performance on target data.","score":2},{"url":"https://www.semanticscholar.org/paper/7500f105fd95191e50fc9e8ca3f38d89d92055ad","title":"Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing","venue":"Transactions of the Association for Computational Linguistics","year":2023,"referenceCount":83,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/07/2023","authors":"Tom Sherborne,Tom Hosking,Mirella Lapata","id":"7500f105fd95191e50fc9e8ca3f38d89d92055ad","summary":"This work proposes a new approach to cross-lingual semantic parsing by explicitly minimizing cross-lingsual divergence between probabilistic latent variables using Optimal Transport, and demonstrates how this direct guidance improves parsing from natural languages using fewer examples and less training.","score":2},{"url":"https://www.semanticscholar.org/paper/30f36f68265823c7f9945f902451fe0b1fac790b","title":"Biomedical Language Models are Robust to Sub-optimal Tokenization","venue":"Workshop on Biomedical Natural Language Processing","year":2023,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/06/2023","authors":"Bernal Jimenez Gutierrez,Huan Sun,Yu Su","id":"30f36f68265823c7f9945f902451fe0b1fac790b","summary":"Surprisingly, it is found that pre-training a biomedical LM using a more accurate biomedical tokenizer does not improve the entity representation quality of a language model as measured by several intrinsic and extrinsic measures such as masked language modeling prediction (MLM) accuracy as well as NER and entity linking performance.","score":2},{"url":"https://www.semanticscholar.org/paper/8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability","venue":"arXiv.org","year":2022,"referenceCount":117,"citationCount":6,"influentialCitationCount":0,"publicationDate":2022,"authors":"Mai Ha Vu,R. Akbar,Philippe A. Robert,B. Swiatczak,V. Greiff,G. K. Sandve,Dag Trygve Tryslew Haug","id":"8f248b5476666e700390f7f8ff6ca923f97d726c","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/732761bad14162fcff61eccebb4f97f89fd1415b","title":"Controlling Diffusion Input-Output Mapping of the Components of a Diffusion Model as a Potential Approach for Enhanced Model Control Masterâs thesis in Complex adaptive systems","venue":"","year":2023,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Philip F Gard","id":"732761bad14162fcff61eccebb4f97f89fd1415b","summary":"The objective is not to propose a definitive solution for control within diffusion models, but rather to probe the underlying mappings that drive these processes, to increase understanding of the inherent complexity of the models and identify potential control opportunities.","score":2},{"url":"https://www.semanticscholar.org/paper/964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","venue":"arXiv.org","year":2022,"referenceCount":172,"citationCount":1083,"influentialCitationCount":124,"publicationDate":"09/11/2022","authors":"Teven Le Scao,Angela Fan,Christopher Akiki,Ellie Pavlick,Suzana Ili'c,Daniel Hesslow,Roman Castagn'e,A. Luccioni,Franccois Yvon,Matthias GallÃ©,J. Tow,Alexander M. Rush,Stella Biderman,Albert Webson,Pawan Sasanka Ammanamanchi,Thomas Wang,BenoÃ®t Sagot,Niklas Muennighoff,Albert Villanova del Moral,Olatunji Ruwase,Rachel Bawden,Stas Bekman,Angelina McMillan-Major,Iz Beltagy,Huu Nguyen,Lucile Saulnier,Samson Tan,Pedro Ortiz Suarez,Victor Sanh,Hugo Laurenccon,Yacine Jernite,Julien Launay,Margaret Mitchell,Colin Raffel,Aaron Gokaslan,Adi Simhi,Aitor Soroa Etxabe,Alham Fikri Aji,Amit Alfassy,Anna Rogers,Ariel Kreisberg Nitzav,Canwen Xu,Chenghao Mou,Chris C. Emezue,Christopher Klamm,Colin Leong,Daniel Alexander van Strien,David Ifeoluwa Adelani,Dragomir R. Radev,E. G. Ponferrada,Efrat Levkovizh,Ethan Kim,Eyal Natan,F. Toni,GÃ©rard Dupont,GermÃ¡n Kruszewski,Giada Pistilli,Hady ElSahar,Hamza Benyamina,H. Tran,Ian Yu,Idris Abdulmumin,Isaac Johnson,Itziar Gonzalez-Dios,Javier de la Rosa,Jenny Chim,Jesse Dodge,Jian Zhu,Jonathan Chang,Jorg Frohberg,Josephine L. Tobing,J. Bhattacharjee,Khalid Almubarak,Kimbo Chen,Kyle Lo,Leandro von Werra,Leon Weber,Long Phan,Loubna Ben Allal,Ludovic Tanguy,Manan Dey,M. MuÃ±oz,Maraim Masoud,MarÃ­a Grandury,Mario vSavsko,Max Huang,Maximin Coavoux,Mayank Singh,Mike Tian-Jian Jiang,Minh Chien Vu,M. A. Jauhar,Mustafa Ghaleb,Nishant Subramani,Nora Kassner,Nurulaqilla Khamis,Olivier Nguyen,Omar Espejel,Ona de Gibert,Paulo Villegas,Peter Henderson,Pierre Colombo,Priscilla Amuok,Quentin Lhoest,Rheza Harliman,Rishi Bommasani,R. L'opez,Rui Ribeiro,Salomey Osei,Sampo Pyysalo,Sebastian Nagel,Shamik Bose,Shamsuddeen Hassan Muhammad,S. Sharma,S. Longpre,So-maieh Nikpoor,S. Silberberg,S. Pai,S. Zink,Tiago Timponi Torrent,Timo Schick,Tristan Thrush,V. Danchev,Vassilina Nikoulina,Veronika Laippala,Violette Lepercq,V. Prabhu,Zaid Alyafeai,Zeerak Talat,Arun Raja,Benjamin Heinzerling,Chenglei Si,Elizabeth Salesky,Sabrina J. Mielke,Wilson Y. Lee,Abheesht Sharma,Andrea Santilli,Antoine Chaffin,Arnaud Stiegler,Debajyoti Datta,Eliza Szczechla,Gunjan Chhablani,Han Wang,Harshit Pandey,Hendrik Strobelt,Jason Alan Fries,Jos Rozen,Leo Gao,Lintang Sutawika,M Saiful Bari,Maged S. Al-Shaibani,Matteo Manica,Nihal V. Nayak,Ryan Teehan,Samuel Albanie,Sheng Shen,Srulik Ben-David,Stephen H. Bach,Taewoon Kim,T. Bers,Thibault FÃ©vry,Trishala Neeraj,Urmish Thakker,Vikas Raunak,Xiang Tang,Zheng-Xin Yong,Zhiqing Sun,Shaked Brody,Y. Uri,Hadar Tojarieh,Adam Roberts,Hyung Won Chung,Jaesung Tae,Jason Phang,Ofir Press,Conglong Li,D. Narayanan,Hatim Bourfoune,J. Casper,Jeff Rasley,Max Ryabinin,Mayank Mishra,Minjia Zhang,M. Shoeybi,Myriam Peyrounette,N. Patry,Nouamane Tazi,Omar Sanseviero,Patrick von Platen,Pierre Cornette,Pierre Franccois Lavall'ee,R. Lacroix,Samyam Rajbhandari,Sanchit Gandhi,Shaden Smith,S. Requena,Suraj Patil,Tim Dettmers,Ahmed Baruwa,Amanpreet Singh,Anastasia Cheveleva,Anne-Laure Ligozat,Arjun Subramonian,Aur'elie N'ev'eol,Charles Lovering,Daniel H Garrette,D. Tunuguntla,Ehud Reiter,Ekaterina Taktasheva,E. Voloshina,Eli Bogdanov,Genta Indra Winata,Hailey Schoelkopf,Jan-Christoph Kalo,Jekaterina Novikova,J. Forde,Xiangru Tang,Jungo Kasai,Ken Kawamura,Liam Hazan,Marine Carpuat,Miruna Clinciu,Najoung Kim,Newton Cheng,O. Serikov,Omer Antverg,Oskar van der Wal,Rui Zhang,Ruochen Zhang,Sebastian Gehrmann,Shachar Mirkin,S. Pais,Tatiana Shavrina,Thomas Scialom,Tian Yun,Tomasz Limisiewicz,Verena Rieser,Vitaly Protasov,V. Mikhailov,Yada Pruksachatkun,Yonatan Belinkov,Zachary Bamberger,Zdenvek Kasner,ZdenÄk Kasner,A. Pestana,A. Feizpour,Ammar Khan,Amy Faranak,A. Santos,A. Hevia,Antigona Unldreaj,Arash Aghagol,Arezoo Abdollahi,A. Tammour,A. HajiHosseini,Bahareh Behroozi,Benjamin Ayoade Ajibade,B. Saxena,Carlos MuÃ±oz Ferrandis,Danish Contractor,D. Lansky,Davis David,Douwe Kiela,D. A. Nguyen,Edward Tan,Emi Baylor,Ezinwanne Ozoani,F. Mirza,Frankline Ononiwu,Habib Rezanejad,H.A. Jones,Indrani Bhattacharya,Irene Solaiman,Irina Sedenko,I. Nejadgholi,J. Passmore,Joshua Seltzer,Julio Bonis Sanz,Karen Fort,LÃ­via Dutra,Mairon Samagaio,Maraim Elbadri,Margot Mieskes,Marissa Gerchick,Martha Akinlolu,Michael McKenna,Mike Qiu,M. Ghauri,Mykola Burynok,Nafis Abrar,Nazneen Rajani,Nour Elkott,N. Fahmy,Olanrewaju Samuel,Ran An,R. Kromann,Ryan Hao,S. Alizadeh,Sarmad Shubber,Silas L. Wang,Sourav Roy,S. Viguier,Thanh-Cong Le,Tobi Oyebade,T. Le,Yoyo Yang,Z. Nguyen,Abhinav Ramesh Kashyap,A. Palasciano,A. Callahan,Anima Shukla,Antonio Miranda-Escalada,A. Singh,Benjamin Beilharz,Bo Wang,C. Brito,Chenxi Zhou,Chirag Jain,Chuxin Xu,ClÃ©mentine Fourrier,Daniel Le'on Perin'an,Daniel Molano,Dian Yu,Enrique Manjavacas,Fabio Barth,Florian Fuhrimann,Gabriel Altay,Giyaseddin Bayrak,Gully Burns,Helena U. Vrabec,I. Bello,Isha Dash,J. Kang,John Giorgi,Jonas Golde,J. Posada,Karthi Sivaraman,Lokesh Bulchandani,Lu Liu,Luisa Shinzato,Madeleine Hahn de Bykhovetz,Maiko Takeuchi,Marc PÃ mies,M. A. Castillo,Marianna Nezhurina,Mario Sanger,M. Samwald,Michael Cullan,Michael Weinberg,M. Wolf,Mina Mihaljcic,Minna Liu,M. Freidank,Myungsun Kang,Natasha Seelam,N. Dahlberg,N. Broad,N. Muellner,Pascale Fung,Patricia Haller,R. Chandrasekhar,R. Eisenberg,Robert Martin,Rodrigo L. Canalli,Rosaline Su,Ruisi Su,Samuel Cahyawijaya,Samuele Garda,Shlok S Deshmukh,Shubhanshu Mishra,Sid Kiblawi,Simon Ott,Sinee Sang-aroonsiri,Srishti Kumar,Stefan Schweter,S. Bharati,Tanmay Laud,ThÃ©o Gigant,Tomoya Kainuma,Wojciech Kusa,Yanis Labrak,Yashasvi Bajaj,Y. Venkatraman,Yifan Xu,Ying Xu,Yu Xu,Z. Tan,Zhongli Xie,Zifan Ye,M. Bras,Younes Belkada,Thomas Wolf","id":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","summary":"BLOOM is a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers and achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning.","score":2},{"url":"https://www.semanticscholar.org/paper/8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models","venue":"arXiv.org","year":2022,"referenceCount":55,"citationCount":18,"influentialCitationCount":3,"publicationDate":"21/12/2022","authors":"Najoung Kim,Tal Linzen,P. Smolensky","id":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","summary":"It is argued that exposure to pretraining data may break distributional control across training and test to gauge compositional generalization, where certain lexical items only occur in limited contexts during training.","score":2},{"url":"https://www.semanticscholar.org/paper/4e97303aeb299ee736b1b8c29cef046212690354","title":"Generation-based Code Review Automation: How Far Are WeÆ","venue":"IEEE International Conference on Program Comprehension","year":2023,"referenceCount":76,"citationCount":4,"influentialCitationCount":0,"publicationDate":"13/03/2023","authors":"Xin Zhou,Kisub Kim,Bowen Xu,Donggyun Han,Junda He,David Lo","id":"4e97303aeb299ee736b1b8c29cef046212690354","summary":"A comprehensive study by comparing the effectiveness of recent ACR tools as well as the general-purpose pre-trained models, which shows that a general- Purpose pre- trained model CodeT5 can outperform other models in most cases.","score":2},{"url":"https://www.semanticscholar.org/paper/850da23eae35322e0af55dbddb7e96e99482c118","title":"From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":2,"influentialCitationCount":0,"publicationDate":"18/04/2023","authors":"Adarsh Kumar,Pedro Sarmento","id":"850da23eae35322e0af55dbddb7e96e99482c118","summary":"This study suggests that subword tokenization is a promising technique for symbolic music generation and may have broader implications for music composition, particularly in cases involving complex data such as multi-track songs.","score":2},{"url":"https://www.semanticscholar.org/paper/af6c6941acecfb23d899cd5266efdf2e27463f12","title":"Causal Language Model Aided Sequential Decoding With Natural Redundancy","venue":"IEEE Transactions on Communications","year":2023,"referenceCount":47,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/05/2023","authors":"Zhaorui Zhu,Hongyi Yu,Caiyao Shen,Jian-ping Du,Zhixiang Shen,Zhenyu Wang","id":"af6c6941acecfb23d899cd5266efdf2e27463f12","summary":"This paper proposes a sequential decoding algorithm for the robust reception of sources with natural redundancy over the AWGN channel and eliminates the requirement of high-cost and accurate labels, which paves a new way for communication receiver design.","score":2},{"url":"https://www.semanticscholar.org/paper/988a17753dd040867eef4f093fa50a87bf4142b1","title":"Tokenization with Factorized Subword Encoding","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/06/2023","authors":"David Samuel,Lilja Ãvrelid","id":"988a17753dd040867eef4f093fa50a87bf4142b1","summary":"A novel tokenization method is proposed that factorizes subwords onto discrete triplets using a VQ-VAE model, and results indicate that this method is more appropriate and robust for morphological tasks than the commonly used byte-pair encoding (BPE) tokenization algorithm.","score":2},{"url":"https://www.semanticscholar.org/paper/9addef681bc1576d7d9b0104d71c41b9def546c5","title":"Analysis of Subword Tokenization Approaches for Turkish Language","venue":"Signal Processing and Communications Applications Conference","year":2023,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/07/2023","authors":"Erencan Erkaya,Tunga GÃ¼ngÃ¶r","id":"9addef681bc1576d7d9b0104d71c41b9def546c5","summary":"This paper comprehensively analyzes subword tokenizers for Turkish which is a highly inflected and morphologically rich language and proposes a morphologically-based approach and examines how the tokenizer parameters like vocabulary and corpus sizes change the characteristics of tokenizers.","score":2},{"url":"https://www.semanticscholar.org/paper/1158f7a56dd3cc7d715740349d1d9fffbeef10ad","title":"SelfSeg: A Self-supervised Sub-word Segmentation Method for Neural Machine Translation","venue":"ACM Trans. Asian Low Resour. Lang. Inf. Process.","year":2023,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/07/2023","authors":"Haiyue Song,Raj Dabre,Chenhui Chu,S. Kurohashi,E. Sumita","id":"1158f7a56dd3cc7d715740349d1d9fffbeef10ad","summary":"SelfSeg is introduced, a self-supervised neural sub-word segmentation method that is much faster to train/decode and requires only monolingual dictionaries instead of parallel corpora and a regularization mechanism that allows the segmenter to generate various segmentations for one word.","score":2},{"url":"https://www.semanticscholar.org/paper/6693b31ba94069901313e11b3a9f138eb5ab777d","title":"Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/09/2023","authors":"Bar Iluz,Tomasz Limisiewicz,Gabriel Stanovsky,David Marevcek","id":"6693b31ba94069901313e11b3a9f138eb5ab777d","summary":"It is shown that analyzing subword splits provides good estimates of gender-form imbalance in the training data and can be used even when the corpus is not publicly available, and fine-tuning just the token embedding layer can decrease the gap in gender prediction accuracy between female and male forms without impairing the translation quality.","score":2},{"url":"https://www.semanticscholar.org/paper/3856fce2fca56e2d7be0134889bf6f9d2f01b931","title":"Advancing Perception in Artificial Intelligence through Principles of Cognitive Science","venue":"arXiv.org","year":2023,"referenceCount":283,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2023","authors":"Palaash Agrawal,Cheston Tan,Heena Rathore","id":"3856fce2fca56e2d7be0134889bf6f9d2f01b931","summary":"This study reviews all current major theories from various sub-disciplines of cognitive science, and draws parallels with theories and techniques from current practices in AI to present a detailed collection of methods in AI for researchers to build AI systems inspired by cognitive science.","score":2},{"url":"https://www.semanticscholar.org/paper/3895423c2e2ed608ba1439a6a21b66e43f5ef8b8","title":"Subword Evenness (SuE) as a Predictor of Cross-lingual Transfer to Low-resource Languages","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Olga Pelloni,Anastassia Shaitarova,T. SamardÅ¾iÄ","id":"3895423c2e2ed608ba1439a6a21b66e43f5ef8b8","summary":"It is shown that languages written in non-Latin and non-alphabetic scripts (mostly Asian languages) are the best choices for improving performance on the task of Masked Language Modelling (MLM) in a diverse set of 30 low-resource languages and that the success of the transfer is well predicted by the authors' novel measure of Subword Evenness (SuE).","score":2},{"url":"https://www.semanticscholar.org/paper/8929066ce924696f960512c92a720c70bba65586","title":"On Language Spaces, Scales and Cross-Lingual Transfer of UD Parsers","venue":"Conference on Computational Natural Language Learning","year":2022,"referenceCount":51,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"T. SamardÅ¾iÄ,Ximena Gutierrez-Vasques,Rob van der Goot,Max MÃ¼ller-Eberstein,Olga Pelloni,Barbara Plank","id":"8929066ce924696f960512c92a720c70bba65586","summary":"This study is a comprehensive analysis of the impact of linguistic distance on the transfer of UD parsers and proposes three text-based feature spaces that can be more precise predictors, especially on a more local scale, when only shorter distances are taken into account.","score":2},{"url":"https://www.semanticscholar.org/paper/66ef8d777eccbb4138c0f9cc83d99c0bb1405c2a","title":"Optimizing the Size of Subword Vocabularies in Dialect Classification","venue":"Workshop on NLP for Similar Languages, Varieties and Dialects","year":2023,"referenceCount":59,"citationCount":2,"influentialCitationCount":0,"publicationDate":2023,"authors":"Vani Kanjirangat,T. SamardÅ¾iÄ,L. Dolamic,Fabio Rinaldi","id":"66ef8d777eccbb4138c0f9cc83d99c0bb1405c2a","summary":"It is shown that models trained from scratch with an optimal tokenization level perform better than fine-tuned classifiers in the case of highly inconsistent writing, and in the cases of relatively consistent writing, fine- Tuned models remain better regardless of thetokenization level.","score":2},{"url":"https://www.semanticscholar.org/paper/2ac5442a32988f86730e460b3198f475592ae410","title":"Improving Low-Resource Languages in Pre-Trained Multilingual Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":50,"citationCount":6,"influentialCitationCount":0,"publicationDate":2022,"authors":"Viktor Hangya,Hossain Shaikh Saadi,Alexander M. Fraser","id":"2ac5442a32988f86730e460b3198f475592ae410","summary":"This work proposes an unsupervised approach to improve the cross-lingual representations of low-resource languages by bootstrapping word translation pairs from monolingual corpora and using them to improve language alignment in pre-trained language models.","score":2},{"url":"https://www.semanticscholar.org/paper/7abfcf54eeac3a5c298717a4f0fe8a5daceeaa42","title":"Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":48,"citationCount":6,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Zhijun Wang,Xuebo Liu,Min Zhang","id":"7abfcf54eeac3a5c298717a4f0fe8a5daceeaa42","summary":"Experiments show that StrokeNet can provide a significant performance boost over the strong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17 Chinese-English task which is better than any previously reported results without using monolingual data.","score":2},{"url":"https://www.semanticscholar.org/paper/f8665a1a5dcf4c771c146edc67c353f007355911","title":"Leveraging Pre-training Models for Speech Processing","venue":"","year":null,"referenceCount":257,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"f8665a1a5dcf4c771c146edc67c353f007355911","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/e873ddaf58ff92ae0492983cf57221fb25837f4e","title":"Strategies in subword tokenization: humans vs. algorithms","venue":"","year":2021,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"","id":"e873ddaf58ff92ae0492983cf57221fb25837f4e","summary":"A new method to an- 010 alyze subword segmentation strategies relying on a spatial analysis of the distribution of sub- 012 wordsâ lengths is proposed, which shows that humans tend to balance creativity and consistency, while algorithms tend to be either strongly biased or inconsistent.","score":1},{"url":"https://www.semanticscholar.org/paper/ae7fc32df9401a86353185515f4fd5e2020ac922","title":"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations","venue":"arXiv.org","year":2021,"referenceCount":41,"citationCount":5,"influentialCitationCount":1,"publicationDate":2021,"authors":"Theodore Jiang,Li Fang,Kai Wang","id":"ae7fc32df9401a86353185515f4fd5e2020ac922","summary":"This study introduces MutFormer, a transformer-based model for the prediction of pathogenic missense mutations, using reference and mutated amino acid sequences from the human genome as the features, and shows that MutFormer outperforms a variety of existing tools in pathogenicity prediction.","score":1},{"url":"https://www.semanticscholar.org/paper/7175caf7568d46c857380d0e5b64653819d5cc45","title":"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization","venue":"Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)","year":2021,"referenceCount":75,"citationCount":22,"influentialCitationCount":0,"publicationDate":2021,"authors":"R. Goot,Alan Ramponi,A. Zubiaga,Barbara Plank,Benjamin Muller,I. Roncal,Nikola Ljubesic,Ãzlem ÃetinoÄlu,Rahmad Mahendra,Talha ÃolakoÄlu,Timothy Baldwin,Tommaso Caselli,Wladimir Sidorenko,Bruno Kessler","id":"7175caf7568d46c857380d0e5b64653819d5cc45","summary":"The MULTILEXNORM shared task provides the largest publicly available multilingual lexical normalization benchmark including 12 language variants and proposes a homogenized evaluation setup with both intrinsic and extrinsic evaluation.","score":1},{"url":"https://www.semanticscholar.org/paper/de0e1f9980afa7949df64d53b8ae7a2f59c55579","title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages","venue":"arXiv.org","year":2021,"referenceCount":77,"citationCount":5,"influentialCitationCount":0,"publicationDate":2021,"authors":"Kalpesh Krishna,Deepak Nathani,Xavier GarcÃ­a,Bidisha Samanta,P. Talukdar","id":"de0e1f9980afa7949df64d53b8ae7a2f59c55579","summary":"This work pushes the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases, and achieves 2-3x better performance and output diversity in formality transfer and code-mixing addition across five Indian languages.","score":1},{"url":"https://www.semanticscholar.org/paper/c31da40e092e808f20f782eb1ee9d4dec6351708","title":"Overview of EXIST 2022: sEXism Identification in Social neTworks","venue":"Proces. del Leng. Natural","year":2022,"referenceCount":36,"citationCount":104,"influentialCitationCount":11,"publicationDate":2022,"authors":"F. RodrÃ­guezâSÃ¡nchez,Jorge Carrillo-de-Albornoz,Laura Plaza,Julio Gonzalo,Paolo Rosso,Miriam Comet,Trinidad Donoso","id":"c31da40e092e808f20f782eb1ee9d4dec6351708","summary":"The organization, goals, and results of the sEXism Identification in Social neTworks (EXIST)2022 challenge, a shared task proposed for the second year at IberLEF, consists of two challenges: sexism identification and sexism categorization of tweets and gabs, both in Spanish and English.","score":1},{"url":"https://www.semanticscholar.org/paper/3fa2e17332bb2888318f504cf37026001b932900","title":"Comparison of Token- and Character-Level Approaches to Restoration of Spaces, Punctuation, and Capitalization in Various Languages","venue":"International Conference on Natural Language and Speech Processing","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Laurence Dyer,Anthony Hughes,Dhwani Shah,Burcu Can","id":"3fa2e17332bb2888318f504cf37026001b932900","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/710ceab1ff91b96e8596b8400ebf2912ee6e2836","title":"Machine Translation for Multilingual Intent Detection and Slots Filling","venue":"MMNLU","year":2022,"referenceCount":44,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans","id":"710ceab1ff91b96e8596b8400ebf2912ee6e2836","summary":"This work uses the inherent multilingual aspect of translation models for the task of multilingual intent classification and slot filling and reveals that they work equally well with general-purpose multilingual text-to-text models.","score":1},{"url":"https://www.semanticscholar.org/paper/038103632b24619818b159f5ca37b848744817db","title":"DENTRA: Denoising and Translation Pre-training for Multilingual Machine Translation","venue":"Conference on Machine Translation","year":2022,"referenceCount":29,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Samta Kamboj,Sunil Kumar Sahu,Neha Sengupta","id":"038103632b24619818b159f5ca37b848744817db","summary":"DENTRA, a novel pre-training strategy for a multilingual sequence-to-sequence transformer model, combines denoising and translation objectives to incorporate both monolingual and bitext corpora in 24 African, English, and French languages is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/0ffb0b109acddf0067aec252221e0ea04d317ddc","title":"A Framework for Sexism Detection on Social Media via ByT5 and TabNet","venue":"IberLEF@SEPLN","year":2022,"referenceCount":16,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"A. Younus,M. A. Qureshi","id":"0ffb0b109acddf0067aec252221e0ea04d317ddc","summary":"A combination of byte-level model ByT5 with tabular modeling via TabNet that has at its core an ability to take into account platform and language aspects of the challenging task of sexism detection is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","title":"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome","venue":"","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Theodore Jiang,Li Fang,Kai Wang","id":"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","summary":"The introduction of MutFormer, a transformer-based model for the prediction of deleterious missense mutations that uses reference and mutated protein sequences from the human genome as the primary features, and which successfully considers sequence features that are not explored in previous studies.","score":1},{"url":"https://www.semanticscholar.org/paper/6102fe88a512290b80e83ed2fe17606b166e505a","title":"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin","venue":"LT4HALA","year":2022,"referenceCount":12,"citationCount":5,"influentialCitationCount":1,"publicationDate":2022,"authors":"Krzysztof WrÃ³bel,Krzysztof Nowak","id":"6102fe88a512290b80e83ed2fe17606b166e505a","summary":"The paper features a thorough discussion of part-of-speech and lemmatization errors which shows how the system performance may be improved for Classical, Medieval and Neo-Latin texts.","score":1},{"url":"https://www.semanticscholar.org/paper/143659fcf93f33e9139f594cf8111c6bc85c04fa","title":"Overview of the EvaLatin 2022 Evaluation Campaign","venue":"LT4HALA","year":2022,"referenceCount":14,"citationCount":8,"influentialCitationCount":1,"publicationDate":2022,"authors":"R. Sprugnoli,M. Passarotti,F. M. Cecchini,Margherita Fantoli,Giovanni Moretti","id":"143659fcf93f33e9139f594cf8111c6bc85c04fa","summary":"This paper describes the organization and the results of the second edition of EvaLatin, the campaign for the evaluation of Natural Language Processing tools for Latin, and the three shared tasks proposed in EvaLatin 2022, i.","score":1},{"url":"https://www.semanticscholar.org/paper/2005311276a1a90dc17cf6e2ca7725fee2e76e1e","title":"BasqueGLUE: A Natural Language Understanding Benchmark for Basque","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":39,"citationCount":5,"influentialCitationCount":1,"publicationDate":2022,"authors":"Gorka Urbizu,IÃ±aki San Vicente,X. Saralegi,Rodrigo Agerri,Aitor Soroa Etxabe","id":"2005311276a1a90dc17cf6e2ca7725fee2e76e1e","summary":"BasqueGLUE is presented, the first NLU benchmark for Basque, a less-resourced language, which has been elaborated from previously existing datasets and following similar criteria to those used for the construction of GLUE and SuperGLUE.","score":1},{"url":"https://www.semanticscholar.org/paper/b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","title":"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Dibyanayan Bandyopadhyay,Arkadipta De,Baban Gain,Tanik Saikh,Asif Ekbal","id":"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","summary":"It is argued that the transfer learning-based loss objective is model agnostic and thus can be used with other deep learning- based architectures for cross-lingual NLI.","score":1},{"url":"https://www.semanticscholar.org/paper/5f77157038dc7f189db7e72ea58567039222d9df","title":"Examining Single Sentence Label Leakage in Natural Language Inference Datasets","venue":"","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Michael Stephen Saxon,Xinyi Wang,Wenda Xu,William Yang Wang","id":"5f77157038dc7f189db7e72ea58567039222d9df","summary":"This work examines how regular NLI models cheat on single sentence label leakage, and discusses how to ameliorate this.","score":1},{"url":"https://www.semanticscholar.org/paper/2b6b38b66fcca218cb2f381e5d4711b5c99a784f","title":"Training Text-to-Text Transformers with Privacy Guarantees","venue":"Findings","year":2022,"referenceCount":35,"citationCount":13,"influentialCitationCount":1,"publicationDate":2022,"authors":"N. Ponomareva,Jasmijn Bastings,Sergei Vassilvitskii","id":"2b6b38b66fcca218cb2f381e5d4711b5c99a784f","summary":"By using recent advances in JAX and XLA, this work can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE).","score":1},{"url":"https://www.semanticscholar.org/paper/32290d428b50f64d1cfee6ea658dc6ba977b26e9","title":"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia","venue":"LTEDI","year":2022,"referenceCount":31,"citationCount":9,"influentialCitationCount":0,"publicationDate":2022,"authors":"I. S. Upadhyay,KV Aditya Srivatsa,Radhika Mamidi","id":"32290d428b50f64d1cfee6ea658dc6ba977b26e9","summary":"The approach to classify homophobia and transphobia in social media comments using an ensemble of transformer-based models to build a classifier that ranked 2nd for English, 8th for Tamil and 10th for Chennai-English.","score":1},{"url":"https://www.semanticscholar.org/paper/9758782feed4b4b9bf0ec18b802462e8023a7f83","title":"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models","venue":"Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing","year":2022,"referenceCount":33,"citationCount":9,"influentialCitationCount":0,"publicationDate":2022,"authors":"Odunayo Jude Ogundepo,Akintunde Oladipo,Mofetoluwa Adeyemi,Kelechi Ogueji and Jimmy Lin","id":"9758782feed4b4b9bf0ec18b802462e8023a7f83","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","title":"On the Influence of Tokenizers in NLP Pipelines","venue":"","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","summary":"This thesis examines the influence of tokenization in NLP pipelines, by analyzing, reproducing, and quantifying claims from the token-free NLP literature, using the example of NER, and concludes that token- free models, like ByT5, offer significant advantages over their tokenizer-based alternatives.","score":1},{"url":"https://www.semanticscholar.org/paper/7ef91d8896c406865e91d90f60a8592a03ed6ded","title":"TRIP: Accelerating Document-level Multilingual Pre-training via Triangular Document-level Pre-training on Parallel Data Triplets","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Hongyuan Lu,Haoyang Huang,Shuming Ma,Dongdong Zhang,Wai Lam,Zhaochuan Gao,Anthony Aue,Arul Menezes,Furu Wei","id":"7ef91d8896c406865e91d90f60a8592a03ed6ded","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/8a930572177545e7394ba5cd03e9342142da564e","title":"Better Quality Pre-training Data and T5 Models for African Languages","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Akintunde Oladipo,Mofetoluwa Adeyemi,Orevaoghene Ahia,A. Owodunni,Odunayo Ogundepo,David Ifeoluwa Adelani,Jimmy Lin","id":"8a930572177545e7394ba5cd03e9342142da564e","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/104974b36654acbc613226b9f70c4ad503655aa1","title":"The Linearity of the Effect of Surprisal on Reading Times across Languages","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Weijie Xu,Jason Chon,Tianran Liu,Richard Futrell","id":"104974b36654acbc613226b9f70c4ad503655aa1","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/ce093816af5d8c1a7aab74808245048b7f3669a5","title":"The Helsinki-NLP Submissions at NADI 2023 Shared Task: Walking the Baseline","venue":"ARABICNLP","year":2023,"referenceCount":26,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Yves Scherrer,Aleksandra Miletic,Olli Kuparinen","id":"ce093816af5d8c1a7aab74808245048b7f3669a5","summary":"The Helsinki-NLP team participated in the NADI 2023 shared tasks on Arabic dialect translation and used statistical (SMT) and neural machine translation (NMT) methods and explored character- and subword-based data preprocessing.","score":1},{"url":"https://www.semanticscholar.org/paper/c6410d9791f390aa43ed26bf5567fdde6fc89f25","title":"Type Enhanced BERT for Correcting NER Errors","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Kuai Li,Chen Chen,Tao Yang,Tianming Du,Peijie Yu,Dong Du,Feng Zhang","id":"c6410d9791f390aa43ed26bf5567fdde6fc89f25","summary":"A gazetteer containing named entities and corresponding possible entity types is constructed and a type-enhanced BERT (TyBERT), a method that integrates the named entityâs type information into BERT by an adapter layer is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/ee26bb846f04240fe6e82170a4b15f3b8bd3a09e","title":"Designing Human-Centric Foundation Models","venue":"HCAI4U@CHItaly","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Narendra Patwardhan,Shreya Shetye,Lidia Marassi,M. Zuccarini,T. Maiti,Tarry Singh","id":"ee26bb846f04240fe6e82170a4b15f3b8bd3a09e","summary":"The potential of sustainability and programmability principles in architectural design to create a more context-aware and user-focused AI ecosystem is investigated.","score":1},{"url":"https://www.semanticscholar.org/paper/30869d285642a8b981702d2a54be0ac54f01aa01","title":"Composer's Assistant: Interactive Transformers for Multi-Track MIDI Infilling","venue":"arXiv.org","year":2023,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Martin E. Malandro","id":"30869d285642a8b981702d2a54be0ac54f01aa01","summary":"Two T5-like models are trained to solve the task of multi-track MIDI inï¬lling when arbitrary (track, measure) pairs of information have been deleted from a contiguous slice of measures from a MIDI sequence, and their results have implications for the training of neural networks in other small-vocabulary domains.","score":1},{"url":"https://www.semanticscholar.org/paper/29e593736c95fd043b7cbb0211d2f7b5cb70e5bf","title":"LLI-UAM Team at FinancES 2023: Noise, Data Augmentation and Hallucinations","venue":"IberLEF@SEPLN","year":2023,"referenceCount":13,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Jordi Porta Zamorano,Yanco Torterolo,Antonio Moreno-Sandoval","id":"29e593736c95fd043b7cbb0211d2f7b5cb70e5bf","summary":"This paper describes the T5-based system developed for FinancES 2023 Shared Task by the Laboratorio de LingÃ¼Ã­stica InformÃ¡tica at UAM and identifies the best model for each task.","score":1},{"url":"https://www.semanticscholar.org/paper/e4a1e9bb360f29aceb56079f52484c4a4de1298d","title":"IntÃ©gration du raisonnement numÃ©rique dans les modÃ¨les de langue : Ãtat de lâart et direction de recherche","venue":"JEPTALNRECITAL","year":2023,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Sarah Abchiche,Lynda Said Lhadj,V. Guigue,Laure Soulier","id":"e4a1e9bb360f29aceb56079f52484c4a4de1298d","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/f83740ef449e705187a0f7d6f76b819c99340bd1","title":"Exploring the Limits of Small Language Models","venue":"","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Nicholas Lee,Kurt Keutzer Gopala,Krishna Anumanchipalli","id":"f83740ef449e705187a0f7d6f76b819c99340bd1","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/17d2c990dd6d25f433b02ce611ce0a57db038dc5","title":"Jetsons at the FinNLP-2023: Using Synthetic Data and Transfer Learning for Multilingual ESG Issue Classification","venue":"FINNLP","year":2023,"referenceCount":17,"citationCount":2,"influentialCitationCount":0,"publicationDate":2023,"authors":"Parker Glenn,Alolika Gon,Nikhil Kohli,Sihan Zha,Parag Dakle,Preethi Raghavan","id":"17d2c990dd6d25f433b02ce611ce0a57db038dc5","summary":"The various approaches by the Jetsons team for the Multilin-gual ESG Issue Identification Task (ML-ESG) to classify articles into ESG issues they are related to are described.","score":1},{"url":"https://www.semanticscholar.org/paper/8e410a836174847e3fc4e5993f625e89e34dd3d1","title":"RST Discourse Parsing as Text-to-Text Generation","venue":"IEEE/ACM Transactions on Audio Speech and Language Processing","year":2023,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Xinyu Hu,Xiaojun Wan","id":"8e410a836174847e3fc4e5993f625e89e34dd3d1","summary":"This article introduces an end- to-end method for sentence-level RST discourse parsing via transforming it into a text-to-text generation task, which can also be simply applied to document-level parsing.","score":1},{"url":"https://www.semanticscholar.org/paper/659be1ff350634f50cc066d258ee6a45e697e552","title":"SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing","venue":"Special Interest Group on Computational Morphology and Phonology Workshop","year":2023,"referenceCount":18,"citationCount":2,"influentialCitationCount":0,"publicationDate":2023,"authors":"Taiqi He,Lindia Tjuatja,Nathaniel R. Robinson,Shinji Watanabe,David R. Mortensen,Graham Neubig,L. Levin","id":"659be1ff350634f50cc066d258ee6a45e697e552","summary":"In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.","score":1},{"url":"https://www.semanticscholar.org/paper/88108f061379045c299d62f487694cb4e6d6d4ff","title":"Findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing","venue":"Special Interest Group on Computational Morphology and Phonology Workshop","year":2023,"referenceCount":69,"citationCount":5,"influentialCitationCount":2,"publicationDate":2023,"authors":"Michael Ginn,Sarah Moeller,Alexis Palmer,Anna Stacey,Garrett Nicolai,Mans Hulden,Miikka Silfverberg","id":"88108f061379045c299d62f487694cb4e6d6d4ff","summary":"This first iteration of the SIGMORPHON 2023 Shared Task on Interlinear Glossing explores glossing of a set of six typologically diverse languages: Arapaho, Gitksan, Lezgi, NatÃ¼gu, Tsez and Uspanteko.","score":1},{"url":"https://www.semanticscholar.org/paper/ffa4ac4a51148208cadb4084dddab954e5f57400","title":"Resolving Elliptical Compounds in German Medical Text","venue":"Workshop on Biomedical Natural Language Processing","year":2023,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Niklas Kammer,Florian Borchert,Silvia Winkler,Gerard de Melo,M. Schapranow","id":"ffa4ac4a51148208cadb4084dddab954e5f57400","summary":"A generative encoder-decoder Transformer model is proposed, allowing for a simple end-to-end resolution of ECCNPs from raw input strings with very high accuracy, and is compared to an elaborate rule-based baseline, which the generative model outperforms by a large margin.","score":1},{"url":"https://www.semanticscholar.org/paper/c3904ef47bec4fc2bfd3d390370681c33542d62d","title":"Fast Whitespace Correction with Encoder-Only Transformers","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"H. Bast,Matthias Hertel,S. Walter","id":"c3904ef47bec4fc2bfd3d390370681c33542d62d","summary":"This work provides an easy-to-use tool that is over 900 times faster than the previous best tool, with the same high quality, and compares two Transformer-based models, a character-level encoder-decoder model and a byte-levelencoder-only model.","score":1},{"url":"https://www.semanticscholar.org/paper/e5adb9bf3f5ed9c253f38949b22e86775dca443a","title":"Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Jingyu Zhang,Alexandra DeLucia,Chenyu Zhang,Mark Dredze","id":"e5adb9bf3f5ed9c253f38949b22e86775dca443a","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/1d578d2bdb5c920b224cfca73868566731eaeebd","title":"Towards Analysis of Biblical Entities and Names using Deep Learning","venue":"International Journal of Advanced Computer Science and Applications","year":2023,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Mikolaj Martinjak,D. Lauc,Ines Skelac","id":"1d578d2bdb5c920b224cfca73868566731eaeebd","summary":"The findings of this study demonstrate that deep learning could help uncover interesting connections between individuals who may have initially been considered less important and highlighted the critical role of onomastic sciences and the philosophy of language in analyzing the richness and importance of human and other proper names in biblical texts.","score":1},{"url":"https://www.semanticscholar.org/paper/13b8060acc3db1fc555f6e55368f6d02899a1698","title":"FairPrism: Evaluating Fairness-Related Harms in Text Generation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":48,"citationCount":5,"influentialCitationCount":0,"publicationDate":2023,"authors":"Eve Fleisig,Aubrie Amstutz,Chad Atalla,Su Lin Blodgett,Hal DaumÃ©,Alexandra Olteanu,Emily Sheng,Dan Vann,Hanna M. Wallach","id":"13b8060acc3db1fc555f6e55368f6d02899a1698","summary":"FairPrism is introduced, a dataset of 5,000 examples of AI-generated English text with detailed human annotations covering a diverse set of harms relating to gender and sexuality that can be used to diagnose the types of fairness-related harms that AI text generation systems cause.","score":1},{"url":"https://www.semanticscholar.org/paper/82e1313f28afde442930b94bc6ed582d17e8d4b3","title":"Generating Errors: OCR Post-Processing for Icelandic","venue":"Nordic Conference of Computational Linguistics","year":2023,"referenceCount":19,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Atli Jasonarson,SteinÃ¾Ã³r SteingrÃ­msson,E. SigurÃ°sson,Ãrni MagnÃºsson,F. Ingimundarson","id":"82e1313f28afde442930b94bc6ed582d17e8d4b3","summary":null,"score":1}]}
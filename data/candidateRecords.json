{"papers":[{"url":"https://www.semanticscholar.org/paper/9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation","venue":"SIGMORPHON","year":2022,"referenceCount":81,"citationCount":5,"influentialCitationCount":0,"publicationDate":"15/06/2022","authors":"Khuyagbaatar Batsuren,Gábor Bella,Aryaman Arora,Viktor Martinovi'c,Kyle Gorman,Zdenvek vZabokrtsk'y,A. Ganbold,vS'arka Dohnalov'a,Magda vSevvc'ikov'a,Katevrina Pelegrinov'a,Fausto Giunchiglia,Ryan Cotterell,Ekaterina Vylomova","id":"9ccad0208b50042d8378b77700146a98bd22ea3f","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability","venue":"ArXiv","year":2022,"referenceCount":137,"citationCount":3,"influentialCitationCount":0,"publicationDate":"03/07/2022","authors":"Mai Ha Vu,R. Akbar,Philippe A. Robert,B. Swiatczak,V. Greiff,G. K. Sandve,Dag Trygve Tryslew Haug","id":"8f248b5476666e700390f7f8ff6ca923f97d726c","summary":"It is argued that guidance drawn from linguistics, a field specialized in analytical rule extraction from natural language data, can aid with building more interpretable protein LMs that have learned relevant domain-specific rules.","score":3},{"url":"https://www.semanticscholar.org/paper/bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/12/2022","authors":"Nathan Godey,Roman Castagn'e,Eric Villemonte de la Clergerie,Benoît Sagot","id":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","summary":"MANTa is a differentiable tokenizer trained end-to-end with the language model that improves robustness to character perturbations and out-of-domain data and is considerably faster than strictly byte-level models.","score":3},{"url":"https://www.semanticscholar.org/paper/bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019","venue":"IWSLT","year":2019,"referenceCount":31,"citationCount":3,"influentialCitationCount":0,"publicationDate":2019,"authors":"Tejas Srinivasan,Ramon Sanabria,Florian Metze","id":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","summary":"Block Multitask Learning (BMTL) is presented, a novel NMT architecture that predicts multiple targets of different granularities simulta- neously, removing the need to search for the optimal subword segmentation strategy.","score":2},{"url":"https://www.semanticscholar.org/paper/fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation","venue":"IWSLT","year":2019,"referenceCount":32,"citationCount":3,"influentialCitationCount":0,"publicationDate":"27/10/2019","authors":"Tejas Srinivasan,Ramon Sanabria,Florian Metze","id":"fec6def294027a2ce9094267ce7b7d57f78daf74","summary":"Block Multitask Learning (BMTL), a novel NMT architecture that predicts multiple targets of different granularities simultaneously, removing the need to search for the optimal segmentation strategy, is presented.","score":2},{"url":"https://www.semanticscholar.org/paper/debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes","venue":"ArXiv","year":2020,"referenceCount":32,"citationCount":3,"influentialCitationCount":0,"publicationDate":"05/04/2020","authors":"Thamme Gowda,Jonathan May","id":"debb3877b778eeb8689729d37e2b90f9f000d877","summary":"This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and investigates the effect of class imbalance on NMT.","score":2},{"url":"https://www.semanticscholar.org/paper/e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization","venue":"International Conference on Learning Representations","year":2021,"referenceCount":69,"citationCount":55,"influentialCitationCount":11,"publicationDate":"23/06/2021","authors":"Yi Tay,V. Tran,Sebastian Ruder,Jai Gupta,Hyung Won Chung,Dara Bahri,Zhen Qin,Simon Baumgartner,Cong Yu,Donald Metzler","id":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","summary":"This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion and paves the way for highly performant token-free models that are trained completely end-to-end.","score":2},{"url":"https://www.semanticscholar.org/paper/2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?","venue":"EMNLP","year":2021,"referenceCount":50,"citationCount":8,"influentialCitationCount":1,"publicationDate":"02/09/2021","authors":"Chantal Amrhein,Rico Sennrich","id":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","summary":"It is found that learning to analyse and generate morphologically complex surface representations is still challenging, especially for nonconcatenative morphological phenomena like reduplication or vowel harmony and for rare word stems.","score":2},{"url":"https://www.semanticscholar.org/paper/3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class","venue":"ACM Transactions on Software Engineering and Methodology","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/07/2022","authors":"Luke Dramko,Jeremy Lacomis,Pengcheng Yin,Edward J. Schwartz,Miltiadis Allamanis,Graham Neubig,Bogdan Vasilescu,Claire Le Goues","id":"3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","summary":"This work investigates how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains, and evaluates DIRE’s overall performance without respect to data quality.","score":2},{"url":"https://www.semanticscholar.org/paper/e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","title":"Checks and Strategies for Enabling Code-Switched Machine Translation","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Thamme Gowda,Mozhdeh Gheini,Jonathan May","id":"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","summary":"This work explores multilingual NMT models’ ability to handle code-switched text, and proposes checks to measure switching capability and investigates simple and effective data augmentation methods that can enhance an NMT model’s ability to support code- Switched text.","score":2},{"url":"https://www.semanticscholar.org/paper/9d83941a205e31d394f614424cdc553cea9e35f9","title":"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":1,"influentialCitationCount":1,"publicationDate":"02/12/2022","authors":"Laia Tarrés,Gerard I. Gállego,Xavier Giró-i-Nieto,Jordi Torres","id":"9d83941a205e31d394f614424cdc553cea9e35f9","summary":"This paper describes the system developed at the Universitat Politècnica de Catalunya for the Workshop on Machine Translation 2022 Sign Language Translation Task, in particular, for the sign-to-text direction using a Transformer model implemented with the Fairseq modeling toolkit.","score":2},{"url":"https://www.semanticscholar.org/paper/8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey","venue":"Journal of Artificial Intelligence Research","year":2021,"referenceCount":366,"citationCount":20,"influentialCitationCount":0,"publicationDate":"14/04/2021","authors":"Danielle Saunders","id":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","summary":"This work surveys approaches to domain adaptation for NMT, particularly where a system may need to translate across multiple domains, and divides techniques into those revolving around data selection or generation, model architecture, parameter adaptation procedure, and inference procedure.","score":2},{"url":"https://www.semanticscholar.org/paper/b162638cd42b65e6add199b0b34f1b375070fc7c","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models","venue":"AACL/IJCNLP","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2022","authors":"Syrielle Montariol,Arij Riabi,Djamé Seddah","id":"b162638cd42b65e6add199b0b34f1b375070fc7c","summary":"It is shown how hate speech detection models benefit from a cross-lingual knowledge proxy brought by auxiliary tasks fine-tuning and highlight these tasks’ positive impact on bridging the hate speech linguistic and cultural gap between languages.","score":2},{"url":"https://www.semanticscholar.org/paper/cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color","venue":"ACL","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sidsel Boldsen,Manex Agirrezabal,Nora Hollenstein","id":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","summary":"This cross-lingual analysis shows that textual character representations correlate strongly with sound representations for languages using an alphabetic script, while shape correlates with featural scripts.","score":2},{"url":"https://www.semanticscholar.org/paper/3227844af4d38bfa702781e321cf6712bf537e2c","title":"Word-level Perturbation Considering Word Length and Compositional Subwords","venue":"FINDINGS","year":2022,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki","id":"3227844af4d38bfa702781e321cf6712bf537e2c","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a","title":"Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation","venue":"FINDINGS","year":2022,"referenceCount":16,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/03/2022","authors":"Sho Takase,Tatsuya Hiraoka,Naoaki Okazaki","id":"75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a","summary":"An inference strategy that approximates the marginalized likelihood by using multiple segmentations including the most plausible segmentation and several sampled segmentations that improves the performance of models trained with subword regularization in low-resource machine translation tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/cd2cd8cb1aa27e7f2404fe335dd5832e14ebb5f8","title":"Composing Word Embeddings for Compound Words Using Linguistic Knowledge","venue":"ACM Transactions on Asian and Low-Resource Language Information Processing","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/09/2022","authors":"Kanako Komiya,Shinji Kouno,Takumi Seitou,Teruo Hirabayashi","id":"cd2cd8cb1aa27e7f2404fe335dd5832e14ebb5f8","summary":"An attempt to compose word embeddings of a compound word from its constituent words in Japanese using “short unit” and “long unit,” both of which are the units of terms in UniDic——a Japanese dictionary compiled by the National Institute for Japanese Language and Linguistics——for constituent and compound words, respectively.","score":1},{"url":"https://www.semanticscholar.org/paper/8c82d3d758897ef9f166924683831ecf6085f21a","title":"MaxMatch-Dropout: Subword Regularization for WordPiece","venue":"COLING","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/09/2022","authors":"Tatsuya Hiraoka","id":"8c82d3d758897ef9f166924683831ecf6085f21a","summary":"The proposed method, MaxMatch-Dropout, randomly drops words in a search using the maximum matching algorithm for tokenization to realize finetuning with subword regularization for popular pretrained language models such as BERT-base.","score":1},{"url":"https://www.semanticscholar.org/paper/75a05bffa2cc35a876ce04edb0b57f9592716d3b","title":"Extending the Subwording Model of Multilingual Pretrained Models for New Languages","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"K. Imamura,E. Sumita","id":"75a05bffa2cc35a876ce04edb0b57f9592716d3b","summary":"This paper adds new subwords to the SentencePiece tokenizer to apply a multilingual pretrained model to new languages (Inuk-titut in this paper) and applies the mBART-50 pret trained model to English-Inuktituts translation.","score":1},{"url":"https://www.semanticscholar.org/paper/00f899ec60300ca972962e6014d63b4ea835ef38","title":"CMS Optimisation with Deep Learning Techniques","venue":"AIABI@AI*IA","year":2021,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Alberto Schiaffino,Matteo Reina,Ricardo Anibal Matamoros Aragon,F. Epifania,Francesco Ruggeri,Ignazio Maria Castrignano,Luca Marconi","id":"00f899ec60300ca972962e6014d63b4ea835ef38","summary":"The Content Management System (CMS), a system capable of creating and maintaining websites, forums and applications for customers, is studied, which has an intrinsic predisposition to provide services based on Machine Learning (ML), thus being able to add to the solutions offered by the most innovative ML systems.","score":1},{"url":"https://www.semanticscholar.org/paper/ca6d0b5ea60687b707fb5ed35f7433f939537e2a","title":"Sentiment Analysis on Dravidian Code-Mixed YouTube Comments using Paraphrase XLM-RoBERTa Model","venue":"FIRE","year":2021,"referenceCount":21,"citationCount":1,"influentialCitationCount":1,"publicationDate":2021,"authors":"Yandrapati Prakash Babu,R. Eswari","id":"ca6d0b5ea60687b707fb5ed35f7433f939537e2a","summary":"This work uses the Paraphrase XLM-RoBERTa model to solve the sentiment classification problem of YouTube comments in code-mixed language, and ranks first, second and third on Tamil, Malayalam, and Kannada code-Mixed language datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5","title":"Pretrained Language Models for Text Generation: A Survey","venue":"International Joint Conference on Artificial Intelligence","year":2021,"referenceCount":246,"citationCount":50,"influentialCitationCount":5,"publicationDate":"21/05/2021","authors":"Junyi Li,Tianyi Tang,Wayne Xin Zhao,Ji-rong Wen","id":"d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5","summary":"This paper presents an overview of the major advances achieved in the topic of pretrained language models for text generation and discusses how to adapt existing PLMs to model different input data and satisfy special properties in the generated text.","score":1},{"url":"https://www.semanticscholar.org/paper/d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","title":"A Primer on Pretrained Multilingual Language Models","venue":"ArXiv","year":2021,"referenceCount":138,"citationCount":17,"influentialCitationCount":3,"publicationDate":"01/07/2021","authors":"Sumanth Doddapaneni,Gowtham Ramesh,Anoop Kunchukuttan,Pratyush Kumar,Mitesh M. Khapra","id":"d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","summary":"A review of the existing literature covering the above broad areas of research pertaining to MLLMs and some promising directions of future research are recommended.","score":1},{"url":"https://www.semanticscholar.org/paper/7c59a4e11eed57aea54d27bbb28a3e4dbc26b0f3","title":"Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling","venue":"ArXiv","year":2021,"referenceCount":94,"citationCount":14,"influentialCitationCount":0,"publicationDate":"27/08/2021","authors":"Adeep Hande,Karthik Puranik,Konthala Yasaswini,R. Priyadharshini,Sajeetha Thavareesan,Anbukkarasi Sampath,Kogilavani Shanmugavadivel,D. Thenmozhi,Bharathi Raja Chakravarthi","id":"7c59a4e11eed57aea54d27bbb28a3e4dbc26b0f3","summary":"This work intends to classify code-mixed social media comments/posts in the Dravidian languages of Tamil, Kannada, andMalayalam to improve offensive language identification by generating pseudo-labels on the dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/1b6798695de27880009346c6c2023665139b0014","title":"Ontology-Based Question Answering over Corporate Structured Data","venue":"2021 International Symposium on Knowledge, Ontology, and Theory (KNOTH)","year":2021,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/11/2021","authors":"S. Gorshkov,Constantin Kondratiev,Roman Shebalov","id":"1b6798695de27880009346c6c2023665139b0014","summary":"The dialogue engine for a chat bot which can keep the conversation context and ask clarifying questions, simulating some aspects of the human logical thinking is described, which uses graph-based algorithms to avoid gathering datasets, required in the neural nets-based approaches, and provide better explainability of the models.","score":1},{"url":"https://www.semanticscholar.org/paper/6fb5dc674bf0013ad5e269b3905c0f4253c3890b","title":"Investigating Cross-Linguistic Gender Bias in Hindi-English Across Domains","venue":"ArXiv","year":2021,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2021","authors":"Somya Khosla","id":"6fb5dc674bf0013ad5e269b3905c0f4253c3890b","summary":"This review concludes with a discussion of the aims and objectives of the study and a comparison of its initial findings with those of a similar study a decade ago.","score":1},{"url":"https://www.semanticscholar.org/paper/551dc8a18770a5807c2ea0724701c3f946bc7c0c","title":"Hindi/Bengali Sentiment Analysis Using Transfer Learning and Joint Dual Input Learning with Self Attention","venue":"BOHR International Journal of Research on Natural Language Computing","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/02/2022","authors":"Shahrukh Khan,Mahnoor Shahid","id":"551dc8a18770a5807c2ea0724701c3f946bc7c0c","summary":"This work explores how to effectively use deep neural networks in transfer learning and joint dual input learning settings to effectively classify sentiments and detect hate speech in Hindi and Bengali data.","score":1},{"url":"https://www.semanticscholar.org/paper/bdc2819cbb6e950d4d96bd492e7484717177831d","title":"A C OMPARATIVE E VALUATION OF T RANSFORMER M ODELS FOR D E - IDENTIFICATION OF C LINICAL T EXT D ATA","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"bdc2819cbb6e950d4d96bd492e7484717177831d","summary":"Transformer models architectures (after suitable hyper-parameter optimization) offer a satisfactory solution for the clinical text de-identiﬁcation problem; and could be readily adopted in clinical scenarios where clinicians/researchers are looking to use de-identified clinical text data to facilitate quality improvement and enhanced patient care.","score":1},{"url":"https://www.semanticscholar.org/paper/4e8e404a6fff91ac55f9c7476a041e2ba31c0454","title":"Know Better – A Clickbait Resolving Challenge","venue":"LREC","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Benjamin Hättasch,Carsten Binnig","id":"4e8e404a6fff91ac55f9c7476a041e2ba31c0454","summary":"This paper proposes to construct approaches that can automatically extract the relevant information from such an article, which is called clickbait resolving, and shows that models fine-tuned on the data can outperform general question answering models, while providing a systematic approach to evaluate the results.","score":1},{"url":"https://www.semanticscholar.org/paper/f3c6ffbb9ed061ebb17623c066b84c596f55b796","title":"міток маски","venue":"","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"f3c6ffbb9ed061ebb17623c066b84c596f55b796","summary":"A new approach of generating semantic segmentation masks, using only classification labels of the image, is proposed, which requires much less human resources compared to the classical approach and has successfully completed the task of defect localization.","score":1},{"url":"https://www.semanticscholar.org/paper/cbf284fe85795eaeb94bfb3dc9e98276dcd33788","title":"Neural Architecture Search for Transformers: A Survey","venue":"IEEE Access","year":2022,"referenceCount":250,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Krishna Teja Chitty-Venkata,M. Emani,V. Vishwanath,Arun Somani","id":"cbf284fe85795eaeb94bfb3dc9e98276dcd33788","summary":"An in-depth literature review of approximately 50 state-of-the-art Neural Architecture Search methods is provided, targeting the Transformer model and its family of architectures such as Bidirectional Encoder Representations from Transformers (BERT) and Vision Transformers.","score":1},{"url":"https://www.semanticscholar.org/paper/e85c5fdc4c44d3e17c45c239adfad8d0942e2805","title":"Text-to-Text Extraction and Verbalization of Biomedical Event Graphs","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Giacomo Frisoni,G. Moro,L. Balzani","id":"e85c5fdc4c44d3e17c45c239adfad8d0942e2805","summary":"This work presents the first lightweight framework to solve both event extraction and event verbalization with a unified text-to-text approach, and proposes baseline transformer model results according to multiple biomedical text mining benchmarks and NLG metrics.","score":1},{"url":"https://www.semanticscholar.org/paper/3c861f509f9da5133648a3e8f2b891b7f7638491","title":"MaNLP@SMM4H’22: BERT for Classification of Twitter Posts","venue":"SMM4H","year":2022,"referenceCount":5,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Keshav Kapur,Rajitha Harikrishnan,Sanjay Singh","id":"3c861f509f9da5133648a3e8f2b891b7f7638491","summary":"This literature describes the approach that was used to build a binary classification system, that classifies the tweets related to birthday posts into two classes namely, exact age(positive class) and non-exact age(negative class).","score":1},{"url":"https://www.semanticscholar.org/paper/a40ca8eba2e45b453ee98b846edab928f89b5342","title":"PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations","venue":"","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Hang Gao","id":"a40ca8eba2e45b453ee98b846edab928f89b5342","summary":"A new Transformer-based graph neural network: Patch Graph Transformer (PatchGT), which achieves higher expressiveness than 1-WL-type GNNs, and the empirical study shows that PatchGT achieves com-petitive performances on benchmark datasets and provides interpretability to its predictions.","score":1},{"url":"https://www.semanticscholar.org/paper/b42e3a759348f27cca2f918a6bd0b139a5312e44","title":"A Survey of Pretrained Language Models Based Text Generation","venue":"ArXiv","year":2022,"referenceCount":237,"citationCount":12,"influentialCitationCount":0,"publicationDate":2022,"authors":"Junyi Li,Tianyi Tang,Wayne Xin Zhao,J. Nie,Ji-rong Wen","id":"b42e3a759348f27cca2f918a6bd0b139a5312e44","summary":"This survey presents the recent advances achieved in the topic of PLMs for text generation and introduces three key points of applying PLMs to text generation: how to encode the input data as representations preserving input semantics which can be fused into PLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c","title":"AMMU - A Survey of Transformer-based Biomedical Pretrained Language Models","venue":"Journal of Biomedical Informatics","year":2021,"referenceCount":227,"citationCount":18,"influentialCitationCount":0,"publicationDate":"16/04/2021","authors":"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha","id":"420c897bc67e6f438db522d919d925df1a10aa8c","summary":"This survey discusses core concepts of transformer-based PLMs like pretraining methods, pretraining tasks, fine-tuning methods, and various embedding types specific to biomedical domain, and introduces a taxonomy for transformerbased BPLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/489af1ae6db21c198b6efeffc3ba68313f9bd4b3","title":"Neuromorphic Camera Denoising using Graph Neural Network-driven Transformers","venue":"IEEE Transactions on Neural Networks and Learning Systems","year":2021,"referenceCount":52,"citationCount":3,"influentialCitationCount":0,"publicationDate":"17/12/2021","authors":"Yusra Alkendi,Rana Azzam,Abdulla Ayyad,S. Javed,L. Seneviratne,Y. Zweiri","id":"489af1ae6db21c198b6efeffc3ba68313f9bd4b3","summary":"This article proposes a novel noise filtration algorithm, called GNN-Transformer, to eliminate events that do not represent real log-intensity variations in the observed scene and introduces the known-object ground-truth labeling (KoGTL) approach for generating approximate ground- Truth labels of event streams under various illumination conditions.","score":1},{"url":"https://www.semanticscholar.org/paper/da1ee6de15da1c28d0069e3276f447f2371d281c","title":"Video Transformers: A Survey","venue":"ArXiv","year":2022,"referenceCount":285,"citationCount":14,"influentialCitationCount":2,"publicationDate":"16/01/2022","authors":"Javier Selva,Anders S. Johansen,Sergio Escalera,Kamal Nasrollahi,T. Moeslund,Albert Clap'es","id":"da1ee6de15da1c28d0069e3276f447f2371d281c","summary":"This survey analyses and summarizes the main contributions and trends for adapting Transformers to model video data, and explores how videos are embedded and tokenized, finding a very widspread use of large CNN backbones to reduce dimensionality and a predominance of patches and frames as tokens.","score":1},{"url":"https://www.semanticscholar.org/paper/7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey","venue":"AAAI Conference on Artificial Intelligence","year":2022,"referenceCount":82,"citationCount":8,"influentialCitationCount":0,"publicationDate":"28/01/2022","authors":"Prajjwal Bhargava,Vincent Ng","id":"7e5ca499cd9b932921bda84db98f75087d0b0683","summary":"A survey of commonsense knowledge acquisition and reasoning tasks, the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks, and reflects on future research directions are presented.","score":1},{"url":"https://www.semanticscholar.org/paper/b6adc972f4bdbbb4dc7143713d16a3408a71ef7e","title":"Automated Customer Complaint Processing for Water Utilities Based on Natural Language Processing—Case Study of a Dutch Water Utility","venue":"Water","year":2022,"referenceCount":29,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/02/2022","authors":"Xin Tian,I. Vertommen,Lydia Tsiami,P. van Thienen,S. Paraskevopoulos","id":"b6adc972f4bdbbb4dc7143713d16a3408a71ef7e","summary":"Through a case study about the Water Utility Groningen in the Netherlands, it is demonstrated that NLP can parse language structures and extract intents and sentiments from customer complaints.","score":1},{"url":"https://www.semanticscholar.org/paper/627be995900638aef82279a22013a7b03b5d732d","title":"A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification","venue":"FINDINGS","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/03/2022","authors":"Dairui Liu,Derek Greene,Ruihai Dong","id":"627be995900638aef82279a22013a7b03b5d732d","summary":"A novel practical framework is proposed by utilizing a two-tier attention architecture to decouple the complexity of explanation and the decision-making process and is applied in the context of a news article classification task.","score":1},{"url":"https://www.semanticscholar.org/paper/0ec91fd9c5c3ff9758d9cfddc2e5046eaa3c1ab0","title":"Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking","venue":"ArXiv","year":2022,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/03/2022","authors":"Iñigo Urteaga,Moulay Draidia,Tomer Lancewicki,Shahram Khadivi","id":"0ec91fd9c5c3ff9758d9cfddc2e5046eaa3c1ab0","summary":"A novel multi-armed bandit-based online optimization framework for the sequential selection of pre-training hyperparameters to optimize language model performance, and empirically shows how the proposed Gaussian process based Thompson sampling pre-trains robust and well-performing language models.","score":1},{"url":"https://www.semanticscholar.org/paper/94d05045e9bf69e015f5398086ac5c27a70d13e6","title":"A Comparative Evaluation Of Transformer Models For De-Identification Of Clinical Text Data","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/03/2022","authors":"C. Meaney,Wali Hakimpour,S. Kalia,R. Moineddin","id":"94d05045e9bf69e015f5398086ac5c27a70d13e6","summary":"Transformer models architectures (after suitable hyper-parameter optimization) offer a satisfactory solution for the clinical text de-identiﬁcation problem; and could be readily adopted in clinical scenarios where clinicians/researchers are looking to use de-identified clinical text data to facilitate quality improvement and enhanced patient care.","score":1},{"url":"https://www.semanticscholar.org/paper/77c1eed5c13928d39c5b62ba6063b47b10ab6636","title":"Spatial Transformer Network on Skeleton-based Gait Recognition","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/04/2022","authors":"Cun Zhang,Xingyun Chen,Guohui Han,Xiangrong Liu","id":"77c1eed5c13928d39c5b62ba6063b47b10ab6636","summary":"This work proposes a state-of-the-art robust skeleton-based gait recognition model called Gait-TR, which is based on the combination of spatial transformer frameworks and temporal convolutional networks, and shows that the spatial transformer can extract gait features from the human skeleton better than the widely used graph Convolutional network.","score":1},{"url":"https://www.semanticscholar.org/paper/2002883eecd8f8e0c094c357defa5dcc40b081d9","title":"UMass PCL at SemEval-2022 Task 4: Pre-trained Language Model Ensembles for Detecting Patronizing and Condescending Language","venue":"SEMEVAL","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/04/2022","authors":"David Koleczek,Alexander Scarlatos,Siddharth Karakare,Preshma Linet Pereira","id":"2002883eecd8f8e0c094c357defa5dcc40b081d9","summary":"The authors' system for detecting patronizing and condescending language, submitted to SemEval 2022 Task 4: Patronizing and Condescending Language Detection, uses an ensemble of pre-trained language models, data augmentation, and optimizing the threshold for detection.","score":1},{"url":"https://www.semanticscholar.org/paper/52dccf13f442e7451f4b81db203b24e056142557","title":"TransGrasp: A Multi-Scale Hierarchical Point Transformer for 7-DoF Grasp Detection","venue":"IEEE International Conference on Robotics and Automation","year":2022,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Zhixuan Liu,Zibo Chen,Shangjin Xie,Weishi Zheng","id":"52dccf13f442e7451f4b81db203b24e056142557","summary":"This work addresses the 7-DoF (6- DoF with the grasp width) grasp detection by introducing a one- stage Transformer-based hierarchical multi-scale model dubbed TransGrasp, which is aware of object contour to avoid collisions and able to apply analogy reasoning for long-distance geometric structures.","score":1},{"url":"https://www.semanticscholar.org/paper/b9cc54786c790a20d21da609ab6627270c50d8e2","title":"Recipe for a General, Powerful, Scalable Graph Transformer","venue":"ArXiv","year":2022,"referenceCount":68,"citationCount":14,"influentialCitationCount":5,"publicationDate":"25/05/2022","authors":"Ladislav Rampášek,Mikhail Galkin,Vijay Prakash Dwivedi,A. Luu,Guy Wolf,D. Beaini","id":"b9cc54786c790a20d21da609ab6627270c50d8e2","summary":"A recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks is proposed and a modular framework that supports multiple types of encodings and that provides scalability both in small and large graphs is built.","score":1},{"url":"https://www.semanticscholar.org/paper/e86869d44e78d4cffd1bf1b62f2f8e56a519e23c","title":"E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation","venue":"ArXiv","year":2022,"referenceCount":83,"citationCount":5,"influentialCitationCount":0,"publicationDate":"30/05/2022","authors":"Qihuang Zhong,Liang Ding,Juhua Liu,Bo Du,Dacheng Tao","id":"e86869d44e78d4cffd1bf1b62f2f8e56a519e23c","summary":"This work proposes an encoding-enhancedseq2seq pretraining strategy, namely E2S2, which improves the seq2seq models via integrating more efﬁcient self-supervised information into the encoders, and proves that the encoder takes an important but under-exploitation role than the decoder regarding the downstream performance and neuron activation.","score":1},{"url":"https://www.semanticscholar.org/paper/0fb235cc59cd7198b5a1494157b0250cfca04386","title":"Detecting Harmful Online Conversational Content towards LGBTQIA+ Individuals","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/06/2022","authors":"Jamell Dacon,H. Shomer,Shaylynn Crum-Dacon,Jiliang Tang","id":"0fb235cc59cd7198b5a1494157b0250cfca04386","summary":"The findings verify that large language models can achieve very promising performance on detecting online Anti-LGBTQIA+ conversational content detection tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/1429d4b6f6c19d6447d60d8bed05841c5de72b38","title":"FluSa-Tweet: A Benchmark Dataset for Influenza Detection in Saudi Arabia","venue":"2022 13th International Conference on Information and Communication Systems (ICICS)","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/06/2022","authors":"Rawabe Al-Jamaan,M. Ykhlef,A. Alothaim","id":"1429d4b6f6c19d6447d60d8bed05841c5de72b38","summary":"This paper presents the first dataset to detect influenza in Arabic tweets, particularly in Saudi Arabia, and five machine learning algorithms, such as Naive Bayes, Logistic Regression, Linear Support Vector Classifier, Random Forest, XGBoost, and three transformer-based models are trained and tested to evaluate the dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/c27c68597065dad70cb0b4f882cd861826bf28fa","title":"Automated Construction Contract Summarization Using Natural Language Processing and Deep Learning","venue":"Proceedings of the International Symposium on Automation and Robotics in Construction (IAARC)","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/07/2022","authors":"Xiaorui Xue,Yiru Hou,Jiansong Zhang","id":"c27c68597065dad70cb0b4f882cd861826bf28fa","summary":"A new merit-based evaluation method is proposed to evaluate the performance of three deep learning models on text summarization of construction contracts, which were reported the state-of-the-art performance onText summarization tasks in general English corpus.","score":1},{"url":"https://www.semanticscholar.org/paper/5b71f03167bea77cde1ad15cf95979f2d08f9e45","title":"TransDBC: Transformer for Multivariate Time-Series based Driver Behavior Classification","venue":"2022 International Joint Conference on Neural Networks (IJCNN)","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/07/2022","authors":"Jayant Vyas,Nishit Bhardwaj,Bhumika,D. Das","id":"5b71f03167bea77cde1ad15cf95979f2d08f9e45","summary":"This paper proposes an end-to-end transformer-based driver behavior classification framework named Trans-DBC that calculates driver behavior from the multivariate time-series smartphone telematics data by learning short and long-range temporal dependencies effectively and accurately, unlike prior data-driven deep learning models.","score":1},{"url":"https://www.semanticscholar.org/paper/74671b4f898daf2b8234d5a4cdf2f25b81f1e4fd","title":"TIMS: A Novel Approach for Incrementally Few-Shot Text Instance Selection via Model Similarity","venue":"2022 International Joint Conference on Neural Networks (IJCNN)","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/07/2022","authors":"Tianjie Ju,Han Liao,Gongshen Liu","id":"74671b4f898daf2b8234d5a4cdf2f25b81f1e4fd","summary":"This work proposes an incrementally few-shot instance selection approach (TIMS) based on model similarity and outlier detection, which suits the starting step of active learning well and serves as a better benchmark for few- shot learning.","score":1},{"url":"https://www.semanticscholar.org/paper/0ab111fce85d37389b93ea5ca8caca14c755c3cf","title":"Attention-based Dependability Prediction for Industrial Wireless Communication Systems","venue":"Complexity in Engineering","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/07/2022","authors":"Danfeng Sun,Yang Yang,Hongping Wu","id":"0ab111fce85d37389b93ea5ca8caca14c755c3cf","summary":"The attention-based dependability prediction model which includes a sequence-to-sequence model and attention mechanism is proposed and results indicate that the Sinkhorn-based model can meet the real-time requirement and has the best performance, and the Performer- based model has the lowest execution time, which can be applied for harsh real- time industrial applications.","score":1},{"url":"https://www.semanticscholar.org/paper/2a672342035defd8d75b54e08597ef124c6a0172","title":"Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/08/2022","authors":"Vilém Zouhar,Marius Mosbach,D. Klakow","id":"2a672342035defd8d75b54e08597ef124c6a0172","summary":"An LSTM-based autoregressive language model which uses pre-trained on text embeddings from a pretrained masked language model via fusion (e.g. concatenation) to obtain a richer context representation for language modelling to improve the perplexity.","score":1},{"url":"https://www.semanticscholar.org/paper/2227980ce08aebbf18a42d4abea42381062c4bd5","title":"Method of Transformation of Image Classification Labels into Segmentation Masks","venue":"Microsystems, Electronics and Acoustics","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/08/2022","authors":"Volodymyr Serhiiovych Sydorskyi","id":"2227980ce08aebbf18a42d4abea42381062c4bd5","summary":"A new approach of generating semantic segmentation masks, using only classification labels of the image, is proposed, which requires much less human resources compared to the classical approach and has successfully completed the task of defect localization.","score":1},{"url":"https://www.semanticscholar.org/paper/17f2a787db8cf5104ffa71ca619d8f8092b05ca5","title":"A Survey on Generative Diffusion Model","venue":"ArXiv","year":2022,"referenceCount":361,"citationCount":17,"influentialCitationCount":0,"publicationDate":"06/09/2022","authors":"Hanqun Cao,Cheng Tan,Zhangyang Gao,Guangyong Chen,P. Heng,Stan Z. Li","id":"17f2a787db8cf5104ffa71ca619d8f8092b05ca5","summary":"A diverse range of advanced techniques to speed up the diffusion models – training schedule, training-free sampling, mixed-modeling, and score & diffusion uniﬁcation are presented.","score":1},{"url":"https://www.semanticscholar.org/paper/0f3d7b2bb0b3bd3bad87a39c545d93ddfd383362","title":"Selective Token Generation for Few-shot Natural Language Generation","venue":"COLING","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/09/2022","authors":"DaeJin Jo,Taehwan Kwon,Eun-Sol Kim,Sungwoong Kim","id":"0f3d7b2bb0b3bd3bad87a39c545d93ddfd383362","summary":"Experimental results on various few-shot NLG tasks including question answering, data-to-text generation and text summarization demonstrate that the proposed selective token generation significantly outperforms the previous additive learning algorithms based on the PLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/f5669510806e6a671cece0920b7593adafdae7d8","title":"JoeyS2T: Minimalistic Speech-to-Text Modeling with JoeyNMT","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/10/2022","authors":"Mayumi Ohta,Julia Kreutzer,S. Riezler","id":"f5669510806e6a671cece0920b7593adafdae7d8","summary":"Despite its simplicity compared to prior implementations, JoeyS2T performs compet-itively on English speech recognition and English-to-German speech translation benchmarks.","score":1},{"url":"https://www.semanticscholar.org/paper/8971d2a43a11b4beca31afadf88d5ed8cda75297","title":"Block Format Error Bounds and Optimal Block Size Selection","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"I. Soloveychik,I. Lyubomirsky,Xin Wang,S. Bhoja","id":"8971d2a43a11b4beca31afadf88d5ed8cda75297","summary":"This work develops asymptotic bounds on the inner product error in SBFP- and BFP-quantized normally distributed vectors and derives high-dimensional tight bounds for the same errors, and introduces a performance measure assessing accuracy of any block format.","score":1},{"url":"https://www.semanticscholar.org/paper/119e72daf28e3ac9e0950cfa827ad2b1e81857b6","title":"CPSAA: Accelerating Sparse Attention using Crossbar-based Processing-In-Memory Architecture","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Huize Li,Hai Jin,Long Zheng,Yu Huang,X. Liao,Dan Chen,Zhuohui Duan,Cong Liu,Jiahong Xu,Chuanyi Gui","id":"119e72daf28e3ac9e0950cfa827ad2b1e81857b6","summary":"CPSAA, a novel crossbar-based processing-in-memory (PIM)-featured sparse attention accelerator to eliminate off-chip data transmissions is proposed and a novel attention calculation mode is presented to balance the crossbar writing and crossbar processing latency.","score":1},{"url":"https://www.semanticscholar.org/paper/8458052b99daeb81d95716100e916bf785313b0b","title":"Research on the Classification of Policy Instruments Based on BERT Model","venue":"Discrete Dynamics in Nature and Society","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Jiani Zhao,Cheng Li","id":"8458052b99daeb81d95716100e916bf785313b0b","summary":"The research tries to apply the automatic classification algorithm based on BERT (Bidirectional Encoder Representation from Transformer) to the policy instruments to improve the efficiency and accuracy of policy instruments classification.","score":1},{"url":"https://www.semanticscholar.org/paper/13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","title":"MarianCG: a code generation transformer model inspired by machine translation","venue":"Journal of engineering and applied sciences","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Ahmed S. Soliman,M. Hadhoud,S. Shaheen","id":"13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","summary":"MarianCG, a code generation Transformer model used to tackle the code generation challenge of generating python code from natural language descriptions is presented, based on fine-tuning a machine translation pre-trained language model.","score":1},{"url":"https://www.semanticscholar.org/paper/f1f78bbb3e146874501e3c52b56cff6abf731842","title":"Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges","venue":"ArXiv","year":2022,"referenceCount":481,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/11/2022","authors":"K. T. Baghaei,A. Payandeh,Pooya Fayyazsanavi,Shahram Rahimi,Zhiqian Chen,Somayeh Bakhtiari Ramezani","id":"f1f78bbb3e146874501e3c52b56cff6abf731842","summary":"The principles and developments that have been made in the process of learning representations, and converting them into desirable applications are discussed, and for each framework or model, the key issues and open challenges, as well as the advantages, are examined.","score":1},{"url":"https://www.semanticscholar.org/paper/3f27b81416206977a22270fa9eaf520a185d1c42","title":"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/11/2022","authors":"Yuqi Nie,Nam H. Nguyen,Phanwadee Sinthong,J. Kalagnanam","id":"3f27b81416206977a22270fa9eaf520a185d1c42","summary":"The channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models and apply to self-supervised pretraining tasks and attain excellent fine-tuning performance.","score":1},{"url":"https://www.semanticscholar.org/paper/a3129f5e4d6505376f8f2661db137853a582a819","title":"Understanding Postpartum Parents' Experiences via Two Digital Platforms","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/12/2022","authors":"X. Yao,M. Mikhelson,Megan Micheletti,Eunsol Choi,S. Watkins,Edison Thomaz,K. D. Barbaro","id":"a3129f5e4d6505376f8f2661db137853a582a819","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/247328a082d86199ed5a98e1d726aa205c1da9df","title":"Neural Machine Translation","venue":"ArXiv","year":2017,"referenceCount":425,"citationCount":247,"influentialCitationCount":32,"publicationDate":"22/09/2017","authors":"Philipp Koehn","id":"247328a082d86199ed5a98e1d726aa205c1da9df","summary":"A comprehensive treatment of the topic, ranging from introduction to neural networks, computation graphs, description of the currently dominant attentional sequence-to-sequence model, recent refinements, alternative architectures and challenges.","score":1},{"url":"https://www.semanticscholar.org/paper/0ab0fda8774c303be8f8f8c8f684a890dcf5d455","title":"Lattice-Based Transformer Encoder for Neural Machine Translation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2019,"referenceCount":49,"citationCount":37,"influentialCitationCount":5,"publicationDate":"04/06/2019","authors":"Fengshun Xiao,Jiangtong Li,Zhao Hai,Rui Wang,Kehai Chen","id":"0ab0fda8774c303be8f8f8c8f684a890dcf5d455","summary":"This work proposes lattice-based encoders to explore effective word or subword representation in an automatic way during training and proposes two methods: 1) lattice positional encoding and 2) lattICE-aware self-attention to further improve translation performance.","score":1},{"url":"https://www.semanticscholar.org/paper/0aef56962035a79101821480f51897fdc4443945","title":"Character n-gram Embeddings to Improve RNN Language Models","venue":"AAAI Conference on Artificial Intelligence","year":2019,"referenceCount":46,"citationCount":17,"influentialCitationCount":1,"publicationDate":"13/06/2019","authors":"Sho Takase,Jun Suzuki,Masaaki Nagata","id":"0aef56962035a79101821480f51897fdc4443945","summary":"A novel Recurrent Neural Network (RNN) language model that takes advantage of character information based on research in the field of word embedding construction and combines them with ordinary word embeddings is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/9f3f6deeb1f03ebd52f9ab44275ad382fe60d073","title":"Latent Part-of-Speech Sequences for Neural Machine Translation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2019,"referenceCount":48,"citationCount":10,"influentialCitationCount":2,"publicationDate":"30/08/2019","authors":"Xuewen Yang,Yingru Liu,Dongliang Xie,Xin Wang,Niranjan Balasubramanian","id":"9f3f6deeb1f03ebd52f9ab44275ad382fe60d073","summary":"A new latent variable model, LaSyn, is introduced that captures the co-dependence between syntax and semantics, while allowing for effective and efficient inference over the latent space.","score":1},{"url":"https://www.semanticscholar.org/paper/fac7f9b04fba4889b445e309d384644d15ee7e88","title":"Trends and Advances in Neural Machine Translation","venue":"2020 IEEE International Conference for Innovation in Technology (INOCON)","year":2020,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/11/2020","authors":"Purva Kulkarni,Pravina Bhalerao,Kuheli Nayek,R. Deolekar","id":"fac7f9b04fba4889b445e309d384644d15ee7e88","summary":"This paper breaks down different models, approaches, inception, principle development, and structures utilized in NMT to discover a productive strategy to make a translation system and identifying the advances and imperfections of the equivalent.","score":1},{"url":"https://www.semanticscholar.org/paper/ca83469977ab49baae02ef1e12f419120927ecdc","title":"Mixed-Level Neural Machine Translation","venue":"Computational Intelligence and Neuroscience","year":2020,"referenceCount":25,"citationCount":7,"influentialCitationCount":0,"publicationDate":"29/11/2020","authors":"Thien Nguyen,Huu Nguyen,Phuoc Tran","id":"ca83469977ab49baae02ef1e12f419120927ecdc","summary":"A novel heterogeneous translation unit system, considering linguistic characteristics of the synthetic Russian language and the analytic Vietnamese language is proposed, which improves over the existing best homogeneous Russian-Vietnamese translation system by 1.17 BLEU.","score":1},{"url":"https://www.semanticscholar.org/paper/188f913be48a218b44b0bd964662b4926fd0b0b8","title":"Neural Machine Translation: A Review of Methods, Resources, and Tools","venue":"AI Open","year":2020,"referenceCount":169,"citationCount":29,"influentialCitationCount":0,"publicationDate":"31/12/2020","authors":"Zhixing Tan,Shuo Wang,Zonghan Yang,Gang Chen,Xuancheng Huang,Maosong Sun,Yang Liu","id":"188f913be48a218b44b0bd964662b4926fd0b0b8","summary":"A broad review of the methods for NMT is provided and focus on methods relating to architectures, decoding, and data augmentation, with a discussion of possible future research directions.","score":1},{"url":"https://www.semanticscholar.org/paper/bfbfeb0613beae296d76077b2adc9e38a6b678a4","title":"Sub-Subword N-Gram Features for Subword-Level Neural Machine Translation","venue":"Journal of Natural Language Processing","year":2021,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"A. Martinez,Katsuhito Sudoh,Yuji Matsumoto","id":"bfbfeb0613beae296d76077b2adc9e38a6b678a4","summary":"A novel approach that combines subword-level segmentation with character-level information in the form of character n-gram features to construct embedding matrices and softmax output projections for a standard encoderdecoder model that increases the vocabulary size for small training datasets without reducing translation quality.","score":1},{"url":"https://www.semanticscholar.org/paper/c68959f4da94fd35da5e8649465177b24d0dd351","title":"Byte-based Multilingual NMT for Endangered Languages","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Mengjiao Zhang,Jia Xu","id":"c68959f4da94fd35da5e8649465177b24d0dd351","summary":"This work proposes a byte-based multilingual neural machine translation system (BMNMT), which consistently and significantly outperforms subword/word-based baselines on twelve language pairs up to +18.5 BLEU points, an 840% relative improvement.","score":1},{"url":"https://www.semanticscholar.org/paper/2e6028f8b156c8b344e1a68d15d88403a978c71d","title":"Learning Multiscale Transformer Models for Sequence Generation","venue":"International Conference on Machine Learning","year":2022,"referenceCount":58,"citationCount":1,"influentialCitationCount":1,"publicationDate":"19/06/2022","authors":"Bei Li,Tong Zheng,Yi Jing,Chengbo Jiao,Tong Xiao,Jingbo Zhu","id":"2e6028f8b156c8b344e1a68d15d88403a978c71d","summary":"This work built a multiscale Transformer model by establishing relationships among scales based on word-boundary information and phrase-level prior knowledge and yielded consistent performance gains over the strong baseline on several test sets without sacrificing the efficiency.","score":1},{"url":"https://www.semanticscholar.org/paper/2dcdc4ef997507630f2c9b1b6682646ae31bdb7f","title":"SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Abdul Rafae Khan,Hrishikesh Kanade,Girish Amar Budhrani,Preet Jhanglani,Jia Xu","id":"2dcdc4ef997507630f2c9b1b6682646ae31bdb7f","summary":"This paper describes the Stevens Institute of Technology’s submission for the WMT 2022 Shared Task: Code-mixed Machine Translation (MixMT), which consisted of two subtasks, subtask 1 Hindi/English to Hinglish and subtask 2 Hinglit to English translation.","score":1},{"url":"https://www.semanticscholar.org/paper/fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a","title":"Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2019,"referenceCount":33,"citationCount":27,"influentialCitationCount":3,"publicationDate":"01/06/2019","authors":"Elizabeth Salesky,Matthias Sperber,A. Black","id":"fbd47a815c73a83e8a47ee2ed38826c82ffc0c2a","summary":"This work shows that a naive method to create compressed phoneme-like speech representations is far more effective and efficient for translation than traditional frame-level speech features.","score":1},{"url":"https://www.semanticscholar.org/paper/95236a88cc959656958d7a49422f4004015d594d","title":"Comparison between NMT and PBSMT Performance for Translating Noisy User-Generated Content","venue":"NODALIDA","year":2019,"referenceCount":46,"citationCount":9,"influentialCitationCount":1,"publicationDate":"30/09/2019","authors":"José Carlos Rosales Núñez,Djamé Seddah,Guillaume Wisniewski","id":"95236a88cc959656958d7a49422f4004015d594d","summary":"It is shown that, contrary to what could be expected, PBSMT outperforms NMT when translating non-canonical inputs, and suggests new avenue for improving NMT models.","score":1},{"url":"https://www.semanticscholar.org/paper/201fae97e51fb6aea7ed8120147e806e43834de6","title":"Neural Machine Translation: A Review and Survey","venue":"","year":2019,"referenceCount":537,"citationCount":10,"influentialCitationCount":1,"publicationDate":"04/12/2019","authors":"Felix Stahlberg","id":"201fae97e51fb6aea7ed8120147e806e43834de6","summary":"This work traces back the origins of modern NMT architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family and concludes with a survey of recent trends in the field.","score":1},{"url":"https://www.semanticscholar.org/paper/4d08dcd2cc1e9691defe664a10f021424a896a1e","title":"Neural Machine Translation: A Review","venue":"Journal of Artificial Intelligence Research","year":2019,"referenceCount":641,"citationCount":67,"influentialCitationCount":3,"publicationDate":"04/12/2019","authors":"Felix Stahlberg","id":"4d08dcd2cc1e9691defe664a10f021424a896a1e","summary":"This work traces back the origins of modern NMT architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family and concludes with a survey of recent trends in the field.","score":1},{"url":"https://www.semanticscholar.org/paper/8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","title":"Adversarial Subword Regularization for Robust Neural Machine Translation","venue":"FINDINGS","year":2020,"referenceCount":47,"citationCount":3,"influentialCitationCount":0,"publicationDate":"01/04/2020","authors":"Jungsoon Park,Mujeen Sung,Jinhyuk Lee,Jaewoo Kang","id":"8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","summary":"This paper presents adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for exposing diverse subword segmentations and experimentally shows that the model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the performance of N MT models in low-resource and out-domain datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/405cd5cd6e056675d4545eba12742174cc75d7e7","title":"Transfer learning and subword sampling for asymmetric-resource one-to-many neural translation","venue":"Machine Translation","year":2020,"referenceCount":114,"citationCount":4,"influentialCitationCount":0,"publicationDate":"08/04/2020","authors":"Stig-Arne Grönroos,Sami Virpioja,M. Kurimo","id":"405cd5cd6e056675d4545eba12742174cc75d7e7","summary":"These approaches for improving neural machine translation for low-resource languages are reviewed in the context of an asymmetric-resource one-to-many translation task, in which the pair of target languages are related, with one being a very low- resource and the other a higher-resource language.","score":1},{"url":"https://www.semanticscholar.org/paper/38b40ae531ddca434de07015637b78413370d15a","title":"The Roles of Language Models and Hierarchical Models in Neural Sequence-to-Sequence Prediction","venue":"European Association for Machine Translation Conferences/Workshops","year":2020,"referenceCount":767,"citationCount":4,"influentialCitationCount":0,"publicationDate":"16/05/2020","authors":"Felix Stahlberg","id":"38b40ae531ddca434de07015637b78413370d15a","summary":"It is shown how traditional symbolic statistical machine translation models can still improve neural machine translation while reducing the risk of common pathologies of NMT such as hallucinations and neologisms.","score":1},{"url":"https://www.semanticscholar.org/paper/8bcde747a44cbc2601175301808fe4518c9128dc","title":"Optimizing Word Segmentation for Downstream Task","venue":"FINDINGS","year":2020,"referenceCount":38,"citationCount":8,"influentialCitationCount":1,"publicationDate":"01/11/2020","authors":"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki","id":"8bcde747a44cbc2601175301808fe4518c9128dc","summary":"The proposed method, optimizing tokenization (OpTok), is trained to assign a high probability to such appropriate tokenization based on the downstream task loss, and can be used for any downstream task which uses a vector representation of a sentence such as text classification.","score":1},{"url":"https://www.semanticscholar.org/paper/6dc710b46bc510a42af487a3ac220e4fabf4d518","title":"VOLT: Improving Vocabularization via Optimal Transport for Machine Translation","venue":"ArXiv","year":2020,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":2020,"authors":"Jingjing Xu,Hao Zhou,Chun Gan,Zaixiang Zheng,Lei Li","id":"6dc710b46bc510a42af487a3ac220e4fabf4d518","summary":"An exciting relation between an information-theoretic feature and BLEU scores is found and it is found that VOLT beats widely-used vocabularies on diverse scenarios, and one advantage of VOLT lies in its low resource consumption.","score":1},{"url":"https://www.semanticscholar.org/paper/b086b812c867b1d07eb65bcdd206dd0891733f9d","title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":32,"citationCount":33,"influentialCitationCount":4,"publicationDate":"31/12/2020","authors":"Jingjing Xu,Hao Zhou,Chun Gan,Zaixiang Zheng,Lei Li","id":"b086b812c867b1d07eb65bcdd206dd0891733f9d","summary":"This paper proposes VOLT, a simple and efficient solution without trial training that beats widely-used vocabularies in diverse scenarios, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation.","score":1},{"url":"https://www.semanticscholar.org/paper/24fd022750ea88e44e6790c7c7e1923635885c71","title":"Translation Mechanism of Neural Machine Algorithm for Online English Resources","venue":"Complex.","year":2021,"referenceCount":25,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/04/2021","authors":"Yanping Ye","id":"24fd022750ea88e44e6790c7c7e1923635885c71","summary":"This paper proposes a framework that integrates vocabulary alignment structure for neural machine translation at the vocabulary level and uses the word alignment structure of statistical machine translation as the external vocabulary alignment information and introduces it into the decoding step of Neural machine translation.","score":1},{"url":"https://www.semanticscholar.org/paper/58403c6bc061670f7ce6267a3a0cd01ba1bb8e50","title":"Intérêt des modèles de caractères pour la détection d’événements (The interest of character-level models for event detection)","venue":"JEPTALNRECITAL","year":2021,"referenceCount":31,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Emanuela Boros,Romaric Besançon,Olivier Ferret,B. Grau","id":"58403c6bc061670f7ce6267a3a0cd01ba1bb8e50","summary":"D’intégrer des plongements de caractères, qui peuvent capturer des informations morphologiques et de forme sur les mots, à un modèle convolutif pour la détection d’événements, évaluons deux stratégies pour réaliser une telle intégration.","score":1},{"url":"https://www.semanticscholar.org/paper/e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order","venue":"ArXiv","year":2021,"referenceCount":72,"citationCount":9,"influentialCitationCount":1,"publicationDate":2021,"authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","summary":"The insensitivity of natural language models to word-order is investigated by quantifying perturbations and analysing their effect on neural models’ performance on language understanding tasks in GLUE benchmark and it is found that neural language models — pretrained and non-pretrained Transformers, LSTMs, and Convolutional architectures — require local ordering more than the global ordering of tokens.","score":1},{"url":"https://www.semanticscholar.org/paper/9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","title":"You should evaluate your language model on marginal likelihood over tokenisations","venue":"EMNLP","year":2021,"referenceCount":42,"citationCount":3,"influentialCitationCount":0,"publicationDate":2021,"authors":"Kris Cao,Laura Rimell","id":"9635e3c008f7bfa80638ade7134a8fb0ef1b37e1","summary":"It is argued that language models should be evaluated on their marginal likelihood over tokenisations, and it is shown that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data.","score":1},{"url":"https://www.semanticscholar.org/paper/b62a56f629f77acce9ea69456d67dd77de1c7dfb","title":"Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization","venue":"MRL","year":2021,"referenceCount":79,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"R. Bassani,Anders Søgaard,Tejaswini Deoskar","id":"b62a56f629f77acce9ea69456d67dd77de1c7dfb","summary":"This work explores the idea of learning multilingual language models based on clustering of monolingual segments and shows significant improvements over standard multilingual segmentation and training across nine languages on a question answering task, both in a small model regime and for a model of the size of BERT-base.","score":1},{"url":"https://www.semanticscholar.org/paper/0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":102,"citationCount":76,"influentialCitationCount":11,"publicationDate":"31/12/2020","authors":"Phillip Rust,Jonas Pfeiffer,Ivan Vulic,Sebastian Ruder,Iryna Gurevych","id":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","summary":"It is found that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.","score":1},{"url":"https://www.semanticscholar.org/paper/dca4128a33ca22c02031b5c0c28548a0df022d80","title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding","venue":"","year":2021,"referenceCount":56,"citationCount":6,"influentialCitationCount":0,"publicationDate":2021,"authors":"Abhik Bhattacharjee,Tahmid Hasan,Kazi Samin,Md. Saiful Islam,M. S. Rahman,Anindya Iqbal,Rifat Shahriyar","id":"dca4128a33ca22c02031b5c0c28548a0df022d80","summary":"The Embedding Barrier is introduced, a phenomenon that limits the monolingual performance of multilingual models on low-resource languages having unique typologies and a straightforward solution by transcribing languages to a common script is proposed, which can effectively improve the performance of a multilingual model for the Bangla language.","score":1},{"url":"https://www.semanticscholar.org/paper/2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":62,"citationCount":77,"influentialCitationCount":19,"publicationDate":"15/04/2021","authors":"Sebastian Ruder,Noah Constant,Jan A. Botha,Aditya Siddhant,Orhan Firat,Jinlan Fu,Pengfei Liu,Junjie Hu,Graham Neubig,Melvin Johnson","id":"2b9762e91305986ac8a2d624d0a69521304405f3","summary":"This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned, and provides a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.","score":1},{"url":"https://www.semanticscholar.org/paper/a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study","venue":"MRL","year":2021,"referenceCount":72,"citationCount":10,"influentialCitationCount":0,"publicationDate":"16/06/2021","authors":"Ethan C. Chau,Noah A. Smith","id":"a20a802839d72bee1c85f4a1cb77addadacb2179","summary":"These evaluations on part-of-speech tagging, universal dependency parsing, and named entity recognition in nine diverse low-resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low- resource settings.","score":1},{"url":"https://www.semanticscholar.org/paper/31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning","venue":"Journal of Big Data","year":2021,"referenceCount":134,"citationCount":60,"influentialCitationCount":1,"publicationDate":"29/06/2021","authors":"Connor Shorten,T. Khoshgoftaar,B. Furht","id":"31852f9fc732c0868af12d631c72693702d80521","summary":"The major motifs of Data Augmentation are summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form.","score":1},{"url":"https://www.semanticscholar.org/paper/3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5","title":"Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":62,"citationCount":11,"influentialCitationCount":3,"publicationDate":"09/09/2021","authors":"M. Nicosia,Zhongdi Qu,Y. Altun","id":"3e53ca0f1d08e5a0f3f014af3eb59dabe95b07e5","summary":"Experimental results on three multilingual semantic parsing datasets show that data augmentation with TaF reaches accuracies competitive with similar systems which rely on traditional alignment techniques.","score":1},{"url":"https://www.semanticscholar.org/paper/e43a360728e34e43934707665db0f7be02c8e5a7","title":"Interpreting BERT-based stance classification: a case study about the Brazilian COVID vaccination","venue":"SBBD","year":2021,"referenceCount":16,"citationCount":2,"influentialCitationCount":0,"publicationDate":"04/10/2021","authors":"Carlos Abel Córdova Sáenz,Karin Becker","id":"e43a360728e34e43934707665db0f7be02c8e5a7","summary":"This paper proposes a BERT-based stance classification model and an attention-based mechanism to identify the influential words for stance classification and uses these metrics to assess if words with high attention weights correspond to domain intrinsic properties and contribute to the correct classification of stances.","score":1},{"url":"https://www.semanticscholar.org/paper/12fec03c538c7aaeca1a4e1ab8d66aed5f793fc0","title":"reamtchka at SemEval-2022 Task 6: Investigating the effect of different loss functions for Sarcasm detection for unbalanced datasets","venue":"International Workshop on Semantic Evaluation","year":2022,"referenceCount":36,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Reem Abdel-Salam","id":"12fec03c538c7aaeca1a4e1ab8d66aed5f793fc0","summary":"A voting classifier between either multiple different BERT-based models or machine learning models is proposed, as the final model in SemEval-2022 Task 6: Intended Sarcasm Detection in English and Arabic.","score":1},{"url":"https://www.semanticscholar.org/paper/b1be10b76314ea8569249f2310f1e6eead8a5adc","title":"Building Domain-specific Corpora from the Web: the Case of European Digital Service Infrastructures","venue":"BUCC","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Rik van Noord,Cristian García-Romero,M. Esplà-Gomis,Leopoldo Pla Sempere,Antonio Toral","id":"b1be10b76314ea8569249f2310f1e6eead8a5adc","summary":"This paper explores the feasibility of building an automatic classifier that allows to identify which segments in a generic corpus are relevant for a particular DSI, and uses pre-trained (multilingual) language models to perform the classification.","score":1},{"url":"https://www.semanticscholar.org/paper/e11d8663ccf2cc412f408853fa5f19ebac75df54","title":"How to encode arbitrarily complex morphology in word embeddings, no corpus needed","venue":"FIELDMATTERS","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Lane Schwartz,Coleman Haley,Francis M. Tyers","id":"e11d8663ccf2cc412f408853fa5f19ebac75df54","summary":"The word embeddings in this paper are explicitly designed to be both linguistically interpretable and fully capable of handling the broad variety found in the world’s diverse set of 7000 languages, regardless of corpus size or morphological characteristics.","score":1},{"url":"https://www.semanticscholar.org/paper/610088e1d7a8e44f921b2c4894fe1a7b204ce5a9","title":"Vicomtech at LivingNER2022","venue":"IberLEF@SEPLN","year":2022,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Elena Zotova,Aitor García Pablos,Naiara Pérez,Pablo Turón,Montse Cuadros","id":"610088e1d7a8e44f921b2c4894fe1a7b204ce5a9","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":75,"citationCount":115,"influentialCitationCount":29,"publicationDate":"28/05/2021","authors":"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel","id":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","summary":"This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts.","score":1},{"url":"https://www.semanticscholar.org/paper/91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU","venue":"Findings","year":2021,"referenceCount":70,"citationCount":4,"influentialCitationCount":0,"publicationDate":"29/07/2021","authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","summary":"It is empirically shown that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.","score":1},{"url":"https://www.semanticscholar.org/paper/9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs","venue":"International Conference on Learning Representations","year":2021,"referenceCount":105,"citationCount":170,"influentialCitationCount":26,"publicationDate":"30/07/2021","authors":"Andrew Jaegle,Sebastian Borgeaud,Jean-Baptiste Alayrac,Carl Doersch,Catalin Ionescu,David Ding,Skanda Koppula,Andrew Brock,Evan Shelhamer,Olivier J. H'enaff,M. Botvinick,A. Zisserman,Oriol Vinyals,João Carreira","id":"9933a5af7895354087baf6c96b64dc8a8973eaed","summary":"The primary focus of this work is generality, rather than speed on images, and Perceiver IO uses comparable FLOPs to attention-based image classiﬁcation models, especially for the more compact conﬂguration B pretrained on JFT.","score":1},{"url":"https://www.semanticscholar.org/paper/231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models","venue":"NAACL-HLT","year":2021,"referenceCount":38,"citationCount":13,"influentialCitationCount":1,"publicationDate":"26/10/2021","authors":"P. Nawrot,Szymon Tworkowski,Michal Tyrolski,Lukasz Kaiser,Yuhuai Wu,Christian Szegedy,H. Michalewski","id":"231e768f0cd280faa0f725bb353262cb4fed08d1","summary":"Hourglass is a hierarchical Transformer language model that sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efﬁciency on the widely studied enwik8 benchmark.","score":1},{"url":"https://www.semanticscholar.org/paper/2ec281d654f622da7267d7da8145e96bb77a9ede","title":"An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification","venue":"Computers & security","year":2021,"referenceCount":61,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/12/2021","authors":"Ferhat Demirkiran,Aykut Çayir,U. Ünal,Hasan Dag","id":"2ec281d654f622da7267d7da8145e96bb77a9ede","summary":"The experiments demonstrate that the transformer model with one transformer block layer surpass the performance of the widely used base architecture, LSTM, and BERT or CANINE, the pre-trained transformer models, outperforms in classifying highly imbalanced malware families according to evaluation metrics: F1-score and AUC score.","score":1},{"url":"https://www.semanticscholar.org/paper/7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models","venue":"International Conference on Agents and Artificial Intelligence","year":2022,"referenceCount":51,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/01/2022","authors":"Konstantin Todorov,Giovanni Colavizza","id":"7e3081b0d698f8abf16dee626d782f3339482fe7","summary":"It is found that OCR noise poses a significant obstacle to language modelling, with language models increasingly diverging from their noiseless targets as OCR quality lowers, and simpler models including PPMI and Word2Vec consistently outperform transformer-based models in this respect.","score":1},{"url":"https://www.semanticscholar.org/paper/12809bcb734beafeb47876f42e7b438e27fe99fe","title":"General-purpose, long-context autoregressive modeling with Perceiver AR","venue":"International Conference on Machine Learning","year":2022,"referenceCount":74,"citationCount":19,"influentialCitationCount":4,"publicationDate":"15/02/2022","authors":"Curtis Hawthorne,Andrew Jaegle,Cătălina Cangea,Sebastian Borgeaud,C. Nash,Mateusz Malinowski,S. Dieleman,Oriol Vinyals,M. Botvinick,Ian Simon,Hannah R. Sheahan,Neil Zeghidour,Jean-Baptiste Alayrac,João Carreira,Jesse Engel","id":"12809bcb734beafeb47876f42e7b438e27fe99fe","summary":"Perceiver AR is developed, an modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms.","score":1},{"url":"https://www.semanticscholar.org/paper/7ed63a4f928fc126f6780d27e75fa6447a00a8c4","title":"Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference","venue":"NAACL","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/02/2022","authors":"Emils Kadickis,Vaibhav Srivastav,Roman Klinger","id":"7ed63a4f928fc126f6780d27e75fa6447a00a8c4","summary":"This work proposes a simple method for predicting the performance without actually fine-tuning the model of a natural language inference model, and shows that the accuracy of the cosine similarity approach correlates strongly with theuracy of the classification approach with a Pearson correlation coefficient of 0.65.","score":1},{"url":"https://www.semanticscholar.org/paper/0376c01a9320027694f2dca57236c27f0c5b36eb","title":"Multilingual Abusiveness Identification on Code-Mixed Social Media Text","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2022","authors":"Ekagra Ranjan,Naman Poddar","id":"0376c01a9320027694f2dca57236c27f0c5b36eb","summary":"This work proposes an approach for abusiveness identiﬁcation on the multilingual Moj dataset which comprises of Indic languages and tackles the common challenges of non-English social media content and can be extended to other languages as well.","score":1},{"url":"https://www.semanticscholar.org/paper/34f2ed08461edc261984fc1c86a14ff7b4c3d2db","title":"KinyaBERT: a Morphology-aware Kinyarwanda Language Model","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":70,"citationCount":9,"influentialCitationCount":0,"publicationDate":"16/03/2022","authors":"Antoine Nzeyimana,Andre Niyongabo Rubungo","id":"34f2ed08461edc261984fc1c86a14ff7b4c3d2db","summary":"A simple yet effective two-tier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality is proposed, naming the proposed model architecture KinyaBERT.","score":1},{"url":"https://www.semanticscholar.org/paper/c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":69,"citationCount":12,"influentialCitationCount":0,"publicationDate":"12/05/2022","authors":"Jonas Pfeiffer,Naman Goyal,Xi Victoria Lin,Xian Li,James Cross,Sebastian Riedel,Mikel Artetxe","id":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","summary":"This work introduces language-specific modules of their Cross-lingual Modular models from the start, which allows them to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant.","score":1},{"url":"https://www.semanticscholar.org/paper/54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models","venue":"COLING","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/08/2022","authors":"Jan Jezabek,A. Singh","id":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","summary":"A novel method of retroactively adding resilience to misspellings to transformer-based NLP models without the need for re-training of the original NLP model is proposed, which significantly reduces the cost needed to evaluate a model’s resilience to adversarial attacks.","score":1},{"url":"https://www.semanticscholar.org/paper/ba9a592438447c2a35afc203be40f7f0a2d3fb5a","title":"A Compact Pretraining Approach for Neural Language Models","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/08/2022","authors":"Shahriar Golchin,M. Surdeanu,N. Tavabi,A. Kiapour","id":"ba9a592438447c2a35afc203be40f7f0a2d3fb5a","summary":"This study shows that pretrained NLMs learn in-domain information more effectively and faster from a compact subset of the data that focuses on the key information in the domain.","score":1},{"url":"https://www.semanticscholar.org/paper/695ea88d22e7d39fa3d0b9f53e75d41e82be81d8","title":"Transformers with Learnable Activation Functions","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/08/2022","authors":"Haishuo Fang,Ji-Ung Lee,N. Moosavi,Iryna Gurevych","id":"695ea88d22e7d39fa3d0b9f53e75d41e82be81d8","summary":"This paper investigates the effectiveness of using Rational Activation Function (RAF) that is a learnable activation function in the Transformer architecture and opens a new research direction for analyzing and interpreting pre-trained models according to the learned activation functions.","score":1},{"url":"https://www.semanticscholar.org/paper/110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward","venue":"ArXiv","year":2022,"referenceCount":85,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/09/2022","authors":"Alan Ramponi","id":"110250df2a6ca0e0e609eaa800a21c17abeedd77","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/265ebfc1074c73917233dbecad802c0c64921a1c","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Gregor Geigle,Chen Cecilia Liu,Jonas Pfeiffer,Iryna Gurevych","id":"265ebfc1074c73917233dbecad802c0c64921a1c","summary":"This work evaluates whether the information stored within different VEs is complementary, i.e. if providing the model with features from multiple VEs can improve the performance on a target task, and suggests that diverse VEs complement each other, resulting in improved downstream V+L task performance.","score":1},{"url":"https://www.semanticscholar.org/paper/10672baf790962195677c7581a2fe984032e7f98","title":"Token-level Sequence Labeling for Spoken Language Understanding using Compositional End-to-End Models","venue":"ArXiv","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Siddhant Arora,Siddharth Dalmia,Brian Yan,Florian Metze,A. Black,Shinji Watanabe","id":"10672baf790962195677c7581a2fe984032e7f98","summary":"This work builds compositional end-to-end SLU systems that explicitly separate the added complexity of recognizing spoken mentions in SLU from the NLU task of sequence labeling, and relies on intermediate decoders trained for ASR and NLU to trans-form the input modality from speech to token-level representations that can be used in the traditional sequence labeling framework.","score":1},{"url":"https://www.semanticscholar.org/paper/dd568e6838903ad7c381f13c1268c94c5db08b02","title":"Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/11/2022","authors":"Yibo Wang,Congying Xia,Guan Wang,Philip S. Yu","id":"dd568e6838903ad7c381f13c1268c94c5db08b02","summary":"This work reformulates the entity typing task into a textual entailment problem to handle new entities that are not present during training, and designs a model to automatically generate textual entailments hypotheses using a continuous prompt tuning method, which can generate better textual entailsment hypotheses without manual design.","score":1},{"url":"https://www.semanticscholar.org/paper/4061a9941fa0ff106e884272d9ed753650417ec4","title":"Collateral facilitation in humans and language models","venue":"ArXiv","year":2022,"referenceCount":85,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"J. Michaelov,B. Bergen","id":"4061a9941fa0ff106e884272d9ed753650417ec4","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages","venue":"AACL","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"9a5b2dc77bda19759df8481aaf283da353ac7e77","summary":"This work replicates a study on the importance of local structure, and the relative unimportance of global structure, in a multilingual setting and finds that the phenomenon observed on the English language broadly translates to over 120 languages, with a few caveats.","score":1},{"url":"https://www.semanticscholar.org/paper/fc78d652c4d395ba2737ab0406bc53fd025d4aad","title":"Detecting Languages Unintelligible to Multilingual Models through Local Structure Probes","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"fc78d652c4d395ba2737ab0406bc53fd025d4aad","summary":"A general approach that requires only unlabelled text to detect which languages are not well understood by a cross-lingual model, derived from the hypothesis that if a model’s understanding is insensitive to perturbations to text in a language, it is likely to have a limited understanding of that language.","score":1},{"url":"https://www.semanticscholar.org/paper/9769992ca9ff1c3894d19f4bd2b6dac27ed1befd","title":"A review on abusive content automatic detection: approaches, challenges and opportunities","venue":"PeerJ Computer Science","year":2022,"referenceCount":134,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"Bedour F. Alrashidi,Amani Jamal,I. Khan,Ali Alkhathlan","id":"9769992ca9ff1c3894d19f4bd2b6dac27ed1befd","summary":"A review of abusive content automatic detection approaches focusing on the recent contributions that were using natural language processing (NLP) technologies to detect the abusive content in social media and proposes a new taxonomy of abusivecontent automatic detection by covering five different aspects and tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy","venue":"IEEE Access","year":2022,"referenceCount":104,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/11/2022","authors":"Philipp Siebers,Christian Janiesch,Patrick Zschech","id":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","summary":"This work contributes threefold to this lack of compilation, composition, and systematization by providing a survey of current approaches, arranging them in a genealogy, and conceptualizing a taxonomy of text representation methods to examine and explain the state-of-the-art.","score":1},{"url":"https://www.semanticscholar.org/paper/923d2376103dbc9e9b2af4518b56299d7630b46b","title":"J URASSIC -1: T ECHNICAL D ETAILS AND E VALUATION","venue":"","year":2021,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Opher Lieber","id":"923d2376103dbc9e9b2af4518b56299d7630b46b","summary":"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1\"-Large\", and their architecture and training are described and their performance relative to GPT-3 is evaluated.","score":1},{"url":"https://www.semanticscholar.org/paper/518cb6d4247bdebf21e2811f296b0c7372602a0a","title":"PMI-Masking: Principled masking of correlated spans","venue":"International Conference on Learning Representations","year":2020,"referenceCount":30,"citationCount":33,"influentialCitationCount":7,"publicationDate":"05/10/2020","authors":"Yoav Levine,Barak Lenz,Opher Lieber,Omri Abend,Kevin Leyton-Brown,Moshe Tennenholtz,Y. Shoham","id":"518cb6d4247bdebf21e2811f296b0c7372602a0a","summary":"PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word masks, entity/phrase masksing, and random-span masking.","score":1},{"url":"https://www.semanticscholar.org/paper/09c896e30d1021b1284b68ed65b93b593f2c3f4f","title":"ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding","venue":"North American Chapter of the Association for Computational Linguistics","year":2020,"referenceCount":52,"citationCount":13,"influentialCitationCount":2,"publicationDate":"23/10/2020","authors":"Dongling Xiao,Yukun Li,Han Zhang,Yu Sun,Hao Tian,Hua Wu,Haifeng Wang","id":"09c896e30d1021b1284b68ed65b93b593f2c3f4f","summary":"ERNIE-Gram is proposed, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training and outperforms previous pre- training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods.","score":1},{"url":"https://www.semanticscholar.org/paper/964da4ed9ac61b12bc2a7adc1e94e3964cc63861","title":"Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question Answering Data","venue":"EMNLP","year":2021,"referenceCount":80,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/02/2021","authors":"Dian Yu,Kai Sun,Dong Yu,Claire Cardie","id":"964da4ed9ac61b12bc2a7adc1e94e3964cc63861","summary":"A self-teaching paradigm is proposed to better use the generated weakly-labeled MRC instances to improve a target MRC task and the effectiveness of this framework and the usefulness of large-scale subjectarea question-answering data for machine reading comprehension are demonstrated.","score":1},{"url":"https://www.semanticscholar.org/paper/a77643bff6f50ccc4f80ec081e4d078a2e788ae7","title":"Multi-view Subword Regularization","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":40,"citationCount":27,"influentialCitationCount":4,"publicationDate":"15/03/2021","authors":"Xinyi Wang,Sebastian Ruder,Graham Neubig","id":"a77643bff6f50ccc4f80ec081e4d078a2e788ae7","summary":"To take full advantage of different possible input segmentations, the proposed Multi-view Subword Regularization (MVR) method enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations.","score":1},{"url":"https://www.semanticscholar.org/paper/111dbe14083359ab39886790632e7f1421732a8a","title":"Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":33,"citationCount":13,"influentialCitationCount":2,"publicationDate":"15/04/2021","authors":"Yuxuan Lai,Yijia Liu,Yansong Feng,Songfang Huang,Dongyan Zhao","id":"111dbe14083359ab39886790632e7f1421732a8a","summary":"This work proposes a novel pre-training paradigm for Chinese — Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner and achieves new state-of-the-art among base-size models on the CLUE benchmarks.","score":1},{"url":"https://www.semanticscholar.org/paper/7da82508a76698f93e733d7a13d9fb13dbafba3e","title":"SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining","venue":"ArXiv","year":2021,"referenceCount":37,"citationCount":6,"influentialCitationCount":1,"publicationDate":2021,"authors":"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Maosong Sun","id":"7da82508a76698f93e733d7a13d9fb13dbafba3e","summary":"It is found that SHUOWEN and JIEZI tokenizers can generally outperform conventional singlecharacter tokenizers, while Chinese word segmentation shows no benefit as a preprocessing step, and exhibit significantly better robustnesses on handling noisy texts.","score":1},{"url":"https://www.semanticscholar.org/paper/4690eb050572a279f94560b6bbdccaae577b45f5","title":"MVP-BERT: Multi-Vocab Pre-training for Chinese BERT","venue":"ACL","year":2021,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/08/2021","authors":"Wei Zhu","id":"4690eb050572a279f94560b6bbdccaae577b45f5","summary":"Experiments show that MVP training strategies improve PLMs’ downstream performances, especially it can improve the PLM’s performances on span-level tasks, and the AL-MVP outperforms the recent AMBERT (CITATION) after large-scale pre-training, and it is more robust against adversarial attacks.","score":1},{"url":"https://www.semanticscholar.org/paper/27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","title":"LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization","venue":"Findings","year":2021,"referenceCount":23,"citationCount":4,"influentialCitationCount":2,"publicationDate":"02/08/2021","authors":"Weidong Guo,Mingjun Zhao,Lusheng Zhang,Di Niu,Jinwen Luo,Zhenhua Liu,Zhenyang Li,J. Tang","id":"27c39dd62635791a0ec3c0c81c2690e7a9bd62ad","summary":"This paper proposes a simple yet effective pretraining method named LICHEE to efficiently incorporate multi-grained information of input text that can be applied to various pretrained language models and improve their representation capability.","score":1},{"url":"https://www.semanticscholar.org/paper/5722d101859846a6a023b8fa00830742203cd0c1","title":"Span Fine-tuning for Pre-trained Language Models","venue":"EMNLP","year":2021,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/08/2021","authors":"Rongzhou Bao,Zhuosheng Zhang,Hai Zhao","id":"5722d101859846a6a023b8fa00830742203cd0c1","summary":"A novel span fine-tuning method for PrLMs is presented, which facilitates the span setting to be adaptively determined by specific downstream tasks during the fine- Tuning phase.","score":1},{"url":"https://www.semanticscholar.org/paper/976f47bade21fd787f029142b39631cb17f16ec2","title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation","venue":"ArXiv","year":2021,"referenceCount":45,"citationCount":41,"influentialCitationCount":12,"publicationDate":"13/09/2021","authors":"Yunfan Shao,Zhichao Geng,Yitao Liu,Junqi Dai,Fei Yang,Li Zhe,H. Bao,Xipeng Qiu","id":"976f47bade21fd787f029142b39631cb17f16ec2","summary":"The unbalanced Transformer saves the computational and storage cost, which makes CPT competitive and greatly accelerates the inference of text generation.","score":1},{"url":"https://www.semanticscholar.org/paper/35eeeefcbea6fa8edae4c310ee5ee717861c7e60","title":"Improving Constituent Representation with Hypertree Neural Networks","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Hao Zhou,Gongshen Liu,Kewei Tu","id":"35eeeefcbea6fa8edae4c310ee5ee717861c7e60","summary":"This paper aims to improve representations of constituent spans using a novel hypertree neural networks (HTNN) that is structured with constituency parse trees that incorporates both bottom-up and top-down compositional information.","score":1},{"url":"https://www.semanticscholar.org/paper/18dc24da603896db2264e9174116407a95c593d5","title":"“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction","venue":"FINDINGS","year":2022,"referenceCount":26,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/03/2022","authors":"Yong Dai,Linyang Li,Cong Zhou,Zhangyin Feng,Enbo Zhao,Xipeng Qiu,Pijian Li,Duyu Tang","id":"18dc24da603896db2264e9174116407a95c593d5","summary":"Three Chinese BERT models with standard character-level masking (CLM), WWM, and a combination of CLM and WWM are trained and it is found that when one character needs to be inserted or replaced, the model trained with CLM performs the best.","score":1},{"url":"https://www.semanticscholar.org/paper/8e1227b0d58b769a0919f8debdfd093ff6f6a65a","title":"MarkBERT: Marking Word Boundaries Improves Chinese BERT","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":2,"influentialCitationCount":0,"publicationDate":"12/03/2022","authors":"Linyang Li,Yong Dai,Duyu Tang,Zhangyin Feng,Cong Zhou,Xipeng Qiu,Zenglin Xu,Shuming Shi","id":"8e1227b0d58b769a0919f8debdfd093ff6f6a65a","summary":"A Chinese BERT model dubbed MarkBERT that uses word information and inserts boundary markers between contiguous words, which enables the model to handle any words in the same way, no matter they are OOV words or not is presented.","score":1},{"url":"https://www.semanticscholar.org/paper/05ea28584e5db18c0c31d1aac40e9c1905327557","title":"Effectiveness of Fine-tuned BERT Model in Classification of Helpful and Unhelpful Online Customer Reviews","venue":"Electronic Commerce Research","year":2022,"referenceCount":65,"citationCount":9,"influentialCitationCount":0,"publicationDate":"29/04/2022","authors":"Muhammad Bilal,A. A. Almazroi","id":"05ea28584e5db18c0c31d1aac40e9c1905327557","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/f8a2428bee464ae69cdb25ce5c2259e88ce576b4","title":"Clickbait Detection of Indonesian News Headlines using Fine-Tune Bidirectional Encoder Representations from Transformers (BERT)","venue":"Inform : Jurnal Ilmiah Bidang Teknologi Informasi dan Komunikasi","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/07/2022","authors":"Diyah Utami Kusumaning Putri,Dinar Nugroho Pratomo","id":"f8a2428bee464ae69cdb25ce5c2259e88ce576b4","summary":"This study fine-tunes the Bidirectional Encoder Representations from Transformers (BERT) and uses the Indonesian news headlines dataset CLICK-ID to predict clickbait (BerT), and evaluation results indicate that all fine-tuned IndoBERT classifiers outperform all word-vectors-based machine learning classifiers in classifying Clickbait and non-clickbait IndonesianNews headlines.","score":1},{"url":"https://www.semanticscholar.org/paper/31d6e59dc275f33da71a72b6b86b38daeed9ffaa","title":"mmLayout: Multi-grained MultiModal Transformer for Document Understanding","venue":"ACM Multimedia","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/09/2022","authors":"Wenjin Wang,Zhengjie Huang,Bin Luo,Qianglong Chen,Qiming Peng,Yinxu Pan,Weichong Yin,Shi Feng,Yu Sun,Dianhai Yu,Yin Zhang","id":"31d6e59dc275f33da71a72b6b86b38daeed9ffaa","summary":"Experimental results on four tasks, including information extraction and document question answering, show that the proposed method can improve the performance of multimodal Transformers based on fine-grained elements and achieve better performance with fewer parameters.","score":1},{"url":"https://www.semanticscholar.org/paper/beb40cc99a6ab931aa8ea1758c1a0397ecd32847","title":"Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Peijie Jiang,Dingkun Long,Yanzhao Zhang,Pengjun Xie,Meishan Zhang,M. Zhang","id":"beb40cc99a6ab931aa8ea1758c1a0397ecd32847","summary":"This work suggests unsupervised statistical boundary information in-stead and proposes an architecture to encode the information directly into pre-trained language models, resulting in Boundary-Aware BERT (BABERT), which is applied for feature induction of Chinese sequence labeling tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/1d08b41de815c79d3fb874ddb159fbe21828c024","title":"VSCA: A Sentence Matching Model Incorporating Visual Perception","venue":"Cognitive Computation","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Zhe Zhang,Guang Xiao,Yurong Qian,Mengnan Ma,Hongyong Leng,T. Zhang","id":"1d08b41de815c79d3fb874ddb159fbe21828c024","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/701a9882884a473faa92324ea6c1ff6c9dacc3ce","title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks","venue":"","year":2023,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"Xinsong Zhang,Yan Zeng,Jipeng Zhang,Hang Li","id":"701a9882884a473faa92324ea6c1ff6c9dacc3ce","summary":"Extensive experiments on benchmark datasets show that X-FM can significantly outperform existing general foundation models and perform better than or comparable to existing foundation models specifically for language, vision, or vision-language understanding.","score":1},{"url":"https://www.semanticscholar.org/paper/134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":28,"citationCount":2,"influentialCitationCount":0,"publicationDate":"06/09/2022","authors":"Doan Nam Long Vu,N. Moosavi,Steffen Eger","id":"134e4d72e23bca51e290db171d063989883020f4","summary":"It is shown that an embedding-based metric that has the highest correlation with human evaluations on a standard benchmark can have the lowest correlation if the amount of input noise or unknown tokens increases, and the highest robustness is achieved when using character-level embeddings, instead of token-based embeddments, from the first layer of the pretrained model.","score":1},{"url":"https://www.semanticscholar.org/paper/527603f0d3c120bbe56753ce4cd7e5a41e0d5e6a","title":"Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems","venue":"ArXiv","year":2022,"referenceCount":95,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/09/2022","authors":"Sahar Abdelnabi,Mario Fritz","id":"527603f0d3c120bbe56753ce4cd7e5a41e0d5e6a","summary":"This work assumes an adversary that automatically tampers with the online evidence in order to disrupt the fact-checking model via camouﬂaging the relevant evidence or planting a misleading one, and proposes an exploratory taxonomy that spans these two targets and the different threat model dimensions.","score":1},{"url":"https://www.semanticscholar.org/paper/fb41147776fd3ef8c3370ce2574efc15486c9a0f","title":"Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":5,"influentialCitationCount":0,"publicationDate":"07/10/2022","authors":"Kenton Lee,Mandar Joshi,Iulia Turc,Hexiang Hu,Fangyu Liu,Julian Martin Eisenschlos,Urvashi Khandelwal,Peter Shaw,Ming-Wei Chang,Kristina Toutanova","id":"fb41147776fd3ef8c3370ce2574efc15486c9a0f","summary":"This work presents Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be used on tasks containing visually-situated language, and shows that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.","score":1},{"url":"https://www.semanticscholar.org/paper/04cd4c224f61e8f25a405103d4210f161d091d1c","title":"Look It Up: Bilingual Dictionaries Improve Neural Machine Translation","venue":"","year":2020,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2020","authors":"X. Zhong,David Chiang","id":"04cd4c224f61e8f25a405103d4210f161d091d1c","summary":"A new method for “attaching” dictionary definitions to rare words so that the network can learn the best way to use them and demonstrate provements of up to 1.8 BLEU using bilingual dictionaries.","score":1},{"url":"https://www.semanticscholar.org/paper/0a82ffe5c889d4defbd6300fffab4f3fa3dade99","title":"Deep convolution neural network with weighted loss to detect rice seeds vigor based on hyperspectral imaging under the sample-imbalanced condition","venue":"Computers and Electronics in Agriculture","year":2022,"referenceCount":45,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"Na Wu,Shizhuang Weng,Jinxin Chen,Qinlin Xiao,Chu Zhang,Yong He","id":"0a82ffe5c889d4defbd6300fffab4f3fa3dade99","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/cb48e3bcc185ae94899007e1ad3cdb49ff39428b","title":"Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video","venue":"ArXiv","year":2022,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/07/2022","authors":"Meng-Fen Tsai,Rosalie H. Wang,J. Zariffa","id":"cb48e3bcc185ae94899007e1ad3cdb49ff39428b","summary":"Using egocentric video to capture the hand use of stroke survivors at home is feasible and Pose estimation to track finger movements may be beneficial to classifying hand roles in the future.","score":1},{"url":"https://www.semanticscholar.org/paper/8b723be33e62bf5bd9278769244f1c13a9510898","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP","venue":"Neural Information Processing Systems","year":2021,"referenceCount":42,"citationCount":5,"influentialCitationCount":0,"publicationDate":"17/04/2021","authors":"J. Rozner,Christopher Potts,Kyle Mahowald","id":"8b723be33e62bf5bd9278769244f1c13a9510898","summary":"A novel curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words, and investigates model systematicity by perturbing the wordplay part of clues, showing that T5 exhibits behavior partially consistent with human solving strategies.","score":1},{"url":"https://www.semanticscholar.org/paper/f22ba8b56b89b2f19e84fbe3439bf78e8fc926b2","title":"KLASIFIKASI EMOSI PADA CUITAN DI TWITTER DENGAN PRINCIPAL COMPONENT ANALYSIS DAN SUPPORT VECTOR MACHINE","venue":"Mathunesa: Jurnal Ilmiah Matematika","year":2022,"referenceCount":27,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/04/2022","authors":"Abi Nizar Sutranggono","id":"f22ba8b56b89b2f19e84fbe3439bf78e8fc926b2","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/b2ed3ed8d8b5717644be0ae705c13e2c2121a058","title":"Cloud computing architecture for Tagging Arabic Text Using Hybrid Model","venue":"Applied computing Journal","year":2021,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/09/2021","authors":"Wasin Alkishri,Mohammed Almutoory","id":"b2ed3ed8d8b5717644be0ae705c13e2c2121a058","summary":"This paper presents and deploys a cloud computing architecture for Tagging Arabic text using a hybrid model, which will help reduce the efforts and cost and show an excellent accuracy rate in tagging an Arabic text and quickly respond.","score":1},{"url":"https://www.semanticscholar.org/paper/d1d3dde91e3e73ccfbeb176f3af565a4507be077","title":"AI-Based Misogyny Detection from Arabic Levantine Twitter Tweets","venue":"IOCA 2021","year":2021,"referenceCount":23,"citationCount":8,"influentialCitationCount":0,"publicationDate":"19/09/2021","authors":"A. Y. Muaad,Prof. Jayappa(J) Hanumanthappa Davanagere,M. A. Al-antari,J. V. B. Benifa,C. Chola","id":"d1d3dde91e3e73ccfbeb176f3af565a4507be077","summary":"An Arabic text recognition approach is presented for detecting misogyny from Arabic tweets using the Arabic Levantine Twitter Dataset for Misogynistic, and seems to be useful in providing practical smart solutions for detecting Arabic misogyny on social media.","score":1},{"url":"https://www.semanticscholar.org/paper/a6fad9b1b32ace586b735e1764d26f1d67f2f7c7","title":"Arabic Document Classification: Performance Investigation of Preprocessing and Representation Techniques","venue":"Mathematical Problems in Engineering","year":2022,"referenceCount":86,"citationCount":4,"influentialCitationCount":0,"publicationDate":"30/04/2022","authors":"A. Y. Muaad,Hanumanthappa Jayappa Davanagere,D. Guru,J. V. B. Benifa,C. Chola,Hussain Alsalman,A. Gumaei,M. A. Al-antari","id":"a6fad9b1b32ace586b735e1764d26f1d67f2f7c7","summary":"This work aims to identify the effectiveness of machine learning (ML) algorithms through preprocessing and representation techniques and shows that the classification performance strongly depends on the preprocessing technique, representation methods and classification technique, and the nature of datasets used.","score":1},{"url":"https://www.semanticscholar.org/paper/8c8aa03f8940b95336d0d70a9c37e024eff9e7bf","title":"Review on Recent Arabic Information Retrieval Techniques","venue":"EAI Endorsed Transactions on Internet of Things","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Abdelkrim Aarab,A. Oussous,Mohammed Saddoune","id":"8c8aa03f8940b95336d0d70a9c37e024eff9e7bf","summary":"An overview of the Arabic information retrieval process, including various text processing techniques, ranking approaches, evaluation measures, and some important information retrieval models is presented.","score":1},{"url":"https://www.semanticscholar.org/paper/29c09defb807a9e16b4ea7cbf91587c9f4361121","title":"A Review Study on Arabic Text Classification","venue":"Automation, Control, and Information Technology","year":2022,"referenceCount":162,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Musab Mustafa Hijazi,A. Zeki,Amelia Ismail","id":"29c09defb807a9e16b4ea7cbf91587c9f4361121","summary":"A review study on the recent researches that have studied Arabic text classification is presented, highlighting the need for automatic text categorization in the rapidly growing number of electronic documents.","score":1},{"url":"https://www.semanticscholar.org/paper/38375a9961778355f073bf974e643e4e00d6c10e","title":"Visual Cues and Error Correction for Translation Robustness","venue":"EMNLP","year":2021,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/03/2021","authors":"Zhenhao Li,Marek Rei,Lucia Specia","id":"38375a9961778355f073bf974e643e4e00d6c10e","summary":"This paper focuses on three types of realistic noise that are commonly generated by humans and introduces the idea of visual context to improve translation robustness for noisy texts and describes a novel error correction training regime that can be used as an auxiliary task to further improvetranslation robustness.","score":1},{"url":"https://www.semanticscholar.org/paper/a76c98c6814ce8de07707b81c18520af508b7184","title":"BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks","venue":"Findings","year":2021,"referenceCount":36,"citationCount":9,"influentialCitationCount":1,"publicationDate":"02/06/2021","authors":"Y. Keller,J. Mackensen,Steffen Eger","id":"a76c98c6814ce8de07707b81c18520af508b7184","summary":"This work shows that an untrained iterative approach which combines context-independent character-level information with context-dependent information from BERT’s masked language modeling can perform on par with human crowd-workers from Amazon Mechanical Turk supervised via 3-shot learning.","score":1},{"url":"https://www.semanticscholar.org/paper/96fd89de07a69dd2dc94d71f884e64174c5974e2","title":"Interpreting the Robustness of Neural NLP Models to Textual Perturbations","venue":"FINDINGS","year":2021,"referenceCount":46,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/10/2021","authors":"Yunxiang Zhang,Liangming Pan,Samson Tan,Min-Yen Kan","id":"96fd89de07a69dd2dc94d71f884e64174c5974e2","summary":"This work conducts extensive experiments with four prominent NLP models — TextRNN, BERT, RoBERTa and XLNet — over eight types of textual perturbations on three datasets, showing that a model which is better at identifying a perturbation becomes worse at ignoring such a perturgation at test time (lower robustness), providing empirical support for the hypothesis.","score":1},{"url":"https://www.semanticscholar.org/paper/9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e","title":"vTTS: visual-text to speech","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":2,"influentialCitationCount":1,"publicationDate":"28/03/2022","authors":"Yoshifumi Nakano,Takaaki Saeki,Shinnosuke Takamichi,Katsuhito Sudoh,H. Saruwatari","id":"9af10e3d4ad1c6f8a0ff8fac8dec52703faa8e7e","summary":"Experimental results show that visual-text to speech is capable of generating speech with naturalness comparable to or better than a conventional TTS, it can transfer emphasis and emotion attributes in visual text to speech without additional labels and architectures, and it can synthesize more natural and intelligible speech from unseen and rare characters than conventional T TS.","score":1},{"url":"https://www.semanticscholar.org/paper/4743ee49af83d3010549ee105e0c193b36fa239a","title":"Machine Translation Robustness to Natural Asemantic Variation","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Jacob Bremerman,Xiang Ren,Jonathan May","id":"4743ee49af83d3010549ee105e0c193b36fa239a","summary":"It is shown that NAV robustness can be transferred across languages and that synthetic perturbations can achieve some but not all of the ben-eﬁts of human-generated NAV data.","score":1},{"url":"https://www.semanticscholar.org/paper/c9b98d7dd15acfddf8de8448263bfff0feb6c382","title":"Logographic Information Aids Learning Better Representations for Natural Language Inference","venue":"AACL/IJCNLP","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/11/2022","authors":"Zijian Jin,Duygu Ataman","id":"c9b98d7dd15acfddf8de8448263bfff0feb6c382","summary":"Evaluation results in six languages with different typology and writing systems suggest the importance of using multi-modal embeddings in languages with logograhic systems, especially for words with less occurence statistics.","score":1},{"url":"https://www.semanticscholar.org/paper/a94e2ce7f94d189e5f788cfa431c504b3fb49402","title":"A Major Obstacle for NLP Research: Let's Talk about Time Allocation!","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Katharina Kann,Shiran Dudy,Arya D. McCarthy","id":"a94e2ce7f94d189e5f788cfa431c504b3fb49402","summary":"It is demonstrated that, in recent years, subpar time allocation has been a major obstacle for NLP research and multiple concrete problems are out-line together with their negative consequences and remedies to improve the status quo are suggested.","score":1},{"url":"https://www.semanticscholar.org/paper/f742e7a7582d647ffe9b8037ba286c4932eb84dd","title":"Boosting Neural Machine Translation from Finnish to Northern Sámi with Rule-Based Backtranslation","venue":"NODALIDA","year":2021,"referenceCount":20,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Mikko Aulamo,Sami Virpioja,Yves Scherrer,J. Tiedemann","id":"f742e7a7582d647ffe9b8037ba286c4932eb84dd","summary":"A low-resource translation task from Finnish into Northern Sámi is considered, and it is found that the RBMT backtranslation outperforms NMT backtranslation clearly for the out-of-domain test set, but also slightly for the in-domain data, for which the N MT backtranslation model provided clearly better BLEU scores than the R BMT.","score":1},{"url":"https://www.semanticscholar.org/paper/7d4e38cc1c26ef513b3e9764d1778917e5420187","title":"Transformers for Low-Resource Languages: Is Féidir Linn!","venue":"MTSUMMIT","year":2021,"referenceCount":33,"citationCount":2,"influentialCitationCount":0,"publicationDate":2021,"authors":"Séamus Lankford,H. Alfi,Andy Way","id":"7d4e38cc1c26ef513b3e9764d1778917e5420187","summary":"It is demonstrated that choosing appropriate parameters leads to considerable performance improvements and the correct choice of subword model is shown to be the biggest driver of translation performance.","score":1},{"url":"https://www.semanticscholar.org/paper/7cb4b8406255d0f8115afa23d8efea1bb780cfb8","title":"On the Compatibility of Tokenizations Across Languages","venue":"","year":2021,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Antonis Maronikolakis,Philipp Dufter,Hinrich Schütze","id":"7cb4b8406255d0f8115afa23d8efea1bb780cfb8","summary":"It is shown that the compatibility measure proposed allows the system designer to create vocabularies across languages that are compatible – a desideratum that so far has been neglected in multilingual models.","score":1},{"url":"https://www.semanticscholar.org/paper/a49b0997efaec2db61109a6deed1512672c3cd0c","title":"Many-to-English Machine Translation Tools, Data, and Pretrained Models","venue":"ACL","year":2021,"referenceCount":66,"citationCount":5,"influentialCitationCount":1,"publicationDate":"01/04/2021","authors":"Thamme Gowda,Zhao Zhang,C. Mattmann,Jonathan May","id":"a49b0997efaec2db61109a6deed1512672c3cd0c","summary":"This work creates a multilingual neural machine translation model capable of translating from 500 source languages to English, readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages.","score":1},{"url":"https://www.semanticscholar.org/paper/3e32139deb17761a25075f8839daa61ad5992fc9","title":"Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":41,"citationCount":10,"influentialCitationCount":1,"publicationDate":"18/04/2021","authors":"Mozhdeh Gheini,Xiang Ren,Jonathan May","id":"3e32139deb17761a25075f8839daa61ad5992fc9","summary":"It is revealed that fine-tuning only the cross-attention parameters is nearly as effective as fine- Tuning all parameters (i.e., the entire translation model) in terms of mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.","score":1},{"url":"https://www.semanticscholar.org/paper/49a5a1a550d0ce757befbad7715f373d8a5b3b87","title":"On the Strengths of Cross-Attention in Pretrained Transformers for Machine Translation","venue":"ArXiv","year":2021,"referenceCount":42,"citationCount":5,"influentialCitationCount":0,"publicationDate":2021,"authors":"Mozhdeh Gheini,Xiang Ren,Jonathan May","id":"49a5a1a550d0ce757befbad7715f373d8a5b3b87","summary":"It is found that, apart from the new language’s embeddings, only the cross-attention parameters need to be fine-tuned to obtain competitive BLEU performance.","score":1},{"url":"https://www.semanticscholar.org/paper/94772377a9c08ad81e506240f844534b6669b8e9","title":"Diversifying Dialog Generation via Adaptive Label Smoothing","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":46,"citationCount":16,"influentialCitationCount":5,"publicationDate":"30/05/2021","authors":"Yida Wang,Yinhe Zheng,Yong Jiang,Minlie Huang","id":"94772377a9c08ad81e506240f844534b6669b8e9","summary":"An Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts, which outperforms various competitive baselines in producing diverse responses.","score":1},{"url":"https://www.semanticscholar.org/paper/2607dce6dcb9043ca9cae67e25e6a24411f08c0b","title":"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":50,"citationCount":15,"influentialCitationCount":3,"publicationDate":"09/09/2021","authors":"Haoran Xu,Benjamin Van Durme,Kenton Murray","id":"2607dce6dcb9043ca9cae67e25e6a24411f08c0b","summary":"This paper demonstrates that simply using the output of a tailored and suitable bilingual pre-trained language model (dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art translation performance and proposes a stochastic layer selection approach and a dual-directional translation model to ensure the sufficient utilization of contextualized embeddings.","score":1},{"url":"https://www.semanticscholar.org/paper/318853f0448a9b95235dbb99338af0b19d138eaa","title":"Quasi Character-Level Transformers to Improve Neural Machine Translation on Small Datasets","venue":"2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)","year":2021,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/12/2021","authors":"Salvador Carrión,F. Casacuberta","id":"318853f0448a9b95235dbb99338af0b19d138eaa","summary":"This work shows how this standard approach to subword segmentation might be counter-productive for small datasets or low-resource environments, where models trained with quasi character-level vocabularies seem to con-sistently outperform models with large subword vocABularies.","score":1},{"url":"https://www.semanticscholar.org/paper/674265c672777b6d10d5455adc58a6cacb0d0cfe","title":"BPE beyond Word Boundary: How NOT to use Multi Word Expressions in Neural Machine Translation","venue":"INSIGHTS","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Dipesh Kumar,Avijit Thawani","id":"674265c672777b6d10d5455adc58a6cacb0d0cfe","summary":"This work observes that naively extending BPE beyond word boundaries results in incoherent tokens which are themselves better represented as individual words, and finds that Pointwise Mutual Information (PMI) instead of frequency finds better MWEs (e.g., New\\_York, Statue of Liberty, neither .","score":1},{"url":"https://www.semanticscholar.org/paper/52d4e170bc9a34797c521fa7c35c650538b5f909","title":"IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations","venue":"SEMEVAL","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Damir Korenčić,I. Grubisic","id":"52d4e170bc9a34797c521fa7c35c650538b5f909","summary":"This paper gives a detailed overview of the systems that were designed for Definition Modeling and Reverse Dictionary tasks, and that achieved top scores on SemEval-2022 CODWOE challenge in several subtasks.","score":1},{"url":"https://www.semanticscholar.org/paper/b9bba726584babe18ed249c3ff55e9b851a2876c","title":"Automatic Gloss-level Data Augmentation for Sign Language Translation","venue":"LREC","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"J. Jang,Han-Mu Park,Saim Shin,Suna Shin,Byung-Chen Yoon,G. Gweon","id":"b9bba726584babe18ed249c3ff55e9b851a2876c","summary":"It is demonstrated that three different augmentation techniques used in existing Natural Language Processing (NLP) can be applied to sign language and an automatic data augmentation method which generates quality data by utilizing the Korean sign language gloss dictionary is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/9e3f3789ef3bfbf8f2ab3174c4d9a49b92085787","title":"Limitations and Challenges of Unsupervised Cross-lingual Pre-training","venue":"AMTA","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Martín Quesada Zaragoza,F. Casacuberta","id":"9e3f3789ef3bfbf8f2ab3174c4d9a49b92085787","summary":"The results show that unsupervised cross-lingual methods are effective at inducing alignment even for distant languages and they benefit noticeably from subword information, but their effectiveness as pre-training models in machine translation is severely limited.","score":1},{"url":"https://www.semanticscholar.org/paper/4ddd463c22fab24e6d89105b6aa887e149832dac","title":"HFT: High Frequency Tokens for Low-Resource NMT","venue":"LORESMT","year":2022,"referenceCount":12,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Edoardo Signoroni,P. Rychlý","id":"4ddd463c22fab24e6d89105b6aa887e149832dac","summary":"This paper presents “High Frequency Tokenizer”, or HFT, a new language-independent subword segmentation algorithm that addresses low-frequency tokens in the training data and proposes a new metric to measure the frequency coverage of a tokenizer’s vocabulary, based on a frequency rank weighted average of the frequency values of its items.","score":1},{"url":"https://www.semanticscholar.org/paper/3442f4bd36ee6b23196b8db40a0b8f84c55a960d","title":"SeqTrans: Automatic Vulnerability Fix via Sequence to Sequence Learning","venue":"IEEE Transactions on Software Engineering","year":2020,"referenceCount":109,"citationCount":6,"influentialCitationCount":4,"publicationDate":"21/10/2020","authors":"Jianlei Chi,YunHuan Qu,Ting Liu,Q. Zheng,Heng Yin","id":"3442f4bd36ee6b23196b8db40a0b8f84c55a960d","summary":"This paper provides a novel approach called SeqTrans to exploit historical vulnerability fixes to provide suggestions and automatically fix the source code, and proposes to leverage data flow dependencies to construct code sequences and fed them into the state-of-the-art transformer model.","score":1},{"url":"https://www.semanticscholar.org/paper/48d5e267ab336a0b4caf2e1909d7860b02c6a736","title":"Direct Speech-to-Speech Translation With Discrete Units","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":115,"citationCount":48,"influentialCitationCount":2,"publicationDate":"12/07/2021","authors":"Ann Lee,Peng-Jen Chen,Changhan Wang,Jiatao Gu,Xutai Ma,A. Polyak,Yossi Adi,Qing He,Yun Tang,J. Pino,Wei-Ning Hsu","id":"48d5e267ab336a0b4caf2e1909d7860b02c6a736","summary":"A direct speech-to-speech translation model that translates speech from one language to speech in another language without relying on intermediate text generation is presented and is comparable to models that predict spectrograms and are trained with text supervision.","score":1},{"url":"https://www.semanticscholar.org/paper/a821453809f36a23402ce93b6042c76d85a720a6","title":"Frequency-Aware Contrastive Learning for Neural Machine Translation","venue":"AAAI","year":2021,"referenceCount":41,"citationCount":4,"influentialCitationCount":0,"publicationDate":"29/12/2021","authors":"Tong Zhang,Wei Ye,Baosong Yang,Long Zhang,Xingzhang Ren,Dayiheng Liu,Jinan Sun,Shikun Zhang,Haibo Zhang,Wen Zhao","id":"a821453809f36a23402ce93b6042c76d85a720a6","summary":"A frequency-aware token-level contrastive learning method, in which the hidden state of each decoding step is pushed away from the counterparts of other target words in a soft contrastive way based on the corresponding word frequencies, which can not only significantly improve the translation quality but also enhance lexical diversity and optimize word representation space.","score":1},{"url":"https://www.semanticscholar.org/paper/88aeac6f5efe99bf24aae982a9c2d00d02685bf7","title":"IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations","venue":"","year":2022,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/05/2022","authors":"Damir Korenvci'c,Ivan Grubivsi'c","id":"88aeac6f5efe99bf24aae982a9c2d00d02685bf7","summary":"A detailed overview of the systems that are designed for Deﬁnition Modeling and Reverse Dictionary tasks, and that achieved top scores on SemEval-2022 CODWOE challenge in several subtasks are given.","score":1},{"url":"https://www.semanticscholar.org/paper/4f68042a0aa40f34027a49ceec64ad2bbe2211aa","title":"When does Parameter-Efficient Transfer Learning Work for Machine Translation?","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"A. Ustun,Asa Cooper Stickland","id":"4f68042a0aa40f34027a49ceec64ad2bbe2211aa","summary":"It is shown that increas- 018 ing model size, but tuning only 0.03% of total parameters, can outperform tuning 100% of the 020 parameters of a smaller model 1 .","score":1},{"url":"https://www.semanticscholar.org/paper/bb459f27d173b3ffa293a0213ec29e31e44c404d","title":"Reinforcement Learning with Large Action Spaces for Neural Machine Translation","venue":"COLING","year":2022,"referenceCount":47,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Asaf Yehudai,Leshem Choshen,Lior Fox,Omri Abend","id":"bb459f27d173b3ffa293a0213ec29e31e44c404d","summary":"It is found that reducing the size of the vocabulary improves RL’s effectiveness and effectively reducing the dimension of the action space without changing the vocabulary also yields notable improvement as evaluated by BLEU, semantic similarity, and human evaluation.","score":1},{"url":"https://www.semanticscholar.org/paper/a30318cc5281187bd315915f0d0cccd7007f75b9","title":"The VolcTrans System for WMT22 Multilingual Machine Translation Task","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Xian Qian,Kai Hu,Jiaqiang Wang,Yifeng Liu,Xingyuan Pan,Jun Cao,Mingxuan Wang","id":"a30318cc5281187bd315915f0d0cccd7007f75b9","summary":"The VolcTrans system is a transformer-based multilingual model trained on data from multiple sources including the public training set from the data track, NLLB data provided by Meta AI, self-collected parallel corpora, and pseudo bitext from back-translation to clean both bilingual and monolingual texts.","score":1},{"url":"https://www.semanticscholar.org/paper/caec2c201aa002ee23d0c7fea7dff6e7ffa7b267","title":"Towards a general purpose machine translation system for Sranantongo","venue":"ArXiv","year":2022,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Just Zwennicker,David Stap","id":"caec2c201aa002ee23d0c7fea7dff6e7ffa7b267","summary":"This study creates a general purpose machine translation system for srn, introduces the SRNcorpus, a collection of parallel Dutch to srn and monolingual srn data, and experiment with a wide range of proven machine translation methods.","score":1},{"url":"https://www.semanticscholar.org/paper/d458218398f84b6ce259f3ef684582eea9e9158e","title":"String Processing and Information Retrieval: 27th International Symposium, SPIRE 2020, Orlando, FL, USA, October 13–15, 2020, Proceedings","venue":"SPIRE","year":2020,"referenceCount":528,"citationCount":0,"influentialCitationCount":0,"publicationDate":2020,"authors":"A. Nishi,Yuto Nakashima,Shunsuke Inenaga,H. Bannai,M. Takeda","id":"d458218398f84b6ce259f3ef684582eea9e9158e","summary":"This paper proposes a new query that can be more appropriate in these collections of repetitive string collections, which is called contextual pattern matching and presents the first solution of this kind.","score":1},{"url":"https://www.semanticscholar.org/paper/ec69e191e25ff4ab9b59178fdc5bd171f9f147b6","title":"Using Context to Help Predict Speaker's Emotions in Social Dialogue","venue":"HCI","year":2020,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":2020,"authors":"Mei Si","id":"ec69e191e25ff4ab9b59178fdc5bd171f9f147b6","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/25fc1d08e98172afe9ac2e4a64fb8fe359045547","title":"Morphological segmentation method for Turkic language neural machine translation","venue":"","year":2020,"referenceCount":31,"citationCount":5,"influentialCitationCount":0,"publicationDate":"01/01/2020","authors":"U. Tukeyev,A. Karibayeva,Z. Zhumanov","id":"25fc1d08e98172afe9ac2e4a64fb8fe359045547","summary":"A new morphological segmentation approach for Turkic languages based on the complete set of endings (CSE), which reduces the vocabulary volume of the source corpora and reduced the vocabulary size in NMT by more than a factor of two.","score":1},{"url":"https://www.semanticscholar.org/paper/b0b0dddb8310e01b9407a21674c2d33a23a6e967","title":"Byte Pair Encoding is Suboptimal for Language Model Pretraining","venue":"Findings","year":2020,"referenceCount":29,"citationCount":72,"influentialCitationCount":6,"publicationDate":"07/04/2020","authors":"Kaj Bostrom,Greg Durrett","id":"b0b0dddb8310e01b9407a21674c2d33a23a6e967","summary":"Differences between BPE and unigram LM tokenization are analyzed, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE’s greedy construction procedure.","score":1},{"url":"https://www.semanticscholar.org/paper/308e3090cc3c77712733a5552d10d25f210cd905","title":"Practical Random Access to SLP-Compressed Texts","venue":"SPIRE","year":2020,"referenceCount":36,"citationCount":10,"influentialCitationCount":0,"publicationDate":"20/07/2020","authors":"T. Gagie,I. Tomohiro,G. Manzini,G. Navarro,H. Sakamoto,Louisa Seelbach Benkner,Yoshimasa Takabatake","id":"308e3090cc3c77712733a5552d10d25f210cd905","summary":"A new encoding of grammars that is about as small as the practical state of the art but with significantly faster queries and the possibility of supporting fast random access is given.","score":1},{"url":"https://www.semanticscholar.org/paper/d36d6abf8f9f1e80124d8a72dc5203802a6fdb26","title":"IIITT at CASE 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection","venue":"CASE","year":2021,"referenceCount":38,"citationCount":5,"influentialCitationCount":1,"publicationDate":2021,"authors":"Pawan Kalyan,D.Ramohan Reddy,Adeep Hande,R. Priyadharshini,Ratnasingam Sakuntharaj,Bharathi Raja Chakravarthi","id":"d36d6abf8f9f1e80124d8a72dc5203802a6fdb26","summary":"This paper demonstrates its work on the sentence classification subtask of multilingual protest detection in CASE@ACL-IJCNLP 2021 by employing various multilingual pre-trained transformer models to classify if any sentence contains information about an event that has transpired or not.","score":1},{"url":"https://www.semanticscholar.org/paper/617c0f6fc2d6807a9cfa72f487b0542b25342fd0","title":"Optimizing Word Alignments with Better Subword Tokenization","venue":"MTSUMMIT","year":2021,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Anh Khoa Ngo Ho,François Yvon","id":"617c0f6fc2d6807a9cfa72f487b0542b25342fd0","summary":"This paper thoroughly study how this preprocessing step interacts with the word alignment task and proposes several tokenization strategies to obtain well-segmented parallel corpora and improves baseline word-based alignment models for six language pairs.","score":1},{"url":"https://www.semanticscholar.org/paper/1c98fcd62a5889c70ea9da4cd168b36253c7101c","title":"No Features Needed: Using BPE Sequence Embeddings for Web Log Anomaly Detection","venue":"IWSPA@CODASPY","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/04/2022","authors":"Nitesh Sehwani","id":"1c98fcd62a5889c70ea9da4cd168b36253c7101c","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/2e53fe6ff7d6d74da962957a5c0b282eaabd4edf","title":"How Effective is Byte Pair Encoding for Out-Of-Vocabulary Words in Neural Machine Translation?","venue":"Conference of the Association for Machine Translation in the Americas","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/08/2022","authors":"Ali Araabi,Christof Monz,Vlad Niculae","id":"2e53fe6ff7d6d74da962957a5c0b282eaabd4edf","summary":"This paper analyzes the translation quality of OOV words based on word type, number of segments, cross-attention weights, and the frequency of segment n-grams in the training data and highlights the slightly higher effectiveness of BPE in translating OOV Words for special cases, such as named-entities and when the languages involved are linguistically close to each other.","score":1},{"url":"https://www.semanticscholar.org/paper/c464352b7ab5f74bc201b5cf94bb4b9a14f5f487","title":"Masked Part-Of-Speech Model: Does Modeling Long Context Help Unsupervised POS-tagging?","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/06/2022","authors":"Xiaoping Zhou,Shiyue Zhang,Mohit Bansal","id":"c464352b7ab5f74bc201b5cf94bb4b9a14f5f487","summary":"A Masked Part-of-Speech Model (MPoSM) is proposed, inspired by the recent success of Masked Language Models (MLM), that can model arbitrary tag dependency and perform POS induction through the objective of masked POS reconstruction and achieves overall better performance.","score":1}]}
{"papers":[{"url":"https://www.semanticscholar.org/paper/f8665a1a5dcf4c771c146edc67c353f007355911","title":"Leveraging Pre-training Models for Speech Processing","venue":"","year":null,"referenceCount":257,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"f8665a1a5dcf4c771c146edc67c353f007355911","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/afbcc8ba3959f55dd5cc3b4d34c4238cc1622382","title":"Diacritization for the World’s Scripts","venue":"","year":null,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Kyle Gorman,Yuval Pinter,Michael Elhadad","id":"afbcc8ba3959f55dd5cc3b4d34c4238cc1622382","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/3c55d5e88c301237ecfb362cae5adcd87a1bc4af","title":"VU Research Portal Analyzing Cognitive Plausibility of Subword Tokenization","venue":"","year":null,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Lisa Beinborn,Yuval Pinter","id":"3c55d5e88c301237ecfb362cae5adcd87a1bc4af","summary":"This work presents a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization, and analyzes the correlation of the tokenizer output with the response time and accuracy of human performance on a lexical decision task.","score":1},{"url":"https://www.semanticscholar.org/paper/28f68458f8c73da7ba1493e1a2c50862580fe0b4","title":"EncT5: Fine-tuning T5 Encoder for Discriminative Tasks","venue":"","year":2021,"referenceCount":17,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Prafulla,Dhariwal,Girish Sastry,Sam McCandlish","id":"28f68458f8c73da7ba1493e1a2c50862580fe0b4","summary":"This work proposes 010 EncT5 as a way to efﬁciently efﬁciently tune pre-trained encoder-decoder T5 models for classiﬁcation and regression tasks by using 013 the encoder layers.","score":1},{"url":"https://www.semanticscholar.org/paper/f31a11257aad37847893b5495024865ca5f41ef9","title":"End-to-end style-conditioned poetry generation: What does it take to learn from examples alone?","venue":"LATECHCLFL","year":2021,"referenceCount":53,"citationCount":6,"influentialCitationCount":0,"publicationDate":2021,"authors":"Jörg Wöckener,T. Haider,Tristan Miller,The-Khang Nguyen,Thanh Tung Linh Nguyen,Minh Vu Pham,Jonas Belouadi,Steffen Eger","id":"f31a11257aad37847893b5495024865ca5f41ef9","summary":"This work designs an end-to-end model for poetry generation based on conditioned recurrent neural network (RNN) language models whose goal is to learn stylistic features (poem length, sentiment, alliteration, and rhyming) from examples alone and shows it successfully learns the ‘meaning’ of length and sentiment.","score":1},{"url":"https://www.semanticscholar.org/paper/4c70347a3b9b5a27f4964c8703ae22d015bb49c1","title":"Machine Reading Comprehension: Generative or Extractive Reader?","venue":"","year":2021,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"T5-Gen T5-Gen,Hao Cheng,Yelong Shen,Pengcheng Xiaodong Liu,Kevin Clark,Minh-Thang Luong,Quoc V. Le,Christopher D. Manning. 2020,J. Devlin,Ming-Wei Chang,Kenton Lee,Martin Fajcik,Martin Docekal,Karel Ondrej,P. S. 2021,V. Karpukhin,Barlas Oğuz,Sewon Min,Patrick,Ledell Lewis,Sergey Wu,Danqi Edunov,Chen,Clement Chaumond,Anthony Delangue,Pier-339 Moi,Tim ric Cistac,Rémi Rault,Morgan Louf,Funtow-900 Joe,Sam Davison,Patrick Shleifer,von Platen,Clara Ma,Yacine Jernite,Julien Plu,Canwen Xu,Xiang Wenhan Xiong,Lorraine Li,Srini Iyer,Linting Xue,Aditya Barua,Noah Constant,Rami Al-695 Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Zhengyan Zhang,Xu Han,Zhiyuan Liu,Xin Jiang","id":"4c70347a3b9b5a27f4964c8703ae22d015bb49c1","summary":"This paper designs multiple transformer-based models and systematically compares these two readers, and suggests that, although an encoder-023 only pre-trained language model (PrLM) is an intuitive choice for extractive readers, the en-025 coder from encoder-decoder PrLM is a strong alternative that performs competitively.","score":1},{"url":"https://www.semanticscholar.org/paper/103dc8d8572eadafa984e4b4c15855f9cdc754fb","title":"Few-shot Controllable Style Transfer for Low-Resource Multilinugal Settings","venue":"","year":2021,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Balamurugan Anandan,Chris Clifton,Wei Jiang,Mummoorthy Murugesan,Pedro Pastrana-583 Camacho,Kalika Bali,Jatin Sharma,M. Choudhury,J. Devlin,Ming-Wei Chang,Kenton Lee,Xavier Garcia,Noah Constant,Mandy Guo,Kelvin Guu,Tatsunori Hashimoto,Yonatan Oren,Dirk Hovy,Federico Bianchi,Tommaso Forna-687,Daphne Ippolito,Daniel Duckworth,Chris Callison-693 Burch,Douglas Eck. 2020,Benjamin Marie,Atsushi Fujita,Raphael Rubino,Kishore Papineni,S. Roukos,Todd Ward,Jonas Pfeiffer,Andreas Rücklé,Clifton A. Poth,Aishwarya Kamath,Ivan Vuli´c,Sebastian Ruder,Iryna Gurevych,Peng Xu,Jackie Chi,Kit Cheung,Yanshuai Cao,Wei Xu,Christopher Callison-Burch,Courtney Napoles,Alan Ritter,Bill Dolan,R. Grishman,Linting Xue,Aditya Barua,Rami Al-695 Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Yinfei Yang,Daniel Cer,Amin Ahmad,Jax Guo,Noah Law,Gustavo Constant,Steve Hernan-958 dez Abrego,Chris Yuan,Yun-hsuan Tar,Sung","id":"103dc8d8572eadafa984e4b4c15855f9cdc754fb","summary":"This work studies a relevant low-resource setting: style trans-009 fer for languages where no style-labelled cor-010 pora are available, and pushes the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases.","score":1},{"url":"https://www.semanticscholar.org/paper/8496c62bb8ecc67dfb0959ef9321f875d5ff067b","title":"Phone-ing it in: Towards Flexible, Multi-Modal Language Model Training using Phonetic Representations of Data","venue":"","year":2021,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Shamsuddeen Hassan Muhammad,Joyce Nakatumba-Nabende,Perez Ogayo,Anuoluwapo Aremu,Catherine Gitau,Derguene,J. Mbaye,Seid Muhie Alabi,Tajuddeen R Yimam,Ignatius 515 Gwadabe,Rubungo Ezeani,Andre Jonathan,Verrah A Mukiibi,Iroro Otiende,Paul Rayson,Mofetoluwa Adeyemi,Gerald Muriuki,E. Anebi,Chiamaka Ijeoma,Chukwuneke,N. Odu,Eric Peter Wairagala,S. Oyerinde,Tobius Clemencia Siro,Saul Bateesa,Deborah Nabagereka,Maurice Katusiime,Ayodele,Mouhamadane Awokoya,Dibora Mboup,Gebrey-525 Henok,Kelechi Tilaye,Nwaike,Degaga,Chantal Amrhein,Rico Sennrich. 2020,On Roman-535,Rosana Ardila,Megan Branson,Kelly Davis,Michael Henretty,Josh Kohler,Reuben Meyer,Alexei Baevski,Wei-Ning Hsu,Alexis Conneau,Tom Brown,Benjamin Mann,Nick Ryder,Jared D Subbiah,Prafulla Kaplan,A. Dhariwal,P. Neelakantan,Girish Shyam,Amanda Sastry,Sandhini Askell,Ariel Agarwal,Herbert-Voss,Gretchen Krueger,T. Henighan,R. Child,Aditya Ramesh,Daniel M. Ziegler,Jeffrey Wu,Clemens Winter,Chris Hesse,Mark Chen,Eric Sigler,Mateusz Litwin,S. Gray,B. Chess,Christopher Clark,Sam Berner,Alec McCandlish,Ilya Radford,Sutskever Dario,Amodei,Davis David,Linting Xue,Aditya Barua,Noah Constant,Rami Al-695 Rfou,Sharan Narang,Mihir Kale,Adam Roberts","id":"8496c62bb8ecc67dfb0959ef9321f875d5ff067b","summary":"A multi-modal approach to train lan-008 guage models using whatever text and/or audio data might be available in a language using models pre-trained on phone data shows an improvement of up to 6% F1-score above models that are trained from scratch.","score":1},{"url":"https://www.semanticscholar.org/paper/ae7fc32df9401a86353185515f4fd5e2020ac922","title":"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations","venue":"arXiv.org","year":2021,"referenceCount":43,"citationCount":5,"influentialCitationCount":1,"publicationDate":2021,"authors":"Theodore Jiang,Li Fang,Kai Wang","id":"ae7fc32df9401a86353185515f4fd5e2020ac922","summary":"This study introduces MutFormer, a transformer-based model for the prediction of pathogenic missense mutations, using reference and mutated amino acid sequences from the human genome as the features, and shows that MutFormer outperforms a variety of existing tools in pathogenicity prediction.","score":1},{"url":"https://www.semanticscholar.org/paper/b8146800f8fd63c29e5f00b1e1b441da55f28c05","title":"Sentence-Level Discourse Parsing as Text-to-Text Generation","venue":"","year":2021,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Parminder Bhatia,Yangfeng Ji,Lynn Carlson,Daniel Marcu,Mary Ellen,Seeger Fisher,Brian Roark. 2007,Shima Gerani,Yashar Mehdad,G. Carenini,T. Ng,Bita Nejat. 2014,Abstractive,Hugo Hernault,H. Prendinger,David duVerle,Peter Jansen,M. Surdeanu,Pengfei Liu,Weizhe Yuan,Jinlan Fu,Zhengbao Jiang,Annie Louis,Aravind K. Joshi,A. Nenkova,William C Mann,Sandra A Thompson,Mitchell P. Marcus,Beatrice Santorini,Mary Ann,Thanh-Tung Nguyen,Xuan-Phi Nguyen,Shafiq R. Joty,Giovanni Paolini,Ben Athiwaratkun,Jason Krone","id":"b8146800f8fd63c29e5f00b1e1b441da55f28c05","summary":"This work introduces an end-to-end method for sentence-level RST discourse parsing via trans-007 forming it into a text-to-text generation task and demonstrates that the proposed method outperforms existing 016 methods in both tasks of sentence-level RST parsing and discourse segmentation.","score":1},{"url":"https://www.semanticscholar.org/paper/e873ddaf58ff92ae0492983cf57221fb25837f4e","title":"Strategies in subword tokenization: humans vs. algorithms","venue":"","year":2021,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Jonathan H. Clark,Daniel H Garrette,Iulia Turc,J. Devlin,Ming-Wei Chang,Kenton Lee,Nadir Durrani,Fahim Dalvi,Hassan Sajjad","id":"e873ddaf58ff92ae0492983cf57221fb25837f4e","summary":"A new method to an-010 alyze subword segmentation strategies relying on a spatial analysis of the distribution of sub-012 words’ lengths is proposed, which shows that humans tend to balance creativity and consistency, while algorithms tend to be either strongly biased or inconsistent.","score":1},{"url":"https://www.semanticscholar.org/paper/e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order","venue":"arXiv.org","year":2021,"referenceCount":74,"citationCount":11,"influentialCitationCount":1,"publicationDate":2021,"authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","summary":"Investigating the insensitivity of natural language models to word-order by quantifying perturbations and analysing their effect on neural models’ performance on language understanding tasks in GLUE benchmark finds that neural language models require local ordering more so than the global ordering of tokens.","score":1},{"url":"https://www.semanticscholar.org/paper/921c1216edbf6b2931b15874f24847ff1007ad8c","title":"EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks","venue":"arXiv.org","year":2021,"referenceCount":15,"citationCount":19,"influentialCitationCount":6,"publicationDate":2021,"authors":"Frederick Liu,Siamak Shakeri,Hongkun Yu,Jing Li","id":"921c1216edbf6b2931b15874f24847ff1007ad8c","summary":"EncT5 is proposed as a way to efﬁciently tune pre-trained encoder-decoder T5 models for classi-cation and regression tasks by using the encoder layers and the experimental results show that EncT5 with less than half of the parameters of T5 performs similarly to T 5 models on GLUE benchmark.","score":1},{"url":"https://www.semanticscholar.org/paper/7175caf7568d46c857380d0e5b64653819d5cc45","title":"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization","venue":"Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)","year":2021,"referenceCount":75,"citationCount":27,"influentialCitationCount":3,"publicationDate":2021,"authors":"R. Goot,Alan Ramponi,A. Zubiaga,Barbara Plank,Benjamin Muller,I. Roncal,Nikola Ljubesic,Özlem Çetinoğlu,Rahmad Mahendra,Talha Çolakoğlu,Timothy Baldwin,Tommaso Caselli,Wladimir Sidorenko,Bruno Kessler","id":"7175caf7568d46c857380d0e5b64653819d5cc45","summary":"The MULTILEXNORM shared task provides the largest publicly available multilingual lexical normalization benchmark including 12 language variants and proposes a homogenized evaluation setup with both intrinsic and extrinsic evaluation.","score":1},{"url":"https://www.semanticscholar.org/paper/de0e1f9980afa7949df64d53b8ae7a2f59c55579","title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages","venue":"arXiv.org","year":2021,"referenceCount":81,"citationCount":4,"influentialCitationCount":0,"publicationDate":2021,"authors":"Kalpesh Krishna,Deepak Nathani,Xavier García,Bidisha Samanta,P. Talukdar","id":"de0e1f9980afa7949df64d53b8ae7a2f59c55579","summary":"This work pushes the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases, and achieves 2-3x better performance and output diversity in formality transfer and code-mixing addition across Indian languages.","score":1},{"url":"https://www.semanticscholar.org/paper/5a6f6f44a2e05709d81245526786f8dc8f8ab263","title":"Relation Leakage in Elicited Natural Language Inference Datasets","venue":"","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Michael Stephen Saxon,Xinyi Wang,Wenda Xu,William Yang Wang","id":"5a6f6f44a2e05709d81245526786f8dc8f8ab263","summary":"This work analyzes the problem of elicited sentence relation leakage in 8 modern NLI datasets, using a combination of previously established and novel model-based techniques, to enable ameliorating this leakage in future NLI datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/2b6b38b66fcca218cb2f381e5d4711b5c99a784f","title":"Training Text-to-Text Transformers with Privacy Guarantees","venue":"Findings","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Natalia Ponomareva,Jasmijn Bastings,Sergei Vassilvitskii","id":"2b6b38b66fcca218cb2f381e5d4711b5c99a784f","summary":"By using recent advances in JAX and XLA, it is shown that by using recent advances in JAX and XLA the authors can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE).","score":1},{"url":"https://www.semanticscholar.org/paper/c31da40e092e808f20f782eb1ee9d4dec6351708","title":"Overview of EXIST 2022: sEXism Identification in Social neTworks","venue":"Proces. del Leng. Natural","year":2022,"referenceCount":36,"citationCount":63,"influentialCitationCount":3,"publicationDate":2022,"authors":"F. Rodríguez‐Sánchez,Jorge Carrillo-de-Albornoz,Laura Plaza,Julio Gonzalo,Paolo Rosso,Miriam Comet,Trinidad Donoso","id":"c31da40e092e808f20f782eb1ee9d4dec6351708","summary":"The organization, goals, and results of the sEXism Identification in Social neTworks (EXIST)2022 challenge, a shared task proposed for the second year at IberLEF, consists of two challenges: sexism identification and sexism categorization of tweets and gabs, both in Spanish and English.","score":1},{"url":"https://www.semanticscholar.org/paper/3fa2e17332bb2888318f504cf37026001b932900","title":"Comparison of Token- and Character-Level Approaches to Restoration of Spaces, Punctuation, and Capitalization in Various Languages","venue":"International Conference on Natural Language and Speech Processing","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Laurence Dyer,Anthony Hughes,Dhwani Shah,Burcu Can","id":"3fa2e17332bb2888318f504cf37026001b932900","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/710ceab1ff91b96e8596b8400ebf2912ee6e2836","title":"Machine Translation for Multilingual Intent Detection and Slots Filling","venue":"MMNLU","year":2022,"referenceCount":48,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans","id":"710ceab1ff91b96e8596b8400ebf2912ee6e2836","summary":"This work uses the inherent multilingual aspect of translation models for the task of multilingual intent classification and slot filling and reveals that they work equally well with general-purpose multilingual text-to-text models.","score":1},{"url":"https://www.semanticscholar.org/paper/038103632b24619818b159f5ca37b848744817db","title":"DENTRA: Denoising and Translation Pre-training for Multilingual Machine Translation","venue":"Conference on Machine Translation","year":2022,"referenceCount":29,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Samta Kamboj,Sunil Kumar Sahu,Neha Sengupta","id":"038103632b24619818b159f5ca37b848744817db","summary":"DENTRA, a novel pre-training strategy for a multilingual sequence-to-sequence transformer model, combines denoising and translation objectives to incorporate both monolingual and bitext corpora in 24 African, English, and French languages is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/4da5c8acd25a7dc7357d6070bb0a89bad187ebd8","title":"A Framework for Sexism Detection on Social Media via ByT5 and TabNet","venue":"IberLEF@SEPLN","year":2022,"referenceCount":16,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"A. Younus,M. A. Qureshi","id":"4da5c8acd25a7dc7357d6070bb0a89bad187ebd8","summary":"A combination of byte-level model ByT5 with tabular modeling via TabNet that has at its core an ability to take into account platform and language aspects of the challenging task of sexism detection is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation","venue":"Conference of the Association for Machine Translation in the Americas","year":2022,"referenceCount":39,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Salvador Carrión-Ponz,F. Casacuberta","id":"373588ff1fb9f7590db000a04de8d838b1516e5a","summary":"This work suggests that quasi-character-level models have practically the same generalization capabilities as character-based models but at lower computational costs and appear to help achieve greater consistency between domains than standard subword- level models, although the catastrophic forgetting problem is not mitigated.","score":1},{"url":"https://www.semanticscholar.org/paper/7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","title":"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome","venue":"","year":2022,"referenceCount":51,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Theodore Jiang,Li Fang,Kai Wang","id":"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","summary":"The introduction of MutFormer, a transformer-based model for the prediction of deleterious missense mutations that uses reference and mutated protein sequences from the human genome as the primary features, and which successfully considers sequence features that are not explored in previous studies.","score":1},{"url":"https://www.semanticscholar.org/paper/6102fe88a512290b80e83ed2fe17606b166e505a","title":"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin","venue":"LT4HALA","year":2022,"referenceCount":13,"citationCount":5,"influentialCitationCount":1,"publicationDate":2022,"authors":"Krzysztof Wróbel,Krzysztof Nowak","id":"6102fe88a512290b80e83ed2fe17606b166e505a","summary":"The paper features a thorough discussion of part-of-speech and lemmatization errors which shows how the system performance may be improved for Classical, Medieval and Neo-Latin texts.","score":1},{"url":"https://www.semanticscholar.org/paper/143659fcf93f33e9139f594cf8111c6bc85c04fa","title":"Overview of the EvaLatin 2022 Evaluation Campaign","venue":"LT4HALA","year":2022,"referenceCount":15,"citationCount":8,"influentialCitationCount":1,"publicationDate":2022,"authors":"R. Sprugnoli,M. Passarotti,F. M. Cecchini,Margherita Fantoli,Giovanni Moretti","id":"143659fcf93f33e9139f594cf8111c6bc85c04fa","summary":"This paper describes the organization and the results of the second edition of EvaLatin, the campaign for the evaluation of Natural Language Processing tools for Latin, and the three shared tasks proposed in EvaLatin 2022, i.","score":1},{"url":"https://www.semanticscholar.org/paper/2005311276a1a90dc17cf6e2ca7725fee2e76e1e","title":"BasqueGLUE: A Natural Language Understanding Benchmark for Basque","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":38,"citationCount":6,"influentialCitationCount":1,"publicationDate":2022,"authors":"Gorka Urbizu,Iñaki San Vicente,X. Saralegi,Rodrigo Agerri,Aitor Soroa Etxabe","id":"2005311276a1a90dc17cf6e2ca7725fee2e76e1e","summary":"BasqueGLUE is presented, the first NLU benchmark for Basque, a less-resourced language, which has been elaborated from previously existing datasets and following similar criteria to those used for the construction of GLUE and SuperGLUE.","score":1},{"url":"https://www.semanticscholar.org/paper/b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","title":"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Dibyanayan Bandyopadhyay,Arkadipta De,Baban Gain,Tanik Saikh,Asif Ekbal","id":"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","summary":"It is argued that the transfer learning-based loss objective is model agnostic and thus can be used with other deep learning- based architectures for cross-lingual NLI.","score":1},{"url":"https://www.semanticscholar.org/paper/e75037b638aca1dfc8a9b013bb7dcb8d19633986","title":"Examining Single Sentence Label Leakage in Natural Language Inference Datasets","venue":"","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Michael Stephen Saxon,Xinyi Wang,Wenda Xu,William Yang Wang","id":"e75037b638aca1dfc8a9b013bb7dcb8d19633986","summary":"This work examines how regular NLI models cheat on single sentence label leakage, and discusses how to ameliorate this.","score":1},{"url":"https://www.semanticscholar.org/paper/2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":56,"citationCount":24,"influentialCitationCount":3,"publicationDate":2022,"authors":"Valentin Hofmann,Hinrich Schütze,J. Pierrehumbert","id":"2f07f97563a73d9b691ec6144e4bba25a347ab87","summary":"FLOTA (Few Longest Token Approximation) leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.","score":1},{"url":"https://www.semanticscholar.org/paper/32290d428b50f64d1cfee6ea658dc6ba977b26e9","title":"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia","venue":"LTEDI","year":2022,"referenceCount":33,"citationCount":9,"influentialCitationCount":0,"publicationDate":2022,"authors":"I. S. Upadhyay,KV Aditya Srivatsa,Radhika Mamidi","id":"32290d428b50f64d1cfee6ea658dc6ba977b26e9","summary":"The approach to classify homophobia and transphobia in social media comments using an ensemble of transformer-based models to build a classifier that ranked 2nd for English, 8th for Tamil and 10th for Chennai-English.","score":1},{"url":"https://www.semanticscholar.org/paper/b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":44,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Colin Leong,Daniel Whitenack","id":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","summary":"Initial experiments using Swahili and Kinyarwanda data suggest the viability of the multi-modal approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch.","score":1},{"url":"https://www.semanticscholar.org/paper/f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms","venue":"arXiv.org","year":2022,"referenceCount":118,"citationCount":114,"influentialCitationCount":23,"publicationDate":2022,"authors":"Yi Tay,Mostafa Dehghani,Vinh Q. Tran,Xavier García,Dara Bahri,Tal Schuster,H. Zheng,N. Houlsby,Donald Metzler","id":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","summary":"This paper presents a uniﬁed framework for pre-training models that are universally effective across datasets and setups, and proposes Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together.","score":1},{"url":"https://www.semanticscholar.org/paper/b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA","venue":"","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Stanford CS224N,Peng Chen","id":"b639124771f9c62cd656a24e8e685a456918e0ff","summary":"A question answering model that encodes text inputs at character level to utilize subword structures and mitigate the out-of-vocabulary problem is proposed, and both S4 and character-level encoding improve the model performance on the question answering task.","score":1},{"url":"https://www.semanticscholar.org/paper/9758782feed4b4b9bf0ec18b802462e8023a7f83","title":"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models","venue":"Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing","year":2022,"referenceCount":35,"citationCount":10,"influentialCitationCount":0,"publicationDate":2022,"authors":"Odunayo Jude Ogundepo,Akintunde Oladipo,Mofetoluwa Adeyemi,Kelechi Ogueji and Jimmy Lin","id":"9758782feed4b4b9bf0ec18b802462e8023a7f83","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","title":"On the Influence of Tokenizers in NLP Pipelines","venue":"","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","summary":"This thesis examines the influence of tokenization in NLP pipelines, by analyzing, reproducing, and quantifying claims from the token-free NLP literature, using the example of NER, and concludes that token- free models, like ByT5, offer significant advantages over their tokenizer-based alternatives.","score":1},{"url":"https://www.semanticscholar.org/paper/64668e46d52302851d62afbc9779ce8c4d46c5eb","title":"Dialect-to-Standard Normalization: A Large-Scale Multilingual Evaluation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":46,"citationCount":2,"influentialCitationCount":0,"publicationDate":2023,"authors":"Olli Kuparinen,Aleksandra Miletic,Yves Scherrer","id":"64668e46d52302851d62afbc9779ce8c4d46c5eb","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/cbeb7f5050790d5abc8c617f6bfd70706c52ae0e","title":"Automatic Translation of Span-Prediction Datasets","venue":"International Joint Conference on Natural Language Processing","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Ofri Masad,Kfir Bar,Amir Cohen","id":"cbeb7f5050790d5abc8c617f6bfd70706c52ae0e","summary":"This paper proposes a new approach for translating NLP datasets that relies on a two-phase pipeline and online translation services and focuses on solving the alignment problem that affects span prediction tasks and utilizes automatically labeled data for training an alignment model.","score":1},{"url":"https://www.semanticscholar.org/paper/7ef91d8896c406865e91d90f60a8592a03ed6ded","title":"TRIP: Accelerating Document-level Multilingual Pre-training via Triangular Document-level Pre-training on Parallel Data Triplets","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Hongyuan Lu,Haoyang Huang,Shuming Ma,Dongdong Zhang,Wai Lam,Zhaochuan Gao,Anthony Aue,Arul Menezes,Furu Wei","id":"7ef91d8896c406865e91d90f60a8592a03ed6ded","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/8a930572177545e7394ba5cd03e9342142da564e","title":"Better Quality Pre-training Data and T5 Models for African Languages","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Akintunde Oladipo,Mofetoluwa Adeyemi,Orevaoghene Ahia,A. Owodunni,Odunayo Ogundepo,David Ifeoluwa Adelani,Jimmy Lin","id":"8a930572177545e7394ba5cd03e9342142da564e","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/104974b36654acbc613226b9f70c4ad503655aa1","title":"The Linearity of the Effect of Surprisal on Reading Times across Languages","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":50,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Weijie Xu,Jason Chon,Tianran Liu,Richard Futrell","id":"104974b36654acbc613226b9f70c4ad503655aa1","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/ce093816af5d8c1a7aab74808245048b7f3669a5","title":"The Helsinki-NLP Submissions at NADI 2023 Shared Task: Walking the Baseline","venue":"ARABICNLP","year":2023,"referenceCount":26,"citationCount":2,"influentialCitationCount":0,"publicationDate":2023,"authors":"Yves Scherrer,Aleksandra Miletic,Olli Kuparinen","id":"ce093816af5d8c1a7aab74808245048b7f3669a5","summary":"The Helsinki-NLP team participated in the NADI 2023 shared tasks on Arabic dialect translation and used statistical (SMT) and neural machine translation (NMT) methods and explored character- and subword-based data preprocessing.","score":1},{"url":"https://www.semanticscholar.org/paper/c6410d9791f390aa43ed26bf5567fdde6fc89f25","title":"Type Enhanced BERT for Correcting NER Errors","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Kuai Li,Chen Chen,Tao Yang,Tianming Du,Peijie Yu,Dong Du,Feng Zhang","id":"c6410d9791f390aa43ed26bf5567fdde6fc89f25","summary":"A gazetteer containing named entities and corresponding possible entity types is constructed and a type-enhanced BERT (TyBERT), a method that integrates the named entity’s type information into BERT by an adapter layer is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/ee26bb846f04240fe6e82170a4b15f3b8bd3a09e","title":"Designing Human-Centric Foundation Models","venue":"HCAI4U@CHItaly","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Narendra Patwardhan,Shreya Shetye,Lidia Marassi,M. Zuccarini,T. Maiti,Tarry Singh","id":"ee26bb846f04240fe6e82170a4b15f3b8bd3a09e","summary":"The potential of sustainability and programmability principles in architectural design to create a more context-aware and user-focused AI ecosystem is investigated.","score":1},{"url":"https://www.semanticscholar.org/paper/30869d285642a8b981702d2a54be0ac54f01aa01","title":"Composer's Assistant: Interactive Transformers for Multi-Track MIDI Infilling","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Martin E. Malandro","id":"30869d285642a8b981702d2a54be0ac54f01aa01","summary":"Two T5-like models are trained to solve the task of multi-track MIDI inﬁlling when arbitrary (track, measure) pairs of information have been deleted from a contiguous slice of measures from a MIDI sequence, and their results have implications for the training of neural networks in other small-vocabulary domains.","score":1},{"url":"https://www.semanticscholar.org/paper/2b4369d50ac7310b9908a2baef89a63c68cbd893","title":"Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models","venue":"Recent Advances in Natural Language Processing","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Shun Kiyono,Sho Takase,Shengzhe Li,Toshinori Sato","id":"2b4369d50ac7310b9908a2baef89a63c68cbd893","summary":"The principle of the method is to apply the subword regularization technique to generate a mixture of subword- and character-level segmentation, which can halve the computational cost of pretraining.","score":1},{"url":"https://www.semanticscholar.org/paper/29e593736c95fd043b7cbb0211d2f7b5cb70e5bf","title":"LLI-UAM Team at FinancES 2023: Noise, Data Augmentation and Hallucinations","venue":"IberLEF@SEPLN","year":2023,"referenceCount":13,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Jordi Porta Zamorano,Yanco Torterolo,Antonio Moreno-Sandoval","id":"29e593736c95fd043b7cbb0211d2f7b5cb70e5bf","summary":"This paper describes the T5-based system developed for FinancES 2023 Shared Task by the Laboratorio de Lingüística Informática at UAM and identifies the best model for each task.","score":1},{"url":"https://www.semanticscholar.org/paper/e4a1e9bb360f29aceb56079f52484c4a4de1298d","title":"Intégration du raisonnement numérique dans les modèles de langue : État de l’art et direction de recherche","venue":"JEPTALNRECITAL","year":2023,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Sarah Abchiche,Lynda Said Lhadj,V. Guigue,Laure Soulier","id":"e4a1e9bb360f29aceb56079f52484c4a4de1298d","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/f83740ef449e705187a0f7d6f76b819c99340bd1","title":"Exploring the Limits of Small Language Models","venue":"","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Nicholas Lee,Kurt Keutzer Gopala,Krishna Anumanchipalli","id":"f83740ef449e705187a0f7d6f76b819c99340bd1","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/17d2c990dd6d25f433b02ce611ce0a57db038dc5","title":"Jetsons at the FinNLP-2023: Using Synthetic Data and Transfer Learning for Multilingual ESG Issue Classification","venue":"FINNLP","year":2023,"referenceCount":17,"citationCount":2,"influentialCitationCount":0,"publicationDate":2023,"authors":"Parker Glenn,Alolika Gon,Nikhil Kohli,Sihan Zha,Parag Dakle,Preethi Raghavan","id":"17d2c990dd6d25f433b02ce611ce0a57db038dc5","summary":"The various approaches by the Jetsons team for the Multilin-gual ESG Issue Identification Task (ML-ESG) to classify articles into ESG issues they are related to are described.","score":1},{"url":"https://www.semanticscholar.org/paper/8e410a836174847e3fc4e5993f625e89e34dd3d1","title":"RST Discourse Parsing as Text-to-Text Generation","venue":"IEEE/ACM Transactions on Audio Speech and Language Processing","year":2023,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Xinyu Hu,Xiaojun Wan","id":"8e410a836174847e3fc4e5993f625e89e34dd3d1","summary":"This article introduces an end- to-end method for sentence-level RST discourse parsing via transforming it into a text-to-text generation task, which can also be simply applied to document-level parsing.","score":1},{"url":"https://www.semanticscholar.org/paper/4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations","venue":"CAWL","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Manex Agirrezabal,Sidsel Boldsen,Nora Hollenstein","id":"4044c062683533d92f598715afe2508be29c739c","summary":"The multilingual contextual CANINE model is probed, including Faroese as an additional zero-shot instance, and it is observed that some phonetic information is indeed encoded in the character representations, as consonants and vowels can be well distinguished using a linear classifier.","score":1},{"url":"https://www.semanticscholar.org/paper/659be1ff350634f50cc066d258ee6a45e697e552","title":"SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing","venue":"Special Interest Group on Computational Morphology and Phonology Workshop","year":2023,"referenceCount":18,"citationCount":2,"influentialCitationCount":0,"publicationDate":2023,"authors":"Taiqi He,Lindia Tjuatja,Nathaniel R. Robinson,Shinji Watanabe,David R. Mortensen,Graham Neubig,L. Levin","id":"659be1ff350634f50cc066d258ee6a45e697e552","summary":"In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.","score":1},{"url":"https://www.semanticscholar.org/paper/88108f061379045c299d62f487694cb4e6d6d4ff","title":"Findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing","venue":"Special Interest Group on Computational Morphology and Phonology Workshop","year":2023,"referenceCount":69,"citationCount":7,"influentialCitationCount":1,"publicationDate":2023,"authors":"Michael Ginn,Sarah Moeller,Alexis Palmer,Anna Stacey,Garrett Nicolai,Mans Hulden,Miikka Silfverberg","id":"88108f061379045c299d62f487694cb4e6d6d4ff","summary":"This first iteration of the SIGMORPHON 2023 Shared Task on Interlinear Glossing explores glossing of a set of six typologically diverse languages: Arapaho, Gitksan, Lezgi, Natügu, Tsez and Uspanteko.","score":1},{"url":"https://www.semanticscholar.org/paper/ffa4ac4a51148208cadb4084dddab954e5f57400","title":"Resolving Elliptical Compounds in German Medical Text","venue":"Workshop on Biomedical Natural Language Processing","year":2023,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Niklas Kammer,Florian Borchert,Silvia Winkler,Gerard de Melo,M. Schapranow","id":"ffa4ac4a51148208cadb4084dddab954e5f57400","summary":"A generative encoder-decoder Transformer model is proposed, allowing for a simple end-to-end resolution of ECCNPs from raw input strings with very high accuracy, and is compared to an elaborate rule-based baseline, which the generative model outperforms by a large margin.","score":1},{"url":"https://www.semanticscholar.org/paper/c3904ef47bec4fc2bfd3d390370681c33542d62d","title":"Fast Whitespace Correction with Encoder-Only Transformers","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Hannah Bast,Matthias Hertel,S. Walter","id":"c3904ef47bec4fc2bfd3d390370681c33542d62d","summary":"This work provides an easy-to-use tool that is over 900 times faster than the previous best tool, with the same high quality, and compares two Transformer-based models, a character-level encoder-decoder model and a byte-levelencoder-only model.","score":1},{"url":"https://www.semanticscholar.org/paper/e5adb9bf3f5ed9c253f38949b22e86775dca443a","title":"Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Jingyu Zhang,Alexandra DeLucia,Chenyu Zhang,Mark Dredze","id":"e5adb9bf3f5ed9c253f38949b22e86775dca443a","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/035315281c72763a3e0956775732e64f5f193d82","title":"Natural Language Generation with Pixels","venue":"","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Gautam Mittal,Rajan Vivek,•. Mentor,Yuan Gao","id":"035315281c72763a3e0956775732e64f5f193d82","summary":"A conditional diffusion-based decoder for modeling rendered natural language that is capable of generating coherent, plausible natural language rendered as images and compared with strong transformer-based autoregressive baselines in both the unconditional and sequence-to-sequence setting.","score":1},{"url":"https://www.semanticscholar.org/paper/1d578d2bdb5c920b224cfca73868566731eaeebd","title":"Towards Analysis of Biblical Entities and Names using Deep Learning","venue":"International Journal of Advanced Computer Science and Applications","year":2023,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Mikolaj Martinjak,D. Lauc,Ines Skelac","id":"1d578d2bdb5c920b224cfca73868566731eaeebd","summary":"The findings of this study demonstrate that deep learning could help uncover interesting connections between individuals who may have initially been considered less important and highlighted the critical role of onomastic sciences and the philosophy of language in analyzing the richness and importance of human and other proper names in biblical texts.","score":1},{"url":"https://www.semanticscholar.org/paper/13b8060acc3db1fc555f6e55368f6d02899a1698","title":"FairPrism: Evaluating Fairness-Related Harms in Text Generation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":48,"citationCount":9,"influentialCitationCount":1,"publicationDate":2023,"authors":"Eve Fleisig,Aubrie Amstutz,Chad Atalla,Su Lin Blodgett,Hal Daumé,Alexandra Olteanu,Emily Sheng,Dan Vann,Hanna M. Wallach","id":"13b8060acc3db1fc555f6e55368f6d02899a1698","summary":"FairPrism is introduced, a dataset of 5,000 examples of AI-generated English text with detailed human annotations covering a diverse set of harms relating to gender and sexuality that can be used to diagnose the types of fairness-related harms that AI text generation systems cause.","score":1},{"url":"https://www.semanticscholar.org/paper/82e1313f28afde442930b94bc6ed582d17e8d4b3","title":"Generating Errors: OCR Post-Processing for Icelandic","venue":"Nordic Conference of Computational Linguistics","year":2023,"referenceCount":20,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Atli Jasonarson,Steinþór Steingrímsson,E. Sigurðsson,Árni Magnússon,F. Ingimundarson","id":"82e1313f28afde442930b94bc6ed582d17e8d4b3","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/380605105531d27474190451183bf6ad6126cac8","title":"GPoeT: a Language Model Trained for Rhyme Generation on Synthetic Data","venue":"LATECHCLFL","year":2023,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Andrei Popescu-Belis,Àlex R. Atrio,Bastien Bernath,Etienne Boisson,Teo Ferrari,Xavier Theimer-lienhard,Giorgos Vernikos","id":"380605105531d27474190451183bf6ad6126cac8","summary":"A novel solution for learning to rhyme is proposed, based on synthetic data generated with a rule-based rhyming algorithm and an evaluation metric that uses a phonetic dictionary and the definitions of perfect and assonant rhymes.","score":1},{"url":"https://www.semanticscholar.org/paper/5088cd22bbd7e1b798df39eb7f3c4ae305ab7625","title":"Findings from the Bambara - French Machine Translation Competition (BFMT 2023)","venue":"LORESMT","year":2023,"referenceCount":55,"citationCount":2,"influentialCitationCount":0,"publicationDate":2023,"authors":"Ninoh Agostinho Da Silva,Tunde Ajayi,A. Antonov,Panga Azazia Kamate,Moussa L. Coulibaly,Mason Del Rio,Yacouba Diarra,Sebastian Diarra,Chris C. Emezue,Joel Hamilcaro","id":"5088cd22bbd7e1b798df39eb7f3c4ae305ab7625","summary":"This paper details each team’s different approaches and motivation for ongoing work in Bambara and the broader low-resource machine translation domain.","score":1},{"url":"https://www.semanticscholar.org/paper/d53d4ff9f3bda51534184344845a78f8c02badd9","title":"DiatopIt: A Corpus of Social Media Posts for the Study of Diatopic Language Variation in Italy","venue":"Workshop on NLP for Similar Languages, Varieties and Dialects","year":2023,"referenceCount":33,"citationCount":13,"influentialCitationCount":2,"publicationDate":2023,"authors":"Alan Ramponi,Camilla Casula","id":"d53d4ff9f3bda51534184344845a78f8c02badd9","summary":"The first corpus specifically focused on diatopic language variation in Italy for language varieties other than Standard Italian is introduced, and the representativeness of DiatopIt is assessed, and it is shown that the density of non-Standard Italian content across areas correlates with actual language use.","score":1},{"url":"https://www.semanticscholar.org/paper/cfe7cb66390ea0b433383d498e6eff555a198c54","title":"Murreviikko - A Dialectologically Annotated and Normalized Dataset of Finnish Tweets","venue":"Workshop on NLP for Similar Languages, Varieties and Dialects","year":2023,"referenceCount":31,"citationCount":3,"influentialCitationCount":0,"publicationDate":2023,"authors":"Olli Kuparinen","id":"cfe7cb66390ea0b433383d498e6eff555a198c54","summary":"It is found that there are significant differences in normalization difficulty between the dialects, and that a character-level statistical machine translation model performs best on the Murreviikko tweet dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/2efefcb2d48eecf308f856cab48f23621e4a88f1","title":"Scaling Neural ITN for Numbers and Temporal Expressions in Tamil: Findings for an Agglutinative Low-resource Language","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Bhavuk Singhal,Sindhuja Gopalan,Amrith Krishna,Malolan Chetlur","id":"2efefcb2d48eecf308f856cab48f23621e4a88f1","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/8cc3c64e1aee320609b6b964a9a6f6f50177e20f","title":"Publish or Hold? Automatic Comment Moderation in Luxembourgish News Articles","venue":"Recent Advances in Natural Language Processing","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Tharindu Ranasinghe,A. Plum,Christoph Purschke,Marcos Zampieri","id":"8cc3c64e1aee320609b6b964a9a6f6f50177e20f","summary":"The first large-scale qualitative analysis of more than one million Luxembourgish comments posted over the course of 14 years is performed, evaluating the performance of state-of-the-art transformer models in Luxembourgish news article comment moderation and how the language of LuxembourgishNews article comments has changed over time.","score":1},{"url":"https://www.semanticscholar.org/paper/117a0c2c23a5a558cf3b39468d917a750bca720c","title":"Are Character-level Translations Worth the Wait? Comparing Character-and Subword-level Models for Machine Translation","venue":"","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Lukas Edman,Gabriele Sarti,Antonio Toral,Gertjan van Noord,Arianna Bisazza","id":"117a0c2c23a5a558cf3b39468d917a750bca720c","summary":"This work performs an extensive comparison across multiple languages and experimental conditions of state-of-the-art character-and subword-level pre-trained models (ByT5 and mT5, respectively) on NMT, showing the effectiveness of character-level modeling in translation, particularly in cases where training data is limited.","score":1},{"url":"https://www.semanticscholar.org/paper/ed7b51e4a5c4835218f6697b280afb2849211939","title":"Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation","venue":"arXiv.org","year":2023,"referenceCount":32,"citationCount":2,"influentialCitationCount":0,"publicationDate":2023,"authors":"Lukas Edman,Antonio Toral,Gertjan van Noord","id":"ed7b51e4a5c4835218f6697b280afb2849211939","summary":"This work performs an extensive comparison across multiple languages and experimental conditions of state-of-the-art character-and subword-level pre-trained models on NMT, and shows that the former not only are effective in translation, but frequently outperform subword models, particularly in cases where training data is limited.","score":1},{"url":"https://www.semanticscholar.org/paper/7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview","venue":"Computational Linguistics","year":2021,"referenceCount":68,"citationCount":85,"influentialCitationCount":5,"publicationDate":"22/02/2021","authors":"Philipp Dufter,Martin Schmitt,Hinrich Schütze","id":"7072db6eddb85ecd2c117365d91bd694760f726e","summary":"An overview and theoretical comparison of existing methods to incorporate position information into Transformer models is provided and what characteristics of an application should be taken into account when selecting a position encoding is indicated.","score":1},{"url":"https://www.semanticscholar.org/paper/d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention","venue":"International Conference on Machine Learning","year":2021,"referenceCount":56,"citationCount":18,"influentialCitationCount":2,"publicationDate":"09/05/2021","authors":"Noam Wies,Yoav Levine,Daniel Jannai,A. Shashua","id":"d13a0c8d49cb268d8d245925baee0316c1fe1875","summary":"This work theoretically predicts the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity, and empirically demonstrates the existence and implications on the depth-to-width interplay of Transformer architectures.","score":1},{"url":"https://www.semanticscholar.org/paper/5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad","title":"Sub-Character Tokenization for Chinese Pretrained Language Models","venue":"Transactions of the Association for Computational Linguistics","year":2021,"referenceCount":61,"citationCount":3,"influentialCitationCount":0,"publicationDate":"01/06/2021","authors":"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Yasheng Wang,Qun Liu,Maosong Sun","id":"5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad","summary":"Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency, and 2) Pronunciation-based SubChartokenization can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to homophone typos.","score":1},{"url":"https://www.semanticscholar.org/paper/4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification","venue":"Neural Processing Letters","year":2021,"referenceCount":65,"citationCount":18,"influentialCitationCount":0,"publicationDate":"14/06/2021","authors":"Zaid Alyafeai,Maged S. Al-Shaibani,Mustafa Ghaleb,Irfan Ahmad","id":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","summary":"This paper introduces three new tokenization algorithms for Arabic and compares them to other three popular tokenizers using unsupervised evaluations, and compares all the six tokenizers by evaluating them on three supervised classification tasks: sentiment analysis, news classification and poem-meter classification, using six publicly available datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization","venue":"International Conference on Learning Representations","year":2021,"referenceCount":71,"citationCount":109,"influentialCitationCount":17,"publicationDate":"23/06/2021","authors":"Yi Tay,Vinh Q. Tran,Sebastian Ruder,Jai Gupta,Hyung Won Chung,Dara Bahri,Zhen Qin,Simon Baumgartner,Cong Yu,Donald Metzler","id":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","summary":"A soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion is introduced that paves the way for highly performant token-free models that are trained completely end-to-end.","score":1},{"url":"https://www.semanticscholar.org/paper/31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning","venue":"Journal of Big Data","year":2021,"referenceCount":144,"citationCount":227,"influentialCitationCount":7,"publicationDate":"29/06/2021","authors":"Connor Shorten,T. Khoshgoftaar,B. Furht","id":"31852f9fc732c0868af12d631c72693702d80521","summary":"The major motifs of Data Augmentation are summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form.","score":1},{"url":"https://www.semanticscholar.org/paper/06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer","venue":"arXiv.org","year":2021,"referenceCount":34,"citationCount":49,"influentialCitationCount":4,"publicationDate":"30/06/2021","authors":"Iulia Turc,Kenton Lee,Jacob Eisenstein,Ming-Wei Chang,Kristina Toutanova","id":"06431546c21d7c2528aaa170c2e1078e0a82d12e","summary":"English is compared against other transfer languages for fine-tuning, and other high-resource languages such as German and Russian often transfer more effectively, especially when the set of target languages is diverse or unknown a priori.","score":1},{"url":"https://www.semanticscholar.org/paper/28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","venue":"ACM Computing Surveys","year":2021,"referenceCount":222,"citationCount":2125,"influentialCitationCount":188,"publicationDate":"28/07/2021","authors":"Pengfei Liu,Weizhe Yuan,Jinlan Fu,Zhengbao Jiang,Hiroaki Hayashi,Graham Neubig","id":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","summary":"The basics of this promising paradigm in natural language processing are introduced, a unified set of mathematical notations that can cover a wide variety of existing work are described, and existing work is organized along several dimensions.","score":1},{"url":"https://www.semanticscholar.org/paper/91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU","venue":"Findings","year":2021,"referenceCount":74,"citationCount":9,"influentialCitationCount":0,"publicationDate":"29/07/2021","authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","summary":"It is empirically shown that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.","score":1},{"url":"https://www.semanticscholar.org/paper/9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs","venue":"International Conference on Learning Representations","year":2021,"referenceCount":104,"citationCount":376,"influentialCitationCount":46,"publicationDate":"30/07/2021","authors":"Andrew Jaegle,Sebastian Borgeaud,Jean-Baptiste Alayrac,Carl Doersch,Catalin Ionescu,David Ding,Skanda Koppula,Andrew Brock,Evan Shelhamer,Olivier J. H'enaff,M. Botvinick,Andrew Zisserman,O. Vinyals,João Carreira","id":"9933a5af7895354087baf6c96b64dc8a8973eaed","summary":"This work proposes Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs and augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering.","score":1},{"url":"https://www.semanticscholar.org/paper/9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information","venue":"arXiv.org","year":2021,"referenceCount":35,"citationCount":6,"influentialCitationCount":0,"publicationDate":"01/08/2021","authors":"Yuval Pinter,A. Stent,Mark Dredze,Jacob Eisenstein","id":"9c2e4e5ee224c20a45c37244924138b50f3fe603","summary":"It is shown that incorporating XRayEmb's learned vectors into sequences of pre- trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on non-standard English text.","score":1},{"url":"https://www.semanticscholar.org/paper/c41819413725668fbf8e3ca1a0bcaf7fb691e582","title":"How Optimal is Greedy Decoding for Extractive Question Answering?","venue":"arXiv.org","year":2021,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":"12/08/2021","authors":"Or Castel,Ori Ram,Avia Efrat,Omer Levy","id":"c41819413725668fbf8e3ca1a0bcaf7fb691e582","summary":"This work presents exact-extract, a decoding algorithm that efficiently finds the most probable answer span in the context, and shows that self-supervised training can bias the model towards extractive behavior, increasing performance in the zero-shot setting without resorting to annotated examples.","score":1},{"url":"https://www.semanticscholar.org/paper/89a4f4d1f8c93da85d829a1acfe8cafea2b50c00","title":"Transfer Learning for Multi-lingual Tasks - a Survey","venue":"arXiv.org","year":2021,"referenceCount":106,"citationCount":3,"influentialCitationCount":0,"publicationDate":"28/08/2021","authors":"Amir Reza Jafari,Behnam Heidary,R. Farahbakhsh,Mostafa Salehi,M. Jalili","id":"89a4f4d1f8c93da85d829a1acfe8cafea2b50c00","summary":"This survey provides a comprehensive overview of the existing literature with a focus on transfer learning techniques in multilingual tasks and identifies potential opportunities for further research in this domain.","score":1},{"url":"https://www.semanticscholar.org/paper/972808b92159a782f5ca1c1ab3a8ed3867acfb2d","title":"mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset","venue":"","year":2021,"referenceCount":52,"citationCount":57,"influentialCitationCount":9,"publicationDate":"31/08/2021","authors":"L. Bonifacio,Israel Campiotti,R. Lotufo,Rodrigo Nogueira","id":"972808b92159a782f5ca1c1ab3a8ed3867acfb2d","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation","venue":"arXiv.org","year":2021,"referenceCount":93,"citationCount":5,"influentialCitationCount":0,"publicationDate":"10/09/2021","authors":"Yuval Pinter","id":"d87647784c12517d31964cc508d5b8423cc24f50","summary":"A survey of the distributional, compositional, and relational approaches to addressing the problem of representing the atomic elements of language in modern neural learning systems is presented, with special emphasis on the word level and the out-of-vocabulary phenomenon.","score":1},{"url":"https://www.semanticscholar.org/paper/39262814fa3e47905a2e5facf13465a1f70706b9","title":"Single-Read Reconstruction for DNA Data Storage Using Transformers","venue":"arXiv.org","year":2021,"referenceCount":37,"citationCount":3,"influentialCitationCount":0,"publicationDate":"12/09/2021","authors":"Yotam Nahum,Eyar Ben-Tolila,Leon Anavy","id":"39262814fa3e47905a2e5facf13465a1f70706b9","summary":"This work proposes a novel approach for single-read reconstruction using an encoder-decoder Transformer architecture for DNA based data storage and achieves lower error rates when reconstructing the original data from a single read of each DNA strand compared to state-of-the-art algorithms using 2-3 copies.","score":1},{"url":"https://www.semanticscholar.org/paper/6e0f8a47072de7e549182a5d0fc07c6d0a207325","title":"Predicting Ethnicity from Names with rethnicity: Methodology and Application","venue":"arXiv.org","year":2021,"referenceCount":28,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/09/2021","authors":"Fangzhou Xie","id":"6e0f8a47072de7e549182a5d0fc07c6d0a207325","summary":"A new R package, \\texttt{rethnicity} is provided for predicting ethnicity based on names, and the availability, accuracy, and performance of the package were compared with other solutions.","score":1},{"url":"https://www.semanticscholar.org/paper/a70fc86508bd0133d5d984a4e777abef1934d76c","title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese","venue":"Interspeech","year":2021,"referenceCount":45,"citationCount":26,"influentialCitationCount":6,"publicationDate":"20/09/2021","authors":"Nguyen Luong Tran,Duong Minh Le,Dat Quoc Nguyen","id":"a70fc86508bd0133d5d984a4e777abef1934d76c","summary":"BARTpho is presented, which are the first public large-scale monolingual sequence-to-sequence models pre-trained for Vietnamese, and it is found that it is more effective than mBART on these two tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/c107835a05ca6fda6e73b64e2ed9884de4fcec0f","title":"A proposed conceptual framework for a representational approach to information retrieval","venue":"SIGIR Forum","year":2021,"referenceCount":87,"citationCount":37,"influentialCitationCount":2,"publicationDate":"04/10/2021","authors":"Jimmy J. Lin","id":"c107835a05ca6fda6e73b64e2ed9884de4fcec0f","summary":"A representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model that establishes connections to sentence similarity tasks in natural language processing and information access \"technologies\" prior to the dawn of computing.","score":1},{"url":"https://www.semanticscholar.org/paper/e35357ac461a669fe7e4b877ee1fad0dfda26303","title":"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":79,"citationCount":19,"influentialCitationCount":1,"publicationDate":"14/10/2021","authors":"Kalpesh Krishna,Deepak Nathani,Xavier García,Bidisha Samanta,P. Talukdar","id":"e35357ac461a669fe7e4b877ee1fad0dfda26303","summary":"This work pushes the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases, and achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages.","score":1},{"url":"https://www.semanticscholar.org/paper/f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?","venue":"Findings","year":2021,"referenceCount":75,"citationCount":20,"influentialCitationCount":1,"publicationDate":"15/10/2021","authors":"Jindřich Libovický,Helmut Schmid,Alexander M. Fraser","id":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","summary":"It is shown that even with recent modeling innovations in character-level natural language processing, character- level MT systems still struggle to match their subword-based counterparts, and show neither better domain robustness, nor better morphological generalization.","score":1},{"url":"https://www.semanticscholar.org/paper/11ccb9b509d84e845901ee097e7d0a6419fdc182","title":"EncT5: A Framework for Fine-tuning T5 as Non-autoregressive Models","venue":"","year":2021,"referenceCount":22,"citationCount":5,"influentialCitationCount":0,"publicationDate":"16/10/2021","authors":"Frederick Liu,T. Huang,Shihang Lyu,Siamak Shakeri,Hongkun Yu,Jing Li","id":"11ccb9b509d84e845901ee097e7d0a6419fdc182","summary":"This work proposes a framework for fine-tuning pre-trained encoder-decoder models for tasks such as classification, multi-label classification, and structured prediction, and shows that EncT5 has advantages over T5 such as efficiency and usability out performs BERT when evaluated on publicly available pre- trained checkpoints.","score":1},{"url":"https://www.semanticscholar.org/paper/66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer","venue":"International Conference on Learning Representations","year":2021,"referenceCount":88,"citationCount":73,"influentialCitationCount":5,"publicationDate":"25/10/2021","authors":"Mostafa Dehghani,Anurag Arnab,Lucas Beyer,Ashish Vaswani,Yi Tay","id":"66d735987a31d666a6459566ae026c40ab9a1c3a","summary":"It is demonstrated how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models, and suggestions to improve reporting of efficiency metrics are presented.","score":1},{"url":"https://www.semanticscholar.org/paper/231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models","venue":"NAACL-HLT","year":2021,"referenceCount":39,"citationCount":32,"influentialCitationCount":3,"publicationDate":"26/10/2021","authors":"Piotr Nawrot,Szymon Tworkowski,Michał Tyrolski,Lukasz Kaiser,Yuhuai Wu,Christian Szegedy,H. Michalewski","id":"231e768f0cd280faa0f725bb353262cb4fed08d1","summary":"Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.","score":1},{"url":"https://www.semanticscholar.org/paper/f3bed81c50e293c03f6b650cdb9351d14e9be347","title":"Deciphering “the language of nature”: A transformer-based language model for deleterious mutations in proteins","venue":"Innovation (Cambridge (Mass.))","year":2021,"referenceCount":63,"citationCount":2,"influentialCitationCount":0,"publicationDate":"27/10/2021","authors":"Theodore Jiang,Li Fang,Kai Wang","id":"f3bed81c50e293c03f6b650cdb9351d14e9be347","summary":"This study introduces MutFormer, a transformer-based model for the prediction of deleterious missense mutations, which uses reference and mutated protein sequences from the human genome as the primary features and shows similar or improved performance over a variety of existing tools, including those that used conventional machine-learning approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/6e2682eb2fbec93b329028b23764c1164e232c41","title":"ÚFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5","venue":"WNUT","year":2021,"referenceCount":40,"citationCount":11,"influentialCitationCount":2,"publicationDate":"28/10/2021","authors":"David Samuel,Milan Straka","id":"6e2682eb2fbec93b329028b23764c1164e232c41","summary":"This paper presents the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021, which evaluates lexical-normalization systems on 12 social media datasets in 11 languages with the best performance by a wide margin.","score":1},{"url":"https://www.semanticscholar.org/paper/9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","title":"Large Dual Encoders Are Generalizable Retrievers","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":41,"citationCount":194,"influentialCitationCount":41,"publicationDate":"15/12/2021","authors":"Jianmo Ni,Chen Qu,Jing Lu,Zhuyun Dai,Gustavo Hernández Abrego,Ji Ma,Vincent Zhao,Yi Luan,Keith B. Hall,Ming-Wei Chang,Yinfei Yang","id":"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","summary":"Experimental results show that the dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly and the ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.","score":1},{"url":"https://www.semanticscholar.org/paper/0ae82f96cb8c1d2da5d8284765dd76a139ba6ff6","title":"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":57,"citationCount":4,"influentialCitationCount":0,"publicationDate":"16/12/2021","authors":"Michael Stephen Saxon,Xinyi Wang,Wenda Xu,William Yang Wang","id":"0ae82f96cb8c1d2da5d8284765dd76a139ba6ff6","summary":"It is demonstrated that despite efforts to reduce this leakage, it persists in modern datasets that have been introduced since its 2018 discovery, and a novel model-driven technique is introduced, the progressive evaluation of cluster outliers (PECO), which enables both the objective measurement of leakage, and the automated detection of subpopulations in the data which maximally exhibit it.","score":1},{"url":"https://www.semanticscholar.org/paper/d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP","venue":"arXiv.org","year":2021,"referenceCount":201,"citationCount":74,"influentialCitationCount":4,"publicationDate":"20/12/2021","authors":"Sabrina J. Mielke,Zaid Alyafeai,Elizabeth Salesky,Colin Raffel,Manan Dey,Matthias Gallé,Arun Raja,Chenglei Si,Wilson Y. Lee,Benoît Sagot,Samson Tan","id":"d617f51833860dc50d202af7f80be71304b2e994","summary":"It is concluded that there is and likely will never be a silver bullet singular solution for all applications and that thinking seriously about tokenization remains important for many applications.","score":1},{"url":"https://www.semanticscholar.org/paper/e53f455b3300d95738dac117419c88fbde58ac4e","title":"Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics","venue":"Database J. Biol. Databases Curation","year":2022,"referenceCount":114,"citationCount":5,"influentialCitationCount":1,"publicationDate":"01/01/2022","authors":"Tiago Almeida,Rui Antunes,João F Silva,João Rafael Almeida,Sérgio Matos","id":"e53f455b3300d95738dac117419c88fbde58ac4e","summary":"This manuscript describes a three-stage pipeline that individually performs chemical mention detection, entity normalization and indexing in PubMed full-text articles and proposes rules for identifying the more relevant MeSH codes for each article.","score":1},{"url":"https://www.semanticscholar.org/paper/7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models","venue":"International Conference on Agents and Artificial Intelligence","year":2022,"referenceCount":55,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/01/2022","authors":"Konstantin Todorov,Giovanni Colavizza","id":"7e3081b0d698f8abf16dee626d782f3339482fe7","summary":"It is found that OCR noise poses a significant obstacle to language modelling, with language models increasingly diverging from their noiseless targets as OCR quality lowers, and simpler models including PPMI and Word2Vec consistently outperform transformer-based models in this respect.","score":1},{"url":"https://www.semanticscholar.org/paper/f357b35e068bbfbb8468b48198ab63241713629d","title":"Correcting diacritics and typos with ByT5 transformer model","venue":"Applied Sciences","year":2022,"referenceCount":121,"citationCount":12,"influentialCitationCount":1,"publicationDate":"31/01/2022","authors":"Lukas Stankevicius,M. Lukoševičius,J. Kapočiūtė-Dzikienė,Monika Briediene,Tomas Krilavičius","id":"f357b35e068bbfbb8468b48198ab63241713629d","summary":"This work tackles diacritics restoration and typos correction at once by employing the newly-developed universal ByT5 byte-level seq2seq transformer model that requires no language-specific model structures and strongly outperforms classical spell-checking or dictionary-based approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language","venue":"International Conference on Machine Learning","year":2022,"referenceCount":86,"citationCount":523,"influentialCitationCount":94,"publicationDate":"07/02/2022","authors":"Alexei Baevski,Wei-Ning Hsu,Qiantong Xu,Arun Babu,Jiatao Gu,Michael Auli","id":"8f2bca9d684005675e294b33c26481e36f528cdb","summary":"Data2vec is a framework that uses the same learning method for either speech, NLP or computer vision to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture.","score":1},{"url":"https://www.semanticscholar.org/paper/7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers","venue":"Knowledge Discovery and Data Mining","year":2022,"referenceCount":38,"citationCount":64,"influentialCitationCount":13,"publicationDate":"22/02/2022","authors":"Alyssa Lees,Vinh Q. Tran,Yi Tay,Jeffrey Scott Sorensen,Jai Gupta,Donald Metzler,Lucy Vasserman","id":"7016eb4f34611f97fe8c99176246e314678e03f4","summary":"This paper presents the fundamentals behind the next version of the Perspective API from Google Jigsaw, and presents a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation","venue":"arXiv.org","year":2022,"referenceCount":43,"citationCount":27,"influentialCitationCount":3,"publicationDate":"07/03/2022","authors":"Gabriele Sarti,M. Nissim","id":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","summary":"IT5 is introduced, the first family of encoder-decoder transformer models pretrained specifically on Italian, with the monolingual IT5 models consistently outperforming their multilingual counterparts and setting a new state-of-the-art for most Italian conditional language generation tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/8a4eec70e3d0c43abf26757252ad6210c9717f6c","title":"Towards Lithuanian grammatical error correction","venue":"","year":2022,"referenceCount":35,"citationCount":2,"influentialCitationCount":0,"publicationDate":"18/03/2022","authors":"Lukas Stankevivcius,Mantas Lukovsevivcius","id":"8a4eec70e3d0c43abf26757252ad6210c9717f6c","summary":"This work constructs a grammatical error correction model for Lithuanian, the language rich in archaic features, using the recent advances in transformer architectures and shares the best trained model, achieving F$_{0.5}$=0.92, in an online open-source repository.","score":1},{"url":"https://www.semanticscholar.org/paper/a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":213,"citationCount":52,"influentialCitationCount":5,"publicationDate":"24/03/2022","authors":"Alham Fikri Aji,Genta Indra Winata,Fajri Koto,Samuel Cahyawijaya,Ade Romadhony,Rahmad Mahendra,Kemal Kurniawan,David Moeljadi,Radityo Eko Prasojo,Timothy Baldwin,Jey Han Lau,Sebastian Ruder","id":"a747e8f2659df479c0092301b9658fc582423df1","summary":"An overview of the current state of NLP research for Indonesia's 700+ languages is provided and general recommendations are provided to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.","score":1},{"url":"https://www.semanticscholar.org/paper/b1dad820853464b30c93b52366b32690ba4b99a6","title":"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View","venue":"ACM Computing Surveys","year":2022,"referenceCount":166,"citationCount":12,"influentialCitationCount":1,"publicationDate":"26/03/2022","authors":"Ruiqi Li,Xiang Zhao,M. Moens","id":"b1dad820853464b30c93b52366b32690ba4b99a6","summary":"This survey summarizes the current universal sentence-embedding methods, categorizes them into four groups from a linguistic view, and ultimately analyzes their reported performance.","score":1},{"url":"https://www.semanticscholar.org/paper/1ed66e048bb025e75aa5ea660545285212e5341f","title":"Scaling Up Models and Data with t5x and seqio","venue":"arXiv.org","year":2022,"referenceCount":25,"citationCount":127,"influentialCitationCount":8,"publicationDate":"31/03/2022","authors":"Adam Roberts,Hyung Won Chung,Anselm Levskaya,Gaurav Mishra,James Bradbury,D. Andor,Sharan Narang,Brian Lester,Colin Gaffney,Afroz Mohiuddin,Curtis Hawthorne,Aitor Lewkowycz,Alexandru Salcianu,Marc van Zee,Jacob Austin,Sebastian Goodman,Livio Baldini Soares,Haitang Hu,Sasha Tsvyashchenko,Aakanksha Chowdhery,Jasmijn Bastings,Jannis Bulian,Xavier García,Jianmo Ni,A. Chen,Kathleen Kenealy,J. Clark,Stephan Lee,Daniel H Garrette,J. Lee-Thorp,Colin Raffel,Noam M. Shazeer,Marvin Ritter,Maarten Bosma,Alexandre Passos,Jeremy B. Maitin-Shepard,Noah Fiedel,Mark Omernick,Brennan Saeta,Ryan Sepassi,A. Spiridonov,Joshua Newlan,Andrea Gesmundo","id":"1ed66e048bb025e75aa5ea660545285212e5341f","summary":"Two software libraries are presented that simplifies the process of building and training large language models at scale while maintaining ease of use and provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines.","score":1},{"url":"https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways","venue":"Journal of machine learning research","year":2022,"referenceCount":173,"citationCount":3314,"influentialCitationCount":269,"publicationDate":"05/04/2022","authors":"Aakanksha Chowdhery,Sharan Narang,Jacob Devlin,Maarten Bosma,Gaurav Mishra,Adam Roberts,Paul Barham,Hyung Won Chung,Charles Sutton,Sebastian Gehrmann,Parker Schuh,Kensen Shi,Sasha Tsvyashchenko,Joshua Maynez,Abhishek Rao,Parker Barnes,Yi Tay,Noam M. Shazeer,Vinodkumar Prabhakaran,Emily Reif,Nan Du,B. Hutchinson,Reiner Pope,James Bradbury,Jacob Austin,M. Isard,Guy Gur-Ari,Pengcheng Yin,Toju Duke,Anselm Levskaya,Sanjay Ghemawat,Sunipa Dev,H. Michalewski,Xavier García,Vedant Misra,Kevin Robinson,L. Fedus,Denny Zhou,Daphne Ippolito,D. Luan,Hyeontaek Lim,Barret Zoph,A. Spiridonov,Ryan Sepassi,David Dohan,Shivani Agrawal,Mark Omernick,Andrew M. Dai,Thanumalayan Sankaranarayana Pillai,Marie Pellat,Aitor Lewkowycz,Erica Moreira,R. Child,Oleksandr Polozov,Katherine Lee,Zongwei Zhou,Xuezhi Wang,Brennan Saeta,Mark Díaz,Orhan Firat,Michele Catasta,Jason Wei,K. Meier-Hellstern,D. Eck,J. Dean,Slav Petrov,Noah Fiedel","id":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","summary":"A 540-billion parameter, densely activated, Transformer language model, which is called PaLM achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark.","score":1},{"url":"https://www.semanticscholar.org/paper/943487997ecd26e871a2ab16160bd5640020369d","title":"Toward Best Practices for Training Multilingual Dense Retrieval Models","venue":"ACM Trans. Inf. Syst.","year":2022,"referenceCount":73,"citationCount":18,"influentialCitationCount":2,"publicationDate":"05/04/2022","authors":"Xinyu Crystina Zhang,Kelechi Ogueji,Xueguang Ma,Jimmy J. Lin","id":"943487997ecd26e871a2ab16160bd5640020369d","summary":"This article focuses on the task of monolingual retrieval in a variety of typologically diverse languages using a transformer-based bi-encoder architecture, and offers a guide for practitioners building search applications, particularly for low-resource languages.","score":1},{"url":"https://www.semanticscholar.org/paper/880c8973ea1376c6bff23226b3f64792657aa666","title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion","venue":"Interspeech","year":2022,"referenceCount":36,"citationCount":14,"influentialCitationCount":3,"publicationDate":"06/04/2022","authors":"Jian Zhu,Cong Zhang,David Jurgens","id":"880c8973ea1376c6bff23226b3f64792657aa666","summary":"It is found that ByT5 operating on byte-level inputs significantly outperformed the token-based mT5 model in terms of multilingual G2P, and pairwise comparison with monolingual models in these languages suggests that multilingual ByT 5 models generally lower the phone error rate by jointly learning from a variety of languages.","score":1},{"url":"https://www.semanticscholar.org/paper/e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","venue":"BIGSCIENCE","year":2022,"referenceCount":142,"citationCount":471,"influentialCitationCount":47,"publicationDate":"14/04/2022","authors":"Sid Black,Stella Biderman,Eric Hallahan,Quentin G. Anthony,Leo Gao,Laurence Golding,Horace He,Connor Leahy,Kyle McDonell,Jason Phang,M. Pieler,USVSN Sai Prashanth,Shivanshu Purohit,Laria Reynolds,J. Tow,Benqi Wang,Samuel Weinbach","id":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","summary":"GPT-NeoX-20B is introduced, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.","score":1},{"url":"https://www.semanticscholar.org/paper/0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":27,"citationCount":9,"influentialCitationCount":1,"publicationDate":"16/04/2022","authors":"Mingchen Li,J. Chen,Samuel Mensah,Nikolaos Aletras,Xiulong Yang,Yang Ye","id":"0b9e130c6305de7766697ba7655f56010aaffd61","summary":"A Hierarchical N-Gram framework for Zero-Shot Link Prediction (HNZSLP), which considers the dependencies among character n-grams of the relation surface name for ZSLP.","score":1},{"url":"https://www.semanticscholar.org/paper/cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":31,"citationCount":10,"influentialCitationCount":1,"publicationDate":"19/04/2022","authors":"Matthew Finlayson,Kyle Richardson,Ashish Sabharwal,Peter Clark","id":"cb16b85891172572cd856142880b503db0c2bc61","summary":"This work uses the task of deciding whether a given string matches a regular expression to identify properties of tasks, instructions, and instances that make instruction learning challenging, and proposes Hard RegSet as a challenging instruction learning dataset and a controlled environment for studying instruction learning.","score":1},{"url":"https://www.semanticscholar.org/paper/0de580957d23dd65e31b6c95e6bc5d15bc15c57d","title":"Impact of Tokenization on Language Models: An Analysis for Turkish","venue":"ACM Trans. Asian Low Resour. Lang. Inf. Process.","year":2022,"referenceCount":68,"citationCount":30,"influentialCitationCount":3,"publicationDate":"19/04/2022","authors":"Cagri Toraman,E. Yilmaz,Furkan Şahinuç,Oguzhan Ozcelik","id":"0de580957d23dd65e31b6c95e6bc5d15bc15c57d","summary":"This work compares five tokenizers at different granularity levels, that is, their outputs vary from the smallest pieces of characters to the surface form of words, and finds that increasing the vocabulary size improves the performance of Morphological- and Word-level tokenizers more than that of de facto tokenizers.","score":1},{"url":"https://www.semanticscholar.org/paper/17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?","venue":"Conference of the Association for Machine Translation in the Americas","year":2022,"referenceCount":37,"citationCount":10,"influentialCitationCount":2,"publicationDate":"29/04/2022","authors":"Shiyue Zhang,Vishrav Chaudhary,Naman Goyal,James Cross,Guillaume Wenzek,Mohit Bansal,Francisco Guzmán","id":"17e2977b907aad2532c45185947539e83ac639cd","summary":"This work analyzes how translation performance changes as the data ratios among languages vary in the tokenizer training corpus and finds that while relatively better performance is often observed when languages are more equally sampled, the downstream performance is more robust to language imbalance than the authors usually expected.","score":1},{"url":"https://www.semanticscholar.org/paper/7267812178393b8ae0b99648f02661ca1ff2b412","title":"Bilingual End-to-End ASR with Byte-Level Subwords","venue":"IEEE International Conference on Acoustics, Speech, and Signal Processing","year":2022,"referenceCount":18,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"Liuhui Deng,Roger Hsiao,Arnab Ghoshal","id":"7267812178393b8ae0b99648f02661ca1ff2b412","summary":"This paper investigates how the output representation of an end-to-end neural network affects multilingual automatic speech recognition (ASR), and finds that BBPE with penalty schemes can improve utterance-based bilingual ASR performance by 2% to 5% relative even with smaller number of outputs and fewer parameters.","score":1},{"url":"https://www.semanticscholar.org/paper/063d9fa4861356500219b7e81d5a654aa921da6f","title":"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":76,"citationCount":62,"influentialCitationCount":3,"publicationDate":"04/05/2022","authors":"David Ifeoluwa Adelani,Jesujoba Oluwadara Alabi,Angela Fan,Julia Kreutzer,Xiaoyu Shen,Machel Reid,Dana Ruiter,D. Klakow,Peter Nabende,Ernie Chang,T. Gwadabe,Freshia Sackey,Bonaventure F. P. Dossou,Chris C. Emezue,Colin Leong,Michael Beukman,Shamsuddeen Hassan Muhammad,Guyo Dub Jarso,Oreen Yousuf,Andre Niyongabo Rubungo,Gilles Hacheme,Eric Peter Wairagala,Muhammad Umair Nasir,Benjamin Ayoade Ajibade,Tunde Ajayi,Yvonne Wambui Gitau,Jade Z. Abbott,Mohamed Ahmed,Millicent Ochieng,Anuoluwapo Aremu,Perez Ogayo,Jonathan Mukiibi,F. Kabore,Godson Kalipe,Derguene Mbaye,A. Tapo,V. M. Koagne,Edwin Munkoh-Buabeng,Valencia Wagner,Idris Abdulmumin,Ayodele Awokoya,Happy Buzaaba,Blessing K. Sibanda,Andiswa Bukula,Sam Manthalu","id":"063d9fa4861356500219b7e81d5a654aa921da6f","summary":"It is demonstrated that the most effective strategy for transferring both additional languages and additional domains is to leverage small quantities of high-quality translation data to fine-tune large pre-trained models.","score":1},{"url":"https://www.semanticscholar.org/paper/b21670e8061a06ab97e7d6052c9345a326e84ff8","title":"UL2: Unifying Language Learning Paradigms","venue":"International Conference on Learning Representations","year":2022,"referenceCount":144,"citationCount":163,"influentialCitationCount":27,"publicationDate":"10/05/2022","authors":"Yi Tay,Mostafa Dehghani,Vinh Q. Tran,Xavier García,Jason Wei,Xuezhi Wang,Hyung Won Chung,Dara Bahri,Tal Schuster,H. Zheng,Denny Zhou,N. Houlsby,Donald Metzler","id":"b21670e8061a06ab97e7d6052c9345a326e84ff8","summary":"A unified framework for pre-training models that are universally effective across datasets and setups is presented and Mixture-of-Denoisers (MoD) is proposed, a pre- Training objective that combines diverse pre- training paradigms together.","score":1},{"url":"https://www.semanticscholar.org/paper/c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":80,"citationCount":83,"influentialCitationCount":7,"publicationDate":"12/05/2022","authors":"Jonas Pfeiffer,Naman Goyal,Xi Victoria Lin,Xian Li,James Cross,Sebastian Riedel,Mikel Artetxe","id":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","summary":"This work introduces language-specific modules of their Cross-lingual Modular models from the start, which allows them to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant.","score":1},{"url":"https://www.semanticscholar.org/paper/32afdb07021fda775ceaedd231c58bfed0aa980a","title":"Automated Crossword Solving","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":29,"citationCount":6,"influentialCitationCount":1,"publicationDate":"19/05/2022","authors":"Eric Wallace,Nicholas Tomlin,Albert Xu,Kevin Yang,Eshaan Pathak,Matthew Ginsberg,D. Klein","id":"32afdb07021fda775ceaedd231c58bfed0aa980a","summary":"The Berkeley Crossword Solver is presented, a state-of-the-art approach for automatically solving crossword puzzles that improves exact puzzle accuracy from 57% to 82% on crosswords from The New York Times and obtains 99.9% letter accuracy on themeless puzzles.","score":1},{"url":"https://www.semanticscholar.org/paper/0232715f9089e3a2fc002cff6737bb9939805b8d","title":"Local Byte Fusion for Neural Machine Translation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Makesh Narsimhan Sreedhar,Xiangpeng Wan,Yu-Jie Cheng,Junjie Hu","id":"0232715f9089e3a2fc002cff6737bb9939805b8d","summary":"A Local Byte Fusion (LOBEF) method for byte-based machine translation—utilizing byte n-gram and word boundaries—to aggregate local semantic information to perform competitive to subword models.","score":1},{"url":"https://www.semanticscholar.org/paper/bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation","venue":"arXiv.org","year":2022,"referenceCount":20,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/05/2022","authors":"Lukas Edman,Antonio Toral,Gertjan van Noord","id":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","summary":"The GBST method from Charformer groups (aka downsamples) characters is used, thereby enabling character grouping in the decoder, and promising performance on English--Turkish translation indicate the potential of character-level models for morphologically-rich languages.","score":1},{"url":"https://www.semanticscholar.org/paper/a766ef0678aade6a9798552618819cf4d0fac406","title":"TEVR: Improving Speech Recognition by Token Entropy Variance Reduction","venue":"arXiv.org","year":2022,"referenceCount":21,"citationCount":2,"influentialCitationCount":0,"publicationDate":"25/06/2022","authors":"Hajo N. Krabbenhöft,E. Barth","id":"a766ef0678aade6a9798552618819cf4d0fac406","summary":"TEVR, a speech recognition model designed to minimize the variation in token entropy w.r.t. to the language model, is presented and it is shown that on CommonVoice German, TEVR scores a very competitive 3.64% word error rate, which outperforms the best reported results by a relative 16.89% reduction inword error rate.","score":1},{"url":"https://www.semanticscholar.org/paper/b308511ac52e1c2da915d1e55d5246dfdb4cbe28","title":"Romanian Question Answering Using Transformer Based Neural Networks","venue":"Studia Universitatis Babeș-Bolyai Informatica","year":2022,"referenceCount":5,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/07/2022","authors":"Bogdan-Alexandru Diaconu,Beáta Lázár-Lőrincz","id":"b308511ac52e1c2da915d1e55d5246dfdb4cbe28","summary":"This work presents experiments evaluated on the XQuAD-ro question answering dataset that has been recently published based on the translation of the SQuAD dataset into Romanian, and shows that fine-tuning the model with the addition of the Romanian translation slightly increases the evaluation metrics.","score":1},{"url":"https://www.semanticscholar.org/paper/2ef60a4ea4ea53056be811ff55679eb59fb4b586","title":"Confident Adaptive Language Modeling","venue":"Neural Information Processing Systems","year":2022,"referenceCount":92,"citationCount":67,"influentialCitationCount":14,"publicationDate":"14/07/2022","authors":"Tal Schuster,Adam Fisch,Jai Gupta,Mostafa Dehghani,Dara Bahri,Vinh Q. Tran,Yi Tay,Donald Metzler","id":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","summary":"This work introduces Confident Adaptive Language Modeling (CALM), a framework for dynamically allocating different amounts of compute per input and generation timestep and demonstrates the efficacy of the framework in reducing compute -- potential speedup of up to $\\times 3$ -- while provably maintaining high performance.","score":1},{"url":"https://www.semanticscholar.org/paper/23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels","venue":"International Conference on Learning Representations","year":2022,"referenceCount":145,"citationCount":25,"influentialCitationCount":3,"publicationDate":"14/07/2022","authors":"Phillip Rust,Jonas F. Lotz,Emanuele Bugliarello,Elizabeth Salesky,Miryam de Lhoneux,Desmond Elliott","id":"23f4b6432b74e5db05da04e354341807f5044f7e","summary":"PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels, and is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.","score":1},{"url":"https://www.semanticscholar.org/paper/00d9a73c54053a32e2aba92b53fc3e6bc71d3238","title":"Sequence-to-sequence pretraining for a less-resourced Slovenian language","venue":"Frontiers in Artificial Intelligence","year":2022,"referenceCount":49,"citationCount":4,"influentialCitationCount":0,"publicationDate":"28/07/2022","authors":"Matej Ulčar,Marko Robnik-Sikonja","id":"00d9a73c54053a32e2aba92b53fc3e6bc71d3238","summary":"Two different-sized T5-type sequence-to-sequence models for morphologically rich Slovene language with much fewer resources are trained and they are helpful for generative tasks and provide several useful results.","score":1},{"url":"https://www.semanticscholar.org/paper/54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/08/2022","authors":"Jan Jezabek,A. Singh","id":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","summary":"A novel method of retroactively adding resilience to misspellings to transformer-based NLP models without the need for re-training of the original NLP model is proposed, which significantly reduces the cost needed to evaluate a model’s resilience to adversarial attacks.","score":1},{"url":"https://www.semanticscholar.org/paper/062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/08/2022","authors":"Borun Chen,Hongyin Tang,Jingang Wang,Qifan Wang,Haitao Zheng,Wei Yu Wu,Liqian Yu","id":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","summary":"This work proposes a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations, and implicitly encodes the coarse-grained information into the fine-Grained representations through contrastive learning on multi-grains information.","score":1},{"url":"https://www.semanticscholar.org/paper/a806041621acb5cf648fc780f1ff14939aa3a721","title":"How Much Does Lookahead Matter for Disambiguation? Partial Arabic Diacritization Case Study","venue":"Computational Linguistics","year":2022,"referenceCount":59,"citationCount":2,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"Saeed Esmail,Kfir Bar,N. Dershowitz","id":"a806041621acb5cf648fc780f1ff14939aa3a721","summary":"It is found that the partial diacritizer improves translation quality compared either to their total absence or to random selection, and the benefit of knowing the text that follows the word in focus toward the restoration of short vowels during reading is studied.","score":1},{"url":"https://www.semanticscholar.org/paper/e4437293a703fcf282bf1b38bf7900ed8ca711bb","title":"Training a T5 Using Lab-sized Resources","venue":"arXiv.org","year":2022,"referenceCount":28,"citationCount":4,"influentialCitationCount":0,"publicationDate":"25/08/2022","authors":"Manuel R. Ciosici,Leon Derczynski","id":"e4437293a703fcf282bf1b38bf7900ed8ca711bb","summary":"Various techniques for making it possible to train a large language model using resources that a modest research lab might have, and train it in a reasonable amount of time are presented.","score":1},{"url":"https://www.semanticscholar.org/paper/134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":30,"citationCount":8,"influentialCitationCount":0,"publicationDate":"06/09/2022","authors":"Doan Nam Long Vu,N. Moosavi,Steffen Eger","id":"134e4d72e23bca51e290db171d063989883020f4","summary":"It is shown that an embedding-based metric that has the highest correlation with human evaluations on a standard benchmark can have the lowest correlation if the amount of input noise or unknown tokens increases, and the highest robustness is achieved when using character-level embeddings, instead of token-based embeddments, from the first layer of the pretrained model.","score":1},{"url":"https://www.semanticscholar.org/paper/28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model","venue":"International Conference on Learning Representations","year":2022,"referenceCount":112,"citationCount":357,"influentialCitationCount":61,"publicationDate":"14/09/2022","authors":"Xi Chen,Xiao Wang,Soravit Changpinyo,A. Piergiovanni,Piotr Padlewski,Daniel M. Salz,Sebastian Goodman,Adam Grycner,Basil Mustafa,Lucas Beyer,Alexander Kolesnikov,J. Puigcerver,Nan Ding,Keran Rong,Hassan Akbari,Gaurav Mishra,Linting Xue,Ashish V. Thapliyal,James Bradbury,Weicheng Kuo,Mojtaba Seyedhosseini,Chao Jia,Burcu Karagol Ayan,C. Riquelme,A. Steiner,A. Angelova,Xiaohua Zhai,N. Houlsby,Radu Soricut","id":"28630034bb29760df01ab033b743e30b37f336ae","summary":"The PaLI (Pathways Language and Image model), a model that achieves state-of-the-art in multiple vision and language tasks, while retaining a simple, modular, and scalable design.","score":1},{"url":"https://www.semanticscholar.org/paper/bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","title":"MonoByte: A Pool of Monolingual Byte-level Language Models","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/09/2022","authors":"Hugo Abonizio,Leandro Rodrigues de Souza,R. Lotufo,Rodrigo Nogueira","id":"bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","summary":"Experiments on QA and NLI tasks show that monolingual models achieve competitive performance to the multilingual one, and hence can be served to strengthen the understanding of cross-lingual transferability in language models.","score":1},{"url":"https://www.semanticscholar.org/paper/2a8aac78df5e1e9658a02d381662ed26c6577713","title":"Disease diagnostic method based on cascade backbone network for apple leaf disease classification","venue":"Frontiers in Plant Science","year":2022,"referenceCount":26,"citationCount":4,"influentialCitationCount":0,"publicationDate":"23/09/2022","authors":"Xing Sheng,Fengyun Wang,Huaijun Ruan,Yangyang Fan,Jiye Zheng,Yangyang Zhang,Chen Lyu","id":"2a8aac78df5e1e9658a02d381662ed26c6577713","summary":"This paper is the first to introduce Transformer into apple leaf disease identification, and the results are promising.","score":1},{"url":"https://www.semanticscholar.org/paper/aa94724d1dbc9cad0ad3377903174e776175837a","title":"Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino","venue":"SUSTAINLP","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Lorenzo Jaime Yu Flores","id":"aa94724d1dbc9cad0ad3377903174e776175837a","summary":"An N-Gram + Damerau-Levenshtein distance model with automatic rule extraction achieves good performance and outperforms other deep learning approaches in terms of accuracy and edit distance, highlighting the success of traditional approaches over more complex deep learning models in settings where data is unavailable.","score":1},{"url":"https://www.semanticscholar.org/paper/cd9fdf84f1cbb482e3952ca63de70d03f9030644","title":"Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":42,"citationCount":3,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Asa Cooper Stickland,Sailik Sengupta,Jason Krone,Saab Mansour,He He","id":"cd9fdf84f1cbb482e3952ca63de70d03f9030644","summary":"After investigating several ways to boost the robustness of multilingual models in this setting, this work proposes Robust Contrastive Pretraining (RCP), which combines data augmentation with a contrastive loss term at the pretraining stage and achieves large improvements on noisy data.","score":1},{"url":"https://www.semanticscholar.org/paper/539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72","title":"Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning","venue":"Transactions of the Japanese society for artificial intelligence","year":2022,"referenceCount":63,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Kotaro Funakoshi","id":"539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72","summary":"Non-Axiomatic Term Logic is presented as a theoretical computational framework of humanlike symbolic reasoning in artiﬁcial intelligence and positions the proposed approach in the phylogeny and the literature of logic.","score":1},{"url":"https://www.semanticscholar.org/paper/3e08d84af266026e7d254729f3afc6dcd572d264","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks","venue":"Workshop on Representation Learning for NLP","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Gregor Geigle,Chen Cecilia Liu,Jonas Pfeiffer,Iryna Gurevych","id":"3e08d84af266026e7d254729f3afc6dcd572d264","summary":"This work exhaustively experiments with three popular VEs on six downstream V+L tasks and suggests that diverse VEs complement each other, resulting in improved downstream V-L task performance, where the improvements are not due to simple ensemble effects.","score":1},{"url":"https://www.semanticscholar.org/paper/a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":49,"citationCount":4,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Shaked Yehezkel,Yuval Pinter","id":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","summary":"SaGe, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase, is presented, showing its robustness to language properties such as morphological exponence and agglutination.","score":1},{"url":"https://www.semanticscholar.org/paper/70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models","venue":"Findings","year":2022,"referenceCount":36,"citationCount":3,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Jimin Sun,Patrick Fernandes,Xinyi Wang,Graham Neubig","id":"70dd68c07b322b68836eded1fb4f78c0efcad685","summary":"Surprisingly, it is found that subword-based models might still be the most practical choice in many settings, achieving better performance for lower inference latency and memory usage.","score":1},{"url":"https://www.semanticscholar.org/paper/12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":38,"citationCount":3,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Huiyin Xue,Nikolaos Aletras","id":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","summary":"HashFormers is proposed, a new family of vocabulary-independent pre-trained transformers that support an unlimited vocabulary given a substantially smaller fixed-sized embedding matrix and is empirically demonstrate that HashFormers are more memory efficient compared to standard pre- trained transformers while achieving comparable predictive performance when fine-tuned on multiple text classification tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","title":"Scaling Instruction-Finetuned Language Models","venue":"arXiv.org","year":2022,"referenceCount":106,"citationCount":1468,"influentialCitationCount":210,"publicationDate":"20/10/2022","authors":"Hyung Won Chung,Le Hou,S. Longpre,Barret Zoph,Yi Tay,W. Fedus,Eric Li,Xuezhi Wang,Mostafa Dehghani,Siddhartha Brahma,Albert Webson,S. Gu,Zhuyun Dai,Mirac Suzgun,Xinyun Chen,Aakanksha Chowdhery,Dasha Valter,Sharan Narang,Gaurav Mishra,Adams Wei Yu,Vincent Zhao,Yanping Huang,Andrew M. Dai,Hongkun Yu,Slav Petrov,E. Chi,J. Dean,Jacob Devlin,Adam Roberts,Denny Zhou,Quoc V. Le,Jason Wei","id":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","summary":"It is found that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups, and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation).","score":1},{"url":"https://www.semanticscholar.org/paper/6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc","title":"Graphemic Normalization of the Perso-Arabic Script","venue":"arXiv.org","year":2022,"referenceCount":141,"citationCount":3,"influentialCitationCount":1,"publicationDate":"21/10/2022","authors":"R. Doctor,Alexander Gutkin,Cibu Johny,Brian Roark,R. Sproat","id":"6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc","summary":"It is argued that better understanding and representation of Perso-Arabic script variation within regional orthographic traditions, where those are present, is crucial for further progress of modern computational NLP techniques especially for languages with a paucity of resources.","score":1},{"url":"https://www.semanticscholar.org/paper/d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6","title":"SLING: Sino Linguistic Evaluation of Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":64,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Yixiao Song,Kalpesh Krishna,R. Bhatt,Mohit Iyyer","id":"d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6","summary":"The benchmark of Sino LINGuistics (SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese grouped into 9 high-level linguistic phenomena, is introduced and it is found that most LMs have a strong gender and number (singular/plural) bias, and they perform better on local phenomena than hierarchical ones.","score":1},{"url":"https://www.semanticscholar.org/paper/5e786d47e3dc5ffa65ba8548ff67139a2764acf8","title":"T5lephone: Bridging Speech and Text Self-Supervised Models for Spoken Language Understanding Via Phoneme Level T5","venue":"IEEE International Conference on Acoustics, Speech, and Signal Processing","year":2022,"referenceCount":28,"citationCount":6,"influentialCitationCount":3,"publicationDate":"01/11/2022","authors":"Chan-Jan Hsu,Ho-Lam Chung,Hung-yi Lee,Yu Tsao","id":"5e786d47e3dc5ffa65ba8548ff67139a2764acf8","summary":"This work conducts extensive studies on how PLMs with different tokenization strategies affect spoken language understanding task including spoken question answering (SQA) and speech translation (ST) and creates T5lephone1, a variant of T5 that is pretrained using phonemicized text.","score":1},{"url":"https://www.semanticscholar.org/paper/92302ab168429c7c3a8f699b35ba8302916c6e7c","title":"Bridging Speech and Textual Pre-Trained Models With Unsupervised ASR","venue":"IEEE International Conference on Acoustics, Speech, and Signal Processing","year":2022,"referenceCount":44,"citationCount":5,"influentialCitationCount":1,"publicationDate":"06/11/2022","authors":"Jiatong Shi,Chan-Jan Hsu,Ho-Lam Chung,Dongji Gao,Leibny Paola García-Perera,Shinji Watanabe,Ann Lee,Hung-yi Lee","id":"92302ab168429c7c3a8f699b35ba8302916c6e7c","summary":"Unsupervised automatic speech recognition (ASR) is proposed to use as a connector that bridges different modalities used in speech and textual pre-trained models, resulting in an unsupervised speech-to-semantic pre- trained model for various tasks in SLU.","score":1},{"url":"https://www.semanticscholar.org/paper/9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages","venue":"AACL","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"9a5b2dc77bda19759df8481aaf283da353ac7e77","summary":"This work replicates a study on the importance of local structure, and the relative unimportance of global structure, in a multilingual setting and finds that the phenomenon observed on the English language broadly translates to over 120 languages, with a few caveats.","score":1},{"url":"https://www.semanticscholar.org/paper/d7e0f0cec28c34710fa631df410b717186741db5","title":"A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces","venue":"","year":2022,"referenceCount":35,"citationCount":3,"influentialCitationCount":1,"publicationDate":"14/11/2022","authors":"Dominic Rampas,Pablo Pernias,M. Aubreville","id":"d7e0f0cec28c34710fa631df410b717186741db5","summary":"Despite its remarkable simplicity, the proposed streamlined approach for text-to-image generation yields aesthetically pleasing images with few sampling iterations, allows for intriguing ways for conditioning the model, and imparts advantages absent in state-of-the-art techniques.","score":1},{"url":"https://www.semanticscholar.org/paper/d50b9db750ded246bde13e2c263e341bcbd8a335","title":"A Benchmark and Dataset for Post-OCR text correction in Sanskrit","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":31,"citationCount":4,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Ayush Maheshwari,Nikhil Singh,Amrith Krishna,Ganesh Ramakrishnan","id":"d50b9db750ded246bde13e2c263e341bcbd8a335","summary":"This work releases a post-OCR text correction dataset containing around 218,000 sentences, with 1.5 million words, from 30 different books of Sanskrit, and finds that the best-performing model, consisting of byte level tokenization in conjunction with phonetic encoding (Byt5+SLP1), yields a 23% point increase over the OCR output in terms of word and character error rates.","score":1},{"url":"https://www.semanticscholar.org/paper/af277778904463965c60626e22783d3e1740058b","title":"Introducing Semantics into Speech Encoders","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":69,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Derek Xu,Shuyan Dong,Changhan Wang,Suyoun Kim,Zhaojiang Lin,Akshat Shrivastava,Shang-Wen Li,Liang-Hsuan Tseng,Alexei Baevski,Guan-Ting Lin,Hung-yi Lee,Yizhou Sun,Wei Wang","id":"af277778904463965c60626e22783d3e1740058b","summary":"This work proposes a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions, and improves existing speech encoder spoken language understanding performance by over 5% on intent classification.","score":1},{"url":"https://www.semanticscholar.org/paper/0783c214623c18f6a8ad96b8eaf4a67a382e68ee","title":"QAmeleon: Multilingual QA with Only 5 Examples","venue":"Transactions of the Association for Computational Linguistics","year":2022,"referenceCount":65,"citationCount":13,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Priyanka Agrawal,Chris Alberti,Fantine Huot,Joshua Maynez,Ji Ma,Sebastian Ruder,Kuzman Ganchev,Dipanjan Das,Mirella Lapata","id":"0783c214623c18f6a8ad96b8eaf4a67a382e68ee","summary":"This approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are fine-tuned, thus avoiding costly annotation and shows that few-shot prompt tuning for data synthesis scales across languages and is a viable alternative to large-scale annotation.","score":1},{"url":"https://www.semanticscholar.org/paper/5e52d654fd31f04c1bd884cd5480e6af8c95ad50","title":"Efficient Transformers with Dynamic Token Pooling","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":55,"citationCount":13,"influentialCitationCount":1,"publicationDate":"17/11/2022","authors":"Piotr Nawrot,J. Chorowski,Adrian La'ncucki,E. Ponti","id":"5e52d654fd31f04c1bd884cd5480e6af8c95ad50","summary":"The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget.","score":1},{"url":"https://www.semanticscholar.org/paper/9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling","venue":"arXiv.org","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Chul Lee,Qipeng Guo,Xipeng Qiu","id":"9f4351600c72d5dac0251cd49984f691dca2fcd1","summary":"This work overhauls the existing character-aware method that takes character-level inputs but makes word-level sequence modeling and prediction by introducing a cross-attention network that builds word- level representation directly from bytes, and a sub-word level prediction based on word-levels hidden states to avoid the time and space requirement ofword-level prediction.","score":1},{"url":"https://www.semanticscholar.org/paper/e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy","venue":"IEEE Access","year":2022,"referenceCount":107,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/11/2022","authors":"Philipp Siebers,Christian Janiesch,Patrick Zschech","id":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","summary":"This work contributes threefold to this lack of compilation, composition, and systematization by providing a survey of current approaches, arranging them in a genealogy, and conceptualizing a taxonomy of text representation methods to examine and explain the state-of-the-art.","score":1},{"url":"https://www.semanticscholar.org/paper/ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":27,"citationCount":5,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Lukas Edman,Antonio Toral,Gertjan van Noord","id":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","summary":"This work analyzes the problems of previous downsampling methods and introduces a noveldownsampling method which is informed by subwords, showing that downsamplings characters can be done without sacrificing quality, and leads to promising performance compared to subword models for translation.","score":1},{"url":"https://www.semanticscholar.org/paper/3af872a4bd314db9e7964876fee811823628f83f","title":"Text Normalization on Code-Mixed Twitter Text using Language Detection","venue":"International Conference on Intelligent Computing","year":2022,"referenceCount":10,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Rafi Dwi Rizqullah,I. Budi","id":"3af872a4bd314db9e7964876fee811823628f83f","summary":"The research shows that proposed method for text normalization still has problems related to language, either on identifying language or normalize a word, but the approach is using language detection module alongside with transformer model.","score":1},{"url":"https://www.semanticscholar.org/paper/be2df0fafa89b6355d1ff1336c10e0d4c8d27276","title":"Massively Multilingual Natural Language Understanding 2022 (MMNLU-22) Workshop and Competition","venue":"MMNLU","year":2022,"referenceCount":9,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"C. Hench,Charith S. Peris,Jack G. M. FitzGerald,Kay Rottmann","id":"be2df0fafa89b6355d1ff1336c10e0d4c8d27276","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling","venue":"arXiv.org","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":1,"publicationDate":"14/12/2022","authors":"Nathan Godey,Roman Castagn'e,Eric Villemonte de la Clergerie,Benoît Sagot","id":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","summary":"MANTa is a differentiable tokenizer trained end-to-end with the language model that offers a trade-off between the expressiveness of byte-level models and the speed of models trained using subword tokenization.","score":1},{"url":"https://www.semanticscholar.org/paper/7c1c95f5fb7fce563171fcc0060c850390753b3c","title":"Advancing Multilingual Pre-training: TRIP Triangular Document-level Pre-training for Multilingual Language Models","venue":"","year":2022,"referenceCount":53,"citationCount":3,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Hongyuan Lu,Haoyang Huang,Shuming Ma,Dongdong Zhang,W. Lam,Furu Wei","id":"7c1c95f5fb7fce563171fcc0060c850390753b3c","summary":"Triangular Document-level Trilingual Re-training (TRIP), which is the first in the field to accelerate the conventional monolingual and bilingual objectives into a trilingual objective with a novel method called Grafting, is presented.","score":1},{"url":"https://www.semanticscholar.org/paper/4aa09cba27a489ca02471fad011ea4854fc63cc1","title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"William B. Held,Christopher Hidey,Fei Liu,Eric Zhu,Rahul Goel,Diyi Yang,Rushin Shah","id":"4aa09cba27a489ca02471fad011ea4854fc63cc1","summary":"The Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer parameters.","score":1},{"url":"https://www.semanticscholar.org/paper/ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only","venue":"Computer Vision and Pattern Recognition","year":2022,"referenceCount":87,"citationCount":25,"influentialCitationCount":1,"publicationDate":"15/12/2022","authors":"M. Tschannen,Basil Mustafa,N. Houlsby","id":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","summary":"The fact that CLIPPO does not require a tokenizer is exploited to show that it can achieve strong performance on multilingual multimodal retrieval without modifications, and it can obtain good accuracy in visual question answering, simply by rendering the question and image together.","score":1},{"url":"https://www.semanticscholar.org/paper/2f272ca3394656835bf32a30b80c6f8ba1c8f4c4","title":"Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":1,"publicationDate":"19/12/2022","authors":"Kaiser Sun,Peng Qi,Yuhao Zhang,Lan Liu,William Yang Wang,Zhiheng Huang","id":"2f272ca3394656835bf32a30b80c6f8ba1c8f4c4","summary":"It is shown that, with consistent tokenization, the model performs better in both in-domain and out-of-domain datasets, with a notable average of +1.7 F2 gain when a BART model is trained on SQuAD and evaluated on 8 QA datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":61,"citationCount":8,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Jing Huang,Zhengxuan Wu,Kyle Mahowald,Christopher Potts","id":"205cc15fca6963b355e4c071071368e874ee103e","summary":"This work develops a causal intervention framework to learn robust and interpretable character representations inside subword-based language models and introduces a suite of character-level tasks that systematically vary in their dependence on meaning and sequence-level context.","score":1},{"url":"https://www.semanticscholar.org/paper/f8ef90a8cac1a8edf9477688aac24864c547d6cd","title":"Character-Aware Models Improve Visual Text Rendering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":182,"citationCount":28,"influentialCitationCount":6,"publicationDate":"20/12/2022","authors":"Rosanne Liu,Daniel H Garrette,Chitwan Saharia,William Chan,Adam Roberts,Sharan Narang,Irina Blok,R. Mical,Mohammad Norouzi,Noah Constant","id":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","summary":"This paper trains a suite of image generation models, and shows that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (the authors' DrawText benchmark), and sets a much higher state-of-the-art on visual spelling.","score":1},{"url":"https://www.semanticscholar.org/paper/7cdebb73662387d9040da4f27a7dc04dbffa3c3e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":80,"citationCount":9,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Jonas Belouadi,Steffen Eger","id":"7cdebb73662387d9040da4f27a7dc04dbffa3c3e","summary":"This work successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with the authors' styles, and shows that ByG PT5 outperforms other models such as mT5, ByT4, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans.","score":1},{"url":"https://www.semanticscholar.org/paper/8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models","venue":"arXiv.org","year":2022,"referenceCount":55,"citationCount":18,"influentialCitationCount":2,"publicationDate":"21/12/2022","authors":"Najoung Kim,Tal Linzen,P. Smolensky","id":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","summary":"It is argued that exposure to pretraining data may break distributional control across training and test to gauge compositional generalization, where certain lexical items only occur in limited contexts during training.","score":1},{"url":"https://www.semanticscholar.org/paper/20c7d73609ba98fa27b1edc7b537ef59442e4ba2","title":"Truveta Mapper: A Zero-shot Ontology Alignment Framework","venue":"OM@ISWC","year":2023,"referenceCount":36,"citationCount":4,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Mariyam Amir,Murchana Baruah,Mahsa Eslamialishah,Sina Ehsani,Alireza Bahramali,Sadra Naddaf-sh,Saman Zarandioon","id":"20c7d73609ba98fa27b1edc7b537ef59442e4ba2","summary":"A new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task that offers log-linear complexity, and overall makes the OM task efficient and more straightforward without much post-processing involving mapping extension or mapping repair.","score":1},{"url":"https://www.semanticscholar.org/paper/62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c","title":"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":37,"citationCount":29,"influentialCitationCount":6,"publicationDate":"25/01/2023","authors":"Davis Liang,Hila Gonen,Yuning Mao,Rui Hou,Naman Goyal,Marjan Ghazvininejad,Luke Zettlemoyer,Madian Khabsa","id":"62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c","summary":"A new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/0704a96e1c57c12031f1c3ca492a91dbed1f61ce","title":"Distillation of encoder-decoder transformers for sequence labelling","venue":"Findings","year":2023,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"M. Farina,D. Pappadopulo,Anant Gupta,Leslie Huang,Ozan Irsoy,T. Solorio","id":"0704a96e1c57c12031f1c3ca492a91dbed1f61ce","summary":"This work builds on recent work to propose a hallucination-free framework for sequence tagging that is especially suited for distillation and shows empirical results of new state-of-the-art performance across multiple sequence labelling datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/a794a92e1fa516250390514eeeb3c3b3140876a3","title":"RetVec: Resilient and Efficient Text Vectorizer","venue":"Neural Information Processing Systems","year":2023,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/02/2023","authors":"Elie Bursztein,Marina Zhang,Owen Vallis,Xinyu Jia,Alexey Kurakin","id":"a794a92e1fa516250390514eeeb3c3b3140876a3","summary":"Evaluated and compared RETVec to state-of-the-art vectorizers and word embeddings on popular model architectures and datasets demonstrate thatRETVec leads to competitive, multilingual models that are significantly more resilient to typos and adversarial text attacks.","score":1},{"url":"https://www.semanticscholar.org/paper/1e4b6567ff66de6cdcbe58c863538241e2f62b45","title":"Aligning Text-to-Image Models using Human Feedback","venue":"arXiv.org","year":2023,"referenceCount":50,"citationCount":94,"influentialCitationCount":9,"publicationDate":"23/02/2023","authors":"Kimin Lee,Hao Liu,M. Ryu,Olivia Watkins,Yuqing Du,Craig Boutilier,P. Abbeel,M. Ghavamzadeh,S. Gu","id":"1e4b6567ff66de6cdcbe58c863538241e2f62b45","summary":"A fine-tuning method for aligning text-to-image models using human feedback, comprising three stages, that generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model.","score":1},{"url":"https://www.semanticscholar.org/paper/67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/02/2023","authors":"Du-Yeong Kim,Jeeeun Kim","id":"67c28d697460b684a0ba97d989719b4ed3c9cffc","summary":"Using this novel approach, the standard transformer architecture can be reused for all levels of language representations and be able to process much longer sequences at the same time-complexity without\"any\"architectural modification and additional overhead.","score":1},{"url":"https://www.semanticscholar.org/paper/96b005d1d92211cf053e4114a85a5e64e428d896","title":"Extending English IR methods to multi-lingual IR","venue":"arXiv.org","year":2023,"referenceCount":17,"citationCount":1,"influentialCitationCount":0,"publicationDate":"28/02/2023","authors":"Carlos Lassance","id":"96b005d1d92211cf053e4114a85a5e64e428d896","summary":"This paper describes the participation in the 2023 WSDM CUP - MIRACL challenge and trains the first SPLADE model that is effectively capable of working in more than 10 languages.","score":1},{"url":"https://www.semanticscholar.org/paper/2c8935ec872eca14636a090e5f6b49bc1c90c30d","title":"Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation","venue":"","year":2023,"referenceCount":47,"citationCount":2,"influentialCitationCount":0,"publicationDate":"28/02/2023","authors":"Lukas Edman,Gabriele Sarti,Antonio Toral,Gertjan van Noord,Arianna Bisazza","id":"2c8935ec872eca14636a090e5f6b49bc1c90c30d","summary":"This work performs an extensive comparison across multiple languages and experimental conditions of character- and subword-level pretrained models (ByT5 and mT5, respectively) on NMT, and shows the effectiveness of character-level modeling in translation, particularly in cases where fine-tuning data is limited.","score":1},{"url":"https://www.semanticscholar.org/paper/b169f2ce55e14584d2db6f64eeb5ad2702d39d40","title":"Artificial intelligence foundation and pre-trained models: Fundamentals, applications, opportunities, and social impacts","venue":"Simulation modelling practice and theory","year":2023,"referenceCount":73,"citationCount":11,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Adam Kolides,Alyna Nawaz,Anshu Rathor,Denzel Beeman,Muzammil Hashmi,Sana Fatima,David Berdik,M. Al-Ayyoub,Yaser Jararweh","id":"b169f2ce55e14584d2db6f64eeb5ad2702d39d40","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/f3d894cf6f7be14a545019f4621ccce41f45b088","title":"Learning the Legibility of Visual Text Perturbations","venue":"Conference of the European Chapter of the Association for Computational Linguistics","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/03/2023","authors":"D. Seth,Rickard Stureborg,Danish Pruthi,Bhuwan Dhingra","id":"f3d894cf6f7be14a545019f4621ccce41f45b088","summary":"It is discovered that legible perturbations from the LEGIT dataset are more effective at lowering the performance of NLP models than best-known attack strategies, suggesting that current models may be vulnerable to a broad range of perturbation beyond what is captured by existing visual attacks.","score":1},{"url":"https://www.semanticscholar.org/paper/2c5460afa19ad6fc2568b7e210115acacc14a40c","title":"An Overview on Language Models: Recent Developments and Outlook","venue":"APSIPA Transactions on Signal and Information Processing","year":2023,"referenceCount":220,"citationCount":15,"influentialCitationCount":1,"publicationDate":"10/03/2023","authors":"Chengwei Wei,Yun Cheng Wang,Bin Wang,C.-C. Jay Kuo","id":"2c5460afa19ad6fc2568b7e210115acacc14a40c","summary":"This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, architectures, training methods, evaluation methods, and applications.","score":1},{"url":"https://www.semanticscholar.org/paper/0dfc7eecc5f12b8613152f8c62f9ccfadc4f92b7","title":"DTT: An Example-Driven Tabular Transformer for Joinability by Leveraging Large Language Models","venue":"","year":2023,"referenceCount":58,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/03/2023","authors":"Arash Dargahi Nobari,Davood Rafiei","id":"0dfc7eecc5f12b8613152f8c62f9ccfadc4f92b7","summary":"This paper develops a framework that leverages large deep-learning language models to transform tabular data from a source formatting to a desired target representation, and can efficiently learn the patterns for mapping a source formatting into an expected target using just a few examples.","score":1},{"url":"https://www.semanticscholar.org/paper/4e97303aeb299ee736b1b8c29cef046212690354","title":"Generation-based Code Review Automation: How Far Are Weƒ","venue":"IEEE International Conference on Program Comprehension","year":2023,"referenceCount":76,"citationCount":5,"influentialCitationCount":1,"publicationDate":"13/03/2023","authors":"Xin Zhou,Kisub Kim,Bowen Xu,Donggyun Han,Junda He,David Lo","id":"4e97303aeb299ee736b1b8c29cef046212690354","summary":"A comprehensive study by comparing the effectiveness of recent ACR tools as well as the general-purpose pre-trained models, which shows that a general- Purpose pre- trained model CodeT5 can outperform other models in most cases.","score":1},{"url":"https://www.semanticscholar.org/paper/0a438980ac42451d6d32dd2ad8ead7b55520408d","title":"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning","venue":"Inf.","year":2023,"referenceCount":129,"citationCount":8,"influentialCitationCount":0,"publicationDate":"16/03/2023","authors":"Evans Kotei,Ramkumar Thirunavukarasu","id":"0a438980ac42451d6d32dd2ad8ead7b55520408d","summary":"A comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks is given and future directions to further improvement in pretrained transformer-based language models are presented.","score":1},{"url":"https://www.semanticscholar.org/paper/eef5b8f3c4e60d596a04101d8261c222ab739861","title":"Fine-Tashkeel: Fine-Tuning Byte-Level Models for Accurate Arabic Text Diacritization","venue":"2023 IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology (JEEIT)","year":2023,"referenceCount":21,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/03/2023","authors":"Bashar Al-Rfooh,Gheith A. Abandah,Rami Al-Rfou","id":"eef5b8f3c4e60d596a04101d8261c222ab739861","summary":"This work fine-tune token-free pre-trained multilingual models (ByT5) to learn to predict and insert missing diacritics in Arabic text, a complex task that requires understanding the sentence semantics and the morphological structure of the tokens.","score":1},{"url":"https://www.semanticscholar.org/paper/0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!","venue":"EUSFLAT/AGOP","year":2023,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/03/2023","authors":"Christos Theodoropoulos,Marie-Francine Moens","id":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","summary":"The main outcome is twofold: tokenization patterns can introduce inductive bias that results in state-of-the-art performance, and the character-based models produce promising results; thus, transitioning to token-free IE models is feasible.","score":1},{"url":"https://www.semanticscholar.org/paper/3add55c068fe19bb2e5392cbe994602a91630ec1","title":"Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams","venue":"arXiv.org","year":2023,"referenceCount":23,"citationCount":16,"influentialCitationCount":0,"publicationDate":"29/03/2023","authors":"Desnes Nunes,Ricardo Primi,Ramon Pires,R. Lotufo,Rodrigo Nogueira","id":"3add55c068fe19bb2e5392cbe994602a91630ec1","summary":"Investigation of the capabilities of Language Models in tackling high-stakes multiple-choice tests, represented here by the Exame Nacional do Ensino M\\'edio (ENEM), a multidisciplinary entrance examination widely adopted by Brazilian universities, finds that the best-performing model, GPT-4 with CoT, achieved an accuracy of 87%, largely surpassing G PT-3.5 by 11 points.","score":1},{"url":"https://www.semanticscholar.org/paper/10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7","title":"GlyphDraw: Seamlessly Rendering Text with Intricate Spatial Structures in Text-to-Image Generation","venue":"","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/03/2023","authors":"Jiancang Ma,Mingjun Zhao,Chen Chen,Ruichen Wang,Di Niu,H. Lu,Xiaodong Lin","id":"10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7","summary":"GlyphDraw is introduced, a general learning framework aiming to endow image generation models with the capacity to generate images coherently embedded with text for any specific language.","score":1},{"url":"https://www.semanticscholar.org/paper/2b8d28149a43b9659a6da2c56014ec4206a912b4","title":"Ticket automation: An insight into current research with applications to multi-level classification scenarios","venue":"Expert systems with applications","year":2023,"referenceCount":143,"citationCount":4,"influentialCitationCount":0,"publicationDate":"01/04/2023","authors":"A. Zangari,Matteo Marcuzzo,Michele Schiavinato,A. Gasparetto,A. Albarelli","id":"2b8d28149a43b9659a6da2c56014ec4206a912b4","summary":"This work provides an overview of support Ticket Automation, what recent proposals are being made in this field, and how well some of these methods can generalize to new scenarios and datasets and showcases an effective way to boost classification by injecting information from the hierarchical structure of the labels into the classifier.","score":1},{"url":"https://www.semanticscholar.org/paper/ba184d335a9a08c52c5d25eabd7f4a8ea987918b","title":"UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining","venue":"International Conference on Learning Representations","year":2023,"referenceCount":47,"citationCount":17,"influentialCitationCount":2,"publicationDate":"18/04/2023","authors":"Hyung Won Chung,Noah Constant,Xavier García,Adam Roberts,Yi Tay,Sharan Narang,Orhan Firat","id":"ba184d335a9a08c52c5d25eabd7f4a8ea987918b","summary":"An extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale, find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases.","score":1},{"url":"https://www.semanticscholar.org/paper/e0d09d91784a6d426ffcd2fd12448dd9c8ceefe9","title":"Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages","venue":"Workshop on NLP for Similar Languages, Varieties and Dialects","year":2023,"referenceCount":53,"citationCount":3,"influentialCitationCount":0,"publicationDate":"20/04/2023","authors":"Verena Blaschke,Hinrich Schütze,Barbara Plank","id":"e0d09d91784a6d426ffcd2fd12448dd9c8ceefe9","summary":"Overall, it is found that the similarity between the percentage of words that get split into subwords in the source and target data (the isplit word ratio difference/i) is the strongest predictor for model performance on target data.","score":1},{"url":"https://www.semanticscholar.org/paper/37d8bc1c268773dc69c92c6f550393c93e9a1f7a","title":"Automatic document classification via transformers for regulations compliance management in large utility companies","venue":"Neural computing & applications (Print)","year":2023,"referenceCount":123,"citationCount":2,"influentialCitationCount":0,"publicationDate":"28/04/2023","authors":"Tolga Dimlioglu,Jing Wang,Devansh Bisla,A. Choromańska,Simon Odie,Leon Bukhman,Afolabi Olomola,James D. Wong","id":"37d8bc1c268773dc69c92c6f550393c93e9a1f7a","summary":"An automatic document classification pipeline that determines whether a document is important for the company or not, and if deemed important it forwards those documents to the departments within the company for further review is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/af6c6941acecfb23d899cd5266efdf2e27463f12","title":"Causal Language Model Aided Sequential Decoding With Natural Redundancy","venue":"IEEE Transactions on Communications","year":2023,"referenceCount":50,"citationCount":3,"influentialCitationCount":0,"publicationDate":"01/05/2023","authors":"Zhaorui Zhu,Hongyi Yu,Caiyao Shen,Jian-ping Du,Zhixiang Shen,Zhenyu Wang","id":"af6c6941acecfb23d899cd5266efdf2e27463f12","summary":"This paper proposes a sequential decoding algorithm for the robust reception of sources with natural redundancy over the AWGN channel and eliminates the requirement of high-cost and accurate labels, which paves a new way for communication receiver design.","score":1},{"url":"https://www.semanticscholar.org/paper/b4790ca1a967b44c3028b73c0c00d501fcd81728","title":"Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages","venue":"European Association for Machine Translation Conferences/Workshops","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/05/2023","authors":"Sonal Sannigrahi,Rachel Bawden","id":"b4790ca1a967b44c3028b73c0c00d501fcd81728","summary":"It is found that transliteration does not give pronounced improvements and the analysis suggests that the multilingual MT models trained on original scripts are already robust to cross-script differences even for relatively low-resource languages.","score":1},{"url":"https://www.semanticscholar.org/paper/ea8197cb357af6f89e8b6e5548897236a24719b1","title":"What is the best recipe for character-level encoder-only modelling?","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/05/2023","authors":"Kris Cao","id":"ea8197cb357af6f89e8b6e5548897236a24719b1","summary":"This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level, and finds that the best performing character-level model exceeds the performance of a token-based model trained with the same settings on the same data.","score":1},{"url":"https://www.semanticscholar.org/paper/2183c88e9056e931b07d48f1dc44360785952073","title":"SoundStorm: Efficient Parallel Audio Generation","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":24,"influentialCitationCount":4,"publicationDate":"16/05/2023","authors":"Zalán Borsos,Matthew Sharifi,Damien Vincent,E. Kharitonov,Neil Zeghidour,M. Tagliasacchi","id":"2183c88e9056e931b07d48f1dc44360785952073","summary":"The ability of the model to scale audio generation to longer sequences is demonstrated by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.","score":1},{"url":"https://www.semanticscholar.org/paper/879a7f5abdb7ab803d48172d4f0830965f989d46","title":"Language Model Tokenizers Introduce Unfairness Between Languages","venue":"Neural Information Processing Systems","year":2023,"referenceCount":117,"citationCount":22,"influentialCitationCount":3,"publicationDate":"17/05/2023","authors":"Aleksandar Petrov,Emanuele La Malfa,Philip H. S. Torr,Adel Bibi","id":"879a7f5abdb7ab803d48172d4f0830965f989d46","summary":"It is shown how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked, and the case is made that future language models should be trained using multilingually fair subword tokenizers.","score":1},{"url":"https://www.semanticscholar.org/paper/39bca01efce8765f0a5d3a8981bc30d56f196b96","title":"XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":101,"citationCount":16,"influentialCitationCount":1,"publicationDate":"19/05/2023","authors":"Sebastian Ruder,J. Clark,Alexander Gutkin,Mihir Kale,Min Ma,M. Nicosia,Shruti Rijhwani,Parker Riley,J. A. Sarr,Xinyi Wang,J. Wieting,Nitish Gupta,Anna Katanova,Christo Kirov,Dana L. Dickinson,Brian Roark,Bidisha Samanta,Connie Tao,David Ifeoluwa Adelani,Vera Axelrod,Isaac Caswell,Colin Cherry,Dan Garrette,R. Ingle,Melvin Johnson,Dmitry Panteleev,P. Talukdar","id":"39bca01efce8765f0a5d3a8981bc30d56f196b96","summary":"The proposed XTREME-UP benchmark evaluates the capabilities of language models across 88 under-represented languages over 9 key user-centric technologies including ASR, OCR, MT, and information access tasks that are of general utility.","score":1},{"url":"https://www.semanticscholar.org/paper/7c217cc7524251f42887438834912e06129c3299","title":"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis","venue":"Neural Information Processing Systems","year":2023,"referenceCount":43,"citationCount":22,"influentialCitationCount":1,"publicationDate":"22/05/2023","authors":"Fuzhao Xue,Yao Fu,Wangchunshu Zhou,Zangwei Zheng,Yang You","id":"7c217cc7524251f42887438834912e06129c3299","summary":"This study empirically investigates the consequences of repeating pre-training data, revealing that the model is susceptible to overfitting, leading to multi-epoch degradation, and discovers that leveraging mixture-of-experts (MoE) enables cost-effective and efficient hyper-parameter tuning for computationally intensive dense LLMs with comparable trainable parameters.","score":1},{"url":"https://www.semanticscholar.org/paper/36dfcdc43664f03b15e5e03373a9d46728672e28","title":"mPLM-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models","venue":"","year":2023,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Peiqin Lin,Chengzhi Hu,Zheyu Zhang,André F. T. Martins,Hinrich Schütze","id":"36dfcdc43664f03b15e5e03373a9d46728672e28","summary":"This study proposes mPLMSim, a language similarity measure that induces the similarities across languages from mPLMs using multi-parallel corpora and shows that mPLM-Sim exhibits moderately high correlations with linguistic similarity measures, such as lexicostatistics, genealogical language family, and geographical sprachbund.","score":1},{"url":"https://www.semanticscholar.org/paper/c60736e61f8961ec535ecfdc6f0398925d34d0b8","title":"Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":45,"citationCount":4,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Elizabeth Salesky,Neha Verma,Philipp Koehn,Matt Post","id":"c60736e61f8961ec535ecfdc6f0398925d34d0b8","summary":"This work introduces and demonstrates how to effectively train multilingual machine translation models with pixel representations and explores various properties of pixel representations such as parameter sharing within and across scripts to better understand where they lead to positive transfer.","score":1},{"url":"https://www.semanticscholar.org/paper/d7a379254ac2dee2e31dfdeedb440176f0285aa5","title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding","venue":"Annual Meeting of the Association for Computational Linguistics","year":2023,"referenceCount":54,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Li Sun,F. Luisier,K. Batmanghelich,D. Florêncio,Changrong Zhang","id":"d7a379254ac2dee2e31dfdeedb440176f0285aa5","summary":"This work introduces a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level, and demonstrates that this hierarchical model is robust to textual corruption and domain shift.","score":1}]}
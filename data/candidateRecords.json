{"papers":[{"url":"https://www.semanticscholar.org/paper/9ccad0208b50042d8378b77700146a98bd22ea3f","title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation","venue":"Special Interest Group on Computational Morphology and Phonology Workshop","year":2022,"referenceCount":81,"citationCount":6,"influentialCitationCount":0,"publicationDate":"15/06/2022","authors":"Khuyagbaatar Batsuren,Gábor Bella,Aryaman Arora,Viktor Martinovi'c,Kyle Gorman,Zdenvek vZabokrtsk'y,A. Ganbold,vS'arka Dohnalov'a,Magda vSevvc'ikov'a,Katevrina Pelegrinov'a,Fausto Giunchiglia,Ryan Cotterell,Ekaterina Vylomova","id":"9ccad0208b50042d8378b77700146a98bd22ea3f","summary":"","score":4},{"url":"https://www.semanticscholar.org/paper/e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization","venue":"International Conference on Learning Representations","year":2021,"referenceCount":69,"citationCount":55,"influentialCitationCount":11,"publicationDate":"23/06/2021","authors":"Yi Tay,V. Tran,Sebastian Ruder,Jai Gupta,Hyung Won Chung,Dara Bahri,Zhen Qin,Simon Baumgartner,Cong Yu,Donald Metzler","id":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","summary":"This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion and paves the way for highly performant token-free models that are trained completely end-to-end.","score":3},{"url":"https://www.semanticscholar.org/paper/c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":69,"citationCount":14,"influentialCitationCount":0,"publicationDate":"12/05/2022","authors":"Jonas Pfeiffer,Naman Goyal,Xi Victoria Lin,Xian Li,James Cross,Sebastian Riedel,Mikel Artetxe","id":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","summary":"This work introduces language-specific modules of their Cross-lingual Modular models from the start, which allows them to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant.","score":3},{"url":"https://www.semanticscholar.org/paper/265ebfc1074c73917233dbecad802c0c64921a1c","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Gregor Geigle,Chen Cecilia Liu,Jonas Pfeiffer,Iryna Gurevych","id":"265ebfc1074c73917233dbecad802c0c64921a1c","summary":"This work evaluates whether the information stored within different VEs is complementary, i.e. if providing the model with features from multiple VEs can improve the performance on a target task, and suggests that diverse VEs complement each other, resulting in improved downstream V+L task performance.","score":3},{"url":"https://www.semanticscholar.org/paper/69b8db33a7ea7366e48c465b6f35a8d7f8f1a3b5","title":"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/01/2023","authors":"Davis Liang,Hila Gonen,Yuning Mao,Rui Hou,Naman Goyal,Marjan Ghazvininejad,Luke Zettlemoyer,Madian Khabsa","id":"69b8db33a7ea7366e48c465b6f35a8d7f8f1a3b5","summary":"A new approach for scaling to very large multilingual vocabularies is introduced by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufﬁcient coverage for each individual language.","score":3},{"url":"https://www.semanticscholar.org/paper/3d3f8399d625238fddb366697acb73446129d65c","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":81,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Abbas Ghaddar,Yimeng Wu,Sunyam Bagga,Ahmad Rashid,Khalil Bibi,Mehdi Rezagholizadeh,Chao Xing,Yasheng Wang,Xinyu Duan,Zhefeng Wang,Baoxing Huai,Xin Jiang,Qun Liu,P. Langlais","id":"3d3f8399d625238fddb366697acb73446129d65c","summary":"This work addresses two major problems in existing Arabic PLMs that limit the progress of the Arabic NLU and NLG fields and releases three new Arabic BERT-style models, and achieves a new state-of-the-art performance on discriminative and generative ArabicNLU andNLG tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/8f248b5476666e700390f7f8ff6ca923f97d726c","title":"Advancing protein language models with linguistics: a roadmap for improved interpretability","venue":"ArXiv","year":2022,"referenceCount":137,"citationCount":3,"influentialCitationCount":0,"publicationDate":"03/07/2022","authors":"Mai Ha Vu,R. Akbar,Philippe A. Robert,B. Swiatczak,V. Greiff,G. K. Sandve,Dag Trygve Tryslew Haug","id":"8f248b5476666e700390f7f8ff6ca923f97d726c","summary":"It is argued that guidance drawn from linguistics, a field specialized in analytical rule extraction from natural language data, can aid with building more interpretable protein LMs that have learned relevant domain-specific rules.","score":3},{"url":"https://www.semanticscholar.org/paper/19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":7,"influentialCitationCount":2,"publicationDate":"07/03/2022","authors":"Gabriele Sarti,M. Nissim","id":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","summary":"The monolingual IT5 models are found to provide the best scale-to-performance ratio across tested models, consistently outperforming their multilingual counterparts and setting a new state-of-the-art for most Italian conditional language generation tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a","title":"Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation","venue":"Findings","year":2022,"referenceCount":16,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/03/2022","authors":"Sho Takase,Tatsuya Hiraoka,Naoaki Okazaki","id":"75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a","summary":"An inference strategy that approximates the marginalized likelihood by using multiple segmentations including the most plausible segmentation and several sampled segmentations that improves the performance of models trained with subword regularization in low-resource machine translation tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","title":"A Primer on Pretrained Multilingual Language Models","venue":"ArXiv","year":2021,"referenceCount":138,"citationCount":17,"influentialCitationCount":3,"publicationDate":"01/07/2021","authors":"Sumanth Doddapaneni,Gowtham Ramesh,Anoop Kunchukuttan,Pratyush Kumar,Mitesh M. Khapra","id":"d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","summary":"A review of the existing literature covering the above broad areas of research pertaining to MLLMs and some promising directions of future research are recommended.","score":2},{"url":"https://www.semanticscholar.org/paper/bc39d16c108057e062ad6f1d0e8154df52cafc6a","title":"CMU’s Machine Translation System for IWSLT 2019","venue":"International Workshop on Spoken Language Translation","year":2019,"referenceCount":31,"citationCount":3,"influentialCitationCount":0,"publicationDate":2019,"authors":"Tejas Srinivasan,Ramon Sanabria,Florian Metze","id":"bc39d16c108057e062ad6f1d0e8154df52cafc6a","summary":"Block Multitask Learning (BMTL) is presented, a novel NMT architecture that predicts multiple targets of different granularities simulta- neously, removing the need to search for the optimal subword segmentation strategy.","score":2},{"url":"https://www.semanticscholar.org/paper/fec6def294027a2ce9094267ce7b7d57f78daf74","title":"Multitask Learning For Different Subword Segmentations In Neural Machine Translation","venue":"International Workshop on Spoken Language Translation","year":2019,"referenceCount":32,"citationCount":3,"influentialCitationCount":0,"publicationDate":"27/10/2019","authors":"Tejas Srinivasan,Ramon Sanabria,Florian Metze","id":"fec6def294027a2ce9094267ce7b7d57f78daf74","summary":"Block Multitask Learning (BMTL), a novel NMT architecture that predicts multiple targets of different granularities simultaneously, removing the need to search for the optimal segmentation strategy, is presented.","score":2},{"url":"https://www.semanticscholar.org/paper/debb3877b778eeb8689729d37e2b90f9f000d877","title":"Neural Machine Translation with Imbalanced Classes","venue":"ArXiv","year":2020,"referenceCount":32,"citationCount":3,"influentialCitationCount":0,"publicationDate":"05/04/2020","authors":"Thamme Gowda,Jonathan May","id":"debb3877b778eeb8689729d37e2b90f9f000d877","summary":"This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and investigates the effect of class imbalance on NMT.","score":2},{"url":"https://www.semanticscholar.org/paper/e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order","venue":"ArXiv","year":2021,"referenceCount":72,"citationCount":9,"influentialCitationCount":1,"publicationDate":2021,"authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","summary":"The insensitivity of natural language models to word-order is investigated by quantifying perturbations and analysing their effect on neural models’ performance on language understanding tasks in GLUE benchmark and it is found that neural language models — pretrained and non-pretrained Transformers, LSTMs, and Convolutional architectures — require local ordering more than the global ordering of tokens.","score":2},{"url":"https://www.semanticscholar.org/paper/dca4128a33ca22c02031b5c0c28548a0df022d80","title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding","venue":"","year":2021,"referenceCount":56,"citationCount":6,"influentialCitationCount":0,"publicationDate":2021,"authors":"Abhik Bhattacharjee,Tahmid Hasan,Kazi Samin,Md. Saiful Islam,M. S. Rahman,Anindya Iqbal,Rifat Shahriyar","id":"dca4128a33ca22c02031b5c0c28548a0df022d80","summary":"The Embedding Barrier is introduced, a phenomenon that limits the monolingual performance of multilingual models on low-resource languages having unique typologies and a straightforward solution by transcribing languages to a common script is proposed, which can effectively improve the performance of a multilingual model for the Bangla language.","score":2},{"url":"https://www.semanticscholar.org/paper/a20a802839d72bee1c85f4a1cb77addadacb2179","title":"Specializing Multilingual Language Models: An Empirical Study","venue":"MRL","year":2021,"referenceCount":72,"citationCount":11,"influentialCitationCount":0,"publicationDate":"16/06/2021","authors":"Ethan C. Chau,Noah A. Smith","id":"a20a802839d72bee1c85f4a1cb77addadacb2179","summary":"These evaluations on part-of-speech tagging, universal dependency parsing, and named entity recognition in nine diverse low-resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low- resource settings.","score":2},{"url":"https://www.semanticscholar.org/paper/31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning","venue":"Journal of Big Data","year":2021,"referenceCount":125,"citationCount":66,"influentialCitationCount":3,"publicationDate":"29/06/2021","authors":"Connor Shorten,T. Khoshgoftaar,B. Furht","id":"31852f9fc732c0868af12d631c72693702d80521","summary":"The major motifs of Data Augmentation are summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form.","score":2},{"url":"https://www.semanticscholar.org/paper/91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU","venue":"Findings","year":2021,"referenceCount":70,"citationCount":4,"influentialCitationCount":0,"publicationDate":"29/07/2021","authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","summary":"It is empirically shown that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.","score":2},{"url":"https://www.semanticscholar.org/paper/9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs","venue":"International Conference on Learning Representations","year":2021,"referenceCount":105,"citationCount":176,"influentialCitationCount":28,"publicationDate":"30/07/2021","authors":"Andrew Jaegle,Sebastian Borgeaud,Jean-Baptiste Alayrac,Carl Doersch,Catalin Ionescu,David Ding,Skanda Koppula,Andrew Brock,Evan Shelhamer,Olivier J. H'enaff,M. Botvinick,A. Zisserman,Oriol Vinyals,João Carreira","id":"9933a5af7895354087baf6c96b64dc8a8973eaed","summary":"The primary focus of this work is generality, rather than speed on images, and Perceiver IO uses comparable FLOPs to attention-based image classiﬁcation models, especially for the more compact conﬂguration B pretrained on JFT.","score":2},{"url":"https://www.semanticscholar.org/paper/231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models","venue":"NAACL-HLT","year":2021,"referenceCount":38,"citationCount":13,"influentialCitationCount":1,"publicationDate":"26/10/2021","authors":"Piotr Nawrot,Szymon Tworkowski,Michal Tyrolski,Lukasz Kaiser,Yuhuai Wu,Christian Szegedy,H. Michalewski","id":"231e768f0cd280faa0f725bb353262cb4fed08d1","summary":"Hourglass is a hierarchical Transformer language model that sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efﬁciency on the widely studied enwik8 benchmark.","score":2},{"url":"https://www.semanticscholar.org/paper/7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models","venue":"International Conference on Agents and Artificial Intelligence","year":2022,"referenceCount":51,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/01/2022","authors":"Konstantin Todorov,Giovanni Colavizza","id":"7e3081b0d698f8abf16dee626d782f3339482fe7","summary":"It is found that OCR noise poses a significant obstacle to language modelling, with language models increasingly diverging from their noiseless targets as OCR quality lowers, and simpler models including PPMI and Word2Vec consistently outperform transformer-based models in this respect.","score":2},{"url":"https://www.semanticscholar.org/paper/54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/08/2022","authors":"Jan Jezabek,A. Singh","id":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","summary":"A novel method of retroactively adding resilience to misspellings to transformer-based NLP models without the need for re-training of the original NLP model is proposed, which significantly reduces the cost needed to evaluate a model’s resilience to adversarial attacks.","score":2},{"url":"https://www.semanticscholar.org/paper/110250df2a6ca0e0e609eaa800a21c17abeedd77","title":"NLP for Language Varieties of Italy: Challenges and the Path Forward","venue":"ArXiv","year":2022,"referenceCount":85,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/09/2022","authors":"Alan Ramponi","id":"110250df2a6ca0e0e609eaa800a21c17abeedd77","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages","venue":"AACL","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar","id":"9a5b2dc77bda19759df8481aaf283da353ac7e77","summary":"This work replicates a study on the importance of local structure, and the relative unimportance of global structure, in a multilingual setting and finds that the phenomenon observed on the English language broadly translates to over 120 languages, with a few caveats.","score":2},{"url":"https://www.semanticscholar.org/paper/e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy","venue":"IEEE Access","year":2022,"referenceCount":104,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/11/2022","authors":"Philipp Siebers,Christian Janiesch,Patrick Zschech","id":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","summary":"This work contributes threefold to this lack of compilation, composition, and systematization by providing a survey of current approaches, arranging them in a genealogy, and conceptualizing a taxonomy of text representation methods to examine and explain the state-of-the-art.","score":2},{"url":"https://www.semanticscholar.org/paper/134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":28,"citationCount":2,"influentialCitationCount":0,"publicationDate":"06/09/2022","authors":"Doan Nam Long Vu,N. Moosavi,Steffen Eger","id":"134e4d72e23bca51e290db171d063989883020f4","summary":"It is shown that an embedding-based metric that has the highest correlation with human evaluations on a standard benchmark can have the lowest correlation if the amount of input noise or unknown tokens increases, and the highest robustness is achieved when using character-level embeddings, instead of token-based embeddments, from the first layer of the pretrained model.","score":2},{"url":"https://www.semanticscholar.org/paper/2ffbe6040369a82d5a003c2bb835e221c9d2f896","title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":50,"citationCount":8,"influentialCitationCount":1,"publicationDate":"02/09/2021","authors":"Chantal Amrhein,Rico Sennrich","id":"2ffbe6040369a82d5a003c2bb835e221c9d2f896","summary":"It is found that learning to analyse and generate morphologically complex surface representations is still challenging, especially for nonconcatenative morphological phenomena like reduplication or vowel harmony and for rare word stems.","score":2},{"url":"https://www.semanticscholar.org/paper/7cb4b8406255d0f8115afa23d8efea1bb780cfb8","title":"On the Compatibility of Tokenizations Across Languages","venue":"","year":2021,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Antonis Maronikolakis,Philipp Dufter,Hinrich Schütze","id":"7cb4b8406255d0f8115afa23d8efea1bb780cfb8","summary":"It is shown that the compatibility measure proposed allows the system designer to create vocabularies across languages that are compatible – a desideratum that so far has been neglected in multilingual models.","score":2},{"url":"https://www.semanticscholar.org/paper/3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","title":"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class","venue":"ACM Transactions on Software Engineering and Methodology","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/07/2022","authors":"Luke Dramko,Jeremy Lacomis,Pengcheng Yin,Edward J. Schwartz,Miltiadis Allamanis,Graham Neubig,Bogdan Vasilescu,Claire Le Goues","id":"3fdc5601bc3294c274c5fb8e0fa7efc636c00ff2","summary":"This work investigates how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains, and evaluates DIRE’s overall performance without respect to data quality.","score":2},{"url":"https://www.semanticscholar.org/paper/e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","title":"Checks and Strategies for Enabling Code-Switched Machine Translation","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Thamme Gowda,Mozhdeh Gheini,Jonathan May","id":"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7","summary":"This work explores multilingual NMT models’ ability to handle code-switched text, and proposes checks to measure switching capability and investigates simple and effective data augmentation methods that can enhance an NMT model’s ability to support code- Switched text.","score":2},{"url":"https://www.semanticscholar.org/paper/9d83941a205e31d394f614424cdc553cea9e35f9","title":"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22","venue":"Conference on Machine Translation","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":1,"publicationDate":"02/12/2022","authors":"Laia Tarrés,Gerard I. Gállego,Xavier Giró-i-Nieto,Jordi Torres","id":"9d83941a205e31d394f614424cdc553cea9e35f9","summary":"The system developed at the Universitat Politècnica de Catalunya for the Workshop on Machine Translation 2022 Sign Language Translation Task, in particular, for the sign-to-text direction is described, with poor results for both the baseline and the system, and thus, the unreliability of the findings.","score":2},{"url":"https://www.semanticscholar.org/paper/8a1c54f6e2c5f1453fddb9f15e769a099286f677","title":"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey","venue":"Journal of Artificial Intelligence Research","year":2021,"referenceCount":366,"citationCount":21,"influentialCitationCount":0,"publicationDate":"14/04/2021","authors":"Danielle Saunders","id":"8a1c54f6e2c5f1453fddb9f15e769a099286f677","summary":"This work surveys approaches to domain adaptation for NMT, particularly where a system may need to translate across multiple domains, and divides techniques into those revolving around data selection or generation, model architecture, parameter adaptation procedure, and inference procedure.","score":2},{"url":"https://www.semanticscholar.org/paper/b162638cd42b65e6add199b0b34f1b375070fc7c","title":"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models","venue":"AACL/IJCNLP","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2022","authors":"Syrielle Montariol,Arij Riabi,Djamé Seddah","id":"b162638cd42b65e6add199b0b34f1b375070fc7c","summary":"It is shown how hate speech detection models benefit from a cross-lingual knowledge proxy brought by auxiliary tasks fine-tuning and highlight these tasks’ positive impact on bridging the hate speech linguistic and cultural gap between languages.","score":2},{"url":"https://www.semanticscholar.org/paper/327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","venue":"ArXiv","year":2022,"referenceCount":157,"citationCount":42,"influentialCitationCount":7,"publicationDate":"09/11/2022","authors":"Teven Le Scao,Angela Fan,Christopher Akiki,Elizabeth-Jane Pavlick,Suzana Ili'c,Daniel Hesslow,Roman Castagn'e,A. Luccioni,Franccois Yvon,Matthias Gallé,J. Tow,Alexander M. Rush,Stella Rose Biderman,Albert Webson,Pawan Sasanka Ammanamanchi,Thomas Wang,Benoît Sagot,Niklas Muennighoff,Albert Villanova del Moral,Olatunji Ruwase,Rachel Bawden,Stas Bekman,Angelina McMillan-Major,Iz Beltagy,Huu Nguyen,Lucile Saulnier,Samson Tan,Pedro Ortiz Suarez,Victor Sanh,Hugo Laurenccon,Yacine Jernite,Julien Launay,Margaret Mitchell,Colin Raffel,Aaron Gokaslan,Adi Simhi,Aitor Soroa Etxabe,Alham Fikri Aji,Amit Alfassy,Anna Rogers,Ariel Kreisberg Nitzav,Canwen Xu,Chenghao Mou,Chris C. Emezue,Christopher Klamm,Colin Leong,Daniel Alexander van Strien,David Ifeoluwa Adelani,Dragomir R. Radev,Eduardo G. Ponferrada,Efrat Levkovizh,Ethan Kim,E. Natan,F. Toni,Gérard Dupont,Germán Kruszewski,Giada Pistilli,Hady ElSahar,Hamza Benyamina,Hieu Tran,Ian Yu,Idris Abdulmumin,Isaac Johnson,Itziar Gonzalez-Dios,Javier de la Rosa,Jenny Chim,Jesse Dodge,Jian Zhu,Jonathan Chang,Jorg Frohberg,Josephine L. Tobing,J. Bhattacharjee,Khalid Almubarak,Kimbo Chen,Kyle Lo,Leandro von Werra,Leon Weber,Long Phan,Loubna Ben Allal,Ludovic Tanguy,Manan Dey,M. Muñoz,Maraim Masoud,Mar'ia Grandury,Mario vSavsko,Max Huang,Maximin Coavoux,Mayank Singh,Mike Tian-Jian Jiang,Minh Chien Vu,M. A. Jauhar,Mustafa Ghaleb,Nishant Subramani,Nora Kassner,Nurulaqilla Khamis,Olivier Nguyen,Omar Espejel,Ona de Gibert,Paulo Villegas,Peter Henderson,Pierre Colombo,Priscilla Amuok,Quentin Lhoest,Rheza Harliman,Rishi Bommasani,R. L'opez,Rui Ribeiro,Salomey Osei,Sampo Pyysalo,Sebastian Nagel,Shamik Bose,Shamsuddeen Hassan Muhammad,Shanya Sharma,S. Longpre,Somaieh Nikpoor,Stanislav Silberberg,S. Pai,S. Zink,Tiago Timponi Torrent,Timo Schick,Tristan Thrush,V. Danchev,Vassilina Nikoulina,Veronika Laippala,Violette Lepercq,V. Prabhu,Zaid Alyafeai,Zeerak Talat,Arun Raja,Benjamin Heinzerling,Chenglei Si,Elizabeth Salesky,Sabrina J. Mielke,Wilson Y. Lee,Abheesht Sharma,Andrea Santilli,Antoine Chaffin,Arnaud Stiegler,Debajyoti Datta,Eliza Szczechla,Gunjan Chhablani,Han Wang,Harshit Pandey,Hendrik Strobelt,Jason Alan Fries,Jos Rozen,Leo Gao,Lintang Sutawika,M Saiful Bari,Maged S. Al-shaibani,Matteo Manica,Nihal V. Nayak,Ryan Teehan,Samuel Albanie,Sheng Shen,Srulik Ben-David,Stephen H. Bach,Taewoon Kim,T. Bers,Thibault Févry,Trishala Neeraj,Urmish Thakker,Vikas Raunak,Xiang Tang,Zheng Xin Yong,Zhiqing Sun,Shaked Brody,Y. Uri,Hadar Tojarieh,Adam Roberts,Hyung Won Chung,Jaesung Tae,Jason Phang,Ofir Press,Conglong Li,D. Narayanan,Hatim Bourfoune,J. Casper,Jeff Rasley,Max Ryabinin,Mayank Mishra,Minjia Zhang,M. Shoeybi,Myriam Peyrounette,N. Patry,Nouamane Tazi,Omar Sanseviero,Patrick von Platen,Pierre Cornette,Pierre Franccois Lavall'ee,R. Lacroix,Samyam Rajbhandari,Sanchit Gandhi,Shaden Smith,S. Requena,Suraj Patil,Tim Dettmers,Ahmed Baruwa,Amanpreet Singh,Anastasia Cheveleva,Anne-Laure Ligozat,Arjun Subramonian,Aur'elie N'ev'eol,Charles Lovering,Daniel H Garrette,D. Tunuguntla,Ehud Reiter,Ekaterina Taktasheva,E. Voloshina,Eli Bogdanov,Genta Indra Winata,Hailey Schoelkopf,Jan-Christoph Kalo,Jekaterina Novikova,J. Forde,Jordan Clive,Jungo Kasai,Ken Kawamura,Liam Hazan,Marine Carpuat,Miruna Clinciu,Najoung Kim,Newton Cheng,Oleg Serikov,Omer Antverg,Oskar van der Wal,Rui Zhang,Ruochen Zhang,Sebastian Gehrmann,S. Pais,Tatiana Shavrina,Thomas Scialom,Tian Yun,Tomasz Limisiewicz,Verena Rieser,Vitaly Protasov,V. Mikhailov,Yada Pruksachatkun,Yonatan Belinkov,Zachary Bamberger,Zdenvek Kasner,Alice Rueda,Amanda Pestana,A. Feizpour,Ammar Khan,Amy Faranak,A. Santos,A. Hevia,Antigona Unldreaj,Arash Aghagol,Arezoo Abdollahi,A. Tammour,Azadeh HajiHosseini,Bahareh Behroozi,B. Ajibade,B. Saxena,Carlos Muñoz Ferrandis,Danish Contractor,D. Lansky,Davis David,Douwe Kiela,D. A. Nguyen,Edward Tan,Emily Baylor,Ezinwanne Ozoani,Fatim T Mirza,Frankline Ononiwu,Habib Rezanejad,H.A. Jones,Indrani Bhattacharya,Irene Solaiman,Irina Sedenko,I. Nejadgholi,J. Passmore,Joshua Seltzer,Julio Bonis Sanz,Karen Fort,L. Dutra,Mairon Samagaio,Maraim Elbadri,M. Mieskes,Marissa Gerchick,Martha Akinlolu,Michael McKenna,Mike Qiu,M. Ghauri,Mykola Burynok,Nafis Abrar,Nazneen Rajani,Nour Elkott,N. Fahmy,O. Samuel,Ran An,R. Kromann,Ryan Hao,S. Alizadeh,Sarmad Shubber,Silas L. Wang,Sourav Roy,S. Viguier,Thanh-Cong Le,Tobi Oyebade,T. Le,Yoyo Yang,Z. Nguyen,Abhinav Ramesh Kashyap,Alfredo Palasciano,A. Callahan,Anima Shukla,Antonio Miranda-Escalada,A. Singh,Benjamin Beilharz,Bo Wang,C. Brito,Chenxi Zhou,Chirag Jain,Chuxin Xu,Clémentine Fourrier,Daniel Le'on Perin'an,Daniel Molano,Dian Yu,Enrique Manjavacas,Fabio Barth,Florian Fuhrimann,Gabriel Altay,Giyaseddin Bayrak,Gully A. Burns,Helena U. Vrabec,I. Bello,Isha Dash,J. Kang,John Giorgi,J. Golde,J. Posada,Karthi Sivaraman,Lokesh Bulchandani,Lu Liu,Luisa Shinzato,Madeleine Hahn de Bykhovetz,Maiko Takeuchi,Marc Pàmies,M. A. Castillo,Marianna Nezhurina,Mario Sanger,M. Samwald,Michael Cullan,Michael Weinberg,M. Wolf,Mina Mihaljcic,Minna Liu,M. Freidank,Myungsun Kang,Natasha Seelam,N. Dahlberg,N. Broad,N. Muellner,Pascale Fung,Patricia Haller,R. Chandrasekhar,R. Eisenberg,Robert Martin,Rodrigo L. Canalli,Rosaline Su,Ruisi Su,Samuel Cahyawijaya,Samuele Garda,Shlok S Deshmukh,Shubhanshu Mishra,Sid Kiblawi,Simon Ott,Sinee Sang-aroonsiri,Srishti Kumar,Stefan Schweter,S. Bharati,T. A. Laud,Th'eo Gigant,Tomoya Kainuma,Wojciech Kusa,Yanis Labrak,Yashasvi Bajaj,Y. Venkatraman,Yifan Xu,Ying Xu,Yun-chao Xu,Z. Tan,Zhongli Xie,Zifan Ye,M. Bras,Younes Belkada,Thomas Wolf","id":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","summary":"BLOOM is a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers and achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning.","score":2},{"url":"https://www.semanticscholar.org/paper/8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Najoung Kim,Tal Linzen,P. Smolensky","id":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","summary":"It is argued that exposure to pre-training data may break distributional control across training and test to gauge compositional generalization, where certain lexical items only occur in limited contexts during training.","score":2},{"url":"https://www.semanticscholar.org/paper/cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","title":"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sidsel Boldsen,Manex Agirrezabal,Nora Hollenstein","id":"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe","summary":"This cross-lingual analysis shows that textual character representations correlate strongly with sound representations for languages using an alphabetic script, while shape correlates with featural scripts.","score":2},{"url":"https://www.semanticscholar.org/paper/8929066ce924696f960512c92a720c70bba65586","title":"On Language Spaces, Scales and Cross-Lingual Transfer of UD Parsers","venue":"Conference on Computational Natural Language Learning","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"T. Samardžić,Ximena Gutierrez-Vasques,Rob van der Goot,Max Müller-Eberstein,Olga Pelloni,Barbara Plank","id":"8929066ce924696f960512c92a720c70bba65586","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/3895423c2e2ed608ba1439a6a21b66e43f5ef8b8","title":"Subword Evenness (SuE) as a Predictor of Cross-lingual Transfer to Low-resource Languages","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Olga Pelloni,Anastassia Shaitarova,T. Samardžić","id":"3895423c2e2ed608ba1439a6a21b66e43f5ef8b8","summary":"It is shown that languages written in non-Latin and non-alphabetic scripts (mostly Asian languages) are the best choices for improving performance on the task of Masked Language Modelling (MLM) in a diverse set of 30 low-resource languages and that the success of the transfer is well predicted by the authors' novel measure of Subword Evenness (SuE).","score":2},{"url":"https://www.semanticscholar.org/paper/2ac5442a32988f86730e460b3198f475592ae410","title":"Improving Low-Resource Languages in Pre-Trained Multilingual Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Viktor Hangya,Hossain Shaikh Saadi,Alexander Fraser","id":"2ac5442a32988f86730e460b3198f475592ae410","summary":"This work proposes an unsupervised approach to improve the cross-lingual representations of low-resource languages by bootstrapping word translation pairs from monolingual corpora and using them to improve language alignment in pre-trained language models.","score":2},{"url":"https://www.semanticscholar.org/paper/ae7fc32df9401a86353185515f4fd5e2020ac922","title":"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations","venue":"ArXiv","year":2021,"referenceCount":40,"citationCount":4,"influentialCitationCount":1,"publicationDate":2021,"authors":"Theodore Jiang,Li Fang,Kai Wang","id":"ae7fc32df9401a86353185515f4fd5e2020ac922","summary":"This study introduces MutFormer, a transformer-based model for the prediction of pathogenic missense mutations, using reference and mutated amino acid sequences from the human genome as the features, and shows that MutFormer outperforms a variety of existing tools in pathogenicity prediction.","score":1},{"url":"https://www.semanticscholar.org/paper/e873ddaf58ff92ae0492983cf57221fb25837f4e","title":"Strategies in subword tokenization: humans vs. algorithms","venue":"","year":2021,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"","id":"e873ddaf58ff92ae0492983cf57221fb25837f4e","summary":"A new method to an- 010 alyze subword segmentation strategies relying on a spatial analysis of the distribution of sub- 012 words’ lengths is proposed, which shows that humans tend to balance creativity and consistency, while algorithms tend to be either strongly biased or inconsistent.","score":1},{"url":"https://www.semanticscholar.org/paper/7175caf7568d46c857380d0e5b64653819d5cc45","title":"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization","venue":"Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)","year":2021,"referenceCount":75,"citationCount":13,"influentialCitationCount":0,"publicationDate":2021,"authors":"R. Goot,Alan Ramponi,A. Zubiaga,Barbara Plank,Benjamin Muller,I. Roncal,Nikola Ljubesic,Özlem Çetinoğlu,Rahmad Mahendra,Talha Çolakoglu,Timothy Baldwin,Tommaso Caselli,Wladimir Sidorenko,Bruno Kessler","id":"7175caf7568d46c857380d0e5b64653819d5cc45","summary":"The MULTILEXNORM shared task provides the largest publicly available multilingual lexical normalization benchmark including 12 language variants and proposes a homogenized evaluation setup with both intrinsic and extrinsic evaluation.","score":1},{"url":"https://www.semanticscholar.org/paper/d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention","venue":"International Conference on Machine Learning","year":2021,"referenceCount":52,"citationCount":13,"influentialCitationCount":2,"publicationDate":"09/05/2021","authors":"Noam Wies,Yoav Levine,Daniel Jannai,A. Shashua","id":"d13a0c8d49cb268d8d245925baee0316c1fe1875","summary":"This work empirically demonstrates the existence of an embedding rank bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains.","score":1},{"url":"https://www.semanticscholar.org/paper/06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer","venue":"ArXiv","year":2021,"referenceCount":33,"citationCount":27,"influentialCitationCount":4,"publicationDate":"30/06/2021","authors":"Iulia Turc,Kenton Lee,Jacob Eisenstein,Ming-Wei Chang,Kristina Toutanova","id":"06431546c21d7c2528aaa170c2e1078e0a82d12e","summary":"English is compared against other transfer languages for fine-tuning, and other high-resource languages such as German and Russian often transfer more effectively, especially when the set of target languages is diverse or unknown a priori.","score":1},{"url":"https://www.semanticscholar.org/paper/c41819413725668fbf8e3ca1a0bcaf7fb691e582","title":"How Optimal is Greedy Decoding for Extractive Question Answering?","venue":"ArXiv","year":2021,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":"12/08/2021","authors":"Or Castel,Ori Ram,Avia Efrat,Omer Levy","id":"c41819413725668fbf8e3ca1a0bcaf7fb691e582","summary":"The results suggest that pretrained language models are so good at adapting to extractive question answering, that it is often enough to tune on a small training set for the greedy algorithm to emulate the optimal decoding strategy.","score":1},{"url":"https://www.semanticscholar.org/paper/89a4f4d1f8c93da85d829a1acfe8cafea2b50c00","title":"Transfer Learning for Multi-lingual Tasks - a Survey","venue":"ArXiv","year":2021,"referenceCount":108,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/08/2021","authors":"Amir Reza Jafari,Behnam Heidary,R. Farahbakhsh,Mostafa Salehi,M. Jalili","id":"89a4f4d1f8c93da85d829a1acfe8cafea2b50c00","summary":"This survey provides a comprehensive overview of the existing literature with a focus on transfer learning techniques in multilingual tasks and identifies potential opportunities for further research in this domain.","score":1},{"url":"https://www.semanticscholar.org/paper/972808b92159a782f5ca1c1ab3a8ed3867acfb2d","title":"mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset","venue":"ArXiv","year":2021,"referenceCount":51,"citationCount":21,"influentialCitationCount":5,"publicationDate":"31/08/2021","authors":"L. Bonifacio,Israel Campiotti,R. Lotufo,Rodrigo Nogueira","id":"972808b92159a782f5ca1c1ab3a8ed3867acfb2d","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/39262814fa3e47905a2e5facf13465a1f70706b9","title":"Single-Read Reconstruction for DNA Data Storage Using Transformers","venue":"ArXiv","year":2021,"referenceCount":41,"citationCount":3,"influentialCitationCount":0,"publicationDate":"12/09/2021","authors":"Yotam Nahum,Eyar Ben-Tolila,Leon Anavy","id":"39262814fa3e47905a2e5facf13465a1f70706b9","summary":"This work proposes a novel approach for single-read reconstruction using an encoder-decoder Transformer architecture for DNA based data storage and achieves lower error rates when reconstructing the original data from a single read of each DNA strand compared to state-of-the-art algorithms using 2-3 copies.","score":1},{"url":"https://www.semanticscholar.org/paper/9d13bde760d8e77f059436d60160881becd2d2e0","title":"Predicting Ethnicity from Names with rethnicity: Methodology and Application","venue":"ArXiv","year":2021,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Fangzhou Xie","id":"9d13bde760d8e77f059436d60160881becd2d2e0","summary":"A new R package, rethnicity1 is provided for predicting ethnicity based on names, using the Bidirectional LSTM and Florida Voter Registration as the model and training data.","score":1},{"url":"https://www.semanticscholar.org/paper/4318c9f87221b9f504f286540c9a6d5c1cc4e4c8","title":"Rethnicity: Predicting Ethnicity from Names","venue":"","year":2021,"referenceCount":26,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/09/2021","authors":"Fangzhou Xie","id":"4318c9f87221b9f504f286540c9a6d5c1cc4e4c8","summary":"An R package, rethniicty, is provided for predicting ethnicity from names, using the Bidirectional LSTM as the model and Florida Voter Registration as training data and the availability, accuracy, and performance are compared.","score":1},{"url":"https://www.semanticscholar.org/paper/c107835a05ca6fda6e73b64e2ed9884de4fcec0f","title":"A proposed conceptual framework for a representational approach to information retrieval","venue":"SIGIR Forum","year":2021,"referenceCount":86,"citationCount":21,"influentialCitationCount":1,"publicationDate":"04/10/2021","authors":"Jimmy J. Lin","id":"c107835a05ca6fda6e73b64e2ed9884de4fcec0f","summary":"A representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model that establishes connections to sentence similarity tasks in natural language processing and information access \"technologies\" prior to the dawn of computing.","score":1},{"url":"https://www.semanticscholar.org/paper/de0e1f9980afa7949df64d53b8ae7a2f59c55579","title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages","venue":"ArXiv","year":2021,"referenceCount":77,"citationCount":5,"influentialCitationCount":0,"publicationDate":2021,"authors":"Kalpesh Krishna,Deepak Nathani,Xavier García,Bidisha Samanta,P. Talukdar","id":"de0e1f9980afa7949df64d53b8ae7a2f59c55579","summary":"This work pushes the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases, and achieves 2-3x better performance and output diversity in formality transfer and code-mixing addition across five Indian languages.","score":1},{"url":"https://www.semanticscholar.org/paper/7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","title":"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome","venue":"","year":2021,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/10/2021","authors":"Theodore Jiang,Li Fang,Kai Wang","id":"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","summary":"The introduction of MutFormer, a transformer-based model for the prediction of deleterious missense mutations that uses reference and mutated protein sequences from the human genome as the primary features, and which successfully considers sequence features that are not explored in previous studies.","score":1},{"url":"https://www.semanticscholar.org/paper/6e2682eb2fbec93b329028b23764c1164e232c41","title":"ÚFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5","venue":"WNUT","year":2021,"referenceCount":35,"citationCount":6,"influentialCitationCount":1,"publicationDate":"28/10/2021","authors":"David Samuel,Milan Straka","id":"6e2682eb2fbec93b329028b23764c1164e232c41","summary":"This paper presents the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021, which evaluates lexical-normalization systems on 12 social media datasets in 11 languages with the best performance by a wide margin.","score":1},{"url":"https://www.semanticscholar.org/paper/db3690379953f2ea4f2a015615de02e643d437ea","title":"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers","venue":"","year":2021,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2021","authors":"Michael Stephen Saxon,Xinyi Wang,Wenda Xu,William Yang Wang","id":"db3690379953f2ea4f2a015615de02e643d437ea","summary":"To enable future amelio-ration efforts, introduce a novel model-driven technique, the progressive evaluation of cluster outliers (PECO) which enables both the ob-jective measurement of leakage, and the automated detection of subpopulations in the data which maximally exhibit it.","score":1},{"url":"https://www.semanticscholar.org/paper/5f77157038dc7f189db7e72ea58567039222d9df","title":"Examining Single Sentence Label Leakage in Natural Language Inference Datasets","venue":"","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Michael Stephen Saxon,Xinyi Wang,Wenda Xu,William Yang Wang","id":"5f77157038dc7f189db7e72ea58567039222d9df","summary":"This work examines how regular NLI models cheat on single sentence label leakage, and discusses how to ameliorate this.","score":1},{"url":"https://www.semanticscholar.org/paper/b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Colin Leong,Daniel Whitenack","id":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","summary":"Initial experiments using Swahili and Kinyarwanda data suggest the viability of the multi-modal approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch.","score":1},{"url":"https://www.semanticscholar.org/paper/2b6b38b66fcca218cb2f381e5d4711b5c99a784f","title":"Training Text-to-Text Transformers with Privacy Guarantees","venue":"Findings","year":2022,"referenceCount":37,"citationCount":5,"influentialCitationCount":0,"publicationDate":2022,"authors":"N. Ponomareva,Jasmijn Bastings,Sergei Vassilvitskii","id":"2b6b38b66fcca218cb2f381e5d4711b5c99a784f","summary":"By using recent advances in JAX and XLA, this work can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE).","score":1},{"url":"https://www.semanticscholar.org/paper/32290d428b50f64d1cfee6ea658dc6ba977b26e9","title":"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia","venue":"LTEDI","year":2022,"referenceCount":33,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"I. Upadhyay,KV Aditya Srivatsa,Radhika Mamidi","id":"32290d428b50f64d1cfee6ea658dc6ba977b26e9","summary":"The approach to classify homophobia and transphobia in social media comments using an ensemble of transformer-based models to build a classifier that ranked 2nd for English, 8th for Tamil and 10th for Chennai-English.","score":1},{"url":"https://www.semanticscholar.org/paper/b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","title":"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Dibyanayan Bandyopadhyay,Arkadipta De,Baban Gain,Tanik Saikh,Asif Ekbal","id":"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","summary":"It is argued that the transfer learning-based loss objective is model agnostic and thus can be used with other deep learning- based architectures for cross-lingual NLI.","score":1},{"url":"https://www.semanticscholar.org/paper/2005311276a1a90dc17cf6e2ca7725fee2e76e1e","title":"BasqueGLUE: A Natural Language Understanding Benchmark for Basque","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Gorka Urbizu,Iñaki San Vicente,X. Saralegi,Rodrigo Agerri,Aitor Soroa Etxabe","id":"2005311276a1a90dc17cf6e2ca7725fee2e76e1e","summary":"BasqueGLUE is presented, the first NLU benchmark for Basque, a less-resourced language, which has been elaborated from previously existing datasets and following similar criteria to those used for the construction of GLUE and SuperGLUE.","score":1},{"url":"https://www.semanticscholar.org/paper/143659fcf93f33e9139f594cf8111c6bc85c04fa","title":"Overview of the EvaLatin 2022 Evaluation Campaign","venue":"LT4HALA","year":2022,"referenceCount":14,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"R. Sprugnoli,Marco Passarotti,F. M. Cecchini,Margherita Fantoli,Giovanni Moretti","id":"143659fcf93f33e9139f594cf8111c6bc85c04fa","summary":"This paper describes the organization and the results of the second edition of EvaLatin, the campaign for the evaluation of Natural Language Processing tools for Latin, and the three shared tasks proposed in EvaLatin 2022, i.","score":1},{"url":"https://www.semanticscholar.org/paper/6102fe88a512290b80e83ed2fe17606b166e505a","title":"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin","venue":"LT4HALA","year":2022,"referenceCount":12,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Krzysztof Wróbel,Krzysztof Nowak","id":"6102fe88a512290b80e83ed2fe17606b166e505a","summary":"The paper features a thorough discussion of part-of-speech and lemmatization errors which shows how the system performance may be improved for Classical, Medieval and Neo-Latin texts.","score":1},{"url":"https://www.semanticscholar.org/paper/0ffb0b109acddf0067aec252221e0ea04d317ddc","title":"A Framework for Sexism Detection on Social Media via ByT5 and TabNet","venue":"IberLEF@SEPLN","year":2022,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"A. Younus,M. A. Qureshi","id":"0ffb0b109acddf0067aec252221e0ea04d317ddc","summary":"A combination of byte-level model ByT5 with tabular modeling via TabNet that has at its core an ability to take into account platform and language aspects of the challenging task of sexism detection is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/710ceab1ff91b96e8596b8400ebf2912ee6e2836","title":"Machine Translation for Multilingual Intent Detection and Slots Filling","venue":"MMNLU","year":2022,"referenceCount":44,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans","id":"710ceab1ff91b96e8596b8400ebf2912ee6e2836","summary":"This work uses the inherent multilingual aspect of translation models for the task of multilingual intent classification and slot filling and reveals that they work equally well with general-purpose multilingual text-to-text models.","score":1},{"url":"https://www.semanticscholar.org/paper/038103632b24619818b159f5ca37b848744817db","title":"DENTRA: Denoising and Translation Pre-training for Multilingual Machine Translation","venue":"Conference on Machine Translation","year":2022,"referenceCount":23,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Samta Kamboj,Sunil Kumar Sahu,Neha Sengupta","id":"038103632b24619818b159f5ca37b848744817db","summary":"DENTRA, a novel pre-training strategy for a multilingual sequence-to-sequence transformer model, combines denoising and translation objectives to incorporate both monolingual and bitext corpora in 24 African, English, and French languages is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/9758782feed4b4b9bf0ec18b802462e8023a7f83","title":"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models","venue":"Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing","year":2022,"referenceCount":33,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Odunayo Jude Ogundepo,Akintunde Oladipo,Mofetoluwa Adeyemi,Kelechi Ogueji and Jimmy Lin","id":"9758782feed4b4b9bf0ec18b802462e8023a7f83","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","title":"On the Influence of Tokenizers in NLP Pipelines","venue":"","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","summary":"This thesis examines the influence of tokenization in NLP pipelines, by analyzing, reproducing, and quantifying claims from the token-free NLP literature, using the example of NER, and concludes that token- free models, like ByT5, offer significant advantages over their tokenizer-based alternatives.","score":1},{"url":"https://www.semanticscholar.org/paper/7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview","venue":"Computational Linguistics","year":2021,"referenceCount":67,"citationCount":32,"influentialCitationCount":2,"publicationDate":"22/02/2021","authors":"Philipp Dufter,Martin Schmitt,Hinrich Schütze","id":"7072db6eddb85ecd2c117365d91bd694760f726e","summary":"An overview and theoretical comparison of existing methods to incorporate position information into Transformer models is provided and what characteristics of an application should be taken into account when selecting a position encoding is indicated.","score":1},{"url":"https://www.semanticscholar.org/paper/a70fc86508bd0133d5d984a4e777abef1934d76c","title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese","venue":"Interspeech","year":2021,"referenceCount":46,"citationCount":5,"influentialCitationCount":1,"publicationDate":"20/09/2021","authors":"Nguyen Luong Tran,Duong Minh Le,Dat Quoc Nguyen","id":"a70fc86508bd0133d5d984a4e777abef1934d76c","summary":"BARTpho uses the “large” architecture and the pre-training scheme of the sequence-to-sequence denoising autoencoder BART, thus it is especially suitable for generative NLP tasks and is found to be more effective than mBART on these two tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/e35357ac461a669fe7e4b877ee1fad0dfda26303","title":"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":85,"citationCount":5,"influentialCitationCount":0,"publicationDate":"14/10/2021","authors":"Kalpesh Krishna,Deepak Nathani,Xavier García,Bidisha Samanta,P. Talukdar","id":"e35357ac461a669fe7e4b877ee1fad0dfda26303","summary":"This work pushes the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases, and achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages.","score":1},{"url":"https://www.semanticscholar.org/paper/66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer","venue":"International Conference on Learning Representations","year":2021,"referenceCount":87,"citationCount":36,"influentialCitationCount":3,"publicationDate":"25/10/2021","authors":"M. Dehghani,Anurag Arnab,L. Beyer,Ashish Vaswani,Yi Tay","id":"66d735987a31d666a6459566ae026c40ab9a1c3a","summary":"It is demonstrated how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models, and suggestions to improve reporting of efficiency metrics are presented.","score":1},{"url":"https://www.semanticscholar.org/paper/9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","title":"Large Dual Encoders Are Generalizable Retrievers","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":40,"citationCount":41,"influentialCitationCount":16,"publicationDate":"15/12/2021","authors":"Jianmo Ni,Chen Qu,Jing Lu,Zhuyun Dai,Gustavo Hern'andez 'Abrego,Ji Ma,Vincent Zhao,Yi Luan,Keith B. Hall,Ming-Wei Chang,Yinfei Yang","id":"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","summary":"Experimental results show that the dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly and the ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.","score":1},{"url":"https://www.semanticscholar.org/paper/e53f455b3300d95738dac117419c88fbde58ac4e","title":"Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics","venue":"Database J. Biol. Databases Curation","year":2022,"referenceCount":141,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2022","authors":"Tiago Almeida,Rui Antunes,João F Silva,João Rafael Almeida,Sérgio Matos","id":"e53f455b3300d95738dac117419c88fbde58ac4e","summary":"This manuscript describes a three-stage pipeline that individually performs chemical mention detection, entity normalization and indexing in PubMed full-text articles and proposes rules for identifying the more relevant MeSH codes for each article.","score":1},{"url":"https://www.semanticscholar.org/paper/f357b35e068bbfbb8468b48198ab63241713629d","title":"Correcting diacritics and typos with ByT5 transformer model","venue":"Applied Sciences","year":2022,"referenceCount":138,"citationCount":4,"influentialCitationCount":1,"publicationDate":"31/01/2022","authors":"Lukas Stankevicius,M. Lukoševičius,J. Kapočiūtė-Dzikienė,Monika Briediene,Tomas Krilavičius","id":"f357b35e068bbfbb8468b48198ab63241713629d","summary":"This work tackles diacritics restoration and typos correction at once by employing the newly-developed universal ByT5 byte-level seq2seq transformer model that requires no language-specific model structures and strongly outperforms classical spell-checking or dictionary-based approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language","venue":"International Conference on Machine Learning","year":2022,"referenceCount":87,"citationCount":208,"influentialCitationCount":43,"publicationDate":"07/02/2022","authors":"Alexei Baevski,Wei-Ning Hsu,Qiantong Xu,Arun Babu,Jiatao Gu,Michael Auli","id":"8f2bca9d684005675e294b33c26481e36f528cdb","summary":"Data2vec is a framework that uses the same learning method for either speech, NLP or computer vision to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture.","score":1},{"url":"https://www.semanticscholar.org/paper/7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers","venue":"Knowledge Discovery and Data Mining","year":2022,"referenceCount":37,"citationCount":7,"influentialCitationCount":2,"publicationDate":"22/02/2022","authors":"Alyssa Lees,V. Tran,Yi Tay,Jeffrey Scott Sorensen,Jai Gupta,Donald Metzler,Lucy Vasserman","id":"7016eb4f34611f97fe8c99176246e314678e03f4","summary":"This paper presents the fundamentals behind the next version of the Perspective API from Google Jigsaw, and presents a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/8a4eec70e3d0c43abf26757252ad6210c9717f6c","title":"Towards Lithuanian grammatical error correction","venue":"","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/03/2022","authors":"Lukas Stankevivcius,Mantas Lukovsevivcius","id":"8a4eec70e3d0c43abf26757252ad6210c9717f6c","summary":"This work constructs a grammatical error correction model for Lithuanian, the language rich in archaic features, by employing the recent advances in transformer architectures and shares the best trained model, achieving F0.5 = 0.92.","score":1},{"url":"https://www.semanticscholar.org/paper/a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":204,"citationCount":5,"influentialCitationCount":0,"publicationDate":"24/03/2022","authors":"Alham Fikri Aji,Genta Indra Winata,Fajri Koto,Samuel Cahyawijaya,A. Romadhony,Rahmad Mahendra,Kemal Kurniawan,David Moeljadi,Radityo Eko Prasojo,Timothy Baldwin,Jey Han Lau,Sebastian Ruder","id":"a747e8f2659df479c0092301b9658fc582423df1","summary":"An overview of the current state of NLP research for Indonesia's 700+ languages is provided and general recommendations are provided to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.","score":1},{"url":"https://www.semanticscholar.org/paper/1ed66e048bb025e75aa5ea660545285212e5341f","title":"Scaling Up Models and Data with t5x and seqio","venue":"ArXiv","year":2022,"referenceCount":23,"citationCount":44,"influentialCitationCount":7,"publicationDate":"31/03/2022","authors":"Adam Roberts,Hyung Won Chung,Anselm Levskaya,Gaurav Mishra,James Bradbury,D. Andor,Sharan Narang,Brian Lester,Colin Gaffney,Afroz Mohiuddin,Curtis Hawthorne,Aitor Lewkowycz,Alexandru Salcianu,Marc van Zee,Jacob Austin,Sebastian Goodman,Livio Baldini Soares,Haitang Hu,Sasha Tsvyashchenko,Aakanksha Chowdhery,Jasmijn Bastings,Jannis Bulian,Xavier García,Jianmo Ni,A. Chen,Kathleen Kenealy,J. Clark,Stephan Lee,Daniel H Garrette,J. Lee-Thorp,Colin Raffel,Noam M. Shazeer,Marvin Ritter,Maarten Bosma,Alexandre Passos,Jeremy B. Maitin-Shepard,Noah Fiedel,Mark Omernick,Brennan Saeta,Ryan Sepassi,A. Spiridonov,Joshua Newlan,Andrea Gesmundo","id":"1ed66e048bb025e75aa5ea660545285212e5341f","summary":"Two software libraries are presented: t5x simplifies the process of building and training large language models at scale while maintaining ease of use, and seqio provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines.","score":1},{"url":"https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways","venue":"ArXiv","year":2022,"referenceCount":173,"citationCount":506,"influentialCitationCount":71,"publicationDate":"05/04/2022","authors":"Aakanksha Chowdhery,Sharan Narang,Jacob Devlin,Maarten Bosma,Gaurav Mishra,Adam Roberts,P. Barham,Hyung Won Chung,Charles Sutton,Sebastian Gehrmann,Parker Schuh,Kensen Shi,Sasha Tsvyashchenko,Joshua Maynez,Abhishek Rao,Parker Barnes,Yi Tay,Noam M. Shazeer,Vinodkumar Prabhakaran,Emily Reif,Nan Du,B. Hutchinson,Reiner Pope,James Bradbury,Jacob Austin,M. Isard,Guy Gur-Ari,Pengcheng Yin,Toju Duke,Anselm Levskaya,S. Ghemawat,Sunipa Dev,H. Michalewski,Xavier García,Vedant Misra,Kevin Robinson,L. Fedus,Denny Zhou,Daphne Ippolito,D. Luan,Hyeontaek Lim,Barret Zoph,A. Spiridonov,Ryan Sepassi,David Dohan,Shivani Agrawal,Mark Omernick,Andrew M. Dai,T. S. Pillai,Marie Pellat,Aitor Lewkowycz,Erica Moreira,Rewon Child,Oleksandr Polozov,Katherine Lee,Zongwei Zhou,Xuezhi Wang,Brennan Saeta,Mark Díaz,Orhan Firat,Michele Catasta,Jason Wei,K. Meier-Hellstern,D. Eck,J. Dean,Slav Petrov,Noah Fiedel","id":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","summary":"A 540-billion parameter, densely activated, Transformer language model, which is called PaLM achieves breakthrough performance, outperforming the state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark.","score":1},{"url":"https://www.semanticscholar.org/paper/880c8973ea1376c6bff23226b3f64792657aa666","title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion","venue":"Interspeech","year":2022,"referenceCount":34,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/04/2022","authors":"Jian Zhu,Cong Zhang,David Jurgens","id":"880c8973ea1376c6bff23226b3f64792657aa666","summary":"It is found that ByT5 operating on byte-level inputs signiﬁcantly outperformed the token-based mT5 model in terms of multilingual G2P, and pairwise comparison with monolingual models in these languages suggests that multilingual ByT 5 models generally lower the phone error rate by jointly learning from a variety of languages.","score":1},{"url":"https://www.semanticscholar.org/paper/e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","venue":"BIGSCIENCE","year":2022,"referenceCount":141,"citationCount":70,"influentialCitationCount":9,"publicationDate":"14/04/2022","authors":"Sid Black,Stella Rose Biderman,Eric Hallahan,Quentin G. Anthony,Leo Gao,Laurence Golding,Horace He,Connor Leahy,Kyle McDonell,Jason Phang,M. Pieler,Usvsn Sai Prashanth,Shivanshu Purohit,Laria Reynolds,J. Tow,Benqi Wang,Samuel Weinbach","id":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","summary":"GPT-NeoX-20B is introduced, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.","score":1},{"url":"https://www.semanticscholar.org/paper/0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction","venue":"ArXiv","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/04/2022","authors":"Mingchen Li,Junfan Chen,Samuel Mensah,Nikolaos Aletras,Xiulong Yang,Yang Ye","id":"0b9e130c6305de7766697ba7655f56010aaffd61","summary":"A Hierarchical N -gram framework for Z ero-S hot L ink P rediction (HNZSLP) that lever-ages character n-gram information for ZSLP and achieves state-of-the-art performance on two standard Z SLP datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":30,"citationCount":4,"influentialCitationCount":1,"publicationDate":"19/04/2022","authors":"Matthew Finlayson,Kyle Richardson,Ashish Sabharwal,Peter Clark","id":"cb16b85891172572cd856142880b503db0c2bc61","summary":"This work uses the task of deciding whether a given string matches a regular expression to identify properties of tasks, instructions, and instances that make instruction learning challenging, and proposes Hard RegSet as a challenging instruction learning dataset and a controlled environment for studying instruction learning.","score":1},{"url":"https://www.semanticscholar.org/paper/7267812178393b8ae0b99648f02661ca1ff2b412","title":"Bilingual End-to-End ASR with Byte-Level Subwords","venue":"IEEE International Conference on Acoustics, Speech, and Signal Processing","year":2022,"referenceCount":18,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"Liuhui Deng,Roger Hsiao,Arnab Ghoshal","id":"7267812178393b8ae0b99648f02661ca1ff2b412","summary":"This paper investigates how the output representation of an end-to-end neural network affects multilingual automatic speech recognition (ASR), and finds that BBPE with penalty schemes can improve utterance-based bilingual ASR performance by 2% to 5% relative even with smaller number of outputs and fewer parameters.","score":1},{"url":"https://www.semanticscholar.org/paper/063d9fa4861356500219b7e81d5a654aa921da6f","title":"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":68,"citationCount":18,"influentialCitationCount":1,"publicationDate":"04/05/2022","authors":"David Ifeoluwa Adelani,Jesujoba Oluwadara Alabi,Angela Fan,Julia Kreutzer,Xiaoyu Shen,Machel Reid,Dana Ruiter,D. Klakow,Peter Nabende,Ernie Chang,T. Gwadabe,Freshia Sackey,Bonaventure F. P. Dossou,Chris C. Emezue,Colin Leong,Michael Beukman,Shamsuddeen Hassan Muhammad,Guyo Dub Jarso,Oreen Yousuf,Andre Niyongabo Rubungo,Gilles Hacheme,Eric Peter Wairagala,Muhammad Umair Nasir,Benjamin Ayoade Ajibade,T. Ajayi,Yvonne Wambui Gitau,Jade Z. Abbott,Mohamed Ahmed,Millicent A. Ochieng,Anuoluwapo Aremu,Perez Ogayo,Jonathan Mukiibi,Fatoumata Kabore,Godson Kalipe,Derguene Mbaye,A. Tapo,V. M. Koagne,Edwin Munkoh-Buabeng,Valencia Wagner,Idris Abdulmumin,A. Awokoya,Happy Buzaaba,Blessing K. Sibanda,Andiswa Bukula,Sam Manthalu","id":"063d9fa4861356500219b7e81d5a654aa921da6f","summary":"It is demonstrated that the most effective strategy for transferring both additional languages and additional domains is to leverage small quantities of high-quality translation data to fine-tune large pre-trained models.","score":1},{"url":"https://www.semanticscholar.org/paper/645b10b7802a035e034488e3640fc0bc415de34c","title":"UL2: Unifying Language Learning Paradigms","venue":"","year":2022,"referenceCount":125,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/05/2022","authors":"Yi Tay,M. Dehghani,V. Tran,Xavier García,Jason Wei,Xuezhi Wang,Hyung Won Chung,Dara Bahri,Tal Schuster,Huaixiu Zheng,Denny Zhou,N. Houlsby,Donald Metzler","id":"645b10b7802a035e034488e3640fc0bc415de34c","summary":"By scaling the model up to 20B parameters, this paper achieves SOTA performance on 50 well-established supervised NLP tasks ranging from language generation, language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval.","score":1},{"url":"https://www.semanticscholar.org/paper/f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms","venue":"ArXiv","year":2022,"referenceCount":113,"citationCount":34,"influentialCitationCount":6,"publicationDate":2022,"authors":"Yi Tay,M. Dehghani,V. Tran,Xavier García,Dara Bahri,Tal Schuster,Huaixiu Zheng,N. Houlsby,Donald Metzler","id":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","summary":"UL2 achieves SOTA performance on 50 well-established supervised NLP tasks ranging from language generation, language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval.","score":1},{"url":"https://www.semanticscholar.org/paper/32afdb07021fda775ceaedd231c58bfed0aa980a","title":"Automated Crossword Solving","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":29,"citationCount":2,"influentialCitationCount":1,"publicationDate":"19/05/2022","authors":"Eric Wallace,Nicholas Tomlin,Albert Xu,Kevin Yang,Eshaan Pathak,Matthew Ginsberg,D. Klein","id":"32afdb07021fda775ceaedd231c58bfed0aa980a","summary":"The Berkeley Crossword Solver is presented, a state-of-the-art approach for automatically solving crossword puzzles that improves exact puzzle accuracy from 57% to 82% on crosswords from The New York Times and obtains 99.9% letter accuracy on themeless puzzles.","score":1},{"url":"https://www.semanticscholar.org/paper/a766ef0678aade6a9798552618819cf4d0fac406","title":"TEVR: Improving Speech Recognition by Token Entropy Variance Reduction","venue":"ArXiv","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/06/2022","authors":"Hajo N. Krabbenhöft,E. Barth","id":"a766ef0678aade6a9798552618819cf4d0fac406","summary":"TEVR, a speech recognition model designed to minimize the variation in token entropy w.r.t. to the language model, is presented and it is shown that on CommonVoice German, TEVR scores a very competitive 3 .","score":1},{"url":"https://www.semanticscholar.org/paper/b308511ac52e1c2da915d1e55d5246dfdb4cbe28","title":"Romanian Question Answering Using Transformer Based Neural Networks","venue":"Studia Universitatis Babeș-Bolyai Informatica","year":2022,"referenceCount":5,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/07/2022","authors":"Bogdan-Alexandru Diaconu,Beáta Lázár-Lőrincz","id":"b308511ac52e1c2da915d1e55d5246dfdb4cbe28","summary":"This work presents experiments evaluated on the XQuAD-ro question answering dataset that has been recently published based on the translation of the SQuAD dataset into Romanian, and shows that fine-tuning the model with the addition of the Romanian translation slightly increases the evaluation metrics.","score":1},{"url":"https://www.semanticscholar.org/paper/2ef60a4ea4ea53056be811ff55679eb59fb4b586","title":"Confident Adaptive Language Modeling","venue":"ArXiv","year":2022,"referenceCount":89,"citationCount":10,"influentialCitationCount":0,"publicationDate":"14/07/2022","authors":"Tal Schuster,Adam Fisch,Jai Gupta,M. Dehghani,Dara Bahri,V. Tran,Yi Tay,Donald Metzler","id":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","summary":"This work introduces Conﬁdent Adaptive Language Modeling (CALM), a framework for dynamically allocating different amounts of compute per input and generation timestep, and demonstrates the efﬂcacy of the framework in reducing compute—speedup of up to × 3 —while provably maintaining high performance.","score":1},{"url":"https://www.semanticscholar.org/paper/00d9a73c54053a32e2aba92b53fc3e6bc71d3238","title":"Sequence to sequence pretraining for a less-resourced Slovenian language","venue":"","year":2022,"referenceCount":44,"citationCount":1,"influentialCitationCount":1,"publicationDate":"28/07/2022","authors":"Matej Ulvcar,Marko Robnik-vSikonja","id":"00d9a73c54053a32e2aba92b53fc3e6bc71d3238","summary":"Two diﬀerent sized T5-type sequence to sequence models for morphologically rich Slovene language with much less resources are trained and analyzed, which mostly lag behind the monolingual Slovene SloBERTa model but are useful for the generative tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/a806041621acb5cf648fc780f1ff14939aa3a721","title":"How Much Does Lookahead Matter for Disambiguation? Partial Arabic Diacritization Case Study","venue":"Computational Linguistics","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"Saeed Esmail,Kfir Bar,N. Dershowitz","id":"a806041621acb5cf648fc780f1ff14939aa3a721","summary":"It is found that the partial diacritizer improves translation quality compared either to their total absence or to random selection, and the benefit of knowing the text that follows the word in focus toward the restoration of short vowels during reading is studied.","score":1},{"url":"https://www.semanticscholar.org/paper/e4437293a703fcf282bf1b38bf7900ed8ca711bb","title":"Training a T5 Using Lab-sized Resources","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/08/2022","authors":"Manuel R. Ciosici,Leon Derczynski","id":"e4437293a703fcf282bf1b38bf7900ed8ca711bb","summary":"This paper presents var-ious techniques for making it possible to train a large language model using resources that a modest research lab might have, and train it in a reasonable amount of time.","score":1},{"url":"https://www.semanticscholar.org/paper/9fc5878d49c41beb12b62032b0afe9a8501fe9db","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model","venue":"ArXiv","year":2022,"referenceCount":92,"citationCount":32,"influentialCitationCount":5,"publicationDate":"14/09/2022","authors":"Xi Chen,Xiao Wang,Soravit Changpinyo,A. Piergiovanni,Piotr Padlewski,Daniel M. Salz,Sebastian Goodman,Adam Grycner,Basil Mustafa,L. Beyer,Alexander Kolesnikov,J. Puigcerver,Nan Ding,Keran Rong,Hassan Akbari,Gaurav Mishra,Linting Xue,Ashish V. Thapliyal,James Bradbury,Weicheng Kuo,Mojtaba Seyedhosseini,Chao Jia,Burcu Karagol Ayan,C. Riquelme,A. Steiner,A. Angelova,Xiaohua Zhai,N. Houlsby,Radu Soricut","id":"9fc5878d49c41beb12b62032b0afe9a8501fe9db","summary":"PaLI achieves state-of-the-art in multiple vision and language tasks, while retaining a simple, modular, and scalable design.","score":1},{"url":"https://www.semanticscholar.org/paper/2a8aac78df5e1e9658a02d381662ed26c6577713","title":"Disease diagnostic method based on cascade backbone network for apple leaf disease classification","venue":"Frontiers in Plant Science","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/09/2022","authors":"Xing Sheng,Fengyun Wang,Huaijun Ruan,Yangyang Fan,Jiye Zheng,Yangyang Zhang,Chen Lyu","id":"2a8aac78df5e1e9658a02d381662ed26c6577713","summary":"This paper is the first to introduce Transformer into apple leaf disease identification, and the results are promising.","score":1},{"url":"https://www.semanticscholar.org/paper/595cc15b9db4e985b30a2e175399a38c021d4ce7","title":"Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino","venue":"SUSTAINLP","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Lorenzo Jaime Yu Flores","id":"595cc15b9db4e985b30a2e175399a38c021d4ce7","summary":"An N-Gram + Damerau-Levenshtein distance model with automatic rule extraction achieves good performance and outperforms other deep learning approaches in terms of accuracy and edit distance, highlighting the success of traditional approaches over more complex deep learning models in settings where data is unavailable.","score":1},{"url":"https://www.semanticscholar.org/paper/539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72","title":"Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning","venue":"Transactions of the Japanese society for artificial intelligence","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Kotaro Funakoshi","id":"539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72","summary":"Non-Axiomatic Term Logic is presented as a theoretical computational framework of humanlike symbolic reasoning in artiﬁcial intelligence and positions the proposed approach in the phylogeny and the literature of logic.","score":1},{"url":"https://www.semanticscholar.org/paper/5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models","venue":"ArXiv","year":2022,"referenceCount":104,"citationCount":25,"influentialCitationCount":5,"publicationDate":"20/10/2022","authors":"Hyung Won Chung,Le Hou,S. Longpre,Barret Zoph,Yi Tay,W. Fedus,Eric Li,Xuezhi Wang,M. Dehghani,Siddhartha Brahma,Albert Webson,S. Gu,Zhuyun Dai,Mirac Suzgun,Xinyun Chen,Aakanksha Chowdhery,Dasha Valter,Sharan Narang,Gaurav Mishra,A. Yu,Vincent Zhao,Yanping Huang,Andrew M. Dai,Hongkun Yu,Slav Petrov,E. Chi,J. Dean,Jacob Devlin,Adam Roberts,Denny Zhou,Quoc Le,Jason Wei","id":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","summary":"This result shows that instruction and UL2 continued pre-training are complementary compute-eﬃcient methods to improve the performance of language models without increasing model scale.","score":1},{"url":"https://www.semanticscholar.org/paper/6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc","title":"Graphemic Normalization of the Perso-Arabic Script","venue":"ArXiv","year":2022,"referenceCount":128,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"R. Doctor,Alexander Gutkin,Cibu Johny,Brian Roark,R. Sproat","id":"6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc","summary":"It is argued that better understanding and representation of Perso-Arabic script variation within regional orthographic traditions, where those are present, is crucial for further progress of modern computational NLP techniques, especially for languages with a paucity of resources.","score":1},{"url":"https://www.semanticscholar.org/paper/d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6","title":"SLING: Sino Linguistic Evaluation of Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Yixiao Song,Kalpesh Krishna,R. Bhatt,Mohit Iyyer","id":"d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/5e786d47e3dc5ffa65ba8548ff67139a2764acf8","title":"T5lephone: Bridging Speech and Text Self-supervised Models for Spoken Language Understanding via Phoneme level T5","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Chan-Jan Hsu,Ho-Lam Chung,Hung-yi Lee,Yu Tsao","id":"5e786d47e3dc5ffa65ba8548ff67139a2764acf8","summary":"This work conducts extensive studies on how PLMs with different tokenization strategies affect spoken language understanding task including spoken question answering (SQA) and speech translation (ST) and creates T5lephone, a variant of T5 that is pretrained using phonemicized text.","score":1},{"url":"https://www.semanticscholar.org/paper/92302ab168429c7c3a8f699b35ba8302916c6e7c","title":"Bridging Speech and Textual Pre-trained Models with Unsupervised ASR","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/11/2022","authors":"Jiatong Shi,Chan-Jan Hsu,Ho-Lam Chung,Dongji Gao,Paola García,Shinji Watanabe,Ann Lee,Hung-yi Lee","id":"92302ab168429c7c3a8f699b35ba8302916c6e7c","summary":"The experiments show that unsupervised ASR itself can improve the representations from speech self-supervised models, and it is shown as an efﬁcient connector between speech and textual pre-trained models, improv-ing the performances of different SLU tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/9dab4c20648cd4c7c6830e6274a95294b014aac9","title":"QAmeleon: Multilingual QA with Only 5 Examples","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Priyanka Agrawal,Chris Alberti,Fantine Huot,Joshua Maynez,Ji Ma,Sebastian Ruder,Kuzman Ganchev,Dipanjan Das,Mirella Lapata","id":"9dab4c20648cd4c7c6830e6274a95294b014aac9","summary":"This approach uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation and shows that few-shot prompt tuning for data synthesis scales across languages and is a viable alternative to large-scale annotation.","score":1},{"url":"https://www.semanticscholar.org/paper/d50b9db750ded246bde13e2c263e341bcbd8a335","title":"A Benchmark and Dataset for Post-OCR text correction in Sanskrit","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Ayush Maheshwari,Nikhil Singh,A. Krishna,Ganesh Ramakrishnan","id":"d50b9db750ded246bde13e2c263e341bcbd8a335","summary":"This work releases a post-OCR text correction dataset containing around 218,000 sentences, with 1.5 million words, from 30 different books of Sanskrit, and finds that the best-performing model, consisting of byte level tokenization in conjunction with phonetic encoding (Byt5+SLP1), yields a 23% point increase over the OCR output in terms of word and character error rates.","score":1},{"url":"https://www.semanticscholar.org/paper/3af872a4bd314db9e7964876fee811823628f83f","title":"Text Normalization on Code-Mixed Twitter Text using Language Detection","venue":"International Conference on Intelligent Computing","year":2022,"referenceCount":10,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Rafi Dwi Rizqullah,I. Budi","id":"3af872a4bd314db9e7964876fee811823628f83f","summary":"The research shows that proposed method for text normalization still has problems related to language, either on identifying language or normalize a word, but the approach is using language detection module alongside with transformer model.","score":1},{"url":"https://www.semanticscholar.org/paper/4aa09cba27a489ca02471fad011ea4854fc63cc1","title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"William B. Held,Christopher Hidey,Fei Liu,Eric Zhu,Rahul Goel,Diyi Yang,Rushin Shah","id":"4aa09cba27a489ca02471fad011ea4854fc63cc1","summary":"This work shows that pretraining alignment objectives improve mul- 012 tilingual transfer while also reducing negative negative transfer to English, and introduces a strained optimization method to improve align- 015 ment using domain adversarial training.","score":1},{"url":"https://www.semanticscholar.org/paper/d4c23dc85ea1eba2f863ff6ffbbb5e473e491ff4","title":"TRIP: Triangular Document-level Pre-training for Multilingual Language Models","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Hongyuan Lu,Haoyang Huang,Shuming Ma,Dongdong Zhang,W. Lam,Furu Wei","id":"d4c23dc85ea1eba2f863ff6ffbbb5e473e491ff4","summary":"In-depth analysis indicates that TRIP improves document-level machine translation and captures better document contexts in at least three characteristics: (i) tense consistency, (ii) noun consistency and (iii) conjunction presence.","score":1},{"url":"https://www.semanticscholar.org/paper/b1dad820853464b30c93b52366b32690ba4b99a6","title":"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View","venue":"ACM Computing Surveys","year":2022,"referenceCount":162,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/03/2022","authors":"Ruiqi Li,Xiang Zhao,M. Moens","id":"b1dad820853464b30c93b52366b32690ba4b99a6","summary":"This survey summarizes the current universal sentence-embedding methods, categorizes them into four groups from a linguistic view, and ultimately analyzes their reported performance.","score":1},{"url":"https://www.semanticscholar.org/paper/2c1b92286a6e2a7e521980c637c84cb9fd9495f6","title":"Truveta Mapper: A Zero-shot Ontology Alignment Framework","venue":"ArXiv","year":2023,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Mariyam Amir,Murchana Baruah,Mahsa Eslamialishah,Sina Ehsani,Alireza Bahramali,Sadra Naddaf-Sh,Saman Zarandioon","id":"2c1b92286a6e2a7e521980c637c84cb9fd9495f6","summary":"A new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task that offers log-linear complexity in contrast to quadratic in the existing end-to-end methods, and makes the OM task efﬁcient and more straightforward without much post-processing involving mapping extension or mapping repair.","score":1},{"url":"https://www.semanticscholar.org/paper/30869d285642a8b981702d2a54be0ac54f01aa01","title":"Composer's Assistant: Interactive Transformers for Multi-Track MIDI Infilling","venue":"ArXiv","year":2023,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/01/2023","authors":"Martin E. Malandro","id":"30869d285642a8b981702d2a54be0ac54f01aa01","summary":"Two T5-like models are trained to solve the task of multi-track MIDI inﬁlling when arbitrary (track, measure) pairs of information have been deleted from a contiguous slice of measures from a MIDI sequence, and their results have implications for the training of neural networks in other small-vocabulary domains.","score":1},{"url":"https://www.semanticscholar.org/paper/91ea7176cc41f7b97867d483075df7886aa3dc33","title":"Practical Transformer-based Multilingual Text Classification","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":42,"citationCount":7,"influentialCitationCount":0,"publicationDate":2021,"authors":"Cindy Wang,Michele Banko","id":"91ea7176cc41f7b97867d483075df7886aa3dc33","summary":"Results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages and that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data.","score":1},{"url":"https://www.semanticscholar.org/paper/8484b4da1aff6460edbd2df78bf2f5e801569248","title":"GSI-UPM at IberLEF2021: Emotion Analysis of Spanish Tweets by Fine-tuning the XLM-RoBERTa Language Model","venue":"IberLEF@SEPLN","year":2021,"referenceCount":20,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"D. Vera,Óscar Araque,C. Iglesias","id":"8484b4da1aff6460edbd2df78bf2f5e801569248","summary":"This work proposes the design and development of a fine-tuned neural language model (XLM-RoBERTa) to tackle an emotion classification task of Spanish tweets, categorizing each message into seven emotions.","score":1},{"url":"https://www.semanticscholar.org/paper/7e5133e41d5a06b8ad4bad7f2046ac73edf43e38","title":"TUDa at WMT21: Sentence-Level Direct Assessment with Adapters","venue":"Conference on Machine Translation","year":2021,"referenceCount":30,"citationCount":2,"influentialCitationCount":0,"publicationDate":2021,"authors":"Gregor Geigle,Jonas Stadtmüller,Wei Zhao,Jonas Pfeiffer,Steffen Eger","id":"7e5133e41d5a06b8ad4bad7f2046ac73edf43e38","summary":"This work focuses on utilizing massively multilingual language models which only partly cover the target languages during their pre-training phase and extends the model to new languages and unseen scripts using recent adapter-based methods and achieve on par performance or even surpass models pre-trained on the respective languages.","score":1},{"url":"https://www.semanticscholar.org/paper/892731fd37a1d38901b0c5bc68168d96f43bd5d0","title":"Monolingual Pre-trained Language Models for Tigrinya","venue":"","year":2021,"referenceCount":16,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Fitsum Gaim,Wonsuk Yang,Jong C. Park","id":"892731fd37a1d38901b0c5bc68168d96f43bd5d0","summary":"This work pre-train three monolingual PLMs for Tigrinya on a newly compiled corpus, and compares the models with their multilingual counterparts on two downstream tasks, part-of-speech tagging and sentiment analysis, achieving significantly better results and establishing the state of the art.","score":1},{"url":"https://www.semanticscholar.org/paper/6adc9c231d874ea358554b8680a6aaba4bd6c963","title":"MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":40,"citationCount":30,"influentialCitationCount":2,"publicationDate":2021,"authors":"Alan Ansell,E. Ponti,Jonas Pfeiffer,Sebastian Ruder,Goran Glavas,Ivan Vulic,A. Korhonen","id":"6adc9c231d874ea358554b8680a6aaba4bd6c963","summary":"MAD-G (Multilingual ADapter Generation), which contextually generates language adapters from language representations based on typological features, offers substantial benefits for low-resource languages, particularly on the NER task in low- resource African languages.","score":1},{"url":"https://www.semanticscholar.org/paper/22c8444eb4da5ae8d43829b127d5e7ee9950b151","title":"Crossing the Conversational Chasm: A Primer on Multilingual Task-Oriented Dialogue Systems","venue":"ArXiv","year":2021,"referenceCount":274,"citationCount":14,"influentialCitationCount":2,"publicationDate":2021,"authors":"E. Razumovskaia,Goran Glavas,Olga Majewska,A. Korhonen,Ivan Vulic","id":"22c8444eb4da5ae8d43829b127d5e7ee9950b151","summary":"This work identifies two main challenges that combined hinder the faster progress in multilingual TOD: current state-of-the-art TOD models based on large pretrained neural language models are data hungry; at the same time data acquisition for TOD use cases is expensive and tedious.","score":1},{"url":"https://www.semanticscholar.org/paper/64cc584aa9bafebbab0bc012e225563cf2873ecc","title":"Introduction to Neural Transfer Learning with Transformers for Social Science Text Analysis","venue":"","year":2021,"referenceCount":241,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/02/2021","authors":"Sandra Wankmuller","id":"64cc584aa9bafebbab0bc012e225563cf2873ecc","summary":"This paper explains how Transformer-based models for transfer learning work, why they might be advantageous, and what their limitations are, and demonstrates the benefits these models can bring to textbased social science research.","score":1},{"url":"https://www.semanticscholar.org/paper/a4ab1bd1501668e932c986725b33d065e4f0a233","title":"The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models","venue":"Workshop on Arabic Natural Language Processing","year":2021,"referenceCount":70,"citationCount":51,"influentialCitationCount":13,"publicationDate":"11/03/2021","authors":"Go Inoue,Bashar Alhafni,Nurpeiis Baimukan,Houda Bouamor,Nizar Habash","id":"a4ab1bd1501668e932c986725b33d065e4f0a233","summary":"This paper builds three pre-trained language models across three variants of Arabic: Modern Standard Arabic, dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre- trained on a mix of the three.","score":1},{"url":"https://www.semanticscholar.org/paper/7b99c51d562e33309a46601c846abbe72a65c6a4","title":"What to Pre-Train on? Efficient Intermediate Task Selection","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":92,"citationCount":39,"influentialCitationCount":5,"publicationDate":"16/04/2021","authors":"Clifton Poth,Jonas Pfeiffer,Andreas Ruckl'e,Iryna Gurevych","id":"7b99c51d562e33309a46601c846abbe72a65c6a4","summary":"This work provides a comprehensive comparison of different methods for efficiently identifying beneficial tasks for intermediate transfer learning, focusing on parameter and computationally efficient adapter settings, highlight different data-availability scenarios, and provide expense estimates for each method.","score":1},{"url":"https://www.semanticscholar.org/paper/529edafa160a77901bec123cf8858e6c08f6cd06","title":"When does pretraining help?: assessing self-supervised learning for law and the CaseHOLD dataset of 53,000+ legal holdings","venue":"International Conference on Artificial Intelligence and Law","year":2021,"referenceCount":52,"citationCount":54,"influentialCitationCount":17,"publicationDate":"18/04/2021","authors":"Lucia Zheng,Neel Guha,Brandon R. Anderson,Peter Henderson,Daniel E. Ho","id":"529edafa160a77901bec123cf8858e6c08f6cd06","summary":"It is shown that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task.","score":1},{"url":"https://www.semanticscholar.org/paper/76db3624f1278a4f4f1e69b343bfff7bba306d47","title":"XLM-T: A Multilingual Language Model Toolkit for Twitter","venue":"ArXiv","year":2021,"referenceCount":41,"citationCount":42,"influentialCitationCount":11,"publicationDate":2021,"authors":"Francesco Barbieri,Luis Espinosa Anke,José Camacho-Collados","id":"76db3624f1278a4f4f1e69b343bfff7bba306d47","summary":"This paper introduces XLMT, a framework for using and evaluating multilingual language models in Twitter, a modular framework that can easily be extended to additional tasks, as well as integrated with recent efforts also aimed at the homogenization of Twitter-specific datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/28459083ba624020c8f1c1ed7c3a075f48b4e709","title":"KLUE: Korean Language Understanding Evaluation","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":164,"citationCount":58,"influentialCitationCount":7,"publicationDate":"20/05/2021","authors":"Sungjoon Park,Jihyung Moon,Sungdong Kim,Won Ik Cho,Jiyoon Han,Jangwon Park,Chisung Song,Junseong Kim,Yongsook Song,Tae Hwan Oh,Joohong Lee,Juhyun Oh,Sungwon Lyu,Young-kuk Jeong,I. Lee,Sang-gyu Seo,Dongjun Lee,Hyunwoo Kim,Myeonghwa Lee,Seongbo Jang,Seungwon Do,SunKyoung Kim,Kyungtae Lim,Jongwon Lee,Kyumin Park,Jamin Shin,Seonghyun Kim,Lucy Park,Alice H. Oh,Jung-Woo Ha,Kyunghyun Cho","id":"28459083ba624020c8f1c1ed7c3a075f48b4e709","summary":"KLUE is a collection of 8 Korean natural language understanding tasks, including Topic Classification, Semantic Textual Similarity, Natural Language Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking, and a comprehensive documentation on creating KLUE will facilitate creating similar resources for other languages in the future.","score":1},{"url":"https://www.semanticscholar.org/paper/92f2000f29f1aed7f2cc4e3cb5f783e079ef553f","title":"MergeDistill: Merging Pre-trained Language Models using Distillation","venue":"ArXiv","year":2021,"referenceCount":61,"citationCount":2,"influentialCitationCount":1,"publicationDate":"05/06/2021","authors":"Simran Khanuja,Melvin Johnson,P. Talukdar","id":"92f2000f29f1aed7f2cc4e3cb5f783e079ef553f","summary":"MEGEDISTILL is proposed, a framework to merge pre-trained LMs in a way that can best leverage their assets with minimal dependencies, using task-agnostic knowledge distillation and the applicability of the framework in a practical setting is demonstrated.","score":1},{"url":"https://www.semanticscholar.org/paper/9e8bad21221b88b516edd28fe902e591ac08efa5","title":"Modelling Latent Translations for Cross-Lingual Transfer","venue":"ArXiv","year":2021,"referenceCount":55,"citationCount":5,"influentialCitationCount":1,"publicationDate":"23/07/2021","authors":"E. Ponti,Julia Kreutzer,Ivan Vulic,Siva Reddy","id":"9e8bad21221b88b516edd28fe902e591ac08efa5","summary":"A new technique is proposed that integrates both steps of the traditional pipeline (translation and classification) into a single model, by treating the intermediate translations as a latent random variable, which can be fine-tuned with a variant of Minimum Risk Training.","score":1},{"url":"https://www.semanticscholar.org/paper/657a8a8c83339dff13892b26bb989f97b2acb182","title":"Code-switched inspired losses for spoken dialog representations","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":114,"citationCount":6,"influentialCitationCount":0,"publicationDate":"27/08/2021","authors":"E. Chapuis,Pierre Colombo,Matthieu Labeau,Chloe Clave","id":"657a8a8c83339dff13892b26bb989f97b2acb182","summary":"This work introduces new pretraining losses tailored to learn generic multilingual spoken dialogue representations that achieve a better performance in both monolingual and multilingual settings.","score":1},{"url":"https://www.semanticscholar.org/paper/10a863a33c4dda778cd6a552ecbb02ac42510445","title":"On the ability of monolingual models to learn language-agnostic representations","venue":"ArXiv","year":2021,"referenceCount":29,"citationCount":5,"influentialCitationCount":0,"publicationDate":"04/09/2021","authors":"Leandro Rodrigues de Souza,Rodrigo Nogueira,R. Lotufo","id":"10a863a33c4dda778cd6a552ecbb02ac42510445","summary":"Evidence is provided that a model can achieve language-agnostic representations even when pretrained on a single language, and it is found that monolingual models pretrained and finetuned on different languages achieve competitive performance compared to the ones that use the same target language.","score":1},{"url":"https://www.semanticscholar.org/paper/8dad3a12d2a652122458dd377f62025354b17a2a","title":"Subword Mapping and Anchoring across Languages","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":58,"citationCount":4,"influentialCitationCount":0,"publicationDate":"09/09/2021","authors":"Giorgos Vernikos,Andrei Popescu-Belis","id":"8dad3a12d2a652122458dd377f62025354b17a2a","summary":"This work proposes Subword Mapping and Anchoring across Languages (SMALA), a method to construct bilingual subword vocabularies that improves zero-shot transfer to an unseen language without task-speciﬁc data, but only by sharing subword embeddings.","score":1},{"url":"https://www.semanticscholar.org/paper/445f82be81e37b064730285cf8de70edce80d6a5","title":"Wine is Not v i n. - On the Compatibility of Tokenizations Across Languages","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":32,"citationCount":6,"influentialCitationCount":2,"publicationDate":"13/09/2021","authors":"Antonis Maronikolakis,Philipp Dufter,Hinrich Schütze","id":"445f82be81e37b064730285cf8de70edce80d6a5","summary":"It is shown that the compatibility measure proposed allows the system designer to create vocabularies across languages that are compatible – a desideratum that so far has been neglected in multilingual models.","score":1},{"url":"https://www.semanticscholar.org/paper/633780929ae262e461ea35c20b36a5d7042350e7","title":"MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":58,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/09/2021","authors":"Rasmus Jorgensen,Mareike Hartmann,Xiang Dai,Desmond Elliott","id":"633780929ae262e461ea35c20b36a5d7042350e7","summary":"Evaluation on nine domain-specific datasets show that a single multilingual domain- specific model can outperform the general multilingual model, and performs close to its monolingual counterpart.","score":1},{"url":"https://www.semanticscholar.org/paper/6b2062812d2e353ea884a0cc077e9f6c73351423","title":"On Generalist and Domain-Specific Music Classification Models and Their Impacts on Brazilian Music Genre Recognition","venue":"Anais do XVIII Simpósio Brasileiro de Computação Musical (SBCM 2021)","year":2021,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2021","authors":"Diego Furtado Silva,A. Silva,Luís Felipe Ortolan,R. Marcacini","id":"6b2062812d2e353ea884a0cc077e9f6c73351423","summary":"It is empirically show that models trained on specific-domain data perform better than generalist models to classify music in the same domain, even trained with a smaller dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/070759c65bd32df9f1f57ef52a4c49a77d3057d1","title":"To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource NLP","venue":"","year":2021,"referenceCount":70,"citationCount":4,"influentialCitationCount":0,"publicationDate":"18/11/2021","authors":"Gozde Gul cSahin","id":"070759c65bd32df9f1f57ef52a4c49a77d3057d1","summary":"Three categories of text augmentation methodologies which perform changes on the syntax, token and character levels are investigated, finding the experimented techniques to be effective on morphologically rich languages in general rather than analytic languages such as Vietnamese.","score":1},{"url":"https://www.semanticscholar.org/paper/bd0e39d075553a68e7af6fdf437c2a890ad59a58","title":"Scene-Text Aware Image and Text Retrieval with Dual-Encoder","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shumpei Miyawaki,Taku Hasegawa,Kyosuke Nishida,Takuma Kato,Jun Suzuki","id":"bd0e39d075553a68e7af6fdf437c2a890ad59a58","summary":"This work proposes pre-training methods that encourage a joint understanding of the scene-text and surrounding visual information and demonstrates that these methods improve the retrieval performances of the dual-encoder models.","score":1},{"url":"https://www.semanticscholar.org/paper/d3f72c17404ba7f8e56d6d4d7fdf9d808f559570","title":"Fine-tuning de modèles de langues pour la veille épidémiologique multilingue avec peu de ressources (Fine-tuning Language Models for Low-resource Multilingual Epidemic Surveillance)","venue":"JEPTALNRECITAL","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Stephen Mutuvi,Emanuela Boros,A. Doucet,Adam Jatowt,G. Lejeune,Moses Odeo","id":"d3f72c17404ba7f8e56d6d4d7fdf9d808f559570","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/a4fc3d86b84f351e87bcdbe1376eae64750629ac","title":"Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Gábor Berend","id":"a4fc3d86b84f351e87bcdbe1376eae64750629ac","summary":"This paper advocates for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism and demonstrates the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure.","score":1},{"url":"https://www.semanticscholar.org/paper/f1edc02e4096acb8ea491b4b47c2cbdf0ff3ec91","title":"Writing System and Speaker Metadata for 2,800+ Language Varieties","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":43,"citationCount":8,"influentialCitationCount":1,"publicationDate":2022,"authors":"D. Esch,Tamar Lucassen,Sebastian Ruder,Isaac Caswell,Clara Rivera","id":"f1edc02e4096acb8ea491b4b47c2cbdf0ff3ec91","summary":"An open-source dataset providing metadata for about 2,800 language varieties used in the world today, as well as an estimated speaker count for each variety, which is the largest publicly-available, machine-readable resource with writing system and speaker information for the world’s languages.","score":1},{"url":"https://www.semanticscholar.org/paper/ea786f376c51dba5c3be79eed6e4814e8afc9290","title":"Faster and Cheaper Energy Demand Forecasting at Scale","venue":"","year":2022,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"ea786f376c51dba5c3be79eed6e4814e8afc9290","summary":"Transplit, a new lightweight transformer-based model, is introduced, which significantly decreases this cost by exploiting the seasonality property and learning typical days of power demand.","score":1},{"url":"https://www.semanticscholar.org/paper/02ef976bc4aedceca2a6d7577975113db1e0e26d","title":"Are Multilingual Sentiment Models Equally Right for the Right Reasons?","venue":"BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"R. Jørgensen,Fiammetta Caccavale,C. Igel,Anders Søgaard","id":"02ef976bc4aedceca2a6d7577975113db1e0e26d","summary":"This work provides a new trilingual, parallel corpus of rationale annotations for English, Danish, and Italian sentiment analysis models and uses it to benchmark models and interpretability methods and proposes rank-biased overlap as a better metric for comparing input token attributions to human rationale annotations.","score":1},{"url":"https://www.semanticscholar.org/paper/3885bdef44dbbcaae85a6b4ccaf279593daadb80","title":"Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems","venue":"Journal of Artificial Intelligence Research","year":2021,"referenceCount":317,"citationCount":12,"influentialCitationCount":0,"publicationDate":"17/04/2021","authors":"E. Razumovskaia,Goran Glavavs,Olga Majewska,E. Ponti,A. Korhonen,Ivan Vulic","id":"3885bdef44dbbcaae85a6b4ccaf279593daadb80","summary":"An extensive overview of existing methods and resources in multilingual ToD is provided as an entry point  to this exciting and emerging field and draws parallels between components of the ToD pipeline and other NLP tasks, which can inspire solutions for learning in low-resource scenarios.","score":1},{"url":"https://www.semanticscholar.org/paper/9076b287843441fb749d37b69317162c1fa272e3","title":"XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond","venue":"International Conference on Language Resources and Evaluation","year":2021,"referenceCount":43,"citationCount":22,"influentialCitationCount":5,"publicationDate":"25/04/2021","authors":"Francesco Barbieri,Luis Espinosa Anke,José Camacho-Collados","id":"9076b287843441fb749d37b69317162c1fa272e3","summary":"XLM-T is introduced, a model to train and evaluate multilingual language models in Twitter with a new strong multilingual baseline consisting of an XLM-R model pre-trained on millions of tweets in over thirty languages, alongside starter code to subsequently fine-tune on a target task.","score":1},{"url":"https://www.semanticscholar.org/paper/21230f72afae4f19f59c4ce9c099075e7ecfe77c","title":"gaBERT — an Irish Language Model","venue":"International Conference on Language Resources and Evaluation","year":2021,"referenceCount":55,"citationCount":4,"influentialCitationCount":1,"publicationDate":"27/07/2021","authors":"James Barry,Joachim Wagner,Lauren Cassidy,Alan Cowap,Teresa Lynn,Abigail Walsh,M'iche'al J. 'O Meachair,Jennifer Foster","id":"21230f72afae4f19f59c4ce9c099075e7ecfe77c","summary":"This work introduces, gaBERT, a monolingual BERT model for the Irish language, and compares it to multilingual BERT and shows that gaberT provides better representations for a downstream parsing task.","score":1},{"url":"https://www.semanticscholar.org/paper/071440ccd1084d20d345fafd0bcaa5993f71fb04","title":"xGQA: Cross-Lingual Visual Question Answering","venue":"Findings","year":2021,"referenceCount":81,"citationCount":16,"influentialCitationCount":3,"publicationDate":"13/09/2021","authors":"Jonas Pfeiffer,Gregor Geigle,Aishwarya Kamath,Jan-Martin O. Steitz,S. Roth,Ivan Vulic,Iryna Gurevych","id":"071440ccd1084d20d345fafd0bcaa5993f71fb04","summary":"This work provides xGQA, a new multilingual evaluation benchmark for the visual question answering task, and extends the established English GQA dataset to 7 typologically diverse languages, enabling us to detect and explore crucial challenges in cross-lingualVisual question answering.","score":1},{"url":"https://www.semanticscholar.org/paper/ed4705fc97d35f8b02a4c11633cb6dd6bd316cc7","title":"Cross-lingual Transfer of Monolingual Models","venue":"International Conference on Language Resources and Evaluation","year":2021,"referenceCount":46,"citationCount":4,"influentialCitationCount":0,"publicationDate":"15/09/2021","authors":"Evangelia Gogoulou,Ariel Ekgren,T. Isbister,Magnus Sahlgren","id":"ed4705fc97d35f8b02a4c11633cb6dd6bd316cc7","summary":"This work introduces a method for transferring monolingual models to other languages through continuous pre-training and studies the effects of such transfer from four different languages to English, finding that model knowledge from the source language enhances the learning of syntactic and semantic knowledge in English.","score":1},{"url":"https://www.semanticscholar.org/paper/dec42306af017bc778bbf1496776f3cd4d5bd42e","title":"Pre-trained transformer-based language models for Sundanese","venue":"Journal of Big Data","year":2021,"referenceCount":50,"citationCount":3,"influentialCitationCount":0,"publicationDate":"27/09/2021","authors":"Wilson Wongso,Henry Lucky,Derwin Suhartono","id":"dec42306af017bc778bbf1496776f3cd4d5bd42e","summary":"Three monolingual Transformer-based language models are pre-trained on Sundanese data that outperformed larger multilingual models despite the smaller overall pre-training data.","score":1},{"url":"https://www.semanticscholar.org/paper/eb0024439858af7cc951ce2efa5a6533c3781799","title":"WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":63,"citationCount":7,"influentialCitationCount":2,"publicationDate":"13/12/2021","authors":"Benjamin Minixhofer,Fabian Paischer,Navid Rekabsaz","id":"eb0024439858af7cc951ce2efa5a6533c3781799","summary":"This work introduces a novel method – called WECHSEL – to efficiently and effectively transfer pretrained LMs to new languages and improves over proposed methods for cross-lingual parameter transfer and outperforms models of comparable size trained from scratch with up to 64x less training effort.","score":1},{"url":"https://www.semanticscholar.org/paper/698a183048f9a81f5c37facfd5fd8454aa813f4c","title":"To Augment or Not to Augment? A Comparative Study on Text Augmentation Techniques for Low-Resource NLP","venue":"International Conference on Computational Logic","year":2021,"referenceCount":85,"citationCount":3,"influentialCitationCount":0,"publicationDate":"16/12/2021","authors":"Gözde Gül Sahin","id":"698a183048f9a81f5c37facfd5fd8454aa813f4c","summary":"Three categories of text augmentation methodologies that perform changes on the syntax, token, and character levels are investigated, finding the experimented techniques to be effective on morphologically rich languages in general rather than analytic languages such as Vietnamese.","score":1},{"url":"https://www.semanticscholar.org/paper/44f75282915d468413e38ac0e1f59b3ee6860485","title":"Does Transliteration Help Multilingual Language Modeling?","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/01/2022","authors":"Ibraheem Muhammad Moosa,Mahmud Elahi Akhter,Ashfia Binte Habib","id":"44f75282915d468413e38ac0e1f59b3ee6860485","summary":"This paper pretrain two ALBERT models to empirically measure the effect of transliteration on Multilingual Language Models and finds that transliterating closely related languages that use different writing scripts to a common script may improve the downstream task performance of MLLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/63bafbf3e7cfdb576407870137b5751cbb579864","title":"Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":6,"influentialCitationCount":0,"publicationDate":"24/02/2022","authors":"Zhengxuan Wu,Isabel Papadimitriou,Alex Tamkin","id":"63bafbf3e7cfdb576407870137b5751cbb579864","summary":"It is found that by far the most impactful factor for crosslingual transfer is the challenge of aligning the new embeddings with the existing transformer layers (18% drop), with little additional effect from switching tokenizers or word morphologies.","score":1},{"url":"https://www.semanticscholar.org/paper/b28e23afe988ad83ab5bd9ce7f826c140c533be8","title":"Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":42,"citationCount":3,"influentialCitationCount":1,"publicationDate":"03/03/2022","authors":"Vaidehi Patil,P. Talukdar,Sunita Sarawagi","id":"b28e23afe988ad83ab5bd9ce7f826c140c533be8","summary":"It is argued that relatedness among languages in a language family along the dimension of lexical overlap may be leveraged to overcome some of the corpora limitations of LRLs and proposed Overlap BPE (OBPE), a simple yet effective modification to the BPE vocabulary generation algorithm which enhances overlap across related languages.","score":1},{"url":"https://www.semanticscholar.org/paper/d97867578a50f824a19168c024e138e3ca482746","title":"Deep Lexical Hypothesis: Identifying personality structure in natural language","venue":"Journal of Personality and Social Psychology","year":2022,"referenceCount":138,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/03/2022","authors":"A. Cutler,D. Condon","id":"d97867578a50f824a19168c024e138e3ca482746","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/597cad6c7b9de94eecc153c7cdcaf824905fe915","title":"You Are What You Write: Preserving Privacy in the Era of Large Language Models","venue":"ArXiv","year":2022,"referenceCount":146,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/04/2022","authors":"Richard Plant,V. Giuffrida,Dimitra Gkatzia","id":"597cad6c7b9de94eecc153c7cdcaf824905fe915","summary":"This paper presents the first wide coverage evaluation and comparison of some of the most popular privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment analysis annotated with demographic information (location, age and gender).","score":1},{"url":"https://www.semanticscholar.org/paper/04207549bf872158d117600029dbe1f1cf8e5b59","title":"Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages","venue":"NLPPOWER","year":2022,"referenceCount":46,"citationCount":3,"influentialCitationCount":0,"publicationDate":"12/05/2022","authors":"Kabir Ahuja,Sandipan Dandapat,Sunayana Sitaram,M. Choudhury","id":"04207549bf872158d117600029dbe1f1cf8e5b59","summary":"It is proposed that the recent work done in Performance Prediction for NLP tasks can serve as a potential solution in fixing benchmarking in Multilingual NLP by utilizing features related to data and language typology to estimate the performance of an MMLM on different languages.","score":1},{"url":"https://www.semanticscholar.org/paper/210980149a6b41d3e8d95c12daa41d6aa391681f","title":"Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":51,"citationCount":7,"influentialCitationCount":0,"publicationDate":"12/05/2022","authors":"Kabir Ahuja,Shanu Kumar,Sandipan Dandapat,M. Choudhury","id":"210980149a6b41d3e8d95c12daa41d6aa391681f","summary":"This work builds upon some of the existing techniques for predicting the zero-shot performance on a task, by modeling it as a multi-task learning problem, and identifies a common set of features that influence zero- shot performance across a variety of tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/99be6dc17a7fd399f4af80c4c1cd7ee5247591a1","title":"Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese","venue":"Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing","year":2022,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/05/2022","authors":"Kurt Micallef,Albert Gatt,Marc Tanti,Lonneke van der Plas,Claudia Borg","id":"99be6dc17a7fd399f4af80c4c1cd7ee5247591a1","summary":"The results show that using a mixture of pre-training domains is often superior to using Wikipedia text only, and that a fraction of this corpus is enough to make signiﬁcant leaps in performance over Wikipedia-trained models.","score":1},{"url":"https://www.semanticscholar.org/paper/d69ec0bbc9fc4fe898ac8cb73f629d253358bf66","title":"Stop Filtering: Multi-View Attribute-Enhanced Dialogue Learning","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Yiwei Li,Bin Sun,Shaoxiong Feng,Kan Li","id":"d69ec0bbc9fc4fe898ac8cb73f629d253358bf66","summary":"A multi-view attribute-enhanced dialogue learning framework that strengthens the attribute-related features more robustly and comprehensively and can improve the performance of models by enhancing dialogue attributes and fusing view-speciﬁc knowledge.","score":1},{"url":"https://www.semanticscholar.org/paper/20b0452359524d15a6182269be51302df850f2a7","title":"hmBERT: Historical Multilingual Language Models for Named Entity Recognition","venue":"Conference and Labs of the Evaluation Forum","year":2022,"referenceCount":28,"citationCount":2,"influentialCitationCount":0,"publicationDate":"31/05/2022","authors":"Stefan Schweter,Luisa März,Katharina Schmid,Erion cCano","id":"20b0452359524d15a6182269be51302df850f2a7","summary":"This work tackles NER for historical German, English, French, Swedish, and Finnish by training large historical language models by circumventing the need for labeled data by using unlabeled data for pretraining a language model.","score":1},{"url":"https://www.semanticscholar.org/paper/6fd36dd51da2a18b53fc7bdb9797f279ceb80462","title":"Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold","venue":"Findings","year":2022,"referenceCount":113,"citationCount":3,"influentialCitationCount":0,"publicationDate":"20/06/2022","authors":"Sebastian Ruder,Ivan Vulic,Anders Søgaard","id":"6fd36dd51da2a18b53fc7bdb9797f279ceb80462","summary":"Historical and recent examples of how the square one bias has led researchers to draw false conclusions or make unwise choices are provided, point to promising yet unexplored directions on the research manifold, and make practical recommendations to enable more multi-dimensional research.","score":1},{"url":"https://www.semanticscholar.org/paper/089fd688de463519a68bd25b11ae1c3eb57b207d","title":"Compositional Evaluation on Japanese Textual Entailment and Similarity","venue":"Transactions of the Association for Computational Linguistics","year":2022,"referenceCount":69,"citationCount":1,"influentialCitationCount":1,"publicationDate":"09/08/2022","authors":"Hitomi Yanaka,K. Mineshima","id":"089fd688de463519a68bd25b11ae1c3eb57b207d","summary":"JSICK, a Japanese NLI/STS dataset that was manually translated from the English dataset SICK is intro-duce and a stress-test dataset for compositional inference is presented, created by transforming syntactic structures of sentences in JSICK to investigate whether language models are sensitive to word order and case particles.","score":1},{"url":"https://www.semanticscholar.org/paper/52b65b29a3d9a7d9e8c016a156454ef8ad80858d","title":"BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":55,"citationCount":3,"influentialCitationCount":1,"publicationDate":"16/08/2022","authors":"Vinura Dhananjaya,Piyumal Demotte,Surangika Ranathunga,Sanath Jayasena","id":"52b65b29a3d9a7d9e8c016a156454ef8ad80858d","summary":"Out of the pre-trained multilingual models that include Sinhala (XLM-R, LaBSE, and LASER), XLM-R is the best model by far forSinhala text classification and is robust in situations where labeled data is insufficient for fine-tuning.","score":1},{"url":"https://www.semanticscholar.org/paper/bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","title":"MonoByte: A Pool of Monolingual Byte-level Language Models","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/09/2022","authors":"H. Abonizio,Leandro Rodrigues de Souza,R. Lotufo,Rodrigo Nogueira","id":"bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","summary":"Experiments on QA and NLI tasks show that monolingual models achieve competitive performance to the multilingual one, and hence can be served to strengthen the understanding of cross-lingual transferability in language models.","score":1},{"url":"https://www.semanticscholar.org/paper/e874510a64c90ec41659338b1f1f6baee1736a15","title":"You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models","venue":"ArXiv","year":2022,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Tomasz Limisiewicz,Daniel Malkin,G. Stanovsky","id":"e874510a64c90ec41659338b1f1f6baee1736a15","summary":"This work proposes a novel multilingual training technique based on teacher-student knowledge distillation based on monolingual teacher models optimized for their language that out-performs standard training methods in low-resource languages and retrains performance on high- resource languages, while using the same amount of data.","score":1},{"url":"https://www.semanticscholar.org/paper/22fbef2bfef213a7619ee4f307e8f42d1888e638","title":"LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":58,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/10/2022","authors":"Hongcheng Guo,Jiaheng Liu,Haoyang Huang,Jian Yang,Zhoujun Li,Dongdong Zhang,Furu Wei","id":"22fbef2bfef213a7619ee4f307e8f42d1888e638","summary":"An effective baseline LVP-M3 using visual prompts is proposed to support translations between different languages, which includes three stages (token encoding, language-aware visual prompt generation, and language translation).","score":1},{"url":"https://www.semanticscholar.org/paper/d3a01c0aeddb98b749bb48c97a48310e91e66d36","title":"Multilingual Multimodality: A Taxonomical Survey of Datasets, Techniques, Challenges and Opportunities","venue":"ArXiv","year":2022,"referenceCount":133,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/10/2022","authors":"Khyathi Raghavi Chandu,A. Geramifard","id":"d3a01c0aeddb98b749bb48c97a48310e91e66d36","summary":"The main goal of this work is to catalogue and characterize these works by charting out the categories of tasks, datasets and methods to address MultiX scenarios and review the languages studied, gold or silver data with parallel annotations, and understand how these modalities and languages interact in modeling.","score":1},{"url":"https://www.semanticscholar.org/paper/296c9f9ba0139a3d7c7a1197960a1c9bce5141e7","title":"Punctuation Restoration for Singaporean Spoken Languages: English, Malay, and Mandarin","venue":"Asia-Pacific Signal and Information Processing Association Annual Summit and Conference","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Abhinav K. Rao,Ho Thi-Nga,Chng Eng Siong","id":"296c9f9ba0139a3d7c7a1197960a1c9bce5141e7","summary":"This work adopts a slot-filling approach that predicts the presence and type of punctuation marks at each word boundary, similar to the Masked-Language Model approach employed during the pre-training stages of BERT but instead of predicting the masked word, the model predicts masked punctuation.","score":1},{"url":"https://www.semanticscholar.org/paper/15a802ca1e77e678db79f61e36cf9eaf6b273c24","title":"MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource Code Completion","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Zi Gong,Yinpeng Guo,Pingyi Zhou,Cuiyun Gao,Yasheng Wang,Zenglin Xu","id":"15a802ca1e77e678db79f61e36cf9eaf6b273c24","summary":"The MultiCoder is proposed to enhance the low-resource code completion via MultiPL pre-training and MultiPL Mixture-of-Experts (MoE) layers and a novel PL-level MoE routing strategy ( PL-MoE ) for improving the code completion on all PLs.","score":1},{"url":"https://www.semanticscholar.org/paper/6cc1d05250511e596e3607827fda4614abb6ed74","title":"Neural Transfer Learning with Transformers for Social Science Text Analysis","venue":"Sociological Methods &amp; Research","year":2022,"referenceCount":68,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Sandra Wankmüller","id":"6cc1d05250511e596e3607827fda4614abb6ed74","summary":"Three Transformer-based models for transfer learning, BERT, RoBERTa, and the Longformer, are compared to conventional machine learning algorithms on three applications and the conventional models are consistently outperformed by transfer learning with Transformers, thereby demonstrating the benefits these models can bring to text-based social science research.","score":1},{"url":"https://www.semanticscholar.org/paper/44e67f6bdb132fadff230b961e61304804f32657","title":"Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Kelly Marchisio,Patrick Lewis,Yihong Chen,Mikel Artetxe","id":"44e67f6bdb132fadff230b961e61304804f32657","summary":"mini-model adaptation is proposed, a compute-efﬁcient alternative that builds a shallow mini-model from a fraction of a large model’s parameters and matches the performance of the standard approach using up to 2.4x less compute.","score":1},{"url":"https://www.semanticscholar.org/paper/bfff952fb890f3eb4ba22718f1df70a030741b74","title":"Cross-Linguistic Syntactic Difference in Multilingual BERT: How Good is It and How Does It Affect Transfer?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Ningyu Xu,Tao Gui,Ruotian Ma,Qi Zhang,Jingting Ye,Menghan Zhang,Xuanjing Huang","id":"bfff952fb890f3eb4ba22718f1df70a030741b74","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/055367775f92ab03d53c470f42bc7284d4a256d8","title":"KOAS: Korean Text Offensiveness Analysis System","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"S. Park,Kang-Min Kim,Seonhee Cho,Jun-Hyung Park,Hyuntae Park,Hyuna Kim,Seon-Young Chung,SangKeun Lee","id":"055367775f92ab03d53c470f42bc7284d4a256d8","summary":"KOAS is presented, a system that fully exploits both contextual and linguistic features and estimates an offensiveness score for a text and constructed a Korean dataset for offensive analysis from various domains.","score":1},{"url":"https://www.semanticscholar.org/paper/710033d861e50f80b28c1fa68316c9ea8ab3c42c","title":"Prediction Difference Regularization against Perturbation for Neural Machine Translation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":45,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Dengji Guo,Zhengrui Ma,M. Zhang,Yanghe Feng","id":"710033d861e50f80b28c1fa68316c9ea8ab3c42c","summary":"This paper utilizes prediction difference for ground-truth tokens to analyze the fitting of token-level samples and finds that under-fitting is almost as common as over-fitting, so prediction difference regularization (PD-R) is introduced, a simple and effective method that can reduce over- fitting and under- fitting at the same time.","score":1},{"url":"https://www.semanticscholar.org/paper/457e73be2f876e0b838f0f8d7aa921993b7f607d","title":"Consistency Training with Virtual Adversarial Discrete Perturbation","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":73,"citationCount":7,"influentialCitationCount":1,"publicationDate":"15/04/2021","authors":"Jungsoo Park,Gyuwan Kim,Jaewoo Kang","id":"457e73be2f876e0b838f0f8d7aa921993b7f607d","summary":"This work proposes an augmentation method of adding a discrete noise that would incur the highest divergence between predictions that outperforms other consistency training baselines with text editing, paraphrasing, or a continuous noise on semi-supervised text classification tasks and a robustness benchmark.","score":1},{"url":"https://www.semanticscholar.org/paper/b19fd8f13c8655775f75764f9a46da778dc177a2","title":"TransFool: An Adversarial Attack against Neural Machine Translation Models","venue":"","year":2023,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Sahar Sadrizadeh,L. Dolamic,P. Frossard","id":"b19fd8f13c8655775f75764f9a46da778dc177a2","summary":"This paper investigates the vulner-ability of Neural Machine Translation (NMT) models to adversarial attacks and proposes a new attack algorithm called TransFool, which builds on a multi-term optimization problem and a gradient projection step to fool NMT models.","score":1},{"url":"https://www.semanticscholar.org/paper/5bfb0b5494885d35bc15952c025fa2d8fbbd8c98","title":"Explicit Contextual Semantics for Text Comprehension","venue":"ArXiv","year":2018,"referenceCount":57,"citationCount":27,"influentialCitationCount":5,"publicationDate":"08/09/2018","authors":"Zhuosheng Zhang,Yuwei Wu,Z. Li,Hai Zhao","id":"5bfb0b5494885d35bc15952c025fa2d8fbbd8c98","summary":"This paper makes the first attempt to let SRL enhance text comprehension and inference through specifying verbal predicates and their corresponding semantic roles, and shows that the salient labels can be conveniently added to existing models and significantly improve deep learning models in challenging text comprehension tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/c70df347923d90e05ff19ebd724ffc2fd6232594","title":"Neural-based Pinyin-to-Character Conversion with Adaptive Vocabulary","venue":"ArXiv","year":2018,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/11/2018","authors":"Yafang Huang,Zhuosheng Zhang,Hai Zhao","id":"c70df347923d90e05ff19ebd724ffc2fd6232594","summary":"A neural P2C conversion model augmented by a large online updating vocabulary with a target vocabulary sampling mechanism is proposed that reduces the decoding time on CPUs up to 50$\\%$ on pinyin-to-character tasks at the same or only negligible change in conversion accuracy.","score":1},{"url":"https://www.semanticscholar.org/paper/c8e707a0ea3a1f3bc104f191e72c4444b28be834","title":"SJTU at MRP 2019: A Transition-Based Multi-Task Parser for Cross-Framework Meaning Representation Parsing","venue":"Conference on Computational Natural Language Learning","year":2019,"referenceCount":45,"citationCount":8,"influentialCitationCount":2,"publicationDate":2019,"authors":"Hongxiao Bai,Zhao Hai","id":"c8e707a0ea3a1f3bc104f191e72c4444b28be834","summary":"This work defines a set of the transition actions to once-for-all tackle all the frameworks and train a transition-based model to parse the meaning representation and achieves 42% F1 unified MRP metric score in the CoNLL 2019 Shared Task.","score":1},{"url":"https://www.semanticscholar.org/paper/be4e226afde6879620d4563f3c39cf3443e22d1f","title":"SJTU-NICT at MRP 2019: Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing","venue":"Conference on Computational Natural Language Learning","year":2019,"referenceCount":42,"citationCount":15,"influentialCitationCount":1,"publicationDate":2019,"authors":"Z. Li,Zhao Hai,Zhuosheng Zhang,Rui Wang,M. Utiyama,E. Sumita","id":"be4e226afde6879620d4563f3c39cf3443e22d1f","summary":"This paper describes SJTU-NICT’s system for participating in the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL) and introduces multi-task learning for multiple objectives within the same framework.","score":1},{"url":"https://www.semanticscholar.org/paper/dc477a26c162ddfa8ab36d4f7975f62f5ad04ab5","title":"Open Vocabulary Learning for Neural Chinese Pinyin IME","venue":"Annual Meeting of the Association for Computational Linguistics","year":2018,"referenceCount":49,"citationCount":18,"influentialCitationCount":4,"publicationDate":"11/11/2018","authors":"Zhuosheng Zhang,Yafang Huang,Hai Zhao","id":"dc477a26c162ddfa8ab36d4f7975f62f5ad04ab5","summary":"The proposed neural P2C conversion model augmented by an online updated vocabulary with a sampling mechanism to support open vocabulary learning during IME working outperforms commercial IMEs and state-of-the-art traditional models on standard corpus and true inputting history dataset in terms of multiple metrics.","score":1},{"url":"https://www.semanticscholar.org/paper/72c0e5b7365ae4981db13cfa15ca808a8eb3a8a1","title":"Head-Driven Phrase Structure Grammar Parsing on Penn Treebank","venue":"Annual Meeting of the Association for Computational Linguistics","year":2019,"referenceCount":80,"citationCount":106,"influentialCitationCount":10,"publicationDate":"05/07/2019","authors":"Junru Zhou,Zhao Hai","id":"72c0e5b7365ae4981db13cfa15ca808a8eb3a8a1","summary":"This paper makes the first attempt to formulate a simplified HPSG by integrating constituent and dependency formal representations into head-driven phrase structure, and proposes two parsing algorithms respectively for two converted tree representations, division span and joint span.","score":1},{"url":"https://www.semanticscholar.org/paper/4d68c1b4167f858979c6a8e8b9ad0b484cd48c63","title":"Cross Aggregation of Multi-head Attention for Neural Machine Translation","venue":"Natural Language Processing and Chinese Computing","year":2019,"referenceCount":51,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/10/2019","authors":"Juncheng Cao,Zhao Hai,Kai Yu","id":"4d68c1b4167f858979c6a8e8b9ad0b484cd48c63","summary":"Experimental results on the machine translation task show that the proposed Cross Aggregation with an iterative routing-by-agreement algorithm help the model outperform the strong Transformer baseline significantly.","score":1},{"url":"https://www.semanticscholar.org/paper/b134ce39a3a195f5b0c277a1d2c0d776e5ce6a52","title":"Syntax-aware Transformer Encoder for Neural Machine Translation","venue":"International Conference on Asian Language Processing","year":2019,"referenceCount":41,"citationCount":11,"influentialCitationCount":1,"publicationDate":"01/11/2019","authors":"Sufeng Duan,Hai Zhao,Junru Zhou,Rui Wang","id":"b134ce39a3a195f5b0c277a1d2c0d776e5ce6a52","summary":"This paper empirically compares two ways, positional encoding and input embedding, to exploit syntactic clues from dependency tree over source sentence to explore effective ways to introduce syntax into Transformer for better machine translation.","score":1},{"url":"https://www.semanticscholar.org/paper/3e2a688a97f5158d441fce9387592de1e3ab1264","title":"Porous Lattice-based Transformer Encoder for Chinese NER","venue":"","year":2019,"referenceCount":48,"citationCount":11,"influentialCitationCount":2,"publicationDate":"07/11/2019","authors":"Xue Mengge,Yu Bowen,Liu Tingwen,Wang Bin,Meng Erli,Liu Quangang","id":"3e2a688a97f5158d441fce9387592de1e3ab1264","summary":"A porous lattice-based transformer encoder for Chinese named entity recognition is proposed, capable to better exploit the GPU parallelism and batch the computation owing to the mask mechanism in transformer.","score":1},{"url":"https://www.semanticscholar.org/paper/bafc0d4f9c6f4d05b89e5e12cd9e88b93c70f408","title":"Porous Lattice-based Transformer Encoder for Chinese NER","venue":"ArXiv","year":2019,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":"07/11/2019","authors":"Mengge Xue,Bowen Yu,Tingwen Liu,Bin Wang,Erli Meng,Quangang Li","id":"bafc0d4f9c6f4d05b89e5e12cd9e88b93c70f408","summary":"A porous lattice-based transformer encoder for Chinese named entity recognition is proposed, capable to better exploit the GPU parallelism and batch the computation owing to the mask mechanism in transformer.","score":1},{"url":"https://www.semanticscholar.org/paper/4452fbcd04370f2cfbc46066bef3b749a7b3b5a4","title":"Korean-to-Chinese Machine Translation using Chinese Character as Pivot Clue","venue":"ArXiv","year":2019,"referenceCount":51,"citationCount":5,"influentialCitationCount":0,"publicationDate":"25/11/2019","authors":"Jeonghyeok Park,Zhao Hai","id":"4452fbcd04370f2cfbc46066bef3b749a7b3b5a4","summary":"This work adopts Chinese characters as a translation pivot by converting Sino-Korean words in Korean sentences to Chinese characters and then train the machine translation model with the converted Korean sentences as source sentences to improve translation quality.","score":1},{"url":"https://www.semanticscholar.org/paper/875fe31a7adaf7462ae5b91231572e1ac24f0145","title":"SWITCHING-ALIGNED-WORDS DATA AUGMENTATION","venue":"","year":2020,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":2020,"authors":"","id":"875fe31a7adaf7462ae5b91231572e1ac24f0145","summary":"This paper randomly replaces words or mixup with their aligned alternatives in another language when training neural machine translation models by using only the original training data without extra data, which significantly outperforms the baseline models.","score":1},{"url":"https://www.semanticscholar.org/paper/8c473a8adca5635c3cde5af793ed7b68afec9d77","title":"Dual Co-Matching Network for Multi-choice Reading Comprehension","venue":"AAAI Conference on Artificial Intelligence","year":2019,"referenceCount":41,"citationCount":95,"influentialCitationCount":14,"publicationDate":"27/01/2019","authors":"Shuailiang Zhang,Zhao Hai,Yuwei Wu,Zhuosheng Zhang,Xi Zhou,Xiaoping Zhou","id":"8c473a8adca5635c3cde5af793ed7b68afec9d77","summary":"This work proposes dual co-matching network (DCMN) which models the relationship among passage, question and answer options bidirectionally and integrates two reading strategies into the model, inspired by how humans solve multi-choice questions.","score":1},{"url":"https://www.semanticscholar.org/paper/47a60929d2e3a54511b89d28f8b4f1f43f764808","title":"Named Entity Recognition Only from Word Embeddings","venue":"Conference on Empirical Methods in Natural Language Processing","year":2019,"referenceCount":48,"citationCount":12,"influentialCitationCount":1,"publicationDate":"31/08/2019","authors":"Ying Luo,Hai Zhao,Junlang Zhan","id":"47a60929d2e3a54511b89d28f8b4f1f43f764808","summary":"This work proposes a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings and designs an instance selector based on reinforcement learning to distinguish positive sentences from noisy sentences and refine these coarse-grained annotations through neural networks.","score":1},{"url":"https://www.semanticscholar.org/paper/18621213820fb05948b326e6d52dfff2deae3ea5","title":"Hierarchical Contextualized Representation for Named Entity Recognition","venue":"AAAI Conference on Artificial Intelligence","year":2019,"referenceCount":52,"citationCount":80,"influentialCitationCount":13,"publicationDate":"06/11/2019","authors":"Ying Luo,Fengshun Xiao,Zhao Hai","id":"18621213820fb05948b326e6d52dfff2deae3ea5","summary":"This paper proposes a model augmented with hierarchical contextualized representation: sentence- level representation and document-level representation that takes different contributions of words in a single sentence into consideration to enhance the sentence representation learned from an independent BiLSTM via label embedding attention mechanism.","score":1},{"url":"https://www.semanticscholar.org/paper/011be9f75e5c009eeb4ff8f220a73d716befac76","title":"English to Urdu: Optimizing Sequence Learning in Neural Machine Translation","venue":"2020 3rd International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)","year":2020,"referenceCount":29,"citationCount":2,"influentialCitationCount":1,"publicationDate":"01/01/2020","authors":"Shahid Iqbal Rai,Muhammad U. S. Khan,M. Waqas Anwar","id":"011be9f75e5c009eeb4ff8f220a73d716befac76","summary":"A seq2seq encoder-decoder model named Convolutional English to Urdu Translation (CEUT) has been proposed, which reduces nonlinearity in attention word mapping and achieves 29.94 BLEU score in English toUrdu translation.","score":1},{"url":"https://www.semanticscholar.org/paper/66c81a4cd0ba6f2cfce8e1e76ec1d2dd0e389add","title":"A Hierarchical Clustering Approach to Fuzzy Semantic Representation of Rare Words in Neural Machine Translation","venue":"IEEE transactions on fuzzy systems","year":2020,"referenceCount":43,"citationCount":13,"influentialCitationCount":0,"publicationDate":"27/01/2020","authors":"Muyun Yang,Shujie Liu,Kehai Chen,Hongyang Zhang,Enbo Zhao,T. Zhao","id":"66c81a4cd0ba6f2cfce8e1e76ec1d2dd0e389add","summary":"This article proposes to build a fuzzy semantic representation (FSR) method for rare words through a hierarchical clustering method to group rare words together, and integrate it into the encoder–decoder framework.","score":1},{"url":"https://www.semanticscholar.org/paper/3fa8d2a9e9a9cf3ee9626424a157888580dcfaba","title":"A Survey of Deep Learning Techniques for Neural Machine Translation","venue":"ArXiv","year":2020,"referenceCount":153,"citationCount":56,"influentialCitationCount":3,"publicationDate":"18/02/2020","authors":"Shu Yang,Yuxin Wang,X. Chu","id":"3fa8d2a9e9a9cf3ee9626424a157888580dcfaba","summary":"This literature survey traces back the origin and principal development timeline of NMT, investigates the important branches, categorizes different research orientations, and discusses some future research trends in this field.","score":1},{"url":"https://www.semanticscholar.org/paper/7066df8fd89cca546d1ef3d66679cb15eba48d50","title":"FLAT: Chinese NER Using Flat-Lattice Transformer","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":26,"citationCount":163,"influentialCitationCount":40,"publicationDate":"01/04/2020","authors":"Xiaonan Li,Hang Yan,Xipeng Qiu,Xuanjing Huang","id":"7066df8fd89cca546d1ef3d66679cb15eba48d50","summary":"Experiments show FLAT outperforms other lexicon-based models in performance and efficiency and can fully leverage the lattice information and has an excellent parallel ability.","score":1},{"url":"https://www.semanticscholar.org/paper/876be9b226601821eeade310013506a03f023824","title":"Capsule-Transformer for Neural Machine Translation","venue":"ArXiv","year":2020,"referenceCount":33,"citationCount":3,"influentialCitationCount":0,"publicationDate":"30/04/2020","authors":"Sufeng Duan,Juncheng Cao,Hai Zhao","id":"876be9b226601821eeade310013506a03f023824","summary":"The capsule-Transformer is proposed, which extends the linear transformation into a more general capsule routing algorithm by taking SAN as a special case of capsule network and is capable of obtaining a better attention distribution representation of the input sequence via information aggregation among different heads and words.","score":1},{"url":"https://www.semanticscholar.org/paper/93d22f0c38e1c465d75ce1e4ca870ed95a3ef0b8","title":"Bipartite Flat-Graph Network for Nested Named Entity Recognition","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":40,"citationCount":36,"influentialCitationCount":4,"publicationDate":"01/05/2020","authors":"Ying Luo,Hai Zhao","id":"93d22f0c38e1c465d75ce1e4ca870ed95a3ef0b8","summary":"A novel bipartite flat-graph network for nested named entity recognition (NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner layers, which outperforms previous state-of-the-art models.","score":1},{"url":"https://www.semanticscholar.org/paper/b3eaf6ac2bc38d42585fcb20cfcb40ccf3ac4629","title":"Learning Spoken Language Representations with Neural Lattice Language Modeling","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":35,"citationCount":7,"influentialCitationCount":0,"publicationDate":"01/07/2020","authors":"Chao-Wei Huang,Yun-Nung Vivian","id":"b3eaf6ac2bc38d42585fcb20cfcb40ccf3ac4629","summary":"A framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks and reduces the demands of speech data and has better efficiency is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/479c9cefe82c774471ed96800835db34bf04e3d3","title":"High-order Semantic Role Labeling","venue":"Findings","year":2020,"referenceCount":73,"citationCount":22,"influentialCitationCount":4,"publicationDate":"09/10/2020","authors":"Z. Li,Hai Zhao,Rui Wang,Kevin Parnow","id":"479c9cefe82c774471ed96800835db34bf04e3d3","summary":"A high-order graph structure is introduced for the neural semantic role labeling model, which enables the model to explicitly consider not only the isolated predicate- argument pairs but also the interaction between the predicate-argument pairs.","score":1},{"url":"https://www.semanticscholar.org/paper/067906c924810e8ffc595ff8c9c4b0b2906cca85","title":"SJTU-NICT’s Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task","venue":"Conference on Machine Translation","year":2020,"referenceCount":66,"citationCount":13,"influentialCitationCount":4,"publicationDate":"11/10/2020","authors":"Z. Li,Hai Zhao,Rui Wang,Kehai Chen,M. Utiyama,E. Sumita","id":"067906c924810e8ffc595ff8c9c4b0b2906cca85","summary":"This paper introduced the joint team SJTU-NICT’s participation in the WMT 2020 machine translation shared task and used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning.","score":1}]}
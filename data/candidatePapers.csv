"id","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c",11,"Incorporating Context into Subword Vocabularies","SaGe, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase, is presented, showing its robustness to language properties such as morphological exponence and agglutination.","Conference of the European Chapter of the Association for Computational Linguistics",2022,"Shaked Yehezkel,Yuval Pinter",2,47,0,"https://www.semanticscholar.org/paper/a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c"
"23f4b6432b74e5db05da04e354341807f5044f7e",10,"Language Modelling with Pixels","PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels, and is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.","International Conference on Learning Representations",2022,"Phillip Rust,Jonas F. Lotz,Emanuele Bugliarello,Elizabeth Salesky,Miryam de Lhoneux,Desmond Elliott",17,135,2,"https://www.semanticscholar.org/paper/23f4b6432b74e5db05da04e354341807f5044f7e"
"67c28d697460b684a0ba97d989719b4ed3c9cffc",6,"Elementwise Language Representation","Using this novel approach, the standard transformer architecture can be reused for all levels of language representations and be able to process much longer sequences at the same time-complexity without""any""architectural modification and additional overhead.","arXiv.org",2023,"Du-Yeong Kim,Jeeeun Kim",0,57,0,"https://www.semanticscholar.org/paper/67c28d697460b684a0ba97d989719b4ed3c9cffc"
"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b",5,"CLIPPO: Image-and-Language Understanding from Pixels Only","The fact that CLIPPO does not require a tokenizer is exploited to show that it can achieve strong performance on multilingual multimodal retrieval without modifications, and it can obtain good accuracy in visual question answering, simply by rendering the question and image together.","",2022,"M. Tschannen,Basil Mustafa,N. Houlsby",3,84,0,"https://www.semanticscholar.org/paper/ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b"
"0290943797ce77535c7b448dc86f4f97d344332e",5,"Language Model Tokenizers Introduce Unfairness Between Languages","It is shown how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked, and the case is made that future language models should be trained using multilingually fair tokenizers.","arXiv.org",2023,"Aleksandar Petrov,Emanuele La Malfa,Philip H. S. Torr,Adel Bibi",0,101,0,"https://www.semanticscholar.org/paper/0290943797ce77535c7b448dc86f4f97d344332e"
"96713d3a63f18ce13ded2a3c5569d3eeba161c49",5,"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models","This work systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale and uses self-supervised models to leverage decompounding during the creation of a subword tokenizer, which it refers to as CompoundPiece.","arXiv.org",2023,"Benjamin Minixhofer,Jonas Pfeiffer,Ivan Vulic",0,54,0,"https://www.semanticscholar.org/paper/96713d3a63f18ce13ded2a3c5569d3eeba161c49"
"6237a49297f45ebfdeabbbf67d06de323d1fccd4",5,"Pixel Representations for Multilingual Translation and Data-efficient Cross-lingual Transfer","It is observed that various properties of pixel representations not only enable seamless cross-lingual transfer to unseen scripts, but make pixel representations more data-efficient than alternatives such as vocabulary expansion.","arXiv.org",2023,"Elizabeth Salesky,Neha Verma,Philipp Koehn,Matt Post",0,40,0,"https://www.semanticscholar.org/paper/6237a49297f45ebfdeabbbf67d06de323d1fccd4"
"17fbffb05fa14e21d1c506fd5f0f568b955fe983",5,"Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models","This work conducts a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages and shows evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.","arXiv.org",2023,"Orevaoghene Ahia,Sachin Kumar,Hila Gonen,Jungo Kasai,David R. Mortensen,Noah A. Smith,Yulia Tsvetkov",0,72,0,"https://www.semanticscholar.org/paper/17fbffb05fa14e21d1c506fd5f0f568b955fe983"
"f8ef90a8cac1a8edf9477688aac24864c547d6cd",5,"Character-Aware Models Improve Visual Text Rendering","This paper trains a suite of image generation models, and shows that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (the authors' DrawText benchmark), and sets a much higher state-of-the-art on visual spelling.","Annual Meeting of the Association for Computational Linguistics",2022,"Rosanne Liu,Daniel H Garrette,Chitwan Saharia,William Chan,Adam Roberts,Sharan Narang,Irina Blok,R. Mical,Mohammad Norouzi,Noah Constant",15,41,6,"https://www.semanticscholar.org/paper/f8ef90a8cac1a8edf9477688aac24864c547d6cd"
"035315281c72763a3e0956775732e64f5f193d82",4,"Natural Language Generation with Pixels","A conditional diffusion-based decoder for modeling rendered natural language that is capable of generating coherent, plausible natural language rendered as images and compared with strong transformer-based autoregressive baselines in both the unconditional and sequence-to-sequence setting.","",2023,"",0,44,0,"https://www.semanticscholar.org/paper/035315281c72763a3e0956775732e64f5f193d82"
"0232715f9089e3a2fc002cff6737bb9939805b8d",4,"Local Byte Fusion for Neural Machine Translation","A Local Byte Fusion (LOBEF) method for byte-based machine translation—utilizing byte n-gram and word boundaries—to aggregate local semantic information to perform competitive to subword models.","Annual Meeting of the Association for Computational Linguistics",2022,"Makesh Narsimhan Sreedhar,Xiangpeng Wan,Yu-Jie Cheng,Junjie Hu",0,45,0,"https://www.semanticscholar.org/paper/0232715f9089e3a2fc002cff6737bb9939805b8d"
"117a0c2c23a5a558cf3b39468d917a750bca720c",4,"Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation","This work performs an extensive comparison across multiple languages and experimental conditions of state-of-the-art character- and subword-level pre-trained models on NMT, showing the effectiveness of character-level modeling in translation, particularly in cases where training data is limited.","",2023,"Lukas Edman,Gabriele Sarti,Antonio Toral,Gertjan van Noord,Arianna Bisazza",1,31,0,"https://www.semanticscholar.org/paper/117a0c2c23a5a558cf3b39468d917a750bca720c"
"5b0c0d181f31acebf2442e7b7c7af27f3f6a5f4a",4,"Hints on the data for language modeling of synthetic languages with transformers","To what extent the critical amount of data varies for languages of different morphological typology, in particular those that have a rich inflectional morphology, and whether the tokenization method to preprocess the data can make a difference, is addressed.","Annual Meeting of the Association for Computational Linguistics",2023,"Rodolfo Zevallos,Núria Bel",0,39,0,"https://www.semanticscholar.org/paper/5b0c0d181f31acebf2442e7b7c7af27f3f6a5f4a"
"9ccad0208b50042d8378b77700146a98bd22ea3f",4,"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation","","Special Interest Group on Computational Morphology and Phonology Workshop",2022,"Khuyagbaatar Batsuren,Gábor Bella,Aryaman Arora,Viktor Martinovi'c,Kyle Gorman,Zdenvek vZabokrtsk'y,A. Ganbold,vS'arka Dohnalov'a,Magda vSevvc'ikov'a,Katevrina Pelegrinov'a,Fausto Giunchiglia,Ryan Cotterell,Ekaterina Vylomova",9,81,0,"https://www.semanticscholar.org/paper/9ccad0208b50042d8378b77700146a98bd22ea3f"
"14efaa989b8db89ad907e6a15253d482cbba77d9",4,"Linguistically inspired roadmap for building biologically reliable protein language models","","Nature Machine Intelligence",2022,"Mai Ha Vu,R. Akbar,Philippe A. Robert,B. Swiatczak,G. K. Sandve,V. Greiff,Dag Trygve Tryslew Haug",2,134,0,"https://www.semanticscholar.org/paper/14efaa989b8db89ad907e6a15253d482cbba77d9"
"0f8f20ef4bc90a2b210bc5be08a7f326a214556f",4,"An Information Extraction Study: Take In Mind the Tokenization!","The main outcome is twofold: tokenization patterns can introduce inductive bias that results in state-of-the-art performance, and the character-based models produce promising results; thus, transitioning to token-free IE models is feasible.","arXiv.org",2023,"Christos Theodoropoulos,Marie-Francine Moens",1,38,0,"https://www.semanticscholar.org/paper/0f8f20ef4bc90a2b210bc5be08a7f326a214556f"
"d7a379254ac2dee2e31dfdeedb440176f0285aa5",4,"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding","This work introduces a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level, and demonstrates that this hierarchical model is robust to textual corruption and domain shift.","Annual Meeting of the Association for Computational Linguistics",2023,"Li Sun,F. Luisier,K. Batmanghelich,D. Florêncio,Changrong Zhang",0,52,0,"https://www.semanticscholar.org/paper/d7a379254ac2dee2e31dfdeedb440176f0285aa5"
"e4d2ba4cb6f364079ce3731e86b860264677ae7d",3,"Downstream Task-Oriented Neural Tokenizer Optimization with Vocabulary Restriction as Post Processing","An example of the BiLSTM-based tokenizer with vocabulary restriction is proposed, which can capture wider contextual information for the tokenization process than non-neural-basedtokenization methods used in existing work.","arXiv.org",2023,"Tatsuya Hiraoka,Tomoya Iwakura",0,26,0,"https://www.semanticscholar.org/paper/e4d2ba4cb6f364079ce3731e86b860264677ae7d"
"e79d1206292bc5e67ba19737d87d4b2ea4a37105",3,"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization","This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion and paves the way for highly performant token-free models that are trained completely end-to-end.","International Conference on Learning Representations",2021,"Yi Tay,Vinh Q. Tran,Sebastian Ruder,Jai Gupta,Hyung Won Chung,Dara Bahri,Zhen Qin,Simon Baumgartner,Cong Yu,Donald Metzler",79,69,15,"https://www.semanticscholar.org/paper/e79d1206292bc5e67ba19737d87d4b2ea4a37105"
"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6",3,"Lifting the Curse of Multilinguality by Pre-training Modular Transformers","This work introduces language-specific modules of their Cross-lingual Modular models from the start, which allows them to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant.","North American Chapter of the Association for Computational Linguistics",2022,"Jonas Pfeiffer,Naman Goyal,Xi Victoria Lin,Xian Li,James Cross,Sebastian Riedel,Mikel Artetxe",47,69,3,"https://www.semanticscholar.org/paper/c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6"
"3e08d84af266026e7d254729f3afc6dcd572d264",3,"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks","This work exhaustively experiments with three popular VEs on six downstream V+L tasks and suggests that diverse VEs complement each other, resulting in improved downstream V-L task performance, where the improvements are not due to simple ensemble effects.","arXiv.org",2022,"Gregor Geigle,Chen Cecilia Liu,Jonas Pfeiffer,Iryna Gurevych",0,42,0,"https://www.semanticscholar.org/paper/3e08d84af266026e7d254729f3afc6dcd572d264"
"69b8db33a7ea7366e48c465b6f35a8d7f8f1a3b5",3,"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models","A new approach for scaling to very large multilingual vocabularies is introduced by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language.","arXiv.org",2023,"Davis Liang,Hila Gonen,Yuning Mao,Rui Hou,Naman Goyal,Marjan Ghazvininejad,Luke Zettlemoyer,Madian Khabsa",7,34,2,"https://www.semanticscholar.org/paper/69b8db33a7ea7366e48c465b6f35a8d7f8f1a3b5"
"5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad",3,"Sub-Character Tokenization for Chinese Pretrained Language Models","Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency, and 2) Pronunciation-based SubChartokenization can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to homophone typos.","Transactions of the Association for Computational Linguistics",2021,"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Yasheng Wang,Qun Liu,Maosong Sun",2,60,0,"https://www.semanticscholar.org/paper/5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad"
"0de580957d23dd65e31b6c95e6bc5d15bc15c57d",3,"Impact of Tokenization on Language Models: An Analysis for Turkish","This work compares five tokenizers at different granularity levels, that is, their outputs vary from the smallest pieces of characters to the surface form of words, and finds that increasing the vocabulary size improves the performance of Morphological- and Word-level tokenizers more than that of de facto tokenizers.","ACM Trans. Asian Low Resour. Lang. Inf. Process.",2022,"Cagri Toraman,E. Yilmaz,Furkan Şahinuç,Oguzhan Ozcelik",9,70,3,"https://www.semanticscholar.org/paper/0de580957d23dd65e31b6c95e6bc5d15bc15c57d"
"29a54c072bcf85fa6934b6f701962a7c9aeb6489",3,"Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora","It is shown that a byte-level model enables higher correction quality than a subword approach, not only for simple spelling errors, but also for more complex semantic, stylistic and grammatical issues.","Annual Meeting of the Association for Computational Linguistics",2023,"Svanhv'it Lilja Ing'olfsd'ottir,Pétur Orri Ragnarsson,H. Jónsson,Haukur Barri S'imonarson,Vilhjálmur Þorsteinsson,Vésteinn Snæbjarnarson",0,55,0,"https://www.semanticscholar.org/paper/29a54c072bcf85fa6934b6f701962a7c9aeb6489"
"3d3f8399d625238fddb366697acb73446129d65c",3,"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing","This work addresses two major problems in existing Arabic PLMs that limit the progress of the Arabic NLU and NLG fields and releases three new Arabic BERT-style models, and achieves a new state-of-the-art performance on discriminative and generative ArabicNLU andNLG tasks.","Conference on Empirical Methods in Natural Language Processing",2022,"Abbas Ghaddar,Yimeng Wu,Sunyam Bagga,Ahmad Rashid,Khalil Bibi,Mehdi Rezagholizadeh,Chao Xing,Yasheng Wang,Xinyu Duan,Zhefeng Wang,Baoxing Huai,Xin Jiang,Qun Liu,P. Langlais",0,81,0,"https://www.semanticscholar.org/paper/3d3f8399d625238fddb366697acb73446129d65c"
"5079e188ae86d8330414edb65e898ed306ac450d",3,"Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation","SSMT unifies subword segmentation and MT in a single trainable model that learns to segment target sentence words while jointly learning to generate target sentences and proves more robust on a test set constructed for evaluating morphological compositional generalisation.","arXiv.org",2023,"Francois Meyer,Jan Buys",0,31,0,"https://www.semanticscholar.org/paper/5079e188ae86d8330414edb65e898ed306ac450d"
"36e1b3973c673d3c323014a168e05c5762f84262",3,"OneCAD: One Classifier for All image Datasets using multimodal learning","This work addresses the question: Is it possible to create a number-of-class-agnostic model architecture and proposes a training and inference framework OneCAD (One Classifier for All image Datasets) to achieve close-to number- of-class,agnostic transformer model.","arXiv.org",2023,"S. Wadekar,E. Culurciello",0,32,0,"https://www.semanticscholar.org/paper/36e1b3973c673d3c323014a168e05c5762f84262"
"491eaab2ff93772d52be8c014bf97466d8c6fe73",3,"Tokenization Tractability for Human and Machine Learning Model: An Annotation Study","A quantitative investigation result is provided that shows the tractable tokenizations for humans and machine learning models are not necessarily the same as each other.","arXiv.org",2023,"Tatsuya Hiraoka,Tomoya Iwakura",0,32,0,"https://www.semanticscholar.org/paper/491eaab2ff93772d52be8c014bf97466d8c6fe73"
"7cdebb73662387d9040da4f27a7dc04dbffa3c3e",3,"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models","This work successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with the authors' styles, and shows that ByG PT5 outperforms other models such as mT5, ByT4, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans.","Annual Meeting of the Association for Computational Linguistics",2022,"Jonas Belouadi,Steffen Eger",4,69,0,"https://www.semanticscholar.org/paper/7cdebb73662387d9040da4f27a7dc04dbffa3c3e"
"8f248b5476666e700390f7f8ff6ca923f97d726c",3,"Advancing protein language models with linguistics: a roadmap for improved interpretability",,"arXiv.org",2022,"Mai Ha Vu,R. Akbar,Philippe A. Robert,B. Swiatczak,V. Greiff,G. K. Sandve,Dag Trygve Tryslew Haug",4,118,0,"https://www.semanticscholar.org/paper/8f248b5476666e700390f7f8ff6ca923f97d726c"
"ea8197cb357af6f89e8b6e5548897236a24719b1",3,"What is the best recipe for character-level encoder-only modelling?","This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level, and finds that the best performing character-level model exceeds the performance of a token-based model trained with the same settings on the same data.","Annual Meeting of the Association for Computational Linguistics",2023,"Kris Cao",0,40,0,"https://www.semanticscholar.org/paper/ea8197cb357af6f89e8b6e5548897236a24719b1"
"5917ba80a905aeba41487f7dcdb8b885f18689f5",3,"Tokenization and the Noiseless Channel","It is proposed that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution.","Annual Meeting of the Association for Computational Linguistics",2023,"Vilém Zouhar,Clara Meister,Juan Luis Gastaldi,L. Du,Mrinmaya Sachan,Ryan Cotterell",0,43,0,"https://www.semanticscholar.org/paper/5917ba80a905aeba41487f7dcdb8b885f18689f5"
"ed7b51e4a5c4835218f6697b280afb2849211939",3,"Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation","An extensive comparison across multiple languages and experimental conditions of state-of-the-art character- and subword-level pre-trained models on NMT shows that the former not only are effective in translation, but frequently outperform subword models, particularly in cases where training data is limited.","arXiv.org",2023,"Lukas Edman,Antonio Toral,Gertjan van Noord",0,28,0,"https://www.semanticscholar.org/paper/ed7b51e4a5c4835218f6697b280afb2849211939"
"ca7636c1d3426dd09a185abf25f81f7a8fa594e1",3,"How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese",,"Annual Meeting of the Association for Computational Linguistics",2023,"T. Fujii,Koki Shibata,Atsuki Yamaguchi,Terufumi Morishita,Yasuhiro Sogawa",0,38,0,"https://www.semanticscholar.org/paper/ca7636c1d3426dd09a185abf25f81f7a8fa594e1"
"77221764ce0eb68d29e1fc35ae61a21179e7dbe5",3,"What changes when you randomly choose BPE merge operations? Not much.","Two simple randomized variants of byte pair encoding are introduced and whether randomizing the selection of merge operations substantially affects a downstream machine translation task is explored, hypothesizing that this task may show sensitivity to the method of choosing subwords.","First Workshop on Insights from Negative Results in NLP",2023,"Jonne Saleva,Constantine Lignos",1,20,0,"https://www.semanticscholar.org/paper/77221764ce0eb68d29e1fc35ae61a21179e7dbe5"
"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852",2,"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation","IT5 is introduced, the first family of encoder-decoder transformer models pretrained specifically on Italian, with the monolingual IT5 models consistently outperforming their multilingual counterparts and setting a new state-of-the-art for most Italian conditional language generation tasks.","arXiv.org",2022,"Gabriele Sarti,M. Nissim",12,40,2,"https://www.semanticscholar.org/paper/19e2f3a1e0c3b8340c14cb83e9ca6878a2598852"
"bd6f44bab6dbb5c93854f67f95f82274adc7d2d0",2,"MonoByte: A Pool of Monolingual Byte-level Language Models","Experiments on QA and NLI tasks show that monolingual models achieve competitive performance to the multilingual one, and hence can be served to strengthen the understanding of cross-lingual transferability in language models.","International Conference on Computational Linguistics",2022,"H. Abonizio,Leandro Rodrigues de Souza,R. Lotufo,Rodrigo Nogueira",0,45,0,"https://www.semanticscholar.org/paper/bd6f44bab6dbb5c93854f67f95f82274adc7d2d0"
"75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a",2,"Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation","An inference strategy that approximates the marginalized likelihood by using multiple segmentations including the most plausible segmentation and several sampled segmentations that improves the performance of models trained with subword regularization in low-resource machine translation tasks.","Findings",2022,"Sho Takase,Tatsuya Hiraoka,Naoaki Okazaki",1,16,0,"https://www.semanticscholar.org/paper/75214f960f82a0ed7fcf6fc0cd70b8d8f1847f8a"
"380605105531d27474190451183bf6ad6126cac8",2,"GPoeT: a Language Model Trained for Rhyme Generation on Synthetic Data","A novel solution for learning to rhyme is proposed, based on synthetic data generated with a rule-based rhyming algorithm and an evaluation metric that uses a phonetic dictionary and the definitions of perfect and assonant rhymes.","LATECHCLFL",2023,"Andrei Popescu-Belis,Àlex R. Atrio,Bastien Bernath,Etienne Boisson,Teo Ferrari,Xavier Theimer-lienhard,Giorgos Vernikos",0,28,0,"https://www.semanticscholar.org/paper/380605105531d27474190451183bf6ad6126cac8"
"d725c41b8e0516d0cf84d8fbd25eb7fc01a47342",2,"A Primer on Pretrained Multilingual Language Models","A review of the existing literature covering the above broad areas of research pertaining toMultilingual Language Models and some promising directions of future research are recommended.","arXiv.org",2021,"Sumanth Doddapaneni,Gowtham Ramesh,Anoop Kunchukuttan,Pratyush Kumar,Mitesh M. Khapra",31,137,3,"https://www.semanticscholar.org/paper/d725c41b8e0516d0cf84d8fbd25eb7fc01a47342"
"61c7e5590064b250ebb5a53c430aede492a4d8ab",2,"BERT for Natural Language Processing in Bahasa Indonesia","The findings in this article can be used as ideas for developing NLP using the BERT model in Indonesian and the downstream task of the Indonesian BERT models are sentiment analysis, classification, and text summarization.","2022 2nd International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA)",2022,"Danny Sebastian,H. Purnomo,I. Sembiring",0,65,0,"https://www.semanticscholar.org/paper/61c7e5590064b250ebb5a53c430aede492a4d8ab"
"3909f91811089dce1c12abd594d2638a4e77d961",2,"Sabi\'a: Portuguese Large Language Models","It is demonstrated that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora, and that the majority of the benefits stem from the domain-specific knowledge acquired through monolingUAL pretraining.","",2023,"Ramon Pires,H. Abonizio,Thales Rog'erio,Rodrigo Nogueira",0,78,0,"https://www.semanticscholar.org/paper/3909f91811089dce1c12abd594d2638a4e77d961"
"d7e0f0cec28c34710fa631df410b717186741db5",2,"A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces","Despite its remarkable simplicity, the proposed streamlined approach for text-to-image generation yields aesthetically pleasing images with few sampling iterations, allows for intriguing ways for conditioning the model, and imparts advantages absent in state-of-the-art techniques.","",2022,"Dominic Rampas,Pablo Pernias,M. Aubreville",2,34,1,"https://www.semanticscholar.org/paper/d7e0f0cec28c34710fa631df410b717186741db5"
"1e4b6567ff66de6cdcbe58c863538241e2f62b45",2,"Aligning Text-to-Image Models using Human Feedback","A fine-tuning method for aligning text-to-image models using human feedback, comprising three stages, that generates objects with specified colors, counts and backgrounds more accurately than the pre-trained model.","arXiv.org",2023,"Kimin Lee,Hao Liu,M. Ryu,Olivia Watkins,Yuqing Du,Craig Boutilier,P. Abbeel,M. Ghavamzadeh,S. Gu",30,50,1,"https://www.semanticscholar.org/paper/1e4b6567ff66de6cdcbe58c863538241e2f62b45"
"10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7",2,"GlyphDraw: Seamlessly Rendering Text with Intricate Spatial Structures in Text-to-Image Generation","GlyphDraw is introduced, a general learning framework aiming to endow image generation models with the capacity to generate images coherently embedded with text for any specific language.","",2023,"Jiancang Ma,Mingjun Zhao,Chen Chen,Ruichen Wang,Di Niu,H. Lu,Xiaodong Lin",0,44,0,"https://www.semanticscholar.org/paper/10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7"
"5fbe4c92791fbecb179c1ab79bba9a59b2e155ba",2,"GlyphControl: Glyph Conditional Control for Visual Text Generation","The empirical evaluations demonstrate that GlyphControl outperforms the recent DeepFloyd IF approach in terms of OCR accuracy and CLIP scores, highlighting the efficacy of the method.","arXiv.org",2023,"Yukang Yang,Dongnan Gui,Yuhui Yuan,Haisong Ding,Hang-Rui Hu,Kai Chen",0,28,0,"https://www.semanticscholar.org/paper/5fbe4c92791fbecb179c1ab79bba9a59b2e155ba"
"4c0ac19deb0d76873c32c98dd37a2e7339802a0c",2,"Wuerstchen: Efficient Pretraining of Text-to-Image Models","Wuerstchen is introduced, a novel technique for text-to-image synthesis that unites competitive performance with unprecedented cost-effectiveness and ease of training on constrained hardware, and significantly reduces the computational burden typically associated with state-of-the-art models.","arXiv.org",2023,"Pablo Pernias,Dominic Rampas,M. Aubreville",0,45,0,"https://www.semanticscholar.org/paper/4c0ac19deb0d76873c32c98dd37a2e7339802a0c"
"a903e1e0ffd04dd666f3537f6570d742d7be3486",2,"Grounded Text-to-Image Synthesis with Attention Refocusing","This paper proposes two novel losses to refocus the attention maps according to a given layout during the sampling process and integrates easily and effectively into existing text-to-image methods and consistently improve their alignment between the generated images and the text prompts.","arXiv.org",2023,"Quynh Phung,Songwei Ge,Jia-Bin Huang",1,48,0,"https://www.semanticscholar.org/paper/a903e1e0ffd04dd666f3537f6570d742d7be3486"
"d7890d1906d95c4ae4c430b350455156d6d8aed9",2,"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis","It is demonstrated that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators.","",2023,"Dustin Podell,Zion English,Kyle Lacey,A. Blattmann,Tim Dockhorn,Jonas Muller,Joe Penna,Robin Rombach",0,54,0,"https://www.semanticscholar.org/paper/d7890d1906d95c4ae4c430b350455156d6d8aed9"
"bc39d16c108057e062ad6f1d0e8154df52cafc6a",2,"CMU’s Machine Translation System for IWSLT 2019","Block Multitask Learning (BMTL) is presented, a novel NMT architecture that predicts multiple targets of different granularities simulta- neously, removing the need to search for the optimal subword segmentation strategy.","International Workshop on Spoken Language Translation",2019,"Tejas Srinivasan,Ramon Sanabria,Florian Metze",3,31,0,"https://www.semanticscholar.org/paper/bc39d16c108057e062ad6f1d0e8154df52cafc6a"
"fec6def294027a2ce9094267ce7b7d57f78daf74",2,"Multitask Learning For Different Subword Segmentations In Neural Machine Translation","Block Multitask Learning (BMTL), a novel NMT architecture that predicts multiple targets of different granularities simultaneously, removing the need to search for the optimal segmentation strategy, is presented.","International Workshop on Spoken Language Translation",2019,"Tejas Srinivasan,Ramon Sanabria,Florian Metze",4,32,0,"https://www.semanticscholar.org/paper/fec6def294027a2ce9094267ce7b7d57f78daf74"
"debb3877b778eeb8689729d37e2b90f9f000d877",2,"Neural Machine Translation with Imbalanced Classes","This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and investigates the effect of class imbalance on NMT.","arXiv.org",2020,"Thamme Gowda,Jonathan May",3,32,0,"https://www.semanticscholar.org/paper/debb3877b778eeb8689729d37e2b90f9f000d877"
"dca4128a33ca22c02031b5c0c28548a0df022d80",2,"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding","The Embedding Barrier is introduced, a phenomenon that limits the monolingual performance of multilingual models on low-resource languages having unique typologies and a straightforward solution by transcribing languages to a common script is proposed, which can effectively improve the performance of a multilingual model for the Bangla language.","",2021,"Abhik Bhattacharjee,Tahmid Hasan,Kazi Samin,Md. Saiful Islam,M. Rahman,Anindya Iqbal,Rifat Shahriyar",7,56,1,"https://www.semanticscholar.org/paper/dca4128a33ca22c02031b5c0c28548a0df022d80"
"e5f6506f9332fcdb574f13a791e4f3c42b80ca90",2,"Demystifying Neural Language Models' Insensitivity to Word-Order","Investigating the insensitivity of natural language models to word-order by quantifying perturbations and analysing their effect on neural models’ performance on language understanding tasks in GLUE benchmark finds that neural language models — pretrained and non-pretrained Transformers, LSTMs, and Convolutional architectures — require local ordering more so than the global ordering of tokens.","arXiv.org",2021,"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar",10,69,1,"https://www.semanticscholar.org/paper/e5f6506f9332fcdb574f13a791e4f3c42b80ca90"
"d53d4ff9f3bda51534184344845a78f8c02badd9",2,"DiatopIt: A Corpus of Social Media Posts for the Study of Diatopic Language Variation in Italy","The first corpus specifically focused on diatopic language variation in Italy for language varieties other than Standard Italian is introduced, and the representativeness of DiatopIt is assessed, and it is shown that the density of non-Standard Italian content across areas correlates with actual language use.","Workshop on NLP for Similar Languages, Varieties and Dialects",2023,"Alan Ramponi,Camilla Casula",0,31,0,"https://www.semanticscholar.org/paper/d53d4ff9f3bda51534184344845a78f8c02badd9"
"a20a802839d72bee1c85f4a1cb77addadacb2179",2,"Specializing Multilingual Language Models: An Empirical Study","These evaluations on part-of-speech tagging, universal dependency parsing, and named entity recognition in nine diverse low-resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low- resource settings.","MRL",2021,"Ethan C. Chau,Noah A. Smith",14,72,0,"https://www.semanticscholar.org/paper/a20a802839d72bee1c85f4a1cb77addadacb2179"
"31852f9fc732c0868af12d631c72693702d80521",2,"Text Data Augmentation for Deep Learning","The major motifs of Data Augmentation are summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form.","Journal of Big Data",2021,"Connor Shorten,T. Khoshgoftaar,B. Furht",113,125,5,"https://www.semanticscholar.org/paper/31852f9fc732c0868af12d631c72693702d80521"
"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa",2,"Local Structure Matters Most: Perturbation Study in NLU","It is empirically shown that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure.","Findings",2021,"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar",4,69,0,"https://www.semanticscholar.org/paper/91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa"
"9933a5af7895354087baf6c96b64dc8a8973eaed",2,"Perceiver IO: A General Architecture for Structured Inputs & Outputs","This work proposes Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs and augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering.","International Conference on Learning Representations",2021,"Andrew Jaegle,Sebastian Borgeaud,Jean-Baptiste Alayrac,Carl Doersch,Catalin Ionescu,David Ding,Skanda Koppula,Andrew Brock,Evan Shelhamer,Olivier J. H'enaff,M. Botvinick,Andrew Zisserman,Oriol Vinyals,João Carreira",265,103,45,"https://www.semanticscholar.org/paper/9933a5af7895354087baf6c96b64dc8a8973eaed"
"231e768f0cd280faa0f725bb353262cb4fed08d1",2,"Hierarchical Transformers Are More Efficient Language Models","Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.","NAACL-HLT",2021,"Piotr Nawrot,Szymon Tworkowski,Michał Tyrolski,Lukasz Kaiser,Yuhuai Wu,Christian Szegedy,H. Michalewski",17,38,1,"https://www.semanticscholar.org/paper/231e768f0cd280faa0f725bb353262cb4fed08d1"
"7e3081b0d698f8abf16dee626d782f3339482fe7",2,"An Assessment of the Impact of OCR Noise on Language Models","It is found that OCR noise poses a significant obstacle to language modelling, with language models increasingly diverging from their noiseless targets as OCR quality lowers, and simpler models including PPMI and Word2Vec consistently outperform transformer-based models in this respect.","International Conference on Agents and Artificial Intelligence",2022,"Konstantin Todorov,Giovanni Colavizza",1,51,0,"https://www.semanticscholar.org/paper/7e3081b0d698f8abf16dee626d782f3339482fe7"
"54123c8de8ecacb30ae2e9fb8c4635f4030965b5",2,"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models","A novel method of retroactively adding resilience to misspellings to transformer-based NLP models without the need for re-training of the original NLP model is proposed, which significantly reduces the cost needed to evaluate a model’s resilience to adversarial attacks.","International Conference on Computational Linguistics",2022,"Jan Jezabek,A. Singh",0,26,0,"https://www.semanticscholar.org/paper/54123c8de8ecacb30ae2e9fb8c4635f4030965b5"
"110250df2a6ca0e0e609eaa800a21c17abeedd77",2,"NLP for Language Varieties of Italy: Challenges and the Path Forward","","arXiv.org",2022,"Alan Ramponi",3,85,1,"https://www.semanticscholar.org/paper/110250df2a6ca0e0e609eaa800a21c17abeedd77"
"7bd8859b5920c7b769e6d40dbdbcd857c1770401",2,"Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining","After investigating several ways to boost the robustness of multilingual models in this setting, this work proposes Robust Contrastive Pretraining (RCP), which combines data augmentation with a contrastive loss term at the pretraining stage and achieves large improvements on noisy data.","Conference of the European Chapter of the Association for Computational Linguistics",2022,"Asa Cooper Stickland,Sailik Sengupta,Jason Krone,Saab Mansour,He He",0,41,0,"https://www.semanticscholar.org/paper/7bd8859b5920c7b769e6d40dbdbcd857c1770401"
"9a5b2dc77bda19759df8481aaf283da353ac7e77",2,"Local Structure Matters Most in Most Languages","This work replicates a study on the importance of local structure, and the relative unimportance of global structure, in a multilingual setting and finds that the phenomenon observed on the English language broadly translates to over 120 languages, with a few caveats.","AACL",2022,"Louis Clouâtre,Prasanna Parthasarathi,A. Zouaq,Sarath Chandar",0,37,0,"https://www.semanticscholar.org/paper/9a5b2dc77bda19759df8481aaf283da353ac7e77"
"5e52d654fd31f04c1bd884cd5480e6af8c95ad50",2,"Efficient Transformers with Dynamic Token Pooling","The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget.","Annual Meeting of the Association for Computational Linguistics",2022,"Piotr Nawrot,J. Chorowski,Adrian La'ncucki,E. Ponti",4,55,0,"https://www.semanticscholar.org/paper/5e52d654fd31f04c1bd884cd5480e6af8c95ad50"
"e541fb54b8b9f2b4d8253f678a28830cd4f52d86",2,"A Survey of Text Representation Methods and Their Genealogy","This work contributes threefold to this lack of compilation, composition, and systematization by providing a survey of current approaches, arranging them in a genealogy, and conceptualizing a taxonomy of text representation methods to examine and explain the state-of-the-art.","IEEE Access",2022,"Philipp Siebers,Christian Janiesch,Patrick Zschech",1,104,0,"https://www.semanticscholar.org/paper/e541fb54b8b9f2b4d8253f678a28830cd4f52d86"
"0a438980ac42451d6d32dd2ad8ead7b55520408d",2,"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning","A comprehensive view of transformer architecture, self-supervised learning and pretraining concepts in language models, and their adaptation to downstream tasks is given and future directions to further improvement in pretrained transformer-based language models are presented.","Inf.",2023,"Evans Kotei,Ramkumar Thirunavukarasu",1,128,0,"https://www.semanticscholar.org/paper/0a438980ac42451d6d32dd2ad8ead7b55520408d"
"412e266cddfd87c79087a88ba1e4d11b89a45a13",2,"MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers","Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes, is proposed, establishing the viability of tokenization-free autoregressive sequence modeling at scale.","arXiv.org",2023,"L. Yu,Daniel Simig,Colin Flaherty,Armen Aghajanyan,Luke Zettlemoyer,M. Lewis",5,45,0,"https://www.semanticscholar.org/paper/412e266cddfd87c79087a88ba1e4d11b89a45a13"
"33c017b6298dd6f7d099f88f9667a6ea97131dbc",2,"mPLM-Sim: Unveiling Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models","The experimental results demonstrate that mPLM-Sim is capable of selecting better source languages than linguistic measures, resulting in a 1%-2% improvement in zero-shot cross-lingual transfer performance.","arXiv.org",2023,"Peiqin Lin,Chengzhi Hu,Zheyu Zhang,André F. T. Martins,Hinrich Schütze",0,40,0,"https://www.semanticscholar.org/paper/33c017b6298dd6f7d099f88f9667a6ea97131dbc"
"ec18aef9ccec70b979d6ab3c78d3721736fd5388",2,"Where’s the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation","This work introduces a multilingual punctuation-agnostic sentence segmentation method, currently covering 85 languages, trained in a self-supervised fashion on unsegmented text, by making use of newline characters which implicitly perform segmentation into paragraphs.","Annual Meeting of the Association for Computational Linguistics",2023,"Benjamin Minixhofer,Jonas Pfeiffer,Ivan Vulic",1,55,0,"https://www.semanticscholar.org/paper/ec18aef9ccec70b979d6ab3c78d3721736fd5388"
"e4a95f595b5d60a0858725996b9355f7275492cf",2,"Hierarchical Attention Encoder Decoder","A model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture that independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency is proposed.","arXiv.org",2023,"Asier Mujika",0,33,0,"https://www.semanticscholar.org/paper/e4a95f595b5d60a0858725996b9355f7275492cf"
"5cd4fe8174b582f27d79859e4b79e0eba2a1d5f0",2,"Is Anisotropy Inherent to Transformers?","This paper shows that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences of cross-entropy loss on long-tailed distributions of tokens.","arXiv.org",2023,"Nathan Godey,Eric Villemonte de la Clergerie,Benoît Sagot",0,38,0,"https://www.semanticscholar.org/paper/5cd4fe8174b582f27d79859e4b79e0eba2a1d5f0"
"134e4d72e23bca51e290db171d063989883020f4",2,"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?","It is shown that an embedding-based metric that has the highest correlation with human evaluations on a standard benchmark can have the lowest correlation if the amount of input noise or unknown tokens increases, and the highest robustness is achieved when using character-level embeddings, instead of token-based embeddments, from the first layer of the pretrained model.","International Conference on Computational Linguistics",2022,"Doan Nam Long Vu,N. Moosavi,Steffen Eger",6,28,0,"https://www.semanticscholar.org/paper/134e4d72e23bca51e290db171d063989883020f4"
"d8b0425e83bdf0335092f21147884236956872db",2,"Self-Supervised Multimodal Learning: A Survey","A comprehensive review of the state-of-the-art in SSML, which is categorized along three orthogonal axes: objective functions, data alignment, and model architectures, which correspond to the inherent characteristics of self-supervised learning methods and multimodal data.","arXiv.org",2023,"Yongshuo Zong,Oisin Mac Aodha,Timothy M. Hospedales",1,208,0,"https://www.semanticscholar.org/paper/d8b0425e83bdf0335092f21147884236956872db"
"2ffbe6040369a82d5a003c2bb835e221c9d2f896",2,"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?","It is found that learning to analyse and generate morphologically complex surface representations is still challenging, especially for nonconcatenative morphological phenomena like reduplication or vowel harmony and for rare word stems.","Conference on Empirical Methods in Natural Language Processing",2021,"Chantal Amrhein,Rico Sennrich",9,50,1,"https://www.semanticscholar.org/paper/2ffbe6040369a82d5a003c2bb835e221c9d2f896"
"940e3ee32edfe425e958a672644a0bab2fa30ddc",2,"When Vision Fails: Text Attacks Against ViT and OCR","It is shown how a genetic algorithm can be used to generate visual adversarial examples in a black-box setting, and a user study is conducted to establish that the model-fooling adversarialExamples do not affect human comprehension.","arXiv.org",2023,"Nicholas Boucher,Jenny Blessing,Ilia Shumailov,Ross Anderson,Nicolas Papernot",0,40,0,"https://www.semanticscholar.org/paper/940e3ee32edfe425e958a672644a0bab2fa30ddc"
"8a1c54f6e2c5f1453fddb9f15e769a099286f677",2,"Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey","This work surveys approaches to domain adaptation for NMT, particularly where a system may need to translate across multiple domains, and divides techniques into those revolving around data selection or generation, model architecture, parameter adaptation procedure, and inference procedure.","Journal of Artificial Intelligence Research",2021,"Danielle Saunders",31,366,0,"https://www.semanticscholar.org/paper/8a1c54f6e2c5f1453fddb9f15e769a099286f677"
"7cb4b8406255d0f8115afa23d8efea1bb780cfb8",2,"On the Compatibility of Tokenizations Across Languages","It is shown that the compatibility measure proposed allows the system designer to create vocabularies across languages that are compatible – a desideratum that so far has been neglected in multilingual models.","",2021,"Antonis Maronikolakis,Philipp Dufter,Hinrich Schütze",0,39,0,"https://www.semanticscholar.org/paper/7cb4b8406255d0f8115afa23d8efea1bb780cfb8"
"d680dd4b0e528b6c575fb210db6971c124210b20",2,"Finding the Optimal Vocabulary Size for Turkish Named Entity Recognition","Novel BERT models are presented pretrained with various vocabulary sizes from scratch on BERTurk corpus and fine-tuned for named entity recognition (NER) downstream task in the Turkish language, achieving state-of-the-art performance on the WikiANN dataset.","",2022,"Y. Kaya,A. C. Tantug",0,20,0,"https://www.semanticscholar.org/paper/d680dd4b0e528b6c575fb210db6971c124210b20"
"19b7c860128e5461d451b3d15f3836a9e1680ffc",2,"DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class","This work investigates how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains, and evaluates DIRE’s overall performance without respect to data quality.","ACM Transactions on Software Engineering and Methodology",2022,"Luke Dramko,Jeremy Lacomis,Pengcheng Yin,Edward J. Schwartz,Miltiadis Allamanis,Graham Neubig,Bogdan Vasilescu,Claire Le Goues",1,56,0,"https://www.semanticscholar.org/paper/19b7c860128e5461d451b3d15f3836a9e1680ffc"
"e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7",2,"Checks and Strategies for Enabling Code-Switched Machine Translation","This work explores multilingual NMT models’ ability to handle code-switched text, and proposes checks to measure switching capability and investigates simple and effective data augmentation methods that can enhance an NMT model’s ability to support code- Switched text.","arXiv.org",2022,"Thamme Gowda,Mozhdeh Gheini,Jonathan May",2,45,0,"https://www.semanticscholar.org/paper/e4750f7b56003fe8d2dcc750f91ddcbd7e9b82c7"
"9d83941a205e31d394f614424cdc553cea9e35f9",2,"Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22","The system developed at the Universitat Politècnica de Catalunya for the Workshop on Machine Translation 2022 Sign Language Translation Task, in particular, for the sign-to-text direction is described, with poor results for both the baseline and the system, and thus, the unreliability of the findings.","Conference on Machine Translation",2022,"Laia Tarrés,Gerard I. Gállego,Xavier Giró-i-Nieto,Jordi Torres",2,31,1,"https://www.semanticscholar.org/paper/9d83941a205e31d394f614424cdc553cea9e35f9"
"509e65ee7dd14772bd52e9065340940e45c299b1",2,"MEGA: Multilingual Evaluation of Generative AI","The first comprehensive benchmarking of generative LLMs is presented - MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse tasks and 33 typologically diverse languages and provides directions for future progress in the field.","arXiv.org",2023,"Kabir Ahuja,Rishav Hada,Millicent A. Ochieng,Prachi Jain,Harshita Diddee,Samuel Maina,T. Ganu,Sameer Segal,Maxamed Axmed,Kalika Bali,Sunayana Sitaram",9,68,2,"https://www.semanticscholar.org/paper/509e65ee7dd14772bd52e9065340940e45c299b1"
"fb49e38135302a1c16d644c0f746cef7d5f10ee4",2,"Understanding BLOOM: An empirical study on diverse NLP tasks","Evaluating the smaller BLOOM model variants on several NLP benchmark datasets and popular leaderboards shows that the 560m variant performs similarly to or better than the 1b7 variant, and toxicity analysis of prompt-based text generation using the RealToxicityPrompts dataset shows that it is at least 17\% less toxic than GPT-2 and G PT-3 models.","arXiv.org",2022,"Parag Dakle,Sai Krishna Rallabandi,Preethi Raghavan",0,70,0,"https://www.semanticscholar.org/paper/fb49e38135302a1c16d644c0f746cef7d5f10ee4"
"a9cd3b1ad7ea09dfddf1a22124c27070c6f910e1",2,"Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT","A tokenization approach is proposed that enables us to separate frequency (the first advantage) from compositionality, and shows that frequency alone accounts for 90%-95% of the scores reached by BPE, hence compositionality has less importance than previously thought.","arXiv.org",2023,"Benoist Wolleb,Romain Silvestri,Giorgos Vernikos,Ljiljana Dolamic Andrei Popescu-Belis",0,46,0,"https://www.semanticscholar.org/paper/a9cd3b1ad7ea09dfddf1a22124c27070c6f910e1"
"b162638cd42b65e6add199b0b34f1b375070fc7c",2,"Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models","It is shown how hate speech detection models benefit from a cross-lingual knowledge proxy brought by auxiliary tasks fine-tuning and highlight these tasks’ positive impact on bridging the hate speech linguistic and cultural gap between languages.","AACL/IJCNLP",2022,"Syrielle Montariol,Arij Riabi,Djamé Seddah",1,56,0,"https://www.semanticscholar.org/paper/b162638cd42b65e6add199b0b34f1b375070fc7c"
"e0d09d91784a6d426ffcd2fd12448dd9c8ceefe9",2,"Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages","Overall, it is found that the similarity between the percentage of words that get split into subwords in the source and target data (the isplit word ratio difference/i) is the strongest predictor for model performance on target data.","Workshop on NLP for Similar Languages, Varieties and Dialects",2023,"Verena Blaschke,Hinrich Schütze,Barbara Plank",0,49,0,"https://www.semanticscholar.org/paper/e0d09d91784a6d426ffcd2fd12448dd9c8ceefe9"
"6b4357a3aa86d138cef747b94bf036ccb234ead5",2,"Biomedical Language Models are Robust to Sub-optimal Tokenization","Surprisingly, it is found that pre-training a biomedical LM using a more accurate biomedical tokenizer does not improve the entity representation quality of a language model as measured by several intrinsic and extrinsic measures such as masked language modeling prediction (MLM) accuracy as well as NER and entity linking performance.","arXiv.org",2023,"Bernal Jimenez Gutierrez,Huan Sun,Yu Su",0,46,0,"https://www.semanticscholar.org/paper/6b4357a3aa86d138cef747b94bf036ccb234ead5"
"964bd39b546f0f6625ff3b9ef1083f797807ef2e",2,"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","BLOOM is a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers and achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning.","arXiv.org",2022,"Teven Le Scao,Angela Fan,Christopher Akiki,Elizabeth-Jane Pavlick,Suzana Ili'c,Daniel Hesslow,Roman Castagn'e,A. Luccioni,Franccois Yvon,Matthias Gallé,J. Tow,Alexander M. Rush,Stella Rose Biderman,Albert Webson,Pawan Sasanka Ammanamanchi,Thomas Wang,Benoît Sagot,Niklas Muennighoff,Albert Villanova del Moral,Olatunji Ruwase,Rachel Bawden,Stas Bekman,Angelina McMillan-Major,Iz Beltagy,Huu Nguyen,Lucile Saulnier,Samson Tan,Pedro Ortiz Suarez,Victor Sanh,Hugo Laurenccon,Yacine Jernite,Julien Launay,Margaret Mitchell,Colin Raffel,Aaron Gokaslan,Adi Simhi,Aitor Soroa Etxabe,Alham Fikri Aji,Amit Alfassy,Anna Rogers,Ariel Kreisberg Nitzav,Canwen Xu,Chenghao Mou,Chris C. Emezue,Christopher Klamm,Colin Leong,Daniel Alexander van Strien,David Ifeoluwa Adelani,Dragomir R. Radev,E. G. Ponferrada,Efrat Levkovizh,Ethan Kim,E. Natan,F. Toni,Gérard Dupont,Germán Kruszewski,Giada Pistilli,Hady ElSahar,Hamza Benyamina,H. Tran,Ian Yu,Idris Abdulmumin,Isaac Johnson,Itziar Gonzalez-Dios,Javier de la Rosa,Jenny Chim,Jesse Dodge,Jian Zhu,Jonathan Chang,Jorg Frohberg,Josephine L. Tobing,J. Bhattacharjee,Khalid Almubarak,Kimbo Chen,Kyle Lo,Leandro von Werra,Leon Weber,Long Phan,Loubna Ben Allal,Ludovic Tanguy,Manan Dey,M. Muñoz,Maraim Masoud,Mar'ia Grandury,Mario vSavsko,Max Huang,Maximin Coavoux,Mayank Singh,Mike Tian-Jian Jiang,Minh Chien Vu,M. A. Jauhar,Mustafa Ghaleb,Nishant Subramani,Nora Kassner,Nurulaqilla Khamis,Olivier Nguyen,Omar Espejel,Ona de Gibert,Paulo Villegas,Peter Henderson,Pierre Colombo,Priscilla A. Amuok,Quentin Lhoest,Rheza Harliman,Rishi Bommasani,R. L'opez,Rui Ribeiro,Salomey Osei,Sampo Pyysalo,Sebastian Nagel,Shamik Bose,Shamsuddeen Hassan Muhammad,Shanya Sharma,S. Longpre,Somaieh Nikpoor,Stanislav Silberberg,S. Pai,S. Zink,Tiago Timponi Torrent,Timo Schick,Tristan Thrush,V. Danchev,Vassilina Nikoulina,Veronika Laippala,Violette Lepercq,V. Prabhu,Zaid Alyafeai,Zeerak Talat,Arun Raja,Benjamin Heinzerling,Chenglei Si,Elizabeth Salesky,Sabrina J. Mielke,Wilson Y. Lee,Abheesht Sharma,Andrea Santilli,Antoine Chaffin,Arnaud Stiegler,Debajyoti Datta,Eliza Szczechla,Gunjan Chhablani,Han Wang,Harshit Pandey,Hendrik Strobelt,Jason Alan Fries,Jos Rozen,Leo Gao,Lintang Sutawika,M Saiful Bari,Maged S. Al-shaibani,Matteo Manica,Nihal V. Nayak,Ryan Teehan,Samuel Albanie,Sheng Shen,Srulik Ben-David,Stephen H. Bach,Taewoon Kim,T. Bers,Thibault Févry,Trishala Neeraj,Urmish Thakker,Vikas Raunak,Xiang Tang,Zheng Xin Yong,Zhiqing Sun,Shaked Brody,Y. Uri,Hadar Tojarieh,Adam Roberts,Hyung Won Chung,Jaesung Tae,Jason Phang,Ofir Press,Conglong Li,D. Narayanan,Hatim Bourfoune,J. Casper,Jeff Rasley,Max Ryabinin,Mayank Mishra,Minjia Zhang,M. Shoeybi,Myriam Peyrounette,N. Patry,Nouamane Tazi,Omar Sanseviero,Patrick von Platen,Pierre Cornette,Pierre Franccois Lavall'ee,R. Lacroix,Samyam Rajbhandari,Sanchit Gandhi,Shaden Smith,S. Requena,Suraj Patil,Tim Dettmers,Ahmed Baruwa,Amanpreet Singh,Anastasia Cheveleva,Anne-Laure Ligozat,Arjun Subramonian,Aur'elie N'ev'eol,Charles Lovering,Daniel H Garrette,D. Tunuguntla,Ehud Reiter,Ekaterina Taktasheva,E. Voloshina,Eli Bogdanov,Genta Indra Winata,Hailey Schoelkopf,Jan-Christoph Kalo,Jekaterina Novikova,J. Forde,Xiangru Tang,Jungo Kasai,Ken Kawamura,Liam Hazan,Marine Carpuat,Miruna Clinciu,Najoung Kim,Newton Cheng,O. Serikov,Omer Antverg,Oskar van der Wal,Rui Zhang,Ruochen Zhang,Sebastian Gehrmann,Shachar Mirkin,S. Pais,Tatiana Shavrina,Thomas Scialom,Tian Yun,Tomasz Limisiewicz,Verena Rieser,Vitaly Protasov,V. Mikhailov,Yada Pruksachatkun,Yonatan Belinkov,Zachary Bamberger,Zdenvek Kasner,A. Rueda,A. Pestana,A. Feizpour,Ammar Khan,Amy Faranak,A. Santos,A. Hevia,Antigona Unldreaj,Arash Aghagol,Arezoo Abdollahi,A. Tammour,A. HajiHosseini,Bahareh Behroozi,B. Ajibade,B. Saxena,Carlos Muñoz Ferrandis,Danish Contractor,D. Lansky,Davis David,Douwe Kiela,D. A. Nguyen,Edward Tan,Emily Baylor,Ezinwanne Ozoani,Fatim T Mirza,Frankline Ononiwu,Habib Rezanejad,H.A. Jones,Indrani Bhattacharya,Irene Solaiman,Irina Sedenko,I. Nejadgholi,J. Passmore,Joshua Seltzer,Julio Bonis Sanz,Karen Fort,L. Dutra,Mairon Samagaio,Maraim Elbadri,Margot Mieskes,Marissa Gerchick,Martha Akinlolu,Michael McKenna,Mike Qiu,M. Ghauri,Mykola Burynok,Nafis Abrar,Nazneen Rajani,Nour Elkott,N. Fahmy,Olanrewaju Samuel,Ran An,R. Kromann,Ryan Hao,S. Alizadeh,Sarmad Shubber,Silas L. Wang,Sourav Roy,S. Viguier,Thanh-Cong Le,Tobi Oyebade,T. Le,Yoyo Yang,Z. Nguyen,Abhinav Ramesh Kashyap,A. Palasciano,A. Callahan,Anima Shukla,Antonio Miranda-Escalada,A. Singh,Benjamin Beilharz,Bo Wang,C. Brito,Chenxi Zhou,Chirag Jain,Chuxin Xu,Clémentine Fourrier,Daniel Le'on Perin'an,Daniel Molano,Dian Yu,Enrique Manjavacas,Fabio Barth,Florian Fuhrimann,Gabriel Altay,Giyaseddin Bayrak,Gully Burns,Helena U. Vrabec,I. Bello,Isha Dash,J. Kang,John Giorgi,J. Golde,J. Posada,Karthi Sivaraman,Lokesh Bulchandani,Lu Liu,Luisa Shinzato,Madeleine Hahn de Bykhovetz,Maiko Takeuchi,Marc Pàmies,M. A. Castillo,Marianna Nezhurina,Mario Sanger,M. Samwald,Michael Cullan,Michael Weinberg,M. Wolf,Mina Mihaljcic,Minna Liu,M. Freidank,Myungsun Kang,Natasha Seelam,N. Dahlberg,N. Broad,N. Muellner,Pascale Fung,Patricia Haller,R. Chandrasekhar,R. Eisenberg,Robert Martin,Rodrigo L. Canalli,Rosaline Su,Ruisi Su,Samuel Cahyawijaya,Samuele Garda,Shlok S Deshmukh,Shubhanshu Mishra,Sid Kiblawi,Simon Ott,Sinee Sang-aroonsiri,Srishti Kumar,Stefan Schweter,S. Bharati,T. A. Laud,Th'eo Gigant,Tomoya Kainuma,Wojciech Kusa,Yanis Labrak,Yashasvi Bajaj,Y. Venkatraman,Yifan Xu,Ying Xu,Yun-chao Xu,Z. Tan,Zhongli Xie,Zifan Ye,M. Bras,Younes Belkada,Thomas Wolf",397,171,74,"https://www.semanticscholar.org/paper/964bd39b546f0f6625ff3b9ef1083f797807ef2e"
"8969ea3d254e149aebcfd1ffc8f46910d7cb160e",2,"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models","It is argued that exposure to pretraining data may break distributional control across training and test to gauge compositional generalization, where certain lexical items only occur in limited contexts during training.","arXiv.org",2022,"Najoung Kim,Tal Linzen,P. Smolensky",12,51,1,"https://www.semanticscholar.org/paper/8969ea3d254e149aebcfd1ffc8f46910d7cb160e"
"4e97303aeb299ee736b1b8c29cef046212690354",2,"Generation-based Code Review Automation: How Far Are We?","A comprehensive study by comparing the effectiveness of recent ACR tools as well as the general-purpose pre-trained models, which shows that a general- Purpose pre- trained model CodeT5 can outperform other models in most cases.","arXiv.org",2023,"Xin Zhou,Kisub Kim,Bowen Xu,Donggyun Han,Junda He,David Lo",0,71,0,"https://www.semanticscholar.org/paper/4e97303aeb299ee736b1b8c29cef046212690354"
"850da23eae35322e0af55dbddb7e96e99482c118",2,"From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation","This study suggests that subword tokenization is a promising technique for symbolic music generation and may have broader implications for music composition, particularly in cases involving complex data such as multi-track songs.","arXiv.org",2023,"Adarsh Kumar,Pedro Sarmento",0,48,0,"https://www.semanticscholar.org/paper/850da23eae35322e0af55dbddb7e96e99482c118"
"af6c6941acecfb23d899cd5266efdf2e27463f12",2,"Causal Language Model Aided Sequential Decoding With Natural Redundancy","This paper proposes a sequential decoding algorithm for the robust reception of sources with natural redundancy over the AWGN channel and eliminates the requirement of high-cost and accurate labels, which paves a new way for communication receiver design.","IEEE Transactions on Communications",2023,"Zhaorui Zhu,Hongyi Yu,Caiyao Shen,Jianping Du,Zhixiang Shen,Zhenyu Wang",0,47,0,"https://www.semanticscholar.org/paper/af6c6941acecfb23d899cd5266efdf2e27463f12"
"988a17753dd040867eef4f093fa50a87bf4142b1",2,"Tokenization with Factorized Subword Encoding","A novel tokenization method is proposed that factorizes subwords onto discrete triplets using a VQ-VAE model, and results indicate that this method is more appropriate and robust for morphological tasks than the commonly used byte-pair encoding (BPE) tokenization algorithm.","arXiv.org",2023,"David Samuel,Lilja Øvrelid",0,37,0,"https://www.semanticscholar.org/paper/988a17753dd040867eef4f093fa50a87bf4142b1"
"3895423c2e2ed608ba1439a6a21b66e43f5ef8b8",2,"Subword Evenness (SuE) as a Predictor of Cross-lingual Transfer to Low-resource Languages","It is shown that languages written in non-Latin and non-alphabetic scripts (mostly Asian languages) are the best choices for improving performance on the task of Masked Language Modelling (MLM) in a diverse set of 30 low-resource languages and that the success of the transfer is well predicted by the authors' novel measure of Subword Evenness (SuE).","Conference on Empirical Methods in Natural Language Processing",2022,"Olga Pelloni,Anastassia Shaitarova,T. Samardžić",0,49,0,"https://www.semanticscholar.org/paper/3895423c2e2ed608ba1439a6a21b66e43f5ef8b8"
"8929066ce924696f960512c92a720c70bba65586",2,"On Language Spaces, Scales and Cross-Lingual Transfer of UD Parsers","","Conference on Computational Natural Language Learning",2022,"T. Samardžić,Ximena Gutierrez-Vasques,Rob van der Goot,Max Müller-Eberstein,Olga Pelloni,Barbara Plank",0,51,0,"https://www.semanticscholar.org/paper/8929066ce924696f960512c92a720c70bba65586"
"cde3d57c8f38e5dfb50edbb31faf02c23f1507fe",2,"Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color","This cross-lingual analysis shows that textual character representations correlate strongly with sound representations for languages using an alphabetic script, while shape correlates with featural scripts.","Annual Meeting of the Association for Computational Linguistics",2022,"Sidsel Boldsen,Manex Agirrezabal,Nora Hollenstein",2,77,0,"https://www.semanticscholar.org/paper/cde3d57c8f38e5dfb50edbb31faf02c23f1507fe"
"66ef8d777eccbb4138c0f9cc83d99c0bb1405c2a",2,"Optimizing the Size of Subword Vocabularies in Dialect Classification","It is shown that models trained from scratch with an optimal tokenization level perform better than fine-tuned classifiers in the case of highly inconsistent writing, and in the cases of relatively consistent writing, fine- Tuned models remain better regardless of thetokenization level.","Workshop on NLP for Similar Languages, Varieties and Dialects",2023,"Vani Kanjirangat,T. Samardžić,L. Dolamic,Fabio Rinaldi",0,59,0,"https://www.semanticscholar.org/paper/66ef8d777eccbb4138c0f9cc83d99c0bb1405c2a"
"2ac5442a32988f86730e460b3198f475592ae410",2,"Improving Low-Resource Languages in Pre-Trained Multilingual Language Models","This work proposes an unsupervised approach to improve the cross-lingual representations of low-resource languages by bootstrapping word translation pairs from monolingual corpora and using them to improve language alignment in pre-trained language models.","Conference on Empirical Methods in Natural Language Processing",2022,"Viktor Hangya,Hossain Shaikh Saadi,Alexander M. Fraser",0,46,0,"https://www.semanticscholar.org/paper/2ac5442a32988f86730e460b3198f475592ae410"
"e873ddaf58ff92ae0492983cf57221fb25837f4e",1,"Strategies in subword tokenization: humans vs. algorithms","A new method to an- 010 alyze subword segmentation strategies relying on a spatial analysis of the distribution of sub- 012 words’ lengths is proposed, which shows that humans tend to balance creativity and consistency, while algorithms tend to be either strongly biased or inconsistent.","",2021,"",0,13,0,"https://www.semanticscholar.org/paper/e873ddaf58ff92ae0492983cf57221fb25837f4e"
"ae7fc32df9401a86353185515f4fd5e2020ac922",1,"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations","This study introduces MutFormer, a transformer-based model for the prediction of pathogenic missense mutations, using reference and mutated amino acid sequences from the human genome as the features, and shows that MutFormer outperforms a variety of existing tools in pathogenicity prediction.","arXiv.org",2021,"Theodore Jiang,Li Fang,Kai Wang",4,40,1,"https://www.semanticscholar.org/paper/ae7fc32df9401a86353185515f4fd5e2020ac922"
"9d13bde760d8e77f059436d60160881becd2d2e0",1,"Predicting Ethnicity from Names with rethnicity: Methodology and Application","A new R package, rethnicity1 is provided for predicting ethnicity based on names, using the Bidirectional LSTM and Florida Voter Registration as the model and training data.","arXiv.org",2021,"Fangzhou Xie",0,28,0,"https://www.semanticscholar.org/paper/9d13bde760d8e77f059436d60160881becd2d2e0"
"7175caf7568d46c857380d0e5b64653819d5cc45",1,"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization","The MULTILEXNORM shared task provides the largest publicly available multilingual lexical normalization benchmark including 12 language variants and proposes a homogenized evaluation setup with both intrinsic and extrinsic evaluation.","Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)",2021,"R. Goot,Alan Ramponi,A. Zubiaga,Barbara Plank,Benjamin Muller,I. Roncal,Nikola Ljubesic,Özlem Çetinoğlu,Rahmad Mahendra,Talha Çolakoglu,Timothy Baldwin,Tommaso Caselli,Wladimir Sidorenko,Bruno Kessler",15,75,0,"https://www.semanticscholar.org/paper/7175caf7568d46c857380d0e5b64653819d5cc45"
"de0e1f9980afa7949df64d53b8ae7a2f59c55579",1,"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages","This work pushes the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases, and achieves 2-3x better performance and output diversity in formality transfer and code-mixing addition across five Indian languages.","arXiv.org",2021,"Kalpesh Krishna,Deepak Nathani,Xavier García,Bidisha Samanta,P. Talukdar",5,77,0,"https://www.semanticscholar.org/paper/de0e1f9980afa7949df64d53b8ae7a2f59c55579"
"3fa2e17332bb2888318f504cf37026001b932900",1,"Comparison of Token- and Character-Level Approaches to Restoration of Spaces, Punctuation, and Capitalization in Various Languages",,"International Conference on Natural Language and Speech Processing",2022,"Laurence Dyer,Anthony Hughes,Dhwani Shah,Burcu Can",0,41,0,"https://www.semanticscholar.org/paper/3fa2e17332bb2888318f504cf37026001b932900"
"710ceab1ff91b96e8596b8400ebf2912ee6e2836",1,"Machine Translation for Multilingual Intent Detection and Slots Filling","This work uses the inherent multilingual aspect of translation models for the task of multilingual intent classification and slot filling and reveals that they work equally well with general-purpose multilingual text-to-text models.","MMNLU",2022,"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans",1,44,0,"https://www.semanticscholar.org/paper/710ceab1ff91b96e8596b8400ebf2912ee6e2836"
"038103632b24619818b159f5ca37b848744817db",1,"DENTRA: Denoising and Translation Pre-training for Multilingual Machine Translation","DENTRA, a novel pre-training strategy for a multilingual sequence-to-sequence transformer model, combines denoising and translation objectives to incorporate both monolingual and bitext corpora in 24 African, English, and French languages is introduced.","Conference on Machine Translation",2022,"Samta Kamboj,Sunil Kumar Sahu,Neha Sengupta",1,23,0,"https://www.semanticscholar.org/paper/038103632b24619818b159f5ca37b848744817db"
"0ffb0b109acddf0067aec252221e0ea04d317ddc",1,"A Framework for Sexism Detection on Social Media via ByT5 and TabNet","A combination of byte-level model ByT5 with tabular modeling via TabNet that has at its core an ability to take into account platform and language aspects of the challenging task of sexism detection is proposed.","IberLEF@SEPLN",2022,"A. Younus,M. A. Qureshi",0,16,0,"https://www.semanticscholar.org/paper/0ffb0b109acddf0067aec252221e0ea04d317ddc"
"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0",1,"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome","The introduction of MutFormer, a transformer-based model for the prediction of deleterious missense mutations that uses reference and mutated protein sequences from the human genome as the primary features, and which successfully considers sequence features that are not explored in previous studies.","",2022,"Theodore Jiang,Li Fang,Kai Wang",0,48,0,"https://www.semanticscholar.org/paper/7c496490abcd8d5c6e90aa3d3c82bde02680f5b0"
"6102fe88a512290b80e83ed2fe17606b166e505a",1,"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin","The paper features a thorough discussion of part-of-speech and lemmatization errors which shows how the system performance may be improved for Classical, Medieval and Neo-Latin texts.","LT4HALA",2022,"Krzysztof Wróbel,Krzysztof Nowak",2,12,1,"https://www.semanticscholar.org/paper/6102fe88a512290b80e83ed2fe17606b166e505a"
"143659fcf93f33e9139f594cf8111c6bc85c04fa",1,"Overview of the EvaLatin 2022 Evaluation Campaign","This paper describes the organization and the results of the second edition of EvaLatin, the campaign for the evaluation of Natural Language Processing tools for Latin, and the three shared tasks proposed in EvaLatin 2022, i.","LT4HALA",2022,"R. Sprugnoli,M. Passarotti,F. M. Cecchini,Margherita Fantoli,Giovanni Moretti",6,14,3,"https://www.semanticscholar.org/paper/143659fcf93f33e9139f594cf8111c6bc85c04fa"
"2005311276a1a90dc17cf6e2ca7725fee2e76e1e",1,"BasqueGLUE: A Natural Language Understanding Benchmark for Basque","BasqueGLUE is presented, the first NLU benchmark for Basque, a less-resourced language, which has been elaborated from previously existing datasets and following similar criteria to those used for the construction of GLUE and SuperGLUE.","International Conference on Language Resources and Evaluation",2022,"Gorka Urbizu,Iñaki San Vicente,X. Saralegi,Rodrigo Agerri,Aitor Soroa Etxabe",2,38,1,"https://www.semanticscholar.org/paper/2005311276a1a90dc17cf6e2ca7725fee2e76e1e"
"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83",1,"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference","It is argued that the transfer learning-based loss objective is model agnostic and thus can be used with other deep learning- based architectures for cross-lingual NLI.","International Conference on Language Resources and Evaluation",2022,"Dibyanayan Bandyopadhyay,Arkadipta De,Baban Gain,Tanik Saikh,Asif Ekbal",0,24,0,"https://www.semanticscholar.org/paper/b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83"
"5f77157038dc7f189db7e72ea58567039222d9df",1,"Examining Single Sentence Label Leakage in Natural Language Inference Datasets","This work examines how regular NLI models cheat on single sentence label leakage, and discusses how to ameliorate this.","",2022,"Michael Stephen Saxon,Xinyi Wang,Wenda Xu,William Yang Wang",0,42,0,"https://www.semanticscholar.org/paper/5f77157038dc7f189db7e72ea58567039222d9df"
"2b6b38b66fcca218cb2f381e5d4711b5c99a784f",1,"Training Text-to-Text Transformers with Privacy Guarantees","By using recent advances in JAX and XLA, this work can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE).","Findings",2022,"N. Ponomareva,Jasmijn Bastings,Sergei Vassilvitskii",9,36,1,"https://www.semanticscholar.org/paper/2b6b38b66fcca218cb2f381e5d4711b5c99a784f"
"32290d428b50f64d1cfee6ea658dc6ba977b26e9",1,"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia","The approach to classify homophobia and transphobia in social media comments using an ensemble of transformer-based models to build a classifier that ranked 2nd for English, 8th for Tamil and 10th for Chennai-English.","LTEDI",2022,"I. Upadhyay,KV Aditya Srivatsa,Radhika Mamidi",4,33,0,"https://www.semanticscholar.org/paper/32290d428b50f64d1cfee6ea658dc6ba977b26e9"
"b54edea6cac055d8ff9e35c2781f5e000ebdff89",1,"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data","Initial experiments using Swahili and Kinyarwanda data suggest the viability of the multi-modal approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch.","Annual Meeting of the Association for Computational Linguistics",2022,"Colin Leong,Daniel Whitenack",0,45,0,"https://www.semanticscholar.org/paper/b54edea6cac055d8ff9e35c2781f5e000ebdff89"
"f40aeae3e522ada1f6a9f326841b01ef5c8657b6",1,"Unifying Language Learning Paradigms","This paper presents a uniﬁed framework for pre-training models that are universally effective across datasets and setups, and proposes Mixture-of-Denoisers (MoD), a pre- Training objective that combines diverse pre- training paradigms together.","arXiv.org",2022,"Yi Tay,M. Dehghani,Vinh Q. Tran,Xavier García,Dara Bahri,Tal Schuster,Huaixiu Zheng,N. Houlsby,Donald Metzler",83,113,18,"https://www.semanticscholar.org/paper/f40aeae3e522ada1f6a9f326841b01ef5c8657b6"
"9758782feed4b4b9bf0ec18b802462e8023a7f83",1,"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models","","Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing",2022,"Odunayo Jude Ogundepo,Akintunde Oladipo,Mofetoluwa Adeyemi,Kelechi Ogueji and Jimmy Lin",5,33,0,"https://www.semanticscholar.org/paper/9758782feed4b4b9bf0ec18b802462e8023a7f83"
"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9",1,"On the Influence of Tokenizers in NLP Pipelines","This thesis examines the influence of tokenization in NLP pipelines, by analyzing, reproducing, and quantifying claims from the token-free NLP literature, using the example of NER, and concludes that token- free models, like ByT5, offer significant advantages over their tokenizer-based alternatives.","",2022,"",0,49,0,"https://www.semanticscholar.org/paper/5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9"
"c3904ef47bec4fc2bfd3d390370681c33542d62d",1,"Fast Whitespace Correction with Encoder-Only Transformers","This work provides an easy-to-use tool that is over 900 times faster than the previous best tool, with the same high quality, and compares two Transformer-based models, a character-level encoder-decoder model and a byte-levelencoder-only model.","Annual Meeting of the Association for Computational Linguistics",2023,"Hannah Bast,Matthias Hertel,S. Walter",0,24,0,"https://www.semanticscholar.org/paper/c3904ef47bec4fc2bfd3d390370681c33542d62d"
"d5241be77ff0aec9a2ac1e1b14fb69df897eb8f3",1,"Bi-Phone: Modeling Inter Language Phonetic Influences in Text","FunGLUE is the first benchmark to introduce L1-L2 interactions in text and introduces a new phoneme prediction pre-training task which helps byte models to recover performance close to SuperGLUE.","Annual Meeting of the Association for Computational Linguistics",2023,"Abhirut Gupta,Ananya B. Sai,R. Sproat,Yuri Vasilevski,James Ren,Ambarish Jash,Sukhdeep S. Sodhi,A. Raghuveer",0,30,0,"https://www.semanticscholar.org/paper/d5241be77ff0aec9a2ac1e1b14fb69df897eb8f3"
"e5adb9bf3f5ed9c253f38949b22e86775dca443a",1,"GEO-SEQ2SEQ: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning","GEO-SEQ2SEQ is proposed, a sequence-to-sequence model for Twitter user geolocation that rewrites noisy, multilingual user-provided location strings into structured English location names and reveals that constrained decoding helps the model produce valid locations according to a location database.","",2023,"Jingyu Zhang,Alexandra DeLucia,Chenyu Zhang,Mark Dredze",0,34,0,"https://www.semanticscholar.org/paper/e5adb9bf3f5ed9c253f38949b22e86775dca443a"
"1d578d2bdb5c920b224cfca73868566731eaeebd",1,"Towards Analysis of Biblical Entities and Names using Deep Learning","The findings of this study demonstrate that deep learning could help uncover interesting connections between individuals who may have initially been considered less important and highlighted the critical role of onomastic sciences and the philosophy of language in analyzing the richness and importance of human and other proper names in biblical texts.","",2023,"Mikolaj Martinjak,D. Lauc,Ines Skelac",0,15,0,"https://www.semanticscholar.org/paper/1d578d2bdb5c920b224cfca73868566731eaeebd"
"4bd35d344c635b05f97f4d749741d196ff541bf3",1,"A Primer on Seq2Seq Models for Generative Chatbots","This paper examines recent trends in the development of open-domain data-driven generative chatbots, focusing on the Seq2Seq architectures, and examines possible architecture implementations as well as training and evaluation approaches.","",2023,"Vincenzo Scotti,L. Sbattella,R. Tedesco",0,196,0,"https://www.semanticscholar.org/paper/4bd35d344c635b05f97f4d749741d196ff541bf3"
"13b8060acc3db1fc555f6e55368f6d02899a1698",1,"FairPrism: Evaluating Fairness-Related Harms in Text Generation","FairPrism is introduced, a dataset of 5,000 examples of AI-generated English text with detailed human annotations covering a diverse set of harms relating to gender and sexuality that can be used to diagnose the types of fairness-related harms that AI text generation systems cause.","Annual Meeting of the Association for Computational Linguistics",2023,"Eve Fleisig,Aubrie Amstutz,Chad Atalla,Su Lin Blodgett,Hal Daumé,Alexandra Olteanu,Emily Sheng,Dan Vann,Hanna M. Wallach",1,43,0,"https://www.semanticscholar.org/paper/13b8060acc3db1fc555f6e55368f6d02899a1698"
"82e1313f28afde442930b94bc6ed582d17e8d4b3",1,"Generating Errors: OCR Post-Processing for Icelandic",,"Nordic Conference of Computational Linguistics",2023,"Atli Jasonarson,Steinþór Steingrímsson,E. Sigurðsson,Árni Magnússon,F. Ingimundarson",1,19,0,"https://www.semanticscholar.org/paper/82e1313f28afde442930b94bc6ed582d17e8d4b3"
"5088cd22bbd7e1b798df39eb7f3c4ae305ab7625",1,"Findings from the Bambara - French Machine Translation Competition (BFMT 2023)","This paper details each team’s different approaches and motivation for ongoing work in Bambara and the broader low-resource machine translation domain.","LORESMT",2023,"Ninoh Agostinho Da Silva,T. Ajayi,A. Antonov,Panga Azazia Kamate,Moussa L. Coulibaly,Mason Del Rio,Yacouba Diarra,Sebastian Diarra,Chris C. Emezue,Joel Hamilcaro",0,51,0,"https://www.semanticscholar.org/paper/5088cd22bbd7e1b798df39eb7f3c4ae305ab7625"
"cfe7cb66390ea0b433383d498e6eff555a198c54",1,"Murreviikko - A Dialectologically Annotated and Normalized Dataset of Finnish Tweets","It is found that there are significant differences in normalization difficulty between the dialects, and that a character-level statistical machine translation model performs best on the Murreviikko tweet dataset.","Workshop on NLP for Similar Languages, Varieties and Dialects",2023,"Olli Kuparinen",0,26,0,"https://www.semanticscholar.org/paper/cfe7cb66390ea0b433383d498e6eff555a198c54"
"d5d936b142eb81826002888230649c805532630e",1,"VisText: A Benchmark for Semantically Rich Chart Captioning","This work introduces VisText, a dataset of 12,441 pairs of charts and captions that describe the charts’ construction, report key statistics, and identify perceptual and cognitive phenomena, and fine-tune state-of-the-art language models on the authors' chart captioning task and apply prefix-tuning to produce caption that vary the semantic content they convey.","Annual Meeting of the Association for Computational Linguistics",2023,"Benny J. Tang,Angie Boggust",0,76,0,"https://www.semanticscholar.org/paper/d5d936b142eb81826002888230649c805532630e"
"7072db6eddb85ecd2c117365d91bd694760f726e",1,"Position Information in Transformers: An Overview","An overview and theoretical comparison of existing methods to incorporate position information into Transformer models is provided and what characteristics of an application should be taken into account when selecting a position encoding is indicated.","Computational Linguistics",2021,"Philipp Dufter,Martin Schmitt,Hinrich Schütze",51,67,2,"https://www.semanticscholar.org/paper/7072db6eddb85ecd2c117365d91bd694760f726e"
"d13a0c8d49cb268d8d245925baee0316c1fe1875",1,"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention","This work theoretically predicts the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity, and empirically demonstrates the existence and implications on the depth-to-width interplay of Transformer architectures.","International Conference on Machine Learning",2021,"Noam Wies,Yoav Levine,Daniel Jannai,A. Shashua",13,52,2,"https://www.semanticscholar.org/paper/d13a0c8d49cb268d8d245925baee0316c1fe1875"
"06431546c21d7c2528aaa170c2e1078e0a82d12e",1,"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer","English is compared against other transfer languages for fine-tuning, and other high-resource languages such as German and Russian often transfer more effectively, especially when the set of target languages is diverse or unknown a priori.","arXiv.org",2021,"Iulia Turc,Kenton Lee,Jacob Eisenstein,Ming-Wei Chang,Kristina Toutanova",37,33,4,"https://www.semanticscholar.org/paper/06431546c21d7c2528aaa170c2e1078e0a82d12e"
"c41819413725668fbf8e3ca1a0bcaf7fb691e582",1,"How Optimal is Greedy Decoding for Extractive Question Answering?","This work presents exact-extract, a decoding algorithm that efficiently finds the most probable answer span in the context, and shows that self-supervised training can bias the model towards extractive behavior, increasing performance in the zero-shot setting without resorting to annotated examples.","arXiv.org",2021,"Or Castel,Ori Ram,Avia Efrat,Omer Levy",2,34,0,"https://www.semanticscholar.org/paper/c41819413725668fbf8e3ca1a0bcaf7fb691e582"
"89a4f4d1f8c93da85d829a1acfe8cafea2b50c00",1,"Transfer Learning for Multi-lingual Tasks - a Survey","This survey provides a comprehensive overview of the existing literature with a focus on transfer learning techniques in multilingual tasks and identifies potential opportunities for further research in this domain.","arXiv.org",2021,"Amir Reza Jafari,Behnam Heidary,R. Farahbakhsh,Mostafa Salehi,M. Jalili",1,106,0,"https://www.semanticscholar.org/paper/89a4f4d1f8c93da85d829a1acfe8cafea2b50c00"
"972808b92159a782f5ca1c1ab3a8ed3867acfb2d",1,"mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset",,"arXiv.org",2021,"L. Bonifacio,Israel Campiotti,R. Lotufo,Rodrigo Nogueira",36,52,7,"https://www.semanticscholar.org/paper/972808b92159a782f5ca1c1ab3a8ed3867acfb2d"
"39262814fa3e47905a2e5facf13465a1f70706b9",1,"Single-Read Reconstruction for DNA Data Storage Using Transformers","This work proposes a novel approach for single-read reconstruction using an encoder-decoder Transformer architecture for DNA based data storage and achieves lower error rates when reconstructing the original data from a single read of each DNA strand compared to state-of-the-art algorithms using 2-3 copies.","arXiv.org",2021,"Yotam Nahum,Eyar Ben-Tolila,Leon Anavy",3,37,0,"https://www.semanticscholar.org/paper/39262814fa3e47905a2e5facf13465a1f70706b9"
"4318c9f87221b9f504f286540c9a6d5c1cc4e4c8",1,"Rethnicity: Predicting Ethnicity from Names","An R package, rethniicty, is provided for predicting ethnicity from names, using the Bidirectional LSTM as the model and Florida Voter Registration as training data and the availability, accuracy, and performance are compared.","",2021,"Fangzhou Xie",2,26,0,"https://www.semanticscholar.org/paper/4318c9f87221b9f504f286540c9a6d5c1cc4e4c8"
"a70fc86508bd0133d5d984a4e777abef1934d76c",1,"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese","BARTpho is presented, which are the first public large-scale monolingual sequence-to-sequence models pre-trained for Vietnamese, and it is found that it is more effective than mBART on these two tasks.","Interspeech",2021,"Nguyen Luong Tran,Duong Minh Le,Dat Quoc Nguyen",8,46,3,"https://www.semanticscholar.org/paper/a70fc86508bd0133d5d984a4e777abef1934d76c"
"c107835a05ca6fda6e73b64e2ed9884de4fcec0f",1,"A proposed conceptual framework for a representational approach to information retrieval","A representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model that establishes connections to sentence similarity tasks in natural language processing and information access ""technologies"" prior to the dawn of computing.","SIGIR Forum",2021,"Jimmy J. Lin",26,86,3,"https://www.semanticscholar.org/paper/c107835a05ca6fda6e73b64e2ed9884de4fcec0f"
"e35357ac461a669fe7e4b877ee1fad0dfda26303",1,"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings","This work pushes the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases, and achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages.","Annual Meeting of the Association for Computational Linguistics",2021,"Kalpesh Krishna,Deepak Nathani,Xavier García,Bidisha Samanta,P. Talukdar",11,85,1,"https://www.semanticscholar.org/paper/e35357ac461a669fe7e4b877ee1fad0dfda26303"
"66d735987a31d666a6459566ae026c40ab9a1c3a",1,"The Efficiency Misnomer","It is demonstrated how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models, and suggestions to improve reporting of efficiency metrics are presented.","International Conference on Learning Representations",2021,"M. Dehghani,Anurag Arnab,L. Beyer,Ashish Vaswani,Yi Tay",61,87,6,"https://www.semanticscholar.org/paper/66d735987a31d666a6459566ae026c40ab9a1c3a"
"f3bed81c50e293c03f6b650cdb9351d14e9be347",1,"Deciphering the Language of Nature: A transformer-based language model for deleterious mutations in proteins","This study introduces MutFormer, a transformer-based model for the prediction of deleterious missense mutations, which uses reference and mutated protein sequences from the human genome as the primary features and concludes that MutFormer successfully considers sequence features that are not explored in previous studies.","",2021,"Theodore Jiang,Li Fang,Kai Wang",0,50,0,"https://www.semanticscholar.org/paper/f3bed81c50e293c03f6b650cdb9351d14e9be347"
"6e2682eb2fbec93b329028b23764c1164e232c41",1,"ÚFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5","This paper presents the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021, which evaluates lexical-normalization systems on 12 social media datasets in 11 languages with the best performance by a wide margin.","WNUT",2021,"David Samuel,Milan Straka",7,35,1,"https://www.semanticscholar.org/paper/6e2682eb2fbec93b329028b23764c1164e232c41"
"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e",1,"Large Dual Encoders Are Generalizable Retrievers","Experimental results show that the dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly and the ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.","Conference on Empirical Methods in Natural Language Processing",2021,"Jianmo Ni,Chen Qu,Jing Lu,Zhuyun Dai,Gustavo Hernandez Abrego,Ji Ma,Vincent Zhao,Yi Luan,Keith B. Hall,Ming-Wei Chang,Yinfei Yang",83,40,23,"https://www.semanticscholar.org/paper/9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e"
"0ae82f96cb8c1d2da5d8284765dd76a139ba6ff6",1,"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers","It is demonstrated that despite efforts to reduce this leakage, it persists in modern datasets that have been introduced since its 2018 discovery, and a novel model-driven technique is introduced, the progressive evaluation of cluster outliers (PECO), which enables both the objective measurement of leakage, and the automated detection of subpopulations in the data which maximally exhibit it.","Conference of the European Chapter of the Association for Computational Linguistics",2021,"Michael Stephen Saxon,Xinyi Wang,Wenda Xu,William Yang Wang",3,55,0,"https://www.semanticscholar.org/paper/0ae82f96cb8c1d2da5d8284765dd76a139ba6ff6"
"e53f455b3300d95738dac117419c88fbde58ac4e",1,"Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics","This manuscript describes a three-stage pipeline that individually performs chemical mention detection, entity normalization and indexing in PubMed full-text articles and proposes rules for identifying the more relevant MeSH codes for each article.","Database J. Biol. Databases Curation",2022,"Tiago Almeida,Rui Antunes,João F Silva,João Rafael Almeida,Sérgio Matos",1,116,0,"https://www.semanticscholar.org/paper/e53f455b3300d95738dac117419c88fbde58ac4e"
"f357b35e068bbfbb8468b48198ab63241713629d",1,"Correcting diacritics and typos with ByT5 transformer model","This work tackles diacritics restoration and typos correction at once by employing the newly-developed universal ByT5 byte-level seq2seq transformer model that requires no language-specific model structures and strongly outperforms classical spell-checking or dictionary-based approaches.","Applied Sciences",2022,"Lukas Stankevicius,M. Lukoševičius,J. Kapočiūtė-Dzikienė,Monika Briediene,Tomas Krilavičius",6,119,2,"https://www.semanticscholar.org/paper/f357b35e068bbfbb8468b48198ab63241713629d"
"8f2bca9d684005675e294b33c26481e36f528cdb",1,"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language","Data2vec is a framework that uses the same learning method for either speech, NLP or computer vision to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture.","International Conference on Machine Learning",2022,"Alexei Baevski,Wei-Ning Hsu,Qiantong Xu,Arun Babu,Jiatao Gu,Michael Auli",359,88,66,"https://www.semanticscholar.org/paper/8f2bca9d684005675e294b33c26481e36f528cdb"
"7016eb4f34611f97fe8c99176246e314678e03f4",1,"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers","This paper presents the fundamentals behind the next version of the Perspective API from Google Jigsaw, and presents a single multilingual token-free Charformer model that is applicable across a range of languages, domains, and tasks.","Knowledge Discovery and Data Mining",2022,"Alyssa Lees,Vinh Q. Tran,Yi Tay,Jeffrey Scott Sorensen,Jai Gupta,Donald Metzler,Lucy Vasserman",25,37,8,"https://www.semanticscholar.org/paper/7016eb4f34611f97fe8c99176246e314678e03f4"
"8a4eec70e3d0c43abf26757252ad6210c9717f6c",1,"Towards Lithuanian grammatical error correction","This work constructs a grammatical error correction model for Lithuanian, the language rich in archaic features, using the recent advances in transformer architectures and shares the best trained model, achieving F$_{0.5}$=0.92, in an online open-source repository.","",2022,"Lukas Stankevivcius,Mantas Lukovsevivcius",1,30,0,"https://www.semanticscholar.org/paper/8a4eec70e3d0c43abf26757252ad6210c9717f6c"
"a747e8f2659df479c0092301b9658fc582423df1",1,"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia","An overview of the current state of NLP research for Indonesia's 700+ languages is provided and general recommendations are provided to help develop NLP technology not only for languages of Indonesia but also other underrepresented languages.","Annual Meeting of the Association for Computational Linguistics",2022,"Alham Fikri Aji,Genta Indra Winata,Fajri Koto,Samuel Cahyawijaya,A. Romadhony,Rahmad Mahendra,Kemal Kurniawan,David Moeljadi,Radityo Eko Prasojo,Timothy Baldwin,Jey Han Lau,Sebastian Ruder",11,204,0,"https://www.semanticscholar.org/paper/a747e8f2659df479c0092301b9658fc582423df1"
"b1dad820853464b30c93b52366b32690ba4b99a6",1,"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View","This survey summarizes the current universal sentence-embedding methods, categorizes them into four groups from a linguistic view, and ultimately analyzes their reported performance.","ACM Computing Surveys",2022,"Ruiqi Li,Xiang Zhao,M. Moens",4,162,1,"https://www.semanticscholar.org/paper/b1dad820853464b30c93b52366b32690ba4b99a6"
"1ed66e048bb025e75aa5ea660545285212e5341f",1,"Scaling Up Models and Data with t5x and seqio","Two software libraries are presented: t5x simplifies the process of building and training large language models at scale while maintaining ease of use, and seqio provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines.","arXiv.org",2022,"Adam Roberts,Hyung Won Chung,Anselm Levskaya,Gaurav Mishra,James Bradbury,D. Andor,Sharan Narang,Brian Lester,Colin Gaffney,Afroz Mohiuddin,Curtis Hawthorne,Aitor Lewkowycz,Alexandru Salcianu,Marc van Zee,Jacob Austin,Sebastian Goodman,Livio Baldini Soares,Haitang Hu,Sasha Tsvyashchenko,Aakanksha Chowdhery,Jasmijn Bastings,Jannis Bulian,Xavier García,Jianmo Ni,A. Chen,Kathleen Kenealy,J. Clark,Stephan Lee,Daniel H Garrette,J. Lee-Thorp,Colin Raffel,Noam M. Shazeer,Marvin Ritter,Maarten Bosma,Alexandre Passos,Jeremy B. Maitin-Shepard,Noah Fiedel,Mark Omernick,Brennan Saeta,Ryan Sepassi,A. Spiridonov,Joshua Newlan,Andrea Gesmundo",84,23,10,"https://www.semanticscholar.org/paper/1ed66e048bb025e75aa5ea660545285212e5341f"
"094ff971d6a8b8ff870946c9b3ce5aa173617bfb",1,"PaLM: Scaling Language Modeling with Pathways","A 540-billion parameter, densely activated, Transformer language model, which is called PaLM achieves breakthrough performance, outperforming the state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark.","arXiv.org",2022,"Aakanksha Chowdhery,Sharan Narang,Jacob Devlin,Maarten Bosma,Gaurav Mishra,Adam Roberts,P. Barham,Hyung Won Chung,Charles Sutton,Sebastian Gehrmann,Parker Schuh,Kensen Shi,Sasha Tsvyashchenko,Joshua Maynez,Abhishek Rao,Parker Barnes,Yi Tay,Noam M. Shazeer,Vinodkumar Prabhakaran,Emily Reif,Nan Du,B. Hutchinson,Reiner Pope,James Bradbury,Jacob Austin,M. Isard,Guy Gur-Ari,Pengcheng Yin,Toju Duke,Anselm Levskaya,S. Ghemawat,Sunipa Dev,H. Michalewski,Xavier García,Vedant Misra,Kevin Robinson,L. Fedus,Denny Zhou,Daphne Ippolito,D. Luan,Hyeontaek Lim,Barret Zoph,A. Spiridonov,Ryan Sepassi,David Dohan,Shivani Agrawal,Mark Omernick,Andrew M. Dai,T. S. Pillai,Marie Pellat,Aitor Lewkowycz,Erica Moreira,Rewon Child,Oleksandr Polozov,Katherine Lee,Zongwei Zhou,Xuezhi Wang,Brennan Saeta,Mark Díaz,Orhan Firat,Michele Catasta,Jason Wei,K. Meier-Hellstern,D. Eck,J. Dean,Slav Petrov,Noah Fiedel",1400,174,203,"https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb"
"880c8973ea1376c6bff23226b3f64792657aa666",1,"ByT5 model for massively multilingual grapheme-to-phoneme conversion","It is found that ByT5 operating on byte-level inputs signiﬁcantly outperformed the token-based mT5 model in terms of multilingual G2P, and pairwise comparison with monolingual models in these languages suggests that multilingual ByT 5 models generally lower the phone error rate by jointly learning from a variety of languages.","Interspeech",2022,"Jian Zhu,Cong Zhang,David Jurgens",4,34,1,"https://www.semanticscholar.org/paper/880c8973ea1376c6bff23226b3f64792657aa666"
"e37018d3cfab9cfc29a7b78404e6c86ea18a907e",1,"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","GPT-NeoX-20B is introduced, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license.","BIGSCIENCE",2022,"Sid Black,Stella Rose Biderman,Eric Hallahan,Quentin G. Anthony,Leo Gao,Laurence Golding,Horace He,Connor Leahy,Kyle McDonell,Jason Phang,M. Pieler,USVSN Sai Prashanth,Shivanshu Purohit,Laria Reynolds,J. Tow,Benqi Wang,Samuel Weinbach",210,141,28,"https://www.semanticscholar.org/paper/e37018d3cfab9cfc29a7b78404e6c86ea18a907e"
"0b9e130c6305de7766697ba7655f56010aaffd61",1,"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction","A Hierarchical N-Gram framework for Zero-Shot Link Prediction (HNZSLP), which considers the dependencies among character n-grams of the relation surface name for ZSLP.","Conference on Empirical Methods in Natural Language Processing",2022,"Mingchen Li,Junfan Chen,Samuel Mensah,Nikolaos Aletras,Xiulong Yang,Yang Ye",3,24,0,"https://www.semanticscholar.org/paper/0b9e130c6305de7766697ba7655f56010aaffd61"
"cb16b85891172572cd856142880b503db0c2bc61",1,"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment","This work uses the task of deciding whether a given string matches a regular expression to identify properties of tasks, instructions, and instances that make instruction learning challenging, and proposes Hard RegSet as a challenging instruction learning dataset and a controlled environment for studying instruction learning.","Conference on Empirical Methods in Natural Language Processing",2022,"Matthew Finlayson,Kyle Richardson,Ashish Sabharwal,Peter Clark",7,30,3,"https://www.semanticscholar.org/paper/cb16b85891172572cd856142880b503db0c2bc61"
"7267812178393b8ae0b99648f02661ca1ff2b412",1,"Bilingual End-to-End ASR with Byte-Level Subwords","This paper investigates how the output representation of an end-to-end neural network affects multilingual automatic speech recognition (ASR), and finds that BBPE with penalty schemes can improve utterance-based bilingual ASR performance by 2% to 5% relative even with smaller number of outputs and fewer parameters.","IEEE International Conference on Acoustics, Speech, and Signal Processing",2022,"Liuhui Deng,Roger Hsiao,Arnab Ghoshal",1,18,0,"https://www.semanticscholar.org/paper/7267812178393b8ae0b99648f02661ca1ff2b412"
"063d9fa4861356500219b7e81d5a654aa921da6f",1,"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation","It is demonstrated that the most effective strategy for transferring both additional languages and additional domains is to leverage small quantities of high-quality translation data to fine-tune large pre-trained models.","North American Chapter of the Association for Computational Linguistics",2022,"David Ifeoluwa Adelani,Jesujoba Oluwadara Alabi,Angela Fan,Julia Kreutzer,Xiaoyu Shen,Machel Reid,Dana Ruiter,D. Klakow,Peter Nabende,Ernie Chang,T. Gwadabe,Freshia Sackey,Bonaventure F. P. Dossou,Chris C. Emezue,Colin Leong,Michael Beukman,Shamsuddeen Hassan Muhammad,Guyo Dub Jarso,Oreen Yousuf,Andre Niyongabo Rubungo,Gilles Hacheme,Eric Peter Wairagala,Muhammad Umair Nasir,Benjamin Ayoade Ajibade,T. Ajayi,Yvonne Wambui Gitau,Jade Z. Abbott,Mohamed Ahmed,Millicent A. Ochieng,Anuoluwapo Aremu,Perez Ogayo,Jonathan Mukiibi,F. Kabore,Godson Kalipe,Derguene Mbaye,A. Tapo,V. M. Koagne,Edwin Munkoh-Buabeng,Valencia Wagner,Idris Abdulmumin,A. Awokoya,Happy Buzaaba,Blessing K. Sibanda,Andiswa Bukula,Sam Manthalu",41,68,1,"https://www.semanticscholar.org/paper/063d9fa4861356500219b7e81d5a654aa921da6f"
"b21670e8061a06ab97e7d6052c9345a326e84ff8",1,"UL2: Unifying Language Learning Paradigms","A unified framework for pre-training models that are universally effective across datasets and setups is presented and Mixture-of-Denoisers (MoD) is proposed, a pre- Training objective that combines diverse pre- training paradigms together.","International Conference on Learning Representations",2022,"Yi Tay,M. Dehghani,Vinh Q. Tran,Xavier García,Jason Wei,Xuezhi Wang,Hyung Won Chung,Dara Bahri,Tal Schuster,Huaixiu Zheng,Denny Zhou,N. Houlsby,Donald Metzler",29,127,8,"https://www.semanticscholar.org/paper/b21670e8061a06ab97e7d6052c9345a326e84ff8"
"32afdb07021fda775ceaedd231c58bfed0aa980a",1,"Automated Crossword Solving","The Berkeley Crossword Solver is presented, a state-of-the-art approach for automatically solving crossword puzzles that improves exact puzzle accuracy from 57% to 82% on crosswords from The New York Times and obtains 99.9% letter accuracy on themeless puzzles.","Annual Meeting of the Association for Computational Linguistics",2022,"Eric Wallace,Nicholas Tomlin,Albert Xu,Kevin Yang,Eshaan Pathak,Matthew Ginsberg,D. Klein",3,29,1,"https://www.semanticscholar.org/paper/32afdb07021fda775ceaedd231c58bfed0aa980a"
"a766ef0678aade6a9798552618819cf4d0fac406",1,"TEVR: Improving Speech Recognition by Token Entropy Variance Reduction","TEVR, a speech recognition model designed to minimize the variation in token entropy w.r.t. to the language model, is presented and it is shown that on CommonVoice German, TEVR scores a very competitive 3 .","arXiv.org",2022,"Hajo N. Krabbenhöft,E. Barth",2,21,0,"https://www.semanticscholar.org/paper/a766ef0678aade6a9798552618819cf4d0fac406"
"b308511ac52e1c2da915d1e55d5246dfdb4cbe28",1,"Romanian Question Answering Using Transformer Based Neural Networks","This work presents experiments evaluated on the XQuAD-ro question answering dataset that has been recently published based on the translation of the SQuAD dataset into Romanian, and shows that fine-tuning the model with the addition of the Romanian translation slightly increases the evaluation metrics.","Studia Universitatis Babeș-Bolyai Informatica",2022,"Bogdan-Alexandru Diaconu,Beáta Lázár-Lőrincz",0,5,0,"https://www.semanticscholar.org/paper/b308511ac52e1c2da915d1e55d5246dfdb4cbe28"
"2ef60a4ea4ea53056be811ff55679eb59fb4b586",1,"Confident Adaptive Language Modeling","This work introduces Confident Adaptive Language Modeling (CALM), a framework for dynamically allocating different amounts of compute per input and generation timestep and demonstrates the efficacy of the framework in reducing compute -- potential speedup of up to $\times 3$ -- while provably maintaining high performance.","Neural Information Processing Systems",2022,"Tal Schuster,Adam Fisch,Jai Gupta,M. Dehghani,Dara Bahri,Vinh Q. Tran,Yi Tay,Donald Metzler",34,89,6,"https://www.semanticscholar.org/paper/2ef60a4ea4ea53056be811ff55679eb59fb4b586"
"00d9a73c54053a32e2aba92b53fc3e6bc71d3238",1,"Sequence to sequence pretraining for a less-resourced Slovenian language","Two different sized T5-type sequence to sequence models for morphologically rich Slovene language with much less resources are trained and analyzed, which mostly lag behind the monolingual Slovene SloBERTa model but are useful for the generative tasks.","",2022,"Matej Ulvcar,Marko Robnik-vSikonja",3,44,1,"https://www.semanticscholar.org/paper/00d9a73c54053a32e2aba92b53fc3e6bc71d3238"
"a806041621acb5cf648fc780f1ff14939aa3a721",1,"How Much Does Lookahead Matter for Disambiguation? Partial Arabic Diacritization Case Study","It is found that the partial diacritizer improves translation quality compared either to their total absence or to random selection, and the benefit of knowing the text that follows the word in focus toward the restoration of short vowels during reading is studied.","Computational Linguistics",2022,"Saeed Esmail,Kfir Bar,N. Dershowitz",1,57,0,"https://www.semanticscholar.org/paper/a806041621acb5cf648fc780f1ff14939aa3a721"
"e4437293a703fcf282bf1b38bf7900ed8ca711bb",1,"Training a T5 Using Lab-sized Resources","Various techniques for making it possible to train a large language model using resources that a modest research lab might have, and train it in a reasonable amount of time are presented.","arXiv.org",2022,"Manuel R. Ciosici,Leon Derczynski",2,29,1,"https://www.semanticscholar.org/paper/e4437293a703fcf282bf1b38bf7900ed8ca711bb"
"28630034bb29760df01ab033b743e30b37f336ae",1,"PaLI: A Jointly-Scaled Multilingual Language-Image Model","The PaLI (Pathways Language and Image model), a model that achieves state-of-the-art in multiple vision and language tasks, while retaining a simple, modular, and scalable design.","International Conference on Learning Representations",2022,"Xi Chen,Xiao Wang,Soravit Changpinyo,A. Piergiovanni,Piotr Padlewski,Daniel M. Salz,Sebastian Goodman,Adam Grycner,Basil Mustafa,L. Beyer,Alexander Kolesnikov,J. Puigcerver,Nan Ding,Keran Rong,Hassan Akbari,Gaurav Mishra,Linting Xue,Ashish V. Thapliyal,James Bradbury,Weicheng Kuo,Mojtaba Seyedhosseini,Chao Jia,Burcu Karagol Ayan,C. Riquelme,A. Steiner,A. Angelova,Xiaohua Zhai,N. Houlsby,Radu Soricut",131,108,27,"https://www.semanticscholar.org/paper/28630034bb29760df01ab033b743e30b37f336ae"
"2a8aac78df5e1e9658a02d381662ed26c6577713",1,"Disease diagnostic method based on cascade backbone network for apple leaf disease classification","This paper is the first to introduce Transformer into apple leaf disease identification, and the results are promising.","Frontiers in Plant Science",2022,"Xing Sheng,Fengyun Wang,Huaijun Ruan,Yangyang Fan,Jiye Zheng,Yangyang Zhang,Chen Lyu",1,25,0,"https://www.semanticscholar.org/paper/2a8aac78df5e1e9658a02d381662ed26c6577713"
"595cc15b9db4e985b30a2e175399a38c021d4ce7",1,"Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino","An N-Gram + Damerau-Levenshtein distance model with automatic rule extraction achieves good performance and outperforms other deep learning approaches in terms of accuracy and edit distance, highlighting the success of traditional approaches over more complex deep learning models in settings where data is unavailable.","SUSTAINLP",2022,"Lorenzo Jaime Yu Flores",0,24,0,"https://www.semanticscholar.org/paper/595cc15b9db4e985b30a2e175399a38c021d4ce7"
"539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72",1,"Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning","Non-Axiomatic Term Logic is presented as a theoretical computational framework of humanlike symbolic reasoning in artiﬁcial intelligence and positions the proposed approach in the phylogeny and the literature of logic.","Transactions of the Japanese society for artificial intelligence",2022,"Kotaro Funakoshi",1,58,0,"https://www.semanticscholar.org/paper/539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72"
"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6",1,"Scaling Instruction-Finetuned Language Models","It is found that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups, and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation).","arXiv.org",2022,"Hyung Won Chung,Le Hou,S. Longpre,Barret Zoph,Yi Tay,W. Fedus,Eric Li,Xuezhi Wang,M. Dehghani,Siddhartha Brahma,Albert Webson,S. Gu,Zhuyun Dai,Mirac Suzgun,Xinyun Chen,Aakanksha Chowdhery,Dasha Valter,Sharan Narang,Gaurav Mishra,A. Yu,Vincent Zhao,Yanping Huang,Andrew M. Dai,Hongkun Yu,Slav Petrov,E. Chi,J. Dean,Jacob Devlin,Adam Roberts,Denny Zhou,Quoc V. Le,Jason Wei",316,104,52,"https://www.semanticscholar.org/paper/5484d228bfc50efbac6e86677bc2ec2ee4ede1a6"
"6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc",1,"Graphemic Normalization of the Perso-Arabic Script","It is argued that better understanding and representation of Perso-Arabic script variation within regional orthographic traditions, where those are present, is crucial for further progress of modern computational NLP techniques, especially for languages with a paucity of resources.","arXiv.org",2022,"R. Doctor,Alexander Gutkin,Cibu Johny,Brian Roark,R. Sproat",3,128,1,"https://www.semanticscholar.org/paper/6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc"
"d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6",1,"SLING: Sino Linguistic Evaluation of Large Language Models","","Conference on Empirical Methods in Natural Language Processing",2022,"Yixiao Song,Kalpesh Krishna,R. Bhatt,Mohit Iyyer",1,61,0,"https://www.semanticscholar.org/paper/d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6"
"5e786d47e3dc5ffa65ba8548ff67139a2764acf8",1,"T5lephone: Bridging Speech and Text Self-supervised Models for Spoken Language Understanding via Phoneme level T5","This work conducts extensive studies on how PLMs with different tokenization strategies affect spoken language understanding task including spoken question answering (SQA) and speech translation (ST) and creates T5lephone, a variant of T5 that is pretrained using phonemicized text.","IEEE International Conference on Acoustics, Speech, and Signal Processing",2022,"Chan-Jan Hsu,Ho-Lam Chung,Hung-yi Lee,Yu Tsao",2,28,0,"https://www.semanticscholar.org/paper/5e786d47e3dc5ffa65ba8548ff67139a2764acf8"
"92302ab168429c7c3a8f699b35ba8302916c6e7c",1,"Bridging Speech and Textual Pre-trained Models with Unsupervised ASR","Unsupervised automatic speech recognition (ASR) is proposed to use as a connector that bridges different modalities used in speech and textual pre-trained models, resulting in an unsupervised speech-to-semantic pre- trained model for various tasks in SLU.","IEEE International Conference on Acoustics, Speech, and Signal Processing",2022,"Jiatong Shi,Chan-Jan Hsu,Ho-Lam Chung,Dongji Gao,Leibny Paola García-Perera,Shinji Watanabe,Ann Lee,Hung-yi Lee",2,44,0,"https://www.semanticscholar.org/paper/92302ab168429c7c3a8f699b35ba8302916c6e7c"
"9dab4c20648cd4c7c6830e6274a95294b014aac9",1,"QAmeleon: Multilingual QA with Only 5 Examples","This approach uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation and shows that few-shot prompt tuning for data synthesis scales across languages and is a viable alternative to large-scale annotation.","arXiv.org",2022,"Priyanka Agrawal,Chris Alberti,Fantine Huot,Joshua Maynez,Ji Ma,Sebastian Ruder,Kuzman Ganchev,Dipanjan Das,Mirella Lapata",11,57,0,"https://www.semanticscholar.org/paper/9dab4c20648cd4c7c6830e6274a95294b014aac9"
"d50b9db750ded246bde13e2c263e341bcbd8a335",1,"A Benchmark and Dataset for Post-OCR text correction in Sanskrit","This work releases a post-OCR text correction dataset containing around 218,000 sentences, with 1.5 million words, from 30 different books of Sanskrit, and finds that the best-performing model, consisting of byte level tokenization in conjunction with phonetic encoding (Byt5+SLP1), yields a 23% point increase over the OCR output in terms of word and character error rates.","Conference on Empirical Methods in Natural Language Processing",2022,"Ayush Maheshwari,Nikhil Singh,A. Krishna,Ganesh Ramakrishnan",3,25,0,"https://www.semanticscholar.org/paper/d50b9db750ded246bde13e2c263e341bcbd8a335"
"af277778904463965c60626e22783d3e1740058b",1,"Introducing Semantics into Speech Encoders","This work proposes a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions, and improves existing speech encoder spoken language understanding performance by over 5% on intent classification.","Annual Meeting of the Association for Computational Linguistics",2022,"Derek Xu,Shuyan Dong,Changhan Wang,Suyoun Kim,Zhaojiang Lin,Akshat Shrivastava,Shang-Wen Li,Liang-Hsuan Tseng,Alexei Baevski,Guan-Ting Lin,Hung-yi Lee,Yizhou Sun,Wei Wang",0,67,0,"https://www.semanticscholar.org/paper/af277778904463965c60626e22783d3e1740058b"
"3af872a4bd314db9e7964876fee811823628f83f",1,"Text Normalization on Code-Mixed Twitter Text using Language Detection","The research shows that proposed method for text normalization still has problems related to language, either on identifying language or normalize a word, but the approach is using language detection module alongside with transformer model.","International Conference on Intelligent Computing",2022,"Rafi Dwi Rizqullah,I. Budi",0,10,0,"https://www.semanticscholar.org/paper/3af872a4bd314db9e7964876fee811823628f83f"
"be2df0fafa89b6355d1ff1336c10e0d4c8d27276",1,"Massively Multilingual Natural Language Understanding 2022 (MMNLU-22) Workshop and Competition",,"MMNLU",2022,"C. Hench,Charith S. Peris,Jack G. M. FitzGerald,Kay Rottmann",1,9,0,"https://www.semanticscholar.org/paper/be2df0fafa89b6355d1ff1336c10e0d4c8d27276"
"7c1c95f5fb7fce563171fcc0060c850390753b3c",1,"TRIP: Triangular Document-level Pre-training for Multilingual Language Models","Triangular Document-level Trilingual Re-training (TRIP), which is the first in the field to accelerate the conventional monolingual and bilingual objectives into a trilingual objective with a novel method called Grafting, is presented.","arXiv.org",2022,"Hongyuan Lu,Haoyang Huang,Shuming Ma,Dongdong Zhang,W. Lam,Furu Wei",2,63,0,"https://www.semanticscholar.org/paper/7c1c95f5fb7fce563171fcc0060c850390753b3c"
"4aa09cba27a489ca02471fad011ea4854fc63cc1",1,"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue","The Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer parameters.","Annual Meeting of the Association for Computational Linguistics",2022,"William B. Held,Christopher Hidey,Fei Liu,Eric Zhu,Rahul Goel,Diyi Yang,Rushin Shah",0,47,0,"https://www.semanticscholar.org/paper/4aa09cba27a489ca02471fad011ea4854fc63cc1"
"b694f4c34d3f7048bd0e4fc08d4bd2f4d23d63c1",1,"Truveta Mapper: A Zero-shot Ontology Alignment Framework","A new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task, which makes the OM task efficient and more straightforward without much post-processing involving mapping extension or mapping repair.","arXiv.org",2023,"Mariyam Amir,Murchana Baruah,Mahsa Eslamialishah,Sina Ehsani,Alireza Bahramali,Sadra Naddaf-Sh,Saman Zarandioon",0,29,0,"https://www.semanticscholar.org/paper/b694f4c34d3f7048bd0e4fc08d4bd2f4d23d63c1"
"30869d285642a8b981702d2a54be0ac54f01aa01",1,"Composer's Assistant: Interactive Transformers for Multi-Track MIDI Infilling","Two T5-like models are trained to solve the task of multi-track MIDI inﬁlling when arbitrary (track, measure) pairs of information have been deleted from a contiguous slice of measures from a MIDI sequence, and their results have implications for the training of neural networks in other small-vocabulary domains.","arXiv.org",2023,"Martin E. Malandro",0,24,0,"https://www.semanticscholar.org/paper/30869d285642a8b981702d2a54be0ac54f01aa01"
"0704a96e1c57c12031f1c3ca492a91dbed1f61ce",1,"Distillation of encoder-decoder transformers for sequence labelling","This work builds on recent work to propose a hallucination-free framework for sequence tagging that is especially suited for distillation and shows empirical results of new state-of-the-art performance across multiple sequence labelling datasets.","Findings",2023,"M. Farina,D. Pappadopulo,Anant Gupta,Leslie Huang,Ozan Irsoy,T. Solorio",0,37,0,"https://www.semanticscholar.org/paper/0704a96e1c57c12031f1c3ca492a91dbed1f61ce"
"d0ab27cfc259fde800e661a714879357714769d5",1,"RetVec: Resilient and Efficient Text Vectorizer","Evaluated and compared RetVec to state-of-the-art tokenizers and word embeddings on common model architectures demonstrate that retVec leads to competitive models that are significantly more resilient to text perturbations across a variety of common tasks.","arXiv.org",2023,"Elie Bursztein,Marina Zhang,Owen Vallis,Xinyu Jia,Alexey Kurakin",0,31,0,"https://www.semanticscholar.org/paper/d0ab27cfc259fde800e661a714879357714769d5"
"96b005d1d92211cf053e4114a85a5e64e428d896",1,"Extending English IR methods to multi-lingual IR","This paper describes the participation in the 2023 WSDM CUP - MIRACL challenge and trains the first SPLADE model that is effectively capable of working in more than 10 languages.","arXiv.org",2023,"Carlos Lassance",0,17,0,"https://www.semanticscholar.org/paper/96b005d1d92211cf053e4114a85a5e64e428d896"
"b169f2ce55e14584d2db6f64eeb5ad2702d39d40",1,"Artificial intelligence foundation and pre-trained models: Fundamentals, applications, opportunities, and social impacts","","Simulation modelling practice and theory",2023,"Adam Kolides,Alyna Nawaz,Anshu Rathor,Denzel Beeman,Muzammil Hashmi,Sana Fatima,David Berdik,M. Al-Ayyoub,Yaser Jararweh",0,73,0,"https://www.semanticscholar.org/paper/b169f2ce55e14584d2db6f64eeb5ad2702d39d40"
"f3d894cf6f7be14a545019f4621ccce41f45b088",1,"Learning the Legibility of Visual Text Perturbations","It is discovered that legible perturbations from the LEGIT dataset are more effective at lowering the performance of NLP models than best-known attack strategies, suggesting that current models may be vulnerable to a broad range of perturbation beyond what is captured by existing visual attacks.","Conference of the European Chapter of the Association for Computational Linguistics",2023,"D. Seth,Rickard Stureborg,Danish Pruthi,Bhuwan Dhingra",0,36,0,"https://www.semanticscholar.org/paper/f3d894cf6f7be14a545019f4621ccce41f45b088"
"b446c44d8b0135a7e7d69640a2a203960117d03d",1,"An Overview on Language Models: Recent Developments and Outlook","This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, architectures, training methods, evaluation methods, and applications.","arXiv.org",2023,"Chen Wei,Yun Cheng Wang,Bin Wang,C.-C. Jay Kuo",1,164,0,"https://www.semanticscholar.org/paper/b446c44d8b0135a7e7d69640a2a203960117d03d"
"1d405cbbfecd02598bab517a23de50d6d90c0e88",1,"DTT: An Example-Driven Tabular Transformer by Leveraging Large Language Models","This paper develops a framework that leverages large deep-learning language models to transform tabular data from a source formatting to a desired target representation, and can efficiently learn the pattern for mapping the source formatting into the expected target using just a few examples.","arXiv.org",2023,"Arash Dargahi Nobari,Davood Rafiei",0,52,0,"https://www.semanticscholar.org/paper/1d405cbbfecd02598bab517a23de50d6d90c0e88"
"eef5b8f3c4e60d596a04101d8261c222ab739861",1,"Fine-Tashkeel: Finetuning Byte-Level Models for Accurate Arabic Text Diacritization","This paper finetune token-free pre-trained multilingual models (ByT5) to learn to predict and insert missing diacritics in Arabic text, a complex task that requires understanding the sentence semantics and the morphological structure of the tokens.","arXiv.org",2023,"Bashar Al-Rfooh,Gheith A. Abandah,Rami Al-Rfou",0,21,0,"https://www.semanticscholar.org/paper/eef5b8f3c4e60d596a04101d8261c222ab739861"
"3add55c068fe19bb2e5392cbe994602a91630ec1",1,"Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams","","arXiv.org",2023,"Desnes Nunes,R. Primi,Ramon Pires,R. Lotufo,Rodrigo Nogueira",4,23,0,"https://www.semanticscholar.org/paper/3add55c068fe19bb2e5392cbe994602a91630ec1"
"2b8d28149a43b9659a6da2c56014ec4206a912b4",1,"Ticket automation: An insight into current research with applications to multi-level classification scenarios","This work provides an overview of support Ticket Automation, what recent proposals are being made in this field, and how well some of these methods can generalize to new scenarios and datasets and showcases an effective way to boost classification by injecting information from the hierarchical structure of the labels into the classifier.","Expert systems with applications",2023,"A. Zangari,Matteo Marcuzzo,Michele Schiavinato,A. Gasparetto,A. Albarelli",0,138,0,"https://www.semanticscholar.org/paper/2b8d28149a43b9659a6da2c56014ec4206a912b4"
"ba184d335a9a08c52c5d25eabd7f4a8ea987918b",1,"UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining","An extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale, find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases.","International Conference on Learning Representations",2023,"Hyung Won Chung,Noah Constant,Xavier García,Adam Roberts,Yi Tay,Sharan Narang,Orhan Firat",3,45,0,"https://www.semanticscholar.org/paper/ba184d335a9a08c52c5d25eabd7f4a8ea987918b"
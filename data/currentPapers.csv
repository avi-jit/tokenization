"id","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning","This work proposes a vocabulary-free neural tokenizer by distilling segmentation information from heuristic-based subword tokenization, which allows end-to-end task learning, resulting in optimal task-specific tokenization.","Workshop on Representation Learning for NLP",2022,"Md. Mofijul Islam,Gustavo Aguilar,Pragaash Ponnusamy,Clint Solomon Mathialagan,Chengyuan Ma,Chenlei Guo",2,23,0,"https://www.semanticscholar.org/paper/ad6c25a46a083e02dbfcdd4b6341945d517b5e31"
"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","What do tokens know about their characters and how do they know it?","The mechanisms through which PLMs acquire English-language character information during training are investigated and it is argued that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.","North American Chapter of the Association for Computational Linguistics",2022,"Ayush Kaushal,Kyle Mahowald",3,79,0,"https://www.semanticscholar.org/paper/9a1a9ae2fc2911f1f275702bef38aa8f79c86986"
"188cd686fb2200f237f688dbda7f64ffc75e67ac","Subword Pooling Makes a Difference","This paper investigates how the choice of subword pooling affects the downstream performance on three tasks: morphological probing, POS tagging and NER, in 9 typologically diverse languages and shows that mBERT is better than XLM-RoBERTa in all 9 languages.","Conference of the European Chapter of the Association for Computational Linguistics",2021,"Judit Ács,'Akos K'ad'ar,András Kornai",13,17,1,"https://www.semanticscholar.org/paper/188cd686fb2200f237f688dbda7f64ffc75e67ac"
"7771aa7badc3375a31bfac8dc47755ff5d5c7780","From characters to words: the turning point of BPE merges","It is shown that text entropy values tend to converge at specific subword levels: relatively few BPE merges lead to the most similar distributions across languages.","Conference of the European Chapter of the Association for Computational Linguistics",2021,"Ximena Gutierrez-Vasques,C. Bentz,O. Sozinova,T. Samardžić",4,31,0,"https://www.semanticscholar.org/paper/7771aa7badc3375a31bfac8dc47755ff5d5c7780"
"17a4477765f27fe3ad037eff9e663ffc6799d7fe","Improving Tokenisation by Alternative Treatment of Spaces","An alterna- 013 tive tokenisation approach where spaces are treated as individual tokens are experiments, which show that the modi- 022 ﬁed algorithms give improved performance on downstream NLP tasks that involve handling 024 complex words, whilst having no detrimental effect on performance in general natural lan- 026 guage understanding tasks.","ArXiv",2022,"Edward Gow-Smith,Harish Tayyar Madabushi,Carolina Scarton,A. Villavicencio",2,45,1,"https://www.semanticscholar.org/paper/17a4477765f27fe3ad037eff9e663ffc6799d7fe"
"373588ff1fb9f7590db000a04de8d838b1516e5a","On the Effectiveness of Quasi Character-Level Models for Machine Translation","This work suggests that quasi-character-level models have practically the same generalization capabilities as character-based models but at lower computational costs and appear to help achieve greater consistency between domains than standard subword- level models, although the catastrophic forgetting problem is not mitigated.","AMTA",2022,"Salvador Carrión-Ponz,F. Casacuberta",0,38,0,"https://www.semanticscholar.org/paper/373588ff1fb9f7590db000a04de8d838b1516e5a"
"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","Beyond Characters: Subword-level Morpheme Segmentation","This paper presents DeepSPIN’s submissions to the SIGMORPHON 2022 Shared Task on Morpheme Segmentation, and challenges the assumption that models for morphological tasks should be trained at the character level by building a transformer that generates morphemes as sequences of unigram language model-induced subwords.","SIGMORPHON",2022,"Ben Peters,André F. T. Martins",2,36,0,"https://www.semanticscholar.org/paper/15db055ac7cb49bac96cc1be1270d9bf2b0b1770"
"d617f51833860dc50d202af7f80be71304b2e994","Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP","This survey connects several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subwordbased approaches based on learned segmentation have been proposed and evaluated.","ArXiv",2021,"Sabrina J. Mielke,Zaid Alyafeai,Elizabeth Salesky,Colin Raffel,Manan Dey,Matthias Gallé,Arun Raja,Chenglei Si,Wilson Y. Lee,Benoît Sagot,Samson Tan",19,186,1,"https://www.semanticscholar.org/paper/d617f51833860dc50d202af7f80be71304b2e994"
"d87647784c12517d31964cc508d5b8423cc24f50","Integrating Approaches to Word Representation","A survey of the distributional, compositional, and relational approaches to addressing the problem of representing the atomic elements of language in modern neural learning systems is presented, with special emphasis on the word level and the out-of-vocabulary phenomenon.","ArXiv",2021,"Yuval Pinter",4,96,0,"https://www.semanticscholar.org/paper/d87647784c12517d31964cc508d5b8423cc24f50"
"f332a615c33c69f54dfcb9a8b14f96e8b5725def","Sub-Character Tokenization for Chinese Pretrained Language Models","Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency, and 2) Pronunciation-based SubChartokenization can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to all homophone typos.","",2021,"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Yasheng Wang,Qun Liu,Maosong Sun",1,42,0,"https://www.semanticscholar.org/paper/f332a615c33c69f54dfcb9a8b14f96e8b5725def"
"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?","This work shows that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models.","WNUT",2021,"Arij Riabi,Benoît Sagot,Djamé Seddah",7,59,0,"https://www.semanticscholar.org/paper/dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5"
"62ece609555bf833a2afd25ef796d72b5f59e767","Local Byte Fusion for Neural Machine Translation","This paper proposes a Lo cal B yt e F usion (LOBEF) method for byte-based machine translation—utilizing byte n -gram and word boundaries—to aggregate local semantic information and indicates that the byte- based models are parameter-efﬁcient and can be trained faster than subword models while showcasing competitive performance.","ArXiv",2022,"Makesh Narsimhan Sreedhar,Xiangpeng Wan,Yu-Jie Cheng,Junjie Hu",0,42,0,"https://www.semanticscholar.org/paper/62ece609555bf833a2afd25ef796d72b5f59e767"
"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","HashFormers: Towards Vocabulary-independent Pre-trained Transformers","It is empirically demonstrated that H ASH F ORMERS are more memory efﬁcient compared to standard pre-trained transformers while achieving comparable predictive performance when ﬁne-tuned on multiple text classiﬂcation tasks.","ArXiv",2022,"Hui-li Xue,Nikolaos Aletras",0,36,0,"https://www.semanticscholar.org/paper/12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1"
"023fd42f0867a88a2206f906c7f127701058feb6","Incorporating Context into Subword Vocabularies","SAGE is presented, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase, and does a better job than current widespread tokenizers in keeping token contexts cohesive, while not incurring a large price in terms of encoding efficiency or domain robustness.","ArXiv",2022,"Shaked Yehezkel,Yuval Pinter",1,41,0,"https://www.semanticscholar.org/paper/023fd42f0867a88a2206f906c7f127701058feb6"
"2f07f97563a73d9b691ec6144e4bba25a347ab87","An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers","FLOTA (Few Longest Token Approximation) leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.","ACL",2022,"Valentin Hofmann,Hinrich Schütze,J. Pierrehumbert",4,54,0,"https://www.semanticscholar.org/paper/2f07f97563a73d9b691ec6144e4bba25a347ab87"
"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","C HARFORMER : F AST C HARACTER T RANSFORMERS VIA G RADIENT - BASED S UBWORD T OKENIZATION","This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion that paves the way for highly performant token-free models that are trained completely end-to-end.","",,"",0,0,0,"https://www.semanticscholar.org/paper/6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01"
"b639124771f9c62cd656a24e8e685a456918e0ff","Character-Level Encoding with S4 for QA","A question answering model that encodes text inputs at character level to utilize subword structures and mitigate the out-of-vocabulary problem is proposed, and both S4 and character-level encoding improve the model performance on the question answering task.","",2022,"Peng Chen",0,21,0,"https://www.semanticscholar.org/paper/b639124771f9c62cd656a24e8e685a456918e0ff"
"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","Why don’t people use character-level machine translation?","It is shown that even with recent modeling innovations in character-level natural language processing, character- level MT systems still struggle to match their subword-based counterparts, and show neither better domain robustness, nor better morphological generalization.","Findings",2021,"Jindřich Libovický,Helmut Schmid,Alexander Fraser",4,78,0,"https://www.semanticscholar.org/paper/f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0"
"2ffacbeeebd3d9e7467610057b4308635a165b6b","Impact of Tokenization on Language Models: An Analysis for Turkish","Morphological and word-level tokenizers outperform de-facto tok- 017 enizers in particular cases and mini models can be competitive to larger state-of-the-art models, such that a 14-times smaller model can recover 94% of the performance of a larger model.","ArXiv",2022,"Cagri Toraman,E. Yilmaz,Furkan Sahinuc,Oguzhan Ozcelik",2,56,1,"https://www.semanticscholar.org/paper/2ffacbeeebd3d9e7467610057b4308635a165b6b"
"17e2977b907aad2532c45185947539e83ac639cd","How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?","This work analyzes how translation performance changes as the data ratios among languages vary in the tokenizer training corpus and finds that while relatively better performance is often observed when languages are more equally sampled, the downstream performance is more robust to language imbalance than the authors usually expected.","Conference of the Association for Machine Translation in the Americas",2022,"Shiyue Zhang,Vishrav Chaudhary,Naman Goyal,James Cross,Guillaume Wenzek,Mohit Bansal,Francisco Guzmán",2,37,0,"https://www.semanticscholar.org/paper/17e2977b907aad2532c45185947539e83ac639cd"
"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations","This work proposes a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations, and implicitly encodes the coarse-grained information into the fine-Grained representations through contrastive learning on multi-grains information.","International Conference on Computational Linguistics",2022,"Borun Chen,Hongyin Tang,Jingang Wang,Qifan Wang,Haitao Zheng,Wei Yu Wu,Liqian Yu",0,39,0,"https://www.semanticscholar.org/paper/062cfd5cbafa47950a5ebbd306cd0059dfd79f75"
"035df9ecf84da7ae475175f326095ab16b97dd47","Investigating the Effectiveness of BPE: The Power of Shorter Sequences","The experiments show that - given a fixed vocabulary size budget - the fewer tokens an algorithm needs to cover the test set, the better the translation (as measured by BLEU).","EMNLP",2019,"Matthias Gallé",13,37,0,"https://www.semanticscholar.org/paper/035df9ecf84da7ae475175f326095ab16b97dd47"
"5e788c833321b12671206b96a438c0e5b1202027","Finding the Optimal Vocabulary Size for Neural Machine Translation","This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and reveals an explanation for why certain vocabulary sizes are better than others.","Findings",2020,"Thamme Gowda,Jonathan May",32,36,1,"https://www.semanticscholar.org/paper/5e788c833321b12671206b96a438c0e5b1202027"
"5c3005e22e6fb218aa76fea49971f3f991993b32","Robust Open-Vocabulary Translation from Visual Text Representations","This work proposes the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows, and demonstrates significant robustness to varied types of noise.","Conference on Empirical Methods in Natural Language Processing",2021,"Elizabeth Salesky,David Etter,Matt Post",13,52,1,"https://www.semanticscholar.org/paper/5c3005e22e6fb218aa76fea49971f3f991993b32"
"2ffbe6040369a82d5a003c2bb835e221c9d2f896","How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?","It is found that learning to analyse and generate morphologically complex surface representations is still challenging, especially for nonconcatenative morphological phenomena like reduplication or vowel harmony and for rare word stems.","EMNLP",2021,"Chantal Amrhein,Rico Sennrich",8,50,1,"https://www.semanticscholar.org/paper/2ffbe6040369a82d5a003c2bb835e221c9d2f896"
"379722c04fb3b54215f82512eb86398cb02d42dd","Subword Segmental Language Modelling for Nguni Languages","A subword segmental language model (SSLM) that learns how to segment words while being trained for autoregressive language modelling, enabling the model to discover morpheme-like subwords that improve its LM capabilities.","ArXiv",2022,"F. Meyer,Jan Buys",0,37,0,"https://www.semanticscholar.org/paper/379722c04fb3b54215f82512eb86398cb02d42dd"
"70dd68c07b322b68836eded1fb4f78c0efcad685","A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models","Surprisingly, it is found that subword-based models might still be the most practical choice in many settings, achieving better performance for lower inference latency and memory usage.","ArXiv",2022,"Jimin Sun,Patrick Fernandes,Xinyi Wang,Graham Neubig",0,22,0,"https://www.semanticscholar.org/paper/70dd68c07b322b68836eded1fb4f78c0efcad685"
"e6b73466bab5e52ce0db19dd06d9353c26557dae","C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE","This work proposes subtokenziation that reduces average length by 17–40% without downstream performance drop, and shows that a carefully chosen subtokenization may significantly improve quality by 0.5-2%, possibly with some length increase.","",2022,"N. Chirkova,Sergey Troshin",0,43,0,"https://www.semanticscholar.org/paper/e6b73466bab5e52ce0db19dd06d9353c26557dae"
"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","Evaluating Various Tokenizers for Arabic Text Classification","This paper introduces three new tokenization algorithms for Arabic and compares them to three other baselines using unsupervised evaluations and shows that the performance of suchtokenization algorithms depends on the size of the dataset, type of the task, and the amount of morphology that exists in the dataset.","Neural Processing Letters",2021,"Zaid Alyafeai,Maged S. Al-shaibani,Mustafa Ghaleb,Irfan Ahmad",5,52,1,"https://www.semanticscholar.org/paper/4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c"
"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","Patching Leaks in the Charformer for Efficient Character-Level Generation","The GBST method from Charformer groups (aka downsamples) characters is used, thereby enabling character grouping in the decoder, and promising performance on English– Turkish translation indicate the potential of character-level models for morphologically rich languages.","ArXiv",2022,"Lukas Edman,Antonio Toral,Gertjan van Noord",1,13,0,"https://www.semanticscholar.org/paper/bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309"
"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings","A robust Hangeul word embedding model against typos is proposed that utilizes a Convolutional Neural Network architecture with a channel attention mechanism that learns to infer the original word embeddings.","EACL",2021,"O-Yeon Kwon,Dohyun Kim,Soobee Lee,Junyoung Choi,SangKeun Lee",1,18,0,"https://www.semanticscholar.org/paper/1babe379f8547a9dc43251e5c5a7bea4a3de5ea1"
"b82f43182aa7a188c12eb9cb6992b6f82cca2120","Language Modelling with Pixels","PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels, and is more robust to noisy text inputs than BERT, further confirming the benefits of modelling language with pixels.","ArXiv",2022,"Phillip Rust,Jonas F. Lotz,Emanuele Bugliarello,Elizabeth Salesky,Miryam de Lhoneux,Desmond Elliott",5,136,1,"https://www.semanticscholar.org/paper/b82f43182aa7a188c12eb9cb6992b6f82cca2120"
"e79d1206292bc5e67ba19737d87d4b2ea4a37105","Charformer: Fast Character Transformers via Gradient-based Subword Tokenization","This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion and paves the way for highly performant token-free models that are trained completely end-to-end.","International Conference on Learning Representations",2021,"Yi Tay,V. Tran,Sebastian Ruder,Jai Gupta,Hyung Won Chung,Dara Bahri,Zhen Qin,Simon Baumgartner,Cong Yu,Donald Metzler",55,69,11,"https://www.semanticscholar.org/paper/e79d1206292bc5e67ba19737d87d4b2ea4a37105"
"9c2e4e5ee224c20a45c37244924138b50f3fe603","Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information","It is shown that incorporating XRAYEMB’s learned vectors into sequences of pre- trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on nonstandard English text.","ArXiv",2021,"Yuval Pinter,A. Stent,Mark Dredze,Jacob Eisenstein",2,32,0,"https://www.semanticscholar.org/paper/9c2e4e5ee224c20a45c37244924138b50f3fe603"
"937b177d2ed7cee27ee45300c690f2f60c81bae5","Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens","The results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell doesn’t appear to enhance its performance on such tasks.","North American Chapter of the Association for Computational Linguistics",2021,"I. Itzhak,Omer Levy",7,16,1,"https://www.semanticscholar.org/paper/937b177d2ed7cee27ee45300c690f2f60c81bae5"
"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts.","International Conference on Topology, Algebra and Categories in Logic",2021,"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel",114,75,28,"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7"
"3b34e79610e5acaba352def4323a59f6d531fac7","Macro-Average: Rare Types Are Important Too","It is found that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance, and can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods’ outputs.","NAACL",2021,"Thamme Gowda,Weiqiu You,Constantine Lignos,Jonathan May",4,42,1,"https://www.semanticscholar.org/paper/3b34e79610e5acaba352def4323a59f6d531fac7"
"59c0076b3d814588e320820b95563965733d1875","AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization","This paper proposes a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine- grained and coarse-grains tokenizations, which outperforms the existing best performing models in almost all cases.","Findings",2020,"Xinsong Zhang,Hang Li",25,46,7,"https://www.semanticscholar.org/paper/59c0076b3d814588e320820b95563965733d1875"
"969287b8a96e242793b11f0dbb99ec341228106f","Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation","Canine is presented, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias.","International Conference on Topology, Algebra and Categories in Logic",2021,"J. Clark,Dan Garrette,Iulia Turc,J. Wieting",68,82,14,"https://www.semanticscholar.org/paper/969287b8a96e242793b11f0dbb99ec341228106f"
"485b3f77b9913e151e7ca897d99497e70e7f30d1","Optimizing segmentation granularity for neural machine translation","This paper incrementally introduces new BPE vocabulary online based on the held-out validation loss, and matches the results found with grid search, optimizing segmentation granularity while significantly reducing overall training time.","Machine Translation",2018,"Elizabeth Salesky,Andrew Runge,Alex Coda,J. Niehues,Graham Neubig",24,26,1,"https://www.semanticscholar.org/paper/485b3f77b9913e151e7ca897d99497e70e7f30d1"
"ab139e341c005929848a7326f3d44f8a6aa9863c","Improving Neural Machine Translation by Incorporating Hierarchical Subword Features","It is confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets and the assumption that in the NMT model, the appropriate subword units for the following three modules can differ is confirmed.","International Conference on Computational Linguistics",2018,"Makoto Morishita,Jun Suzuki,Masaaki Nagata",16,17,0,"https://www.semanticscholar.org/paper/ab139e341c005929848a7326f3d44f8a6aa9863c"
"473921de1b52f98f34f37afd507e57366ff7d1ca","CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters","This work proposes CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters, and shows that this new model improves the performance of Bert on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations.","International Conference on Computational Linguistics",2020,"Hicham El Boukkouri,Olivier Ferret,T. Lavergne,Hiroshi Noji,Pierre Zweigenbaum,Junichi Tsujii",77,36,8,"https://www.semanticscholar.org/paper/473921de1b52f98f34f37afd507e57366ff7d1ca"
"id","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"ad6c25a46a083e02dbfcdd4b6341945d517b5e31","A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning","This work proposes a vocabulary-free neural tokenizer by distilling segmentation information from heuristic-based subword tokenization, which allows end-to-end task learning, resulting in optimal task-specific tokenization.","Workshop on Representation Learning for NLP",2022,"Md. Mofijul Islam,Gustavo Aguilar,Pragaash Ponnusamy,Clint Solomon Mathialagan,Chengyuan Ma,Chenlei Guo",2,23,0,"https://www.semanticscholar.org/paper/ad6c25a46a083e02dbfcdd4b6341945d517b5e31"
"9a1a9ae2fc2911f1f275702bef38aa8f79c86986","What do tokens know about their characters and how do they know it?","The mechanisms through which PLMs acquire English-language character information during training are investigated and it is argued that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.","North American Chapter of the Association for Computational Linguistics",2022,"Ayush Kaushal,Kyle Mahowald",4,79,0,"https://www.semanticscholar.org/paper/9a1a9ae2fc2911f1f275702bef38aa8f79c86986"
"188cd686fb2200f237f688dbda7f64ffc75e67ac","Subword Pooling Makes a Difference","This paper investigates how the choice of subword pooling affects the downstream performance on three tasks: morphological probing, POS tagging and NER, in 9 typologically diverse languages and shows that mBERT is better than XLM-RoBERTa in all 9 languages.","Conference of the European Chapter of the Association for Computational Linguistics",2021,"Judit Ács,'Akos K'ad'ar,András Kornai",15,17,1,"https://www.semanticscholar.org/paper/188cd686fb2200f237f688dbda7f64ffc75e67ac"
"7771aa7badc3375a31bfac8dc47755ff5d5c7780","From characters to words: the turning point of BPE merges","It is shown that text entropy values tend to converge at specific subword levels: relatively few BPE merges lead to the most similar distributions across languages.","Conference of the European Chapter of the Association for Computational Linguistics",2021,"Ximena Gutierrez-Vasques,C. Bentz,O. Sozinova,T. Samardžić",6,31,0,"https://www.semanticscholar.org/paper/7771aa7badc3375a31bfac8dc47755ff5d5c7780"
"17a4477765f27fe3ad037eff9e663ffc6799d7fe","Improving Tokenisation by Alternative Treatment of Spaces","An alterna- 013 tive tokenisation approach where spaces are treated as individual tokens are experiments, which show that the modi- 022 ﬁed algorithms give improved performance on downstream NLP tasks that involve handling 024 complex words, whilst having no detrimental effect on performance in general natural lan- 026 guage understanding tasks.","ArXiv",2022,"Edward Gow-Smith,Harish Tayyar Madabushi,Carolina Scarton,A. Villavicencio",2,45,1,"https://www.semanticscholar.org/paper/17a4477765f27fe3ad037eff9e663ffc6799d7fe"
"373588ff1fb9f7590db000a04de8d838b1516e5a","On the Effectiveness of Quasi Character-Level Models for Machine Translation","This work suggests that quasi-character-level models have practically the same generalization capabilities as character-based models but at lower computational costs and appear to help achieve greater consistency between domains than standard subword- level models, although the catastrophic forgetting problem is not mitigated.","AMTA",2022,"Salvador Carrión-Ponz,F. Casacuberta",1,38,0,"https://www.semanticscholar.org/paper/373588ff1fb9f7590db000a04de8d838b1516e5a"
"d617f51833860dc50d202af7f80be71304b2e994","Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP","This survey connects several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subwordbased approaches based on learned segmentation have been proposed and evaluated.","ArXiv",2021,"Sabrina J. Mielke,Zaid Alyafeai,Elizabeth Salesky,Colin Raffel,Manan Dey,Matthias Gallé,Arun Raja,Chenglei Si,Wilson Y. Lee,Benoît Sagot,Samson Tan",24,186,1,"https://www.semanticscholar.org/paper/d617f51833860dc50d202af7f80be71304b2e994"
"15db055ac7cb49bac96cc1be1270d9bf2b0b1770","Beyond Characters: Subword-level Morpheme Segmentation","This paper presents DeepSPIN’s submissions to the SIGMORPHON 2022 Shared Task on Morpheme Segmentation, and challenges the assumption that models for morphological tasks should be trained at the character level by building a transformer that generates morphemes as sequences of unigram language model-induced subwords.","SIGMORPHON",2022,"Ben Peters,André F. T. Martins",3,36,0,"https://www.semanticscholar.org/paper/15db055ac7cb49bac96cc1be1270d9bf2b0b1770"
"f332a615c33c69f54dfcb9a8b14f96e8b5725def","Sub-Character Tokenization for Chinese Pretrained Language Models","Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency, and 2) Pronunciation-based SubChartokenization can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to all homophone typos.","",2021,"Chenglei Si,Zhengyan Zhang,Yingfa Chen,Fanchao Qi,Xiaozhi Wang,Zhiyuan Liu,Yasheng Wang,Qun Liu,Maosong Sun",2,58,0,"https://www.semanticscholar.org/paper/f332a615c33c69f54dfcb9a8b14f96e8b5725def"
"d87647784c12517d31964cc508d5b8423cc24f50","Integrating Approaches to Word Representation","A survey of the distributional, compositional, and relational approaches to addressing the problem of representing the atomic elements of language in modern neural learning systems is presented, with special emphasis on the word level and the out-of-vocabulary phenomenon.","ArXiv",2021,"Yuval Pinter",4,96,0,"https://www.semanticscholar.org/paper/d87647784c12517d31964cc508d5b8423cc24f50"
"dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5","Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?","This work shows that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models.","WNUT",2021,"Arij Riabi,Benoît Sagot,Djamé Seddah",8,59,0,"https://www.semanticscholar.org/paper/dc018aacbebf81e4e164a2e2c53d3e2c9b1824a5"
"62ece609555bf833a2afd25ef796d72b5f59e767","Local Byte Fusion for Neural Machine Translation","This paper proposes a Lo cal B yt e F usion (LOBEF) method for byte-based machine translation—utilizing byte n -gram and word boundaries—to aggregate local semantic information and indicates that the byte- based models are parameter-efﬁcient and can be trained faster than subword models while showcasing competitive performance.","ArXiv",2022,"Makesh Narsimhan Sreedhar,Xiangpeng Wan,Yu-Jie Cheng,Junjie Hu",0,42,0,"https://www.semanticscholar.org/paper/62ece609555bf833a2afd25ef796d72b5f59e767"
"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","HashFormers: Towards Vocabulary-independent Pre-trained Transformers","It is empirically demonstrated that H ASH F ORMERS are more memory efﬁcient compared to standard pre-trained transformers while achieving comparable predictive performance when ﬁne-tuned on multiple text classiﬂcation tasks.","ArXiv",2022,"Hui-li Xue,Nikolaos Aletras",0,37,0,"https://www.semanticscholar.org/paper/12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1"
"023fd42f0867a88a2206f906c7f127701058feb6","Incorporating Context into Subword Vocabularies","SAGE is presented, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase, and does a better job than current widespread tokenizers in keeping token contexts cohesive, while not incurring a large price in terms of encoding efficiency or domain robustness.","ArXiv",2022,"Shaked Yehezkel,Yuval Pinter",1,46,0,"https://www.semanticscholar.org/paper/023fd42f0867a88a2206f906c7f127701058feb6"
"2f07f97563a73d9b691ec6144e4bba25a347ab87","An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers","FLOTA (Few Longest Token Approximation) leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.","ACL",2022,"Valentin Hofmann,Hinrich Schütze,J. Pierrehumbert",4,54,0,"https://www.semanticscholar.org/paper/2f07f97563a73d9b691ec6144e4bba25a347ab87"
"6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01","C HARFORMER : F AST C HARACTER T RANSFORMERS VIA G RADIENT - BASED S UBWORD T OKENIZATION","This paper introduces a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion that paves the way for highly performant token-free models that are trained completely end-to-end.","",,"",0,0,0,"https://www.semanticscholar.org/paper/6ce74320bdee8bd2a89e7fe1d73a7870b3f7fc01"
"b639124771f9c62cd656a24e8e685a456918e0ff","Character-Level Encoding with S4 for QA","A question answering model that encodes text inputs at character level to utilize subword structures and mitigate the out-of-vocabulary problem is proposed, and both S4 and character-level encoding improve the model performance on the question answering task.","",2022,"Peng Chen",0,21,0,"https://www.semanticscholar.org/paper/b639124771f9c62cd656a24e8e685a456918e0ff"
"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","Why don’t people use character-level machine translation?","It is shown that even with recent modeling innovations in character-level natural language processing, character- level MT systems still struggle to match their subword-based counterparts, and show neither better domain robustness, nor better morphological generalization.","Findings",2021,"Jindřich Libovický,Helmut Schmid,Alexander Fraser",4,78,0,"https://www.semanticscholar.org/paper/f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0"
"2ffacbeeebd3d9e7467610057b4308635a165b6b","Impact of Tokenization on Language Models: An Analysis for Turkish","Morphological and word-level tokenizers outperform de-facto tok- 017 enizers in particular cases and mini models can be competitive to larger state-of-the-art models, such that a 14-times smaller model can recover 94% of the performance of a larger model.","ArXiv",2022,"Cagri Toraman,E. Yilmaz,Furkan Sahinuc,Oguzhan Ozcelik",5,56,1,"https://www.semanticscholar.org/paper/2ffacbeeebd3d9e7467610057b4308635a165b6b"
"17e2977b907aad2532c45185947539e83ac639cd","How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?","This work analyzes how translation performance changes as the data ratios among languages vary in the tokenizer training corpus and finds that while relatively better performance is often observed when languages are more equally sampled, the downstream performance is more robust to language imbalance than the authors usually expected.","Conference of the Association for Machine Translation in the Americas",2022,"Shiyue Zhang,Vishrav Chaudhary,Naman Goyal,James Cross,Guillaume Wenzek,Mohit Bansal,Francisco Guzmán",5,37,0,"https://www.semanticscholar.org/paper/17e2977b907aad2532c45185947539e83ac639cd"
"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations","This work proposes a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations, and implicitly encodes the coarse-grained information into the fine-Grained representations through contrastive learning on multi-grains information.","International Conference on Computational Linguistics",2022,"Borun Chen,Hongyin Tang,Jingang Wang,Qifan Wang,Haitao Zheng,Wei Yu Wu,Liqian Yu",0,39,0,"https://www.semanticscholar.org/paper/062cfd5cbafa47950a5ebbd306cd0059dfd79f75"
"035df9ecf84da7ae475175f326095ab16b97dd47","Investigating the Effectiveness of BPE: The Power of Shorter Sequences","The experiments show that - given a fixed vocabulary size budget - the fewer tokens an algorithm needs to cover the test set, the better the translation (as measured by BLEU).","EMNLP",2019,"Matthias Gallé",13,37,0,"https://www.semanticscholar.org/paper/035df9ecf84da7ae475175f326095ab16b97dd47"
"5e788c833321b12671206b96a438c0e5b1202027","Finding the Optimal Vocabulary Size for Neural Machine Translation","This work casts neural machine translation as a classification task in an autoregressive setting and analyzes the limitations of both classification and autoregression components, and reveals an explanation for why certain vocabulary sizes are better than others.","Findings",2020,"Thamme Gowda,Jonathan May",34,36,1,"https://www.semanticscholar.org/paper/5e788c833321b12671206b96a438c0e5b1202027"
"5c3005e22e6fb218aa76fea49971f3f991993b32","Robust Open-Vocabulary Translation from Visual Text Representations","This work proposes the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows, and demonstrates significant robustness to varied types of noise.","Conference on Empirical Methods in Natural Language Processing",2021,"Elizabeth Salesky,David Etter,Matt Post",14,52,1,"https://www.semanticscholar.org/paper/5c3005e22e6fb218aa76fea49971f3f991993b32"
"379722c04fb3b54215f82512eb86398cb02d42dd","Subword Segmental Language Modelling for Nguni Languages","A subword segmental language model (SSLM) that learns how to segment words while being trained for autoregressive language modelling, enabling the model to discover morpheme-like subwords that improve its LM capabilities.","ArXiv",2022,"F. Meyer,Jan Buys",0,37,0,"https://www.semanticscholar.org/paper/379722c04fb3b54215f82512eb86398cb02d42dd"
"70dd68c07b322b68836eded1fb4f78c0efcad685","A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models","Surprisingly, it is found that subword-based models might still be the most practical choice in many settings, achieving better performance for lower inference latency and memory usage.","ArXiv",2022,"Jimin Sun,Patrick Fernandes,Xinyi Wang,Graham Neubig",0,22,0,"https://www.semanticscholar.org/paper/70dd68c07b322b68836eded1fb4f78c0efcad685"
"e6b73466bab5e52ce0db19dd06d9353c26557dae","C ODE BPE: I NVESTIGATING S UBTOKENIZATION O PTIONS FOR L ARGE L ANGUAGE M ODEL P RETRAINING ON S OURCE C ODE","This work proposes subtokenziation that reduces average length by 17–40% without downstream performance drop, and shows that a carefully chosen subtokenization may significantly improve quality by 0.5-2%, possibly with some length increase.","",2022,"N. Chirkova,Sergey Troshin",0,43,0,"https://www.semanticscholar.org/paper/e6b73466bab5e52ce0db19dd06d9353c26557dae"
"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","Evaluating Various Tokenizers for Arabic Text Classification","This paper introduces three new tokenization algorithms for Arabic and compares them to three other baselines using unsupervised evaluations and shows that the performance of suchtokenization algorithms depends on the size of the dataset, type of the task, and the amount of morphology that exists in the dataset.","Neural Processing Letters",2021,"Zaid Alyafeai,Maged S. Al-shaibani,Mustafa Ghaleb,Irfan Ahmad",9,52,1,"https://www.semanticscholar.org/paper/4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c"
"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","Patching Leaks in the Charformer for Efficient Character-Level Generation","The GBST method from Charformer groups (aka downsamples) characters is used, thereby enabling character grouping in the decoder, and promising performance on English– Turkish translation indicate the potential of character-level models for morphologically rich languages.","ArXiv",2022,"Lukas Edman,Antonio Toral,Gertjan van Noord",1,13,0,"https://www.semanticscholar.org/paper/bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309"
"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings","A robust Hangeul word embedding model against typos is proposed that utilizes a Convolutional Neural Network architecture with a channel attention mechanism that learns to infer the original word embeddings.","EACL",2021,"O-Yeon Kwon,Dohyun Kim,Soobee Lee,Junyoung Choi,SangKeun Lee",1,18,0,"https://www.semanticscholar.org/paper/1babe379f8547a9dc43251e5c5a7bea4a3de5ea1"
"9c2e4e5ee224c20a45c37244924138b50f3fe603","Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information","It is shown that incorporating XRAYEMB’s learned vectors into sequences of pre- trained token embeddings helps performance on both autoregressive and masked pre-trained transformer architectures and on both sequence-level and sequence tagging tasks, particularly on nonstandard English text.","ArXiv",2021,"Yuval Pinter,A. Stent,Mark Dredze,Jacob Eisenstein",3,32,0,"https://www.semanticscholar.org/paper/9c2e4e5ee224c20a45c37244924138b50f3fe603"
"937b177d2ed7cee27ee45300c690f2f60c81bae5","Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens","The results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell doesn’t appear to enhance its performance on such tasks.","North American Chapter of the Association for Computational Linguistics",2021,"I. Itzhak,Omer Levy",8,16,1,"https://www.semanticscholar.org/paper/937b177d2ed7cee27ee45300c690f2f60c81bae5"
"3b34e79610e5acaba352def4323a59f6d531fac7","Macro-Average: Rare Types Are Important Too","It is found that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance, and can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods’ outputs.","NAACL",2021,"Thamme Gowda,Weiqiu You,Constantine Lignos,Jonathan May",5,42,1,"https://www.semanticscholar.org/paper/3b34e79610e5acaba352def4323a59f6d531fac7"
"b82f43182aa7a188c12eb9cb6992b6f82cca2120","Language Modelling with Pixels","PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels, and is more robust to noisy text inputs than BERT, further confirming the benefits of modelling language with pixels.","ArXiv",2022,"Phillip Rust,Jonas F. Lotz,Emanuele Bugliarello,Elizabeth Salesky,Miryam de Lhoneux,Desmond Elliott",5,136,1,"https://www.semanticscholar.org/paper/b82f43182aa7a188c12eb9cb6992b6f82cca2120"
"59c0076b3d814588e320820b95563965733d1875","AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization","This paper proposes a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine- grained and coarse-grains tokenizations, which outperforms the existing best performing models in almost all cases.","Findings",2020,"Xinsong Zhang,Hang Li",26,46,7,"https://www.semanticscholar.org/paper/59c0076b3d814588e320820b95563965733d1875"
"969287b8a96e242793b11f0dbb99ec341228106f","Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation","Canine is presented, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias.","International Conference on Topology, Algebra and Categories in Logic",2021,"J. Clark,Dan Garrette,Iulia Turc,J. Wieting",74,82,14,"https://www.semanticscholar.org/paper/969287b8a96e242793b11f0dbb99ec341228106f"
"485b3f77b9913e151e7ca897d99497e70e7f30d1","Optimizing segmentation granularity for neural machine translation","This paper incrementally introduces new BPE vocabulary online based on the held-out validation loss, and matches the results found with grid search, optimizing segmentation granularity while significantly reducing overall training time.","Machine Translation",2018,"Elizabeth Salesky,Andrew Runge,Alex Coda,J. Niehues,Graham Neubig",24,26,1,"https://www.semanticscholar.org/paper/485b3f77b9913e151e7ca897d99497e70e7f30d1"
"ab139e341c005929848a7326f3d44f8a6aa9863c","Improving Neural Machine Translation by Incorporating Hierarchical Subword Features","It is confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets and the assumption that in the NMT model, the appropriate subword units for the following three modules can differ is confirmed.","International Conference on Computational Linguistics",2018,"Makoto Morishita,Jun Suzuki,Masaaki Nagata",16,17,0,"https://www.semanticscholar.org/paper/ab139e341c005929848a7326f3d44f8a6aa9863c"
"205cc15fca6963b355e4c071071368e874ee103e","Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training","While simple character-level tokenization approaches still perform best on purely form-based tasks like string reversal, this method is superior for more complex tasks that blend form, meaning, and context, such as spelling correction in context and word search games.","",2022,"Jing Huang,Zhengxuan Wu,Kyle Mahowald,Christopher Potts",1,45,0,"https://www.semanticscholar.org/paper/205cc15fca6963b355e4c071071368e874ee103e"
"353d6a18a222aec0be66de0b8be111fbbe67012d","Character-Aware Models Improve Visual Text Rendering","This paper trains a suite of image generation models, and shows that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (the authors' DrawText benchmark), and sets a much higher state-of-the-art on visual spelling.","",2022,"Rosanne Liu,Daniel H Garrette,Chitwan Saharia,William Chan,Adam Roberts,Sharan Narang,Irina Blok,R. Mical,Mohammad Norouzi,Noah Constant",2,36,0,"https://www.semanticscholar.org/paper/353d6a18a222aec0be66de0b8be111fbbe67012d"
"6c761cfdb031701072582e434d8f64d436255da6","AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing","This comprehensive survey paper explains various core concepts like pretraining, Pretraining methods, pretraining tasks, embeddings and downstream adaptation methods, presents a new taxonomy of T-PTLMs and gives brief overview of various benchmarks including both intrinsic and extrinsic.","ArXiv",2021,"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha",64,301,6,"https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6"
"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","Subword-Delimited Downsampling for Better Character-Level Translation","This newdownsampling method not only outperforms existing downsampling methods, showing that downsamplings characters can be done without sacriﬁcing quality, but also leads to promising performance compared to subword models for translation.","ArXiv",2022,"Lukas Edman,Antonio Toral,Gertjan van Noord",1,24,0,"https://www.semanticscholar.org/paper/ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6"
"9f4351600c72d5dac0251cd49984f691dca2fcd1","Word-Level Representation From Bytes For Language Modeling","This work overhauls the existing character-aware method that takes character-level inputs but makes word-level sequence modeling and prediction a token free model with slim input embeddings for downstream tasks by introducing a cross-attention network and a sub-word level prediction based on word- level hidden states.","ArXiv",2022,"Chul Lee,Qipeng Guo,Xipeng Qiu",0,35,0,"https://www.semanticscholar.org/paper/9f4351600c72d5dac0251cd49984f691dca2fcd1"
"11ddb0953eae196dab339bfdc117221594cf945e","ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models","This work successfully pre-train and release ByGPT5, a new token-free decoder-only language model, and successfully tunes it on a large custom corpus of English and German quatrains annotated with the authors' styles, demonstrating its runtime performance and introspect the model’s understanding of style conditions.","",2022,"Jonas Belouadi,Steffen Eger",1,67,0,"https://www.semanticscholar.org/paper/11ddb0953eae196dab339bfdc117221594cf945e"
"660ddea322b193c7428f2a3149ebe3d24ff9d88d","Image-and-Language Understanding from Pixels Only","This work explores an additional uniﬁcation: the use of a pure pixel-based model to perform image, text, and multimodal tasks and exploits the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multi-modal retrieval without modi ﬁcations.","",2022,"M. Tschannen,Basil Mustafa,N. Houlsby",2,77,0,"https://www.semanticscholar.org/paper/660ddea322b193c7428f2a3149ebe3d24ff9d88d"
"0351167c875b8931366e85eb5e517819d7db80cc","What Makes for Good Tokenizers in Vision Transformer?","Through extensive experiments on various transformer architectures, this work observes both improved performance and intriguing properties of these two plug-and-play designs with negligible computational overhead, indicating the importance of the commonly-omitted designs of tokenizers in vision transformer.","IEEE Transactions on Pattern Analysis and Machine Intelligence",2022,"Shengju Qian,Yi Zhu,Wenbo Li,Mu Li,Jiaya Jia",3,91,0,"https://www.semanticscholar.org/paper/0351167c875b8931366e85eb5e517819d7db80cc"
"9f9196beee29c02b8c5837c6edbc69967189b735","Efficient Transformers with Dynamic Token Pooling","Dynamic pooling, which jointly segments and models language, is of-ten both faster and more accurate than vanilla Transformers and ﬁxed-length pooling within the same computational budget.","ArXiv",2022,"P. Nawrot,J. Chorowski,Adrian La'ncucki,E. Ponti",0,55,0,"https://www.semanticscholar.org/paper/9f9196beee29c02b8c5837c6edbc69967189b735"
"e6b252ad22486c10b1b288e0a5e1ad468690be70","Joint Optimization of Tokenization and Downstream Model","Experimental results show that the proposed method improves the performance by determining appropriate tokenizations and can be used to explore the appropriate tokenization for an already trained model as post-processing.","Findings",2021,"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki",12,40,0,"https://www.semanticscholar.org/paper/e6b252ad22486c10b1b288e0a5e1ad468690be70"
"00675f1591392622b0db2d9cd37a8a1f32e37aa8","Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks","It is shown that, with consistent tokenization, the model performs better in both in-domain and out-of-domain datasets, with a notable average of +1.7 F 1 gain when a BART model is trained on SQuAD and evaluated on 8 QA datasets.","",2022,"Kaiser Sun,Peng Qi,Yuhao Zhang,Lan Liu,William Yang Wang,Zhiheng Huang",0,29,0,"https://www.semanticscholar.org/paper/00675f1591392622b0db2d9cd37a8a1f32e37aa8"
"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling","MANTa is a differentiable tokenizer trained end-to-end with the language model that improves robustness to character perturbations and out-of-domain data and is considerably faster than strictly byte-level models.","ArXiv",2022,"Nathan Godey,Roman Castagn'e,Eric Villemonte de la Clergerie,Benoît Sagot",1,29,0,"https://www.semanticscholar.org/paper/bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d"
"3227844af4d38bfa702781e321cf6712bf537e2c","Word-level Perturbation Considering Word Length and Compositional Subwords","","Findings",2022,"Tatsuya Hiraoka,Sho Takase,Kei Uchiumi,Atsushi Keyaki,Naoaki Okazaki",1,30,0,"https://www.semanticscholar.org/paper/3227844af4d38bfa702781e321cf6712bf537e2c"
"8c82d3d758897ef9f166924683831ecf6085f21a","MaxMatch-Dropout: Subword Regularization for WordPiece","The proposed method, MaxMatch-Dropout, randomly drops words in a search using the maximum matching algorithm for tokenization to realize finetuning with subword regularization for popular pretrained language models such as BERT-base.","International Conference on Computational Linguistics",2022,"Tatsuya Hiraoka",1,29,0,"https://www.semanticscholar.org/paper/8c82d3d758897ef9f166924683831ecf6085f21a"
"75a05bffa2cc35a876ce04edb0b57f9592716d3b","Extending the Subwording Model of Multilingual Pretrained Models for New Languages","This paper adds new subwords to the SentencePiece tokenizer to apply a multilingual pretrained model to new languages (Inuk-titut in this paper) and applies the mBART-50 pret trained model to English-Inuktituts translation.","ArXiv",2022,"K. Imamura,E. Sumita",0,29,0,"https://www.semanticscholar.org/paper/75a05bffa2cc35a876ce04edb0b57f9592716d3b"
"0ab0fda8774c303be8f8f8c8f684a890dcf5d455","Lattice-Based Transformer Encoder for Neural Machine Translation","This work proposes lattice-based encoders to explore effective word or subword representation in an automatic way during training and proposes two methods: 1) lattice positional encoding and 2) lattICE-aware self-attention to further improve translation performance.","Annual Meeting of the Association for Computational Linguistics",2019,"Fengshun Xiao,Jiangtong Li,Zhao Hai,Rui Wang,Kehai Chen",38,49,5,"https://www.semanticscholar.org/paper/0ab0fda8774c303be8f8f8c8f684a890dcf5d455"
"8ab8288e3596fecfc2c5482cc8d48c54e97ab42e","Adversarial Subword Regularization for Robust Neural Machine Translation","This paper presents adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for exposing diverse subword segmentations and experimentally shows that the model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the performance of N MT models in low-resource and out-domain datasets.","Findings",2020,"Jungsoo Park,Mujeen Sung,Jinhyuk Lee,Jaewoo Kang",4,47,0,"https://www.semanticscholar.org/paper/8ab8288e3596fecfc2c5482cc8d48c54e97ab42e"
"0d4b5c9a071557f4eb12f63f785dbc89071d4272","How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models","It is found that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.","Annual Meeting of the Association for Computational Linguistics",2020,"Phillip Rust,Jonas Pfeiffer,Ivan Vulic,Sebastian Ruder,Iryna Gurevych",87,102,11,"https://www.semanticscholar.org/paper/0d4b5c9a071557f4eb12f63f785dbc89071d4272"
"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts.","International Conference on Topology, Algebra and Categories in Logic",2021,"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel",135,75,29,"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7"
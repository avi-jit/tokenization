"id","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings","A robust Hangeul word embedding model against typos is proposed that utilizes a Convolutional Neural Network architecture with a channel attention mechanism that learns to infer the original word embeddings.","EACL",2021,"O-Yeon Kwon,Dohyun Kim,Soobee Lee,Junyoung Choi,SangKeun Lee",5,18,0,"https://www.semanticscholar.org/paper/1babe379f8547a9dc43251e5c5a7bea4a3de5ea1"
"6c761cfdb031701072582e434d8f64d436255da6","AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing","This comprehensive survey paper explains various core concepts like pretraining, Pretraining methods, pretraining tasks, embeddings and downstream adaptation methods, presents a new taxonomy of T-PTLMs and gives brief overview of various benchmarks including both intrinsic and extrinsic.","ArXiv",2021,"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha",154,304,6,"https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6"
"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts.","International Conference on Topology, Algebra and Categories in Logic",2021,"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel",271,68,29,"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7"
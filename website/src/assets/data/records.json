{"papers":[{"url":"https://www.semanticscholar.org/paper/1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","title":"Handling Out-Of-Vocabulary Problem in Hangeul Word Embeddings","venue":"EACL","year":2021,"referenceCount":18,"citationCount":6,"influentialCitationCount":0,"publicationDate":2021,"authors":"O-Yeon Kwon,Dohyun Kim,Soobee Lee,Junyoung Choi,SangKeun Lee","citations":[{"paperId":"7a139679ef9b310008e4ad799d8a27fb7c0747a8","title":"Computational Models to Study Language Processing in the Human Brain: A Survey"},{"paperId":"02942af51e5c80fd6c6fa8097ad389e697d53677","title":"DPMS: Data-Driven Promotional Management System of Universities Using Deep Learning on Social Media"},{"paperId":"ce26a9458de83c7394ebfe6f62d26ad73869b0cb","title":"From task to evaluation: an automatic text summarization review"},{"paperId":"e985d34a29a1988659e7971d520496eca1a7a548","title":"BejaGNN: behavior-based Java malware detection via graph neural network"},{"paperId":"96e77dfdd50411fe404e6b6bbde5224df9eee802","title":"Tweets Emotions Analysis of Community Activities Restriction as COVID-19 Policy in Indonesia Using Support Vector Machine"},{"paperId":"f22ba8b56b89b2f19e84fbe3439bf78e8fc926b2","title":"KLASIFIKASI EMOSI PADA CUITAN DI TWITTER DENGAN PRINCIPAL COMPONENT ANALYSIS DAN SUPPORT VECTOR MACHINE"}],"references":[{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"8fa7d1f3f82526935ba122c20c8d0648506301b3","title":"Misspelling Oblivious Word Embeddings"},{"paperId":"9348186c18bbd35d77a9011474fdd76ef98c86c5","title":"Robust Word Vectors: Context-Informed Embeddings for Noisy Texts"},{"paperId":"6c145caf5da0e2f078c34d0b65b399d7124e2fd8","title":"Learning to Generate Word Representations using Subword Information"},{"paperId":"1dff26dc9be229daae61231a340f57efa8126e0d","title":"Subword-level Word Vector Representations for Korean"},{"paperId":"c0fbe0378150bb35e94bdd800bf02a4e57d9f2c2","title":"Morpheme-based Efficient Korean Word Embedding"},{"paperId":"765bdcf27ebc1eb03a14f1e47aefa4dda1e03073","title":"Synthetic and Natural Noise Both Break Neural Machine Translation"},{"paperId":"8e32e1f02b7060ce419a964b800d0927a2e1d69c","title":"Mimicking Word Embeddings using Subword RNNs"},{"paperId":"e2dba792360873aef125572812f3673b1a85d850","title":"Enriching Word Vectors with Subword Information"},{"paperId":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"},{"paperId":"6fe682de00baba4d50bb855ae243f55a4a2a4e14","title":"A Word Embedding and a Josa Vector for Korean Unsupervised Semantic Role Induction"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"b0130277677e5b915d5cd86b3afafd77fd08eb2e","title":"Estimation of probabilities from sparse data for the language model component of a speech recognizer"},{"paperId":null,"title":"Analyzing of hangul search query spelling error patterns and developing query spelling correction system based on user logs"},{"paperId":"b92a80c848d51bb009758c117ab4f05a99913106","title":"THE KOREAN LANGUAGE: Structure, use and context"}],"id":"1babe379f8547a9dc43251e5c5a7bea4a3de5ea1","summary":"A robust Hangeul word embedding model against typos is proposed that utilizes a Convolutional Neural Network architecture with a channel attention mechanism that learns to infer the original word embeddings."},{"url":"https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing","venue":"ArXiv","year":2021,"referenceCount":304,"citationCount":168,"influentialCitationCount":6,"publicationDate":"12/08/2021","authors":"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha","citations":[{"paperId":"651a0579eb3e63806134dcb1d141d2b00fcc8421","title":"Blessing or curse? A survey on the Impact of Generative AI on Fake News"},{"paperId":"c522009902ae5dbfb23db9ffac69da11d2cb778b","title":"AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for Detecting Multi-generator Machine-generated Text"},{"paperId":"5171aa6fb83e97a8b9ee99d1d881b71b389be320","title":"On permutation-invariant neural networks"},{"paperId":"88e6bae24ae33dc1b381213dfd105c1a21b31e6d","title":"CI-STHPAN: Pre-trained Attention Network for Stock Selection with Channel-Independent Spatio-Temporal Hypergraph"},{"paperId":"8bc3f9ff9a26280bf8fc94bb46e26c5d799a540a","title":"Computational Sentence-level Metrics Predicting Human Sentence Comprehension"},{"paperId":"03082a976eff70f37009e56af353eaec30a0193a","title":"Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting"},{"paperId":"da0a4266cc1c1a2b3b65922c037e77f3a828ddf6","title":"Efficient High-Resolution Time Series Classification via Attention Kronecker Decomposition"},{"paperId":"45dadac2e3e52659369595256fde80ab51796246","title":"Inner Product Accelerating Scheme Based on RRAM Array for Attention-Mechanism Neural Network"},{"paperId":"fa3226d7841d4428906a3e9350a7d0fe77d0d1a0","title":"Content Modeling in Multi-Platform Multilingual Social Media Data"},{"paperId":"b3de1e03b3b378e746f16b66b86f4bd3f2111e26","title":"LLMGuard: Guarding against Unsafe LLM Behavior"},{"paperId":"19402cea70b08008fd9c19631669c816981855c4","title":"Enhanced Kidney Tumor Segmentation in CT Scans Using a Simplified UNETR with Organ Information"},{"paperId":"a951bd5f667a63459c4f2f70f39724074634b89a","title":"Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products"},{"paperId":"1df04f33a8ef313cc2067147dbb79c3ca7c5c99f","title":"Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces"},{"paperId":"8686805010697380c8ad1d908ce4074a99c924ab","title":"PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software"},{"paperId":"0499939b6d855b5b49a9b9829ce0d19850b346cd","title":"Engineering A Large Language Model From Scratch"},{"paperId":"3cd81b0123b5f8477f6b5777681030ef6b05dd46","title":"The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts"},{"paperId":"27191876fd26b2eeb0c895e03417f443ae117c00","title":"Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research"},{"paperId":"432e7fb118b94b10ec514fd7c1ebd67cad65ab62","title":"Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing"},{"paperId":"88a30d7676108ecafcd8a85c2c60b3d5d1fbde50","title":"A Survey on Efficient Federated Learning Methods for Foundation Model Training"},{"paperId":"4016fe2e7916b349f58d53f2e8a756bccb9c147a","title":"From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models"},{"paperId":"d839519477bd2f2055ec189f796bce578c578102","title":"Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey"},{"paperId":"076e19395bb0395b889e99ce46c198d8678864fd","title":"QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers"},{"paperId":"b87e2944fc6e3b1fbc21bdcb923dd0c29d4afa04","title":"Vulnerability Analysis of Transformer-based Optical Character Recognition to Adversarial Attacks"},{"paperId":"1adfc77b0c6e6d5a0f4c77c01180df1c430e8543","title":"Sentiment analysis using deep learning techniques: a comprehensive review"},{"paperId":"40df031b916709b39063cc7ef99c655c419fcded","title":"Arabic stance detection of COVID-19 vaccination using transformer-based approaches: a comparison study"},{"paperId":"972db8f6f147c0078b3106ce84db991649e1b287","title":"Hiformer: Heterogeneous Feature Interactions Learning with Transformers for Recommender Systems"},{"paperId":"0a898a2772c7c2941b83a737f1bef806710b047a","title":"Evaluating multiple large language models in pediatric ophthalmology"},{"paperId":"e79a35acb00a6b40d53ea0c71071a28ed6884ae1","title":"SugarViT - Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet"},{"paperId":"e62cfe350fd94e874b441f58985b9444b65ee24d","title":"Attention to COVID-19: Abstractive Summarization of COVID-19 Research with State-of-the-Art Transformers"},{"paperId":"e9bca09bb2457f6f4c861446669f5941d61a5713","title":"FET-OPU: A Flexible and Efficient FPGA-Based Overlay Processor for Transformer Networks"},{"paperId":"8d9f78ee984a5aa986b7fbd4198e0e23d95cd1aa","title":"The Dark Side of the Language: Syntax-Based Neural Networks Rivaling Transformers in Definitely Unseen Sentences"},{"paperId":"18d18d4ffdc070868ce06f216a2a8d040d42a4cb","title":"Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models"},{"paperId":"9ea8105a7bc03dbcde05bf953f3b90f1db61f6bd","title":"URL-BERT: Training Webpage Representations via Social Media Engagements"},{"paperId":"6b5b6384a0400c9188c3577803395b3adc94c7e9","title":"Challenges and opportunities for Arabic question-answering systems: current techniques and future directions"},{"paperId":"7dd4b7e6f5f1588eae707df8334589ffd503bc54","title":"A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models"},{"paperId":"7be93f2daf5753d13a627db9e046e558291f6dcc","title":"PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps"},{"paperId":"e7f45aa6124a5315ccf0cd607637ee15691f90c5","title":"MetaProbformer for Charging Load Probabilistic Forecasting of Electric Vehicle Charging Stations"},{"paperId":"788da23e46632cca4696a4a8286e2ea9b33a1b46","title":"GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction"},{"paperId":"880aa839e9275808573f221988944323bf40ddab","title":"Bias and Fairness in Chatbots: An Overview"},{"paperId":"7ebbc89e7d90e342793bc39cf5fa30bb63dce518","title":"A robust deep learning workflow to predict CD8 + T-cell epitopes"},{"paperId":"d2b5b86a76b3633ee189125ff4046bebafcd4a95","title":"A lightweight transformer for faster and robust EBSD data collection"},{"paperId":"0986eb81994647aaacb6dd879397103438751dad","title":"Fast Training of NMT Model with Data Sorting"},{"paperId":"efd4c155210317cea21741c08042eb9a049d479c","title":"An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM"},{"paperId":"7ba388fdcfc34da2dbb55786f0b70a898b9c0489","title":"Fine-tuning Multilingual Transformers for Hausa-English Sentiment Analysis"},{"paperId":"7db2732db2cd1cd571aea1dc5cc853d886ac30f3","title":"A Spatial-Aware Representation Learning Model for Link Completion in GeoKG: A Case Study on Wikidata and OpenStreetMap"},{"paperId":"1851384056be3deb030686b6e6c9b2245301b15c","title":"Community Structure and Coherence in Digital Humanities Works"},{"paperId":"a06f88e6be4935f7694604b7b5ffedfaa4d9783e","title":"Handling Data Scarcity in Code-Mixing Using Learning-Based Approaches"},{"paperId":"1ba4da21d669b630c86079b60173664abed0c279","title":"A Side-by-side Comparison of Transformers for Implicit Discourse Relation Classification"},{"paperId":"7cd8d611fe158d51a8f6764cdc52b2ef5f1fcd0a","title":"Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study"},{"paperId":"e36b3a9b6071a237ab110219d61107f14c7275e2","title":"No Labels?: No Problem! Experiments with active learning strategies for multi-class classification in imbalanced low-resource settings"},{"paperId":"7d5657c78f3fee9756061c6a82db44db9d413e0b","title":"AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology"},{"paperId":"5fce7d9442b06cab91174fb68ba52ff6bdaa29cc","title":"A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks"},{"paperId":"7b304faf0bbbc6e17891454c2f6a32bc573f2f18","title":"DSHGT: Dual-Supervisors Heterogeneous Graph Transformer - A pioneer study of using heterogeneous graph learning for detecting software vulnerabilities"},{"paperId":"5a90a8f4ec612cef6b1bb9cf4eae897385d33c2d","title":"An Overview on Generative AI at Scale With Edge–Cloud Computing"},{"paperId":"43ec80eeb6f22431ae741796996b25ca3b6bf3e2","title":"Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting"},{"paperId":"1a91b8652baddcbf7722f3087a6e0c2d067ae6f5","title":"Multilingual Multiword Expression Identification Using Lateral Inhibition and Domain Adaptation"},{"paperId":"7609c41065711902ebf56542dd7b95f3bcb807a3","title":"Automatic Classification of Public Expenses in the Fight against COVID-19: A Case Study of TCE/PI"},{"paperId":"79f53eb251af62ecffa784ff89605610e4b14d56","title":"A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting"},{"paperId":"77904c6d1eb07dc128c4776447b7b50e7816383e","title":"TI-former: A Time-Interval Prediction Transformer for Timestamped Sequences"},{"paperId":"97f551d97f6ea4880c11f1da5bcec2277bdb56ef","title":"Sequential Transfer Learning to Decode Heard and Imagined Timbre from fMRI Data"},{"paperId":"4f0c7f4df04f07609bdb67944af2a529d5a4517b","title":"A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation"},{"paperId":"e5dff0d39324dd0bb3fa323f2d256f801043ba4a","title":"A Survey on Time-Series Pre-Trained Models"},{"paperId":"d50f2935deabfc75c93d0d210c365ca4d1ba37ca","title":"Prospects and Challenges of Large Language Models in the Field of Intelligent Building"},{"paperId":"96b75f51fa24018fad2d96878a58536048a41c3f","title":"HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models"},{"paperId":"af3acb9c57f8da06382b5e885ad797d70444827d","title":"MPI-RICAL: Data-Driven MPI Distributed Parallelism Assistance with Transformers"},{"paperId":"736c66e1e6ddbf8311b000e9e6d5289edc185d8c","title":"Comparison of Transformer Models for Performance on Domain Specific Texts: A Systematic Evaluation of Intrinsic Model Performance"},{"paperId":"ddcf4ae15f6620c39dfa30b91403f00614e46fb5","title":"Self-Supervised Pretraining on Paired Sequences of fMRI Data for Transfer Learning to Brain Decoding Tasks"},{"paperId":"729bf8040870d411f9b526eae0861c10004eb746","title":"IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers and Task-adaptive Pretraining"},{"paperId":"ae1db9bf0930cc8506c2c74d3dfc72385675fa4e","title":"ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps"},{"paperId":"8d247cfc133e31047ce98c1758081b2115b62d8a","title":"Evaluation of transformer models for financial targeted sentiment analysis in Spanish"},{"paperId":"8489b55d992e6a3ac54aec7094a42ec8e333012f","title":"SELFormer: molecular representation learning via SELFIES language models"},{"paperId":"5861df95084cf739a6ca3185d6523dd702bd1f10","title":"Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT"},{"paperId":"cd1e691d26793874c244b84f6f1f91ae0a9a2029","title":"Reviewer Assignment Problem: A Systematic Review of the Literature"},{"paperId":"b92b8a32f89f45cd771359e3351f6ddcad61450c","title":"ChatGPT Participates in a Computer Science Exam"},{"paperId":"5e3ffdb21e897afdc4301249a45363c6efb538dc","title":"Transfer learning on large datasets for the accurate prediction of material properties"},{"paperId":"a0d0002be16a1b70dcbb4b152813da55a81c2201","title":"Diffusing Graph Attention"},{"paperId":"2bb67629796b8f53d9cc573beb1b65929d3492a7","title":"A transformer with layer-cross decoding for remaining useful life prediction"},{"paperId":"a171780f04780f1dca6965e6c451915b8ff5458f","title":"Mask-guided BERT for Few Shot Text Classification"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"83f8a272b54a4611ae0dca73bfe41741b18dfb34","title":"Exploring zero-shot and joint training cross-lingual strategies for aspect-based sentiment analysis based on contextualized multilingual language models"},{"paperId":"569e323c84bcc01fc6562ea2c7084c7a2a00e703","title":"Multi-modal Machine Learning in Engineering Design: A Review and Future Directions"},{"paperId":"8539284502ae94def71366f4df6031a0a3d6e949","title":"PSST! Prosodic Speech Segmentation with Transformers"},{"paperId":"3ea31f9b80bb69537f11f2c0e7d39c97d0742e3b","title":"On the Connection Between MPNN and Graph Transformer"},{"paperId":"99c951469b1f79ffd8f44e0da5a3301d6ec10455","title":"Identifying COVID-19 english informative tweets using limited labelled data"},{"paperId":"7389b6ebbf36f4d869a02e305e2ef52ad2c92264","title":"Applications of transformer-based language models in bioinformatics: a survey"},{"paperId":"a7c244e906b8bd09c027ea5f996746b23953d15d","title":"Automated Line Labelling: Dataset for Contour Detection and 3D Reconstruction"},{"paperId":"df49486e50abdfa0ca8ecada65bc36fe747be776","title":"Correlation, path and principal component analysis of few agronomical traits in few elite lines of chickpea (Cicer arietinum L.)"},{"paperId":"3f77a071feb8118fa851265381c9d5d4db132ca9","title":"COVID-19 Fake News Detection With Pre-trained Transformer Models"},{"paperId":"78a0c1b28d3c7543ae5d25aa9080a48807ec9264","title":"CoVEffect: interactive system for mining the effects of SARS-CoV-2 mutations and variants based on deep learning"},{"paperId":"a3129f5e4d6505376f8f2661db137853a582a819","title":"Understanding Postpartum Parents' Experiences via Two Digital Platforms"},{"paperId":"9c2b7c67a4bf516aeda0b960edd4aeb281c6c053","title":"Rate Insight: A Comparative Study on Different Machine Learning and Deep Learning Approaches for Product Review Rating Prediction in Bengali Language"},{"paperId":"61c7e5590064b250ebb5a53c430aede492a4d8ab","title":"BERT for Natural Language Processing in Bahasa Indonesia"},{"paperId":"49ab76e76b0943862d6f3c6b99d78cc1296f91d2","title":"HeartBEiT: Vision Transformer for Electrocardiogram Data Improves Diagnostic Performance at Low Sample Sizes"},{"paperId":"3c861f509f9da5133648a3e8f2b891b7f7638491","title":"MaNLP@SMM4H’22: BERT for Classification of Twitter Posts"},{"paperId":"dad15404d372a23b4b3bf9a63b3124693df3c85e","title":"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers"},{"paperId":"f1f78bbb3e146874501e3c52b56cff6abf731842","title":"Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges"},{"paperId":"b9fa8a915a9f0cf3e91c1659e19ba057ac78a498","title":"PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations"},{"paperId":"13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","title":"MarianCG: a code generation transformer model inspired by machine translation"},{"paperId":"8458052b99daeb81d95716100e916bf785313b0b","title":"Research on the Classification of Policy Instruments Based on BERT Model"},{"paperId":"3a4e97478ad1b113a7f51668f05d5ba85e500f5a","title":"CPSAA: Accelerating Sparse Attention using Crossbar-based Processing-In-Memory Architecture"},{"paperId":"8971d2a43a11b4beca31afadf88d5ed8cda75297","title":"Block Format Error Bounds and Optimal Block Size Selection"},{"paperId":"f5669510806e6a671cece0920b7593adafdae7d8","title":"JoeyS2T: Minimalistic Speech-to-Text Modeling with JoeyNMT"},{"paperId":"0f3d7b2bb0b3bd3bad87a39c545d93ddfd383362","title":"Selective Token Generation for Few-shot Natural Language Generation"},{"paperId":"0fba6742990324dd98d92d147f649bb3a8023102","title":"Method of Transformation of Image Classification Labels into Segmentation Masks"},{"paperId":"247a218762eef55655f992285225718fdd8a8082","title":"Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines"},{"paperId":"2a672342035defd8d75b54e08597ef124c6a0172","title":"Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models"},{"paperId":"5b71f03167bea77cde1ad15cf95979f2d08f9e45","title":"TransDBC: Transformer for Multivariate Time-Series based Driver Behavior Classification"},{"paperId":"74671b4f898daf2b8234d5a4cdf2f25b81f1e4fd","title":"TIMS: A Novel Approach for Incrementally Few-Shot Text Instance Selection via Model Similarity"},{"paperId":"0ab111fce85d37389b93ea5ca8caca14c755c3cf","title":"Attention-based Dependability Prediction for Industrial Wireless Communication Systems"},{"paperId":"c27c68597065dad70cb0b4f882cd861826bf28fa","title":"Automated Construction Contract Summarization Using Natural Language Processing and Deep Learning"},{"paperId":"1429d4b6f6c19d6447d60d8bed05841c5de72b38","title":"FluSa-Tweet: A Benchmark Dataset for Influenza Detection in Saudi Arabia"},{"paperId":"0fb235cc59cd7198b5a1494157b0250cfca04386","title":"Detecting Harmful Online Conversational Content towards LGBTQIA+ Individuals"},{"paperId":"e86869d44e78d4cffd1bf1b62f2f8e56a519e23c","title":"E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation"},{"paperId":"277dd73bfeb5c46513ce305136b0e71fcd2a311c","title":"Recipe for a General, Powerful, Scalable Graph Transformer"},{"paperId":"52dccf13f442e7451f4b81db203b24e056142557","title":"TransGrasp: A Multi-Scale Hierarchical Point Transformer for 7-DoF Grasp Detection"},{"paperId":"24fbe3af030f393e55bda3dc7dae0f57bd270d04","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Understanding"},{"paperId":"5bfff0955b511ce4ecb906c67cbe323b60b5c6d3","title":"Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias"},{"paperId":"2002883eecd8f8e0c094c357defa5dcc40b081d9","title":"UMass PCL at SemEval-2022 Task 4: Pre-trained Language Model Ensembles for Detecting Patronizing and Condescending Language"},{"paperId":"7a25155364476839b6d1fc0653cd8611327ab9ba","title":"mGPT: Few-Shot Learners Go Multilingual"},{"paperId":"77c1eed5c13928d39c5b62ba6063b47b10ab6636","title":"Spatial transformer network on skeleton‐based gait recognition"},{"paperId":"94d05045e9bf69e015f5398086ac5c27a70d13e6","title":"A Comparative Evaluation Of Transformer Models For De-Identification Of Clinical Text Data"},{"paperId":"43fee02f5e63dcc85c58d74966951e89b2331489","title":"Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking"},{"paperId":"627be995900638aef82279a22013a7b03b5d732d","title":"A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification"},{"paperId":"b6adc972f4bdbbb4dc7143713d16a3408a71ef7e","title":"Automated Customer Complaint Processing for Water Utilities Based on Natural Language Processing—Case Study of a Dutch Water Utility"},{"paperId":"551dc8a18770a5807c2ea0724701c3f946bc7c0c","title":"Hindi/Bengali Sentiment Analysis Using Transfer Learning and Joint Dual Input Learning with Self Attention"},{"paperId":"7e5ca499cd9b932921bda84db98f75087d0b0683","title":"Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"},{"paperId":"3e906906d475c73b6d8ce24ac5ebdac9979fd01b","title":"Video Transformers: A Survey"},{"paperId":"0c181f508ec9de8e48f62523ba8a9bcb1f51f83a","title":"Pre-trained Language Models for Text Generation: A Survey"},{"paperId":"e475d7c3b10d548e59590902474ff99206a732f3","title":"The Dark Side of the Language: Pre-trained Transformers in the DarkNet"},{"paperId":"489af1ae6db21c198b6efeffc3ba68313f9bd4b3","title":"Neuromorphic Camera Denoising Using Graph Neural Network-Driven Transformers"},{"paperId":"6fb5dc674bf0013ad5e269b3905c0f4253c3890b","title":"Investigating Cross-Linguistic Gender Bias in Hindi-English Across Domains"},{"paperId":"1b6798695de27880009346c6c2023665139b0014","title":"Ontology-Based Question Answering over Corporate Structured Data"},{"paperId":"7c59a4e11eed57aea54d27bbb28a3e4dbc26b0f3","title":"Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling"},{"paperId":"d725c41b8e0516d0cf84d8fbd25eb7fc01a47342","title":"A Primer on Pretrained Multilingual Language Models"},{"paperId":"420c897bc67e6f438db522d919d925df1a10aa8c","title":"AMMU: A survey of transformer-based biomedical pretrained language models"},{"paperId":"2055e2d803e7854477bb186d28e1a13f890f08f4","title":"TBMF Framework: A Transformer-Based Multilevel Filtering Framework for PD Detection"},{"paperId":"63e98be31bd0d5c73f233a0b1d5b6f997596290c","title":"A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?"},{"paperId":"53b142bace4fbc7ac6bfbff540c31ce97ea98448","title":"A Character Based Steganography Using Masked Language Modeling"},{"paperId":"a81ed24cbdbd77fbe399e0556cae841728a90c51","title":"Advancing Fake News Detection: Hybrid Deep Learning With FastText and Explainable AI"},{"paperId":"392d139040f9656b72374553dbe0d9ae70ee4eb0","title":"Effective Exploitation of Macroeconomic Indicators for Stock Direction Classification Using the Multimodal Fusion Transformer"},{"paperId":"5a9002e2092a0c832cb03aa18a631f4cc85d5b43","title":"Adjusting Dropout in Contrastive Learning of Sentence Embeddings"},{"paperId":"0567a26ccacdbe977ffc1160da1ebe746dc19228","title":"Chick Adams at SemEval-2023 Task 5: Using RoBERTa and DeBERTa to Extract Post and Document-based Features for Clickbait Spoiling"},{"paperId":"3d67937d4d4013a9951f73c56b42a9ffe1d2aceb","title":"Optimizing the Performance of Text Classification Models by Improving the Isotropy of the Embeddings Using a Joint Loss Function"},{"paperId":"15b0060ba45c5da5d74efb8aa2bccc8e0db21a15","title":"Transformers for Multi-Intent Classification and Slot Filling of Supreme Court Decisions Related to Sexual Violence Law"},{"paperId":"ce29a77010e47ed62e2e10c5654ab335e062ae22","title":"Improving Media Bias Detection with state-of-the-art Transformers"},{"paperId":"7f46a645911a57ea34522739def8bfc358afa96c","title":"Understanding Customer Requirements: an Enterprise Knowledge Graph Approach"},{"paperId":"01404a50a3c0c065da3391c60dda49a0cab36251","title":"A Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques"},{"paperId":"e0a4053a8ba5883d6a3f06dae43e32bfde610c61","title":"Cross-lingual Strategies for Low-resource Language Modeling: A Study on Five Indic Dialects"},{"paperId":"049778820d158b87a326352eba41f3a8d6ed6403","title":"Interaction in Transformer for Change Detection in VHR Remote Sensing Images"},{"paperId":"e617b09412d11c27cf78db93a9de7dfca471199c","title":"Modeling Easiness for Training Transformers with Curriculum Learning"},{"paperId":"5f3f7ffe97e0de0b2c6b82412ce2d12e68daddaf","title":"Brain-Inspired Remote Sensing Foundation Models and Open Problems: A Comprehensive Survey"},{"paperId":"c72a138e25369997703aeb2db8f33fb08ef3cee2","title":"Enhancing Scholarly Understanding: A Comparison of Knowledge Injection Strategies in Large Language Models"},{"paperId":"2c62cbeafb8b97959987030bad9bc23205c622e2","title":"Deep Representation Learning: Fundamentals, Technologies, Applications, and Open Challenges"},{"paperId":"b42e3a759348f27cca2f918a6bd0b139a5312e44","title":"A Survey of Pretrained Language Models Based Text Generation"},{"paperId":"0ec91fd9c5c3ff9758d9cfddc2e5046eaa3c1ab0","title":"Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking"},{"paperId":"4e8e404a6fff91ac55f9c7476a041e2ba31c0454","title":"Know Better – A Clickbait Resolving Challenge"},{"paperId":"2227980ce08aebbf18a42d4abea42381062c4bd5","title":"трансформації класифікаційних міток зображення"},{"paperId":"71cc838d8a50a0d62cc9c679536f1f25b2ea6b7f","title":"A Survey on Generative Diffusion Model"},{"paperId":"e85c5fdc4c44d3e17c45c239adfad8d0942e2805","title":"Text-to-Text Extraction and Verbalization of Biomedical Event Graphs"},{"paperId":"cbf284fe85795eaeb94bfb3dc9e98276dcd33788","title":"Neural Architecture Search for Transformers: A Survey"},{"paperId":"3d3f8399d625238fddb366697acb73446129d65c","title":"Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing"},{"paperId":"9f158e69eb2ccf5dce78cd50f4be3cff99b25ca8","title":"NLG-Metricverse: An End-to-End Library for Evaluating Natural Language Generation"},{"paperId":null,"title":"Predicting Human Judgments of Relational Similarity: A Comparison of Computational Models Based on Vector Representations of Meaning"},{"paperId":"f3c6ffbb9ed061ebb17623c066b84c596f55b796","title":"міток маски"},{"paperId":"10007f2b4b6e7e269400bbfd7d3d4204051d0497","title":"Sentiment Analysis on Dravidian Code-Mixed YouTube Comments using Paraphrase XLM-RoBERTa Model"},{"paperId":"00f899ec60300ca972962e6014d63b4ea835ef38","title":"CMS Optimisation with Deep Learning Techniques"},{"paperId":"4a313233cc78ac4e2e2b12f81bb89f242461d961","title":"Enhancing Writing Skills of Chilean Adolescents: Assisted Story Creation with LLMs"},{"paperId":"529b911da40fa9df598cfbb589e84af5b7c18f0d","title":"Evaluation of Transfer Learning and Adaptability in Large Language Models with the GLUE Benchmark"}],"references":[{"paperId":"79926aa63d4daee6af06a8e9a7c2480b31cb7ed9","title":"Spanish Pre-trained BERT Model and Evaluation Data"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"ff2f48fe6438adcaf860aac0f41c584568beafb5","title":"CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision"},{"paperId":"2ee03e28208a9310a9be4032c2b04ebdddb83cc7","title":"FLEX: Unifying Evaluation for Few-Shot NLP"},{"paperId":"4237cbebe788a97174f48dc398082739bbffe95b","title":"FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark"},{"paperId":"4566c0d22ebf3c31180066ab23b6c445aeec78d5","title":"Deduplicating Training Data Makes Language Models Better"},{"paperId":"8b954e1654c6b759a957fd11e66c111e6105fb3f","title":"CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding"},{"paperId":"94b38a1cf2905a9f8dd90f5e22f904a07a59b6bc","title":"XLM-E: Cross-lingual Language Model Pre-training via ELECTRA"},{"paperId":"f07bcf49a92e09437359be788bbe3f9237c5ec40","title":"A Closer Look at How Fine-tuning Changes BERT"},{"paperId":"cf5e670a79847d9be0eb185fb372d99d30d4d98f","title":"Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains"},{"paperId":"114aa720872462b0ca1b97bfdec0ebd56c36fd0a","title":"Towards Understanding and Mitigating Social Biases in Language Models"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"a6a7724763d8adba466519489b0b9d209e7f2d15","title":"BARTScore: Evaluating Generated Text as Text Generation"},{"paperId":"00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d","title":"CPM-2: Large-scale Cost-effective Pre-trained Language Models"},{"paperId":"a93632237958800217341d7bad847200afdd60e3","title":"Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better"},{"paperId":"c6fd846b9b8f9eb0a492d6d6242fffce987c4580","title":"Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning"},{"paperId":"d8d2e574965fe733eb1416e03df2b5c2914fc530","title":"A Survey of Transformers"},{"paperId":"2384c92bbde47f5dbc8d8f175aa67e0f95c413d4","title":"FastSeq: Make Sequence Generation Faster"},{"paperId":"7509c66a666e2e3f14bc8676b969b945ee6e136f","title":"CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"},{"paperId":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"},{"paperId":"981995fd64611f475179b280f4e9c241051ac185","title":"Knowledge Inheritance for Pre-trained Language Models"},{"paperId":"077c713bccd9d2c7fde68d4cbde06ab0f07a6855","title":"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"},{"paperId":"0934952763f1549e75b142261e73f2b27a2f495b","title":"RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model"},{"paperId":"28459083ba624020c8f1c1ed7c3a075f48b4e709","title":"KLUE: Korean Language Understanding Evaluation"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"6563251e69e4378c189d0a0c94d8d19508d552c8","title":"MathBERT: A Pre-Trained Model for Mathematical Formula Understanding"},{"paperId":"c553280c1fc1d0bc7b94683bb75910e309b0d579","title":"Larger-Scale Transformers for Multilingual Masked Language Modeling"},{"paperId":"78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f","title":"PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"},{"paperId":"b4f30496a8fa212a40461ca1bdef32169e998902","title":"Efficient pre-training objectives for Transformers"},{"paperId":"279a19b9eba7afd513394c7a733834b0f41f97fb","title":"mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs"},{"paperId":"c5dd0c12353d683179fae1df079d5c5ae0e2cd23","title":"Dual-View Distilled BERT for Sentence Embedding"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"7b99c51d562e33309a46601c846abbe72a65c6a4","title":"What to Pre-Train on? Efficient Intermediate Task Selection"},{"paperId":"2435c04832d486975304a094e55ecbab8acf8a5f","title":"Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders"},{"paperId":"0e027f8206a4d04f0b50b88dfe31e7f2f46e2d60","title":"IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation"},{"paperId":"2b9762e91305986ac8a2d624d0a69521304405f3","title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"},{"paperId":"b6647280615f667dd7418bfb9b13d828a22c1cfe","title":"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning"},{"paperId":"1cf2e9e198feef3893da2800a7949f6880ddc084","title":"ExplainaBoard: An Explainable Leaderboard for NLP"},{"paperId":"3acedae6febb3d9567e896696f966c92ff406a17","title":"Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages"},{"paperId":"6fd41ee12ae828bb7de965bf7e06bb1bd5259fe7","title":"IndT5: A Text-to-Text Transformer for 10 Indigenous Languages"},{"paperId":"96afd5a4bcf814fc9db32b1fd0324c3dbb63ed61","title":"MuRIL: Multilingual Representations for Indian Languages"},{"paperId":"bc37c6bdb8f39929a58b30464f72d6aa46cddc17","title":"GPT Understands, Too"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"8994bce4b85a8b4087584661c49f8776f868f7dd","title":"Interpretable Bias Mitigation for Textual Data: Reducing Genderization in Patient Notes While Maintaining Classification Performance"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"beb6a2d33b1979d9d2010fc5721fd307e015c342","title":"Bidirectional Representation Learning From Transformers Using Multimodal Electronic Health Record Data to Predict Depression"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"cbd4f06a08eb4223b97c9079007a87dda4339afe","title":"Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models"},{"paperId":"824cd8db8a68732db04f4d8b7139eb4475e59ff2","title":"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"},{"paperId":"fcfc9648561a221750b8085790ad9ba1bebb1800","title":"Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models"},{"paperId":"86b369759bff13ec23b45ca23cc5461292a75415","title":"WangchanBERTa: Pretraining transformer-based Thai Language Models"},{"paperId":"0e8afb694ad0a84c5deafd4eedfb75e953776e65","title":"Cloud-based intelligent self-diagnosis and department recommendation service using Chinese medical BERT"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"9f9fe98b60d75c6407726efff8193e8bee3ee13b","title":"KoreALBERT: Pretraining a Lite BERT Model for Korean Language Understanding"},{"paperId":"3a906b77fa218adc171fecb28bb81c24c14dcc7b","title":"Transformers in Vision: A Survey"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"c342798bafc1eaaa60c652fc90fd738941542133","title":"AraGPT2: Pre-Trained Transformer for Arabic Language Generation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"aea9f18d70a78d39ca927f2baa143e084c486086","title":"AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding"},{"paperId":"1e4cda8be54999ced1324777fa462a85e2c9746c","title":"ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic"},{"paperId":"df7d26339adf4eb0c07160947b9d2973c24911ba","title":"Extracting Training Data from Large Language Models"},{"paperId":"21d347db8adc949b0ad08e9c42b66a63739c866f","title":"ParsiNLU: A Suite of Language Understanding Challenges for Persian"},{"paperId":"092442a694b811dff5b7715fba9e363e0ce4108c","title":"SentiX: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis"},{"paperId":"5eea6a9de39c41715e105f5943ac0fcb98fa245c","title":"Enhancing Clinical BERT Embedding using a Biomedical Knowledge Base"},{"paperId":"aa8f5faf8890b6846d4da9cb7e60a8df6e96ba4a","title":"KD-Lib: A PyTorch library for Knowledge Distillation, Pruning and Quantization"},{"paperId":"5fe78eb0f142902237df11cb67c455787a759172","title":"GLGE: A New General Language Generation Evaluation Benchmark"},{"paperId":"70b0c85638d195dbde56cbedc94ae4363b272b58","title":"A Pre-Training Technique to Localize Medical BERT and to Enhance Biomedical BERT"},{"paperId":"b70ee890b5e0ec7d6db04cfd5471b3bbbd0320fc","title":"Self-Supervised Learning from Contrastive Mixtures for Personalized Speech Enhancement"},{"paperId":"a79b520571f7373cbeb8c6ffc02f6a719b3bce38","title":"CODER: Knowledge-infused cross-lingual medical term embedding for term normalization"},{"paperId":"96c22a88ec3b9d3799daa41098555ab665c24ea8","title":"Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning"},{"paperId":"17aa716dae728e994a2539bf4952c05ad513bd7a","title":"CharBERT: Character-aware Pre-trained Language Model"},{"paperId":"1109d62ebd2b29a7dc148bc30dd6cfc803a63dec","title":"IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP"},{"paperId":"d7b93f247f9e46115d6b78a67a458c44de0f9039","title":"iNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages"},{"paperId":"28ba41305e8268592ec829600f5a3b54cd10fbca","title":"Learning from Unlabelled Data for Clinical Semantic Textual Similarity"},{"paperId":"70efbd71c840e78d0ecee101d389db0aaea93652","title":"exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources"},{"paperId":"210cf704dddaa922e4eafe634dbabf707d6683bc","title":"LIMIT-BERT : Linguistics Informed Multi-Task BERT"},{"paperId":"302bb91ed02b90896e0ee78a80f303672d9c3b1b","title":"RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark"},{"paperId":"b68b2e81ae2de647394ec05ee62ecf108bf2b50a","title":"Eliciting Knowledge from Language Models Using Automatically Generated Prompts"},{"paperId":"9927a15ddf5313d97c98f0111fd191caf507ce72","title":"HateBERT: Retraining BERT for Abusive Language Detection in English"},{"paperId":"9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2","title":"TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"615204452304331004532c5800399ef55d58b4c7","title":"Self-Alignment Pretraining for Biomedical Entity Representations"},{"paperId":"03935e520c612ac9f137d9e9ef388e0c08568b60","title":"UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus"},{"paperId":"5ad57623099f1fb6045a67fee313fee2573ef5ec","title":"A Benchmark for Lease Contract Review"},{"paperId":"1d95011355628f7aef068ab1914198e43258c530","title":"BERTimbau: Pretrained BERT Models for Brazilian Portuguese"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"708dcd8456426cd609c89a86344e0007c04c80bf","title":"X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models"},{"paperId":"6d6595766a35f12a6ad671d05634b5e2159d4f3e","title":"Bio-Megatron: Larger Biomedical Domain Language Model"},{"paperId":"0c775d7ed34fb4690b4291490778649ae75c48d2","title":"TurboTransformers: an efficient GPU serving system for transformer models"},{"paperId":"1c6f94fb3d888167355afb580f04d55cd517ebc6","title":"On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"},{"paperId":"3fbf6339273c50b04e886fa9bd4ad18c952a683d","title":"Rethinking Attention with Performers"},{"paperId":"2274b14cd3f513bee527a92f5859d14aea093aaa","title":"DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue"},{"paperId":"097210dc65924f8ce59523faf444e635523dc714","title":"TernaryBERT: Distillation-aware Ultra-low Bit BERT"},{"paperId":"574c7c7260a795d2906f03b5229ace3f54ee0ab3","title":"An Unsupervised Sentence Embedding Method by Mutual Information Maximization"},{"paperId":"8b0a0f6d1cd6f3aa9b54be45d5127bb016a98171","title":"The birth of Romanian BERT"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"f30444fbb6ad806168e2564db4815cd27faa7fd9","title":"It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"},{"paperId":"03f22e693a0c00bae8a66a64a2fecb0f11a4b034","title":"IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding"},{"paperId":"2b01b3334ce950c76c9c3c2c9146a7f0ce79cc50","title":"Conceptualized Representation Learning for Chinese Biomedical Text Mining"},{"paperId":"5b2ec7a534cab750c6b00fe491500681ae3b1527","title":"PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data"},{"paperId":"9baab08fbe37369856688b2abe5b3c90cce1682c","title":"Compression of Deep Learning Models for Text: A Survey"},{"paperId":"2a218786f4615b82389f78472e7ff22e6ce57490","title":"ConvBERT: Improving BERT with Span-based Dynamic Convolution"},{"paperId":"a2f38d03fd363e920494ad65a5f0ad8bd18cd60b","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"16cca900a850eee1b832937281bb08c84beb86ff","title":"Identification of Semantically Similar Sentences in Clinical Notes: Iterative Intermediate Training Using Multi-Task Learning"},{"paperId":"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3","title":"Big Bird: Transformers for Longer Sequences"},{"paperId":"7c462df4adefcf90af3c27ce7f7a8d83efbff2b0","title":"Measurement of Semantic Textual Similarity in Clinical Texts: Comparison of Transformer-Based Models"},{"paperId":"09bfe057c9285577242636950c6835b8731a07fb","title":"Multi-task learning for natural language processing in the 2020s: where are we going?"},{"paperId":"4ceff7472c04ee6d76bce89d61ba4b445d8dbf74","title":"InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training"},{"paperId":"725264948d7b6946259af5b8d966e996b9570f99","title":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"},{"paperId":"214a0eede75f546b631d6d28871bd9028a66fc46","title":"Playing with Words at the National Library of Sweden - Making a Swedish BERT"},{"paperId":"c0091585ae4f9cccd4c1dba5aa7409c0886553fa","title":"Transferability of Natural Language Inference to Biomedical Question Answering"},{"paperId":"6150a2dab1b63b246eb2cd418fcdb5a6b6b8ae62","title":"exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models"},{"paperId":"1882f194cb43828852cc052887671e55a80f945a","title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"},{"paperId":"49a049dc85e2380dde80501a984878341dd8efdf","title":"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"},{"paperId":"706f756b71f0bf51fc78d98f52c358b1a3aeef8e","title":"Self-Supervised Learning: Generative or Contrastive"},{"paperId":"3578a7792904e6af3db8ffefdff86ab6a387c7c3","title":"FinBERT: A Pretrained Language Model for Financial Communications"},{"paperId":"8b9d77d5e52a70af37451d3db3d32781b83ea054","title":"On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines"},{"paperId":"82453548b97f78ab2cdb9a8626ff858db9ce5a82","title":"Pre-training Polish Transformer-based Language Models at Scale"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"90abbc2cf38462b954ae1b772fac9532e2ccd8b0","title":"Language Models are Few-Shot Learners"},{"paperId":"be158d5ab493b2f2dae736a2ca92afcd66ed5be4","title":"ParsBERT: Transformer-based Model for Persian Language Understanding"},{"paperId":"5d4de0fa45aeddc31142e6a24666d06ed7923f1e","title":"Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction"},{"paperId":"72cdd6ebe0221fb568ef20534f44ba5b35190a56","title":"BERTweet: A pre-trained language model for English Tweets"},{"paperId":"126fb7df6bcab2b70000dfe5b940ada63ae1ba6a","title":"COVID-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter"},{"paperId":"1b0c8b26affd13e10ace5770e85478d60dcc368e","title":"GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"},{"paperId":"02809fc23aecf33e3ed95b83d1d03b54fb5c3d0a","title":"An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining"},{"paperId":"f527bcef68aeda601aae314fe5c75185c716e579","title":"KLEJ: Comprehensive Benchmark for Polish Language Understanding"},{"paperId":"8ae8e5e840c5f43d4d72cbc4595690fba01aa799","title":"LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation"},{"paperId":"c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c","title":"Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset"},{"paperId":"673e970fd835c7dd1bb1e071c5a37e9df99b7c8e","title":"Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?"},{"paperId":"11c226588d4d6a255967fe9c1cee92b081b405cd","title":"Low Resource Multi-Task Sequence Tagging - Revisiting Dynamic Conditional Random Fields"},{"paperId":"00cd2650a89734105fa0c0aba3bf07935b318290","title":"GLUECoS: An Evaluation Benchmark for Code-Switched NLP"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"64b4d2c2181e3fb4ecacf797bf5f35db203c1437","title":"MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask Learning"},{"paperId":"d27669c82faf78ea08cceaa0a171b540cccc304d","title":"ETC: Encoding Long and Structured Inputs in Transformers"},{"paperId":"0171ad4cc87cc7db25b4ec3169e293eed9a13b39","title":"Training with Quantization Noise for Extreme Model Compression"},{"paperId":"5b015296730273921889e54a0a31e3b173017026","title":"TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue"},{"paperId":"1c8aea2bfb61f4661b6907018a5a8bca390900dd","title":"PALM: Pre-training an Autoencoding&autoregressive Language Model for Context-conditioned Generation"},{"paperId":"18318b10e7c2dd4ad292208f4399eb1d4dca5768","title":"CLUE: A Chinese Language Understanding Evaluation Benchmark"},{"paperId":"925ad2897d1b5decbea320d07e99afa9110e09b2","title":"Longformer: The Long-Document Transformer"},{"paperId":"2b9955bc08fc5f4ddba73082ddabcfaabdbb4416","title":"Poor Man's BERT: Smaller and Faster Transformer Models"},{"paperId":"3b504f939e55d567652737ef093c1087cd40689b","title":"Analyzing Redundancy in Pretrained Transformer Models"},{"paperId":"25a49187e0d1e3ebebda71c7e77f31bc49358044","title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA"},{"paperId":"2573af4e13d9a5dddb257d22cd38a600528d9a8b","title":"MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"},{"paperId":"10467a1466aeec246ac0a577bfc311ec4de110de","title":"Alternating Language Modeling for Cross-Lingual Pre-Training"},{"paperId":"297ad41c0e7264e67ae078921e2a57436293ce72","title":"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation"},{"paperId":"dbfbfcc2633ef46c53e2343525ee87c700f2cfc3","title":"Extending Multilingual BERT to Low-Resource Languages"},{"paperId":"b2fd96a52ded7a64f60c1e54f5bb488c787629c0","title":"What Happens To BERT Embeddings During Fine-tuning?"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","title":"Pre-trained models for natural language processing: A survey"},{"paperId":"e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286","title":"A Survey on Contextual Embeddings"},{"paperId":"0dde065405210ebc399c58ab6b7e843a18caad51","title":"CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model"},{"paperId":"a622332550eaf535cf0f0f6c3a3f3ba197c39cac","title":"PhoBERT: Pre-trained language models for Vietnamese"},{"paperId":"501a8b86428563539667e8117cd8409674ef97c3","title":"TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing"},{"paperId":"738215a396f6eee1709c6b521a6199769f0ce674","title":"Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"60a4a3a886338d0c8e3579d392cb32f493430255","title":"MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"d9b824dbecbe3a1f0b1489f9e4521a532a63818d","title":"Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"},{"paperId":"baf60d13c98916b77b09bc525ede1cd610ed1db5","title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"},{"paperId":"1359d2ef45f1550941e22bf046026c89f6edf315","title":"AraBERT: Transformer-based Model for Arabic Language Understanding"},{"paperId":"81ed0e757ae2d66a43d73407ad6f7e0359adf6d7","title":"PMIndia - A Collection of Parallel Corpora of Languages of India"},{"paperId":"dc5da5ac3aff86e4b0156c52d9641d05dc1eeace","title":"MT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b","title":"RobBERT: a Dutch RoBERTa-based Language Model"},{"paperId":"e344ae0473651d04bf322849f1c71a5edc75f887","title":"Modified Bidirectional Encoder Representations From Transformers Extractive Summarization Model for Hospital Information Systems Based on Character-Level Tokens (AlphaBERT): Development and Performance Evaluation"},{"paperId":"055fd6a9f7293269f1b22c1470e63bd02d8d9500","title":"Reformer: The Efficient Transformer"},{"paperId":"a4d5e425cac0bf84c86c0c9f720b6339d6288ffa","title":"BERTje: A Dutch BERT Model"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"477d66dcd2c08243dcc69822d6da7ec06393773a","title":"Multilingual is not enough: BERT for Finnish"},{"paperId":"069e0d896da7c79faeee4cf057548d5da7ce885e","title":"FlauBERT: Unsupervised Language Model Pre-training for French"},{"paperId":"04690b6e72d1d236d91196b93fe64a48f4666a52","title":"Automated Brain Image Classification Based on VGG-16 and Transfer Learning"},{"paperId":"a75649771901a4881b44c0ceafa469fcc6e6f968","title":"How Can We Know What Language Models Know?"},{"paperId":"b61c6405f4de381758e8b52a20313554d68a9d85","title":"CamemBERT: a Tasty French Language Model"},{"paperId":"46b8201f1b84950f141cbbb5eeccaa1437159ff4","title":"A Massive Collection of Cross-Lingual Web-Document Pairs"},{"paperId":"2bd5b4aed18400bf1a1cc866d9b8d931aa047290","title":"E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT"},{"paperId":"68c1bf884f0fc0e86641466a1f1fa67e79f16a17","title":"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76","title":"Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"},{"paperId":"4099c4d272c12081b562392606e6d567e4ae7031","title":"Masked Language Model Scoring"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"ce106590145e89ea4b621c99665862967ccf5dac","title":"Q8BERT: Quantized 8Bit BERT"},{"paperId":"4e561318668f0ae190217ffe82bf44c9c33b9c0d","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"12dc43176fd607557d6cc8d46af8b8d77c121b47","title":"Domain-Relevant Embeddings for Medical Question Similarity"},{"paperId":"a54b56af24bb4873ed0163b77df63b92bd018ddc","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"},{"paperId":"7a064df1aeada7e69e5173f7d4c8606f4470365b","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"},{"paperId":"f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1","title":"Reducing Transformer Depth on Demand with Structured Dropout"},{"paperId":"222b9a7b8038120671a1610e857d3edbc7ac5550","title":"Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models"},{"paperId":"aa2e8b6eecaed5c4af018c03abe5eb8adcabaf47","title":"Cross-Lingual Natural Language Generation via Pre-Training"},{"paperId":"0cbf97173391b0430140117027edcaf1a37968c7","title":"TinyBERT: Distilling BERT for Natural Language Understanding"},{"paperId":"1a00229c25dcc740fd0388ac1e98c42eaa52912e","title":"Pre-trained Language Model for Biomedical Question Answering"},{"paperId":"4fb8fd55b476909a26a8dc594e0ae98d4923ad4d","title":"Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"},{"paperId":"bfeb827d06c1a3583b5cc6d25241203a81f6af09","title":"Knowledge Enhanced Contextual Word Representations"},{"paperId":"e14c93e69cbf9645abb70cef09391f21f644d6d8","title":"Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity"},{"paperId":"65f788fb964901e3f1149a0a53317535ca85ed7d","title":"Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"7102bb3fe73bd057ff161d9db5214a267c1ef312","title":"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"},{"paperId":"80cf2a6af4200ecfca1c18fc89de16148f1cd4bf","title":"Patient Knowledge Distillation for BERT Model Compression"},{"paperId":"d78aed1dac6656affa4a04cbf225ced11a83d103","title":"Revealing the Dark Secrets of BERT"},{"paperId":"3caf34532597683c980134579b156cd0d7db2f40","title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"772717eb2e369cd68c11b7da7aa779450dced9d0","title":"SenseBERT: Driving Some Sense into BERT"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"d56c1fc337fb07ec004dc846f80582c327af717c","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding"},{"paperId":"8aa1d9145640b4a63258b82bc8180c3683d072b5","title":"KU_ai at MEDIQA 2019: Domain-specific Pre-training and Transfer Learning for Medical NLI"},{"paperId":"1cd8167b2a6be4fdc0f2bfeec4e6f23a9bbb7090","title":"Tuning Multilingual Transformers for Language-Specific Named Entity Recognition"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"81f5810fbbab9b7203b9556f4ce3c741875407bc","title":"SpanBERT: Improving Pre-training by Representing and Predicting Spans"},{"paperId":"57633ff5c6f0708be25e651f51eef29d2fbfe48b","title":"BEHRT: Transformer for Electronic Health Records"},{"paperId":"92343cecdc990380de362b969eec60081959f507","title":"Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"},{"paperId":"f48f90464d9694e2ea18767f14842c64c9a1e8fb","title":"WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"347bac45298f37cd83c3e79d99b826dc65a70c46","title":"Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"3b3f47170ec5c4fabac510585b33aeb87b384396","title":"Variational Pretraining for Semi-supervised Text Classification"},{"paperId":"ad7129af0644dbcafa9aa2f111cb76526ea444a1","title":"Defending Against Neural Fake News"},{"paperId":"4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9","title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"},{"paperId":"07a64686ce8e43ac475a8d820a8a9f1d87989583","title":"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"cc94d15ba408c260c8fe4fa4f1cb6797a996dd21","title":"Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"145b8b5d99a2beba6029418ca043585b90138d12","title":"MASS: Masked Sequence to Sequence Pre-training for Language Generation"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"b03c7ff961822183bab66b2e594415e585d3fd09","title":"Are Sixteen Heads Really Better than One?"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"a9c3a009d754a110379574b069b48c0b4c75db40","title":"Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking"},{"paperId":"e61a3a5ba2b93458774f2ccbe480f3cf6cd74fa1","title":"Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?"},{"paperId":"2a567ebd78939d0861d788f0fedff8d40ae62bf2","title":"Publicly Available Clinical BERT Embeddings"},{"paperId":"bc789aef715498e79a74f857fa090ece9e383bf1","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"658721bc13b0fa97366d38c05a96bf0a9f4bb0ac","title":"Multi-Task Deep Neural Networks for Natural Language Understanding"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"06a1bf4a7333bbc78dbd7470666b33bd9e26882b","title":"Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling"},{"paperId":"b47381e04739ea3f392ba6c8faaf64105493c196","title":"Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"ac4dafdef1d2b685b7f28a11837414573d39ff4e","title":"Universal Transformers"},{"paperId":"11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25","title":"T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"},{"paperId":"8c207ece66e0a63627869c49fb37c6811072539b","title":"The brWaC Corpus: A New Open Resource for Brazilian Portuguese"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"25c0e5ad89b77ae9c7ff91765ebd4e1e21abdcb3","title":"The IIT Bombay English-Hindi Parallel Corpus"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"786f95cada23d4639aa1a8b922cdb9fb9a9c03fa","title":"Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b","title":"Recurrent Neural Network for Text Classification with Multi-Task Learning"},{"paperId":"95cd83603a0d2b6918a8e34a5637a8f382da96f5","title":"MIMIC-III, a freely accessible critical care database"},{"paperId":"800366078f063a637e6a4880c0c49c217c7905ea","title":"The United Nations Parallel Corpus v1.0"},{"paperId":"36f652172792f8aab1cf3c4441a72a1bf79d17c8","title":"Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering"},{"paperId":"9a501e501a60b431b6031f81dc2c19b390b0aff3","title":"Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"23ffaa0fe06eae05817f527a47ac3291077f9e58","title":"Rethinking the Inception Architecture for Computer Vision"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"424561d8585ff8ebce7d5d07de8dbf7aae5e7270","title":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","title":"Distilling the Knowledge in a Neural Network"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"27725a2d2a8cee9bf9fffc6c2167017103aba0fa","title":"A Convolutional Neural Network for Modelling Sentences"},{"paperId":"d770060812fb646b3846a7d398a3066145b5e3c8","title":"Do Deep Nets Really Need to be Deep?"},{"paperId":"032d67d27ecacbf6c5b82eb67e5d02d81fb43a7a","title":"A Survey on Multi-view Learning"},{"paperId":"f6b51c8753a871dc94ff32152c00c01e94f90f09","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"1b97b4623cf2f183340e548e0aa53abf0f2963d8","title":"Representing General Relational Knowledge in ConceptNet 5"},{"paperId":"a25fbcbbae1e8f79c4360d26aa11a3abf1a11972","title":"A Survey on Transfer Learning"},{"paperId":"0d2336389dff3031910bd21dd1c44d1b4cd51725","title":"Why Does Unsupervised Pre-training Help Deep Learning?"},{"paperId":"30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9","title":"Model compression"},{"paperId":"45a23651bcc5a6cc993d722e71b0d301a6dc9dee","title":"Open Mind Common Sense: Knowledge Acquisition from the General Public"},{"paperId":"a9693b7b57f203940889de6d3f979c70c09202ed","title":"Shuffled-token Detection for Refining Pre-trained RoBERTa"},{"paperId":"50068fbea4d1cafcf4c99873ab272c701c08dfcb","title":"OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language Models"},{"paperId":"76db3624f1278a4f4f1e69b343bfff7bba306d47","title":"XLM-T: A Multilingual Language Model Toolkit for Twitter"},{"paperId":"acf2dd4e2853f90832c01c556a2e716e7c720bc2","title":"G ENIE A Leaderboard for Human-in-the-Loop Evaluation of Text Generation"},{"paperId":"1554887c6bd76c443a477b27dbcab35877787b27","title":"LightSeq: A High Performance Inference Library for Transformers"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"2ada5bea5e567313fdac9541725fac0cdc49bc36","title":"BanglaBERT: Combating Embedding Barrier for Low-Resource Language Understanding"},{"paperId":"b9478e237b58160c65acd2c41894493d27e2c277","title":"WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"},{"paperId":"cbd78779af4e83fe101ba3f7ba4d4786388d12d8","title":"Semantic Re-tuning with Contrastive Tension"},{"paperId":"5322e5936e4a46195b1a92001467a2350fe72782","title":"KART: Privacy Leakage Framework of Language Models Pre-trained with Clinical Records"},{"paperId":"2af4e8d14d03cd9031ae4a6b1ef39fce2ab3f504","title":"NetBERT: A Pre-trained Language Representation Model for Computer Networking"},{"paperId":null,"title":"“Legal-bert: The muppets straight out of law school,”"},{"paperId":"888c3a3788c52d6637d45dc4238691083884589d","title":"Investigating Learning Dynamics of BERT Fine-Tuning"},{"paperId":"17d5884215b5afa53545cd7cb6135de5478da4ec","title":"CERT: Contrastive Self-supervised Learning for Language Understanding"},{"paperId":"5c5751d45e298cea054f32b392c12c61027d2fe7","title":"S2ORC: The Semantic Scholar Open Research Corpus"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"Electra: Pretraining text encoders as discriminators rather than generators"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"2a0870bc2ecfd17dfb1b96cc34613bb73bb4506a","title":"BLACK BOX ATTACKS ON TRANSFORMER LANGUAGE MODELS"},{"paperId":"e1e43d6bdb1419e08af833cf4899a460f70da26c","title":"AlBERTo: Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets"},{"paperId":null,"title":"“Bertviz: A tool for visualizing multihead self-attention in the bert model,”"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"“Wordnet,”"},{"paperId":"1f1eaf19e38b541eec8a02f099e3090536a4c936","title":"The Unified Medical Language System (UMLS): integrating biomedical terminology"},{"paperId":null,"title":"Bert and pals : Projected attention layers for efficient adaptation in multitask learning"},{"paperId":null,"title":"Sentilare: Sentimentaware language representation learning with linguistic knowledge"},{"paperId":null,"title":"“Umlsbert: Clinical domain knowledge augmentation of con-38"}],"id":"6c761cfdb031701072582e434d8f64d436255da6","summary":"This comprehensive survey paper explains various core concepts like pretraining, Pretraining methods, pretraining tasks, embeddings and downstream adaptation methods, presents a new taxonomy of T-PTLMs and gives brief overview of various benchmarks including both intrinsic and extrinsic."},{"url":"https://www.semanticscholar.org/paper/1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","title":"ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models","venue":"International Conference on Topology, Algebra and Categories in Logic","year":2021,"referenceCount":68,"citationCount":293,"influentialCitationCount":29,"publicationDate":"28/05/2021","authors":"Linting Xue,Aditya Barua,Noah Constant,Rami Al-Rfou,Sharan Narang,Mihir Kale,Adam Roberts,Colin Raffel","citations":[{"paperId":"e27c9c39ecb67e8e7708d8d53bec2f891cfa7a40","title":"Training LLMs over Neurally Compressed Text"},{"paperId":"cd77f734f0c54bccbea1b75c9458cbde121a38dc","title":"CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models"},{"paperId":"5b838e3be86f9f86828f9892aca4f937ec882da8","title":"CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech"},{"paperId":"f914e6e4cd65a4f04a78d4f6af2fd8aa093b79cd","title":"An Analysis of BPE Vocabulary Trimming in Neural Machine Translation"},{"paperId":"0169ee108d2cf9e388d9c6c3773402b84e95ab7b","title":"Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens"},{"paperId":"8f8e46b7d454406ec35358f525ce9ce17ff3f2b0","title":"LeanReasoner: Boosting Complex Logical Reasoning with Lean"},{"paperId":"01574fb3abdf10867850b0970bc47b178027ee6b","title":"Wav2Gloss: Generating Interlinear Glossed Text from Speech"},{"paperId":"c20942cedd92dcc4e6270f85af780464da655c4c","title":"Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models"},{"paperId":"8efaa8206874c2f7a79bba2a9bcba542e4cabf31","title":"MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling"},{"paperId":"9b13450581dc2a04366ad95fb60e30cd69453be2","title":"Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering"},{"paperId":"cef7c8825b59713324b62110a626c3b0589150a5","title":"Embedded Translations for Low-resource Automated Glossing"},{"paperId":"55c15c1bc0660fd6a532d4ad9a1175064166991e","title":"GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing"},{"paperId":"02a694bc78a8178f2409387b709039849f36bd83","title":"Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance"},{"paperId":"93a71c09e8ea389750c7e3aa6ec5577a8ba227f7","title":"LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition"},{"paperId":"4d31aae7d8555279fda45a0fc00e6aca2904772b","title":"A Bit of a Problem: Measurement Disparities in Dataset Sizes Across Languages"},{"paperId":"99ed8dd210fb84de7417424adf8f10ae463d2b99","title":"Advancing Generative AI for Portuguese with Open Decoder Gervásio PT"},{"paperId":"25cee6fe99fc51b60760c31de4b28195edca1b6c","title":"Beyond Language Models: Byte Models are Digital World Simulators"},{"paperId":"7861d04b41c9848905cb73268040d10e23409c77","title":"Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning"},{"paperId":"441d9c239ba5fee0be1ac122330052c7b6bf822e","title":"An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference"},{"paperId":"9dee70e17a06fa067cf408d1a4a862f2ec77baa3","title":"Knowledge of Pretrained Language Models on Surface Information of Tokens"},{"paperId":"f7f2d9e56aed2aef09a11e98fab14940af726c97","title":"Pixel Sentence Representation Learning"},{"paperId":"a640215755e23e2649f4b3d3246a47b14fea93f7","title":"Getting the most out of your tokenizer for pre-training and domain adaptation"},{"paperId":"a6e2dca754f3dc625a9da5f10f9b7a57079bfd27","title":"MambaByte: Token-free Selective State Space Model"},{"paperId":"de643836277d108e1f8410ea85b0c3ed0e686163","title":"MaLA-500: Massive Language Adaptation of Large Language Models"},{"paperId":"16daf858baef5ff2a698596bf26999a618fa521e","title":"Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion"},{"paperId":"96c88b196e3e432710debab39f49ee72f2b96a10","title":"Anisotropy Is Inherent to Self-Attention in Transformers"},{"paperId":"ece1c04e9dcf89cbc6170fcccdec8412d11503d8","title":"A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation"},{"paperId":"9b782ec0e038287227c8ff28f1bbcae548cdd721","title":"Pheme: Efficient and Conversational Speech Generation"},{"paperId":"0eef63fe71efba4404a9a883bf89ec634dbaad82","title":"Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method"},{"paperId":"7845058565eb71e3e423ec72641abe9d7ba2ed0a","title":"UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models"},{"paperId":"7354a57261d27a281e56dc428b6ec146b9992afd","title":"Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text"},{"paperId":"1c6e2a4da1ead685a95c079751bf4d7a727d8180","title":"TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering"},{"paperId":"e58147980d42f845879c4231840c2857d82337eb","title":"MELA: Multilingual Evaluation of Linguistic Acceptability"},{"paperId":"c4ae033f14a89db8b6b6b4da598375f177a1fac4","title":"Memory Augmented Language Models through Mixture of Word Experts"},{"paperId":"cba0b336e7a2001d891d8d29275542367cd52925","title":"Lexical Normalization Using Generative Transformer Model (LN-GTM)"},{"paperId":"5f3d6c077712a2d692bb29671c084e098854c738","title":"Learning Mutually Informed Representations for Characters and Subwords"},{"paperId":"c0e62e324dfc7bd45ec0a5ce4055823d1f0c03f1","title":"Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval"},{"paperId":"ab2cd01090bebd7c52193c432e0a0fce4226982e","title":"Generating Pragmatic Examples to Train Neural Program Synthesizers"},{"paperId":"4d51e88131a7f1acc6e74a3d750f18bad527dcd7","title":"Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision"},{"paperId":"9cb33adc68daa6f27d8a362532e6c54205e0e22b","title":"Too Much Information: Keeping Training Simple for BabyLMs"},{"paperId":"fd665a3d8c4a5fde1e1b00f78ab231d99b22abd0","title":"Explicit Morphological Knowledge Improves Pre-training of Language Models for Hebrew"},{"paperId":"d921732ca5920049b6a8194f559917689f02d096","title":"Text Rendering Strategies for Pixel Language Models"},{"paperId":"d9655aad891e0dba404bc132c8419f93c098c8f8","title":"Learning to Abstract with Nonparametric Variational Information Bottleneck"},{"paperId":"0a550ee375df53c845f46084de4f0e7f16170d9a","title":"NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task"},{"paperId":"17d0cc24fd6a267e7fd59ae62054de3e5c552096","title":"Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"61644c4b76a11501d3b9b85fcfed513c56f184a7","title":"Enhancing Neural Machine Translation with Semantic Units"},{"paperId":"a401510c434b2274b299e9444085df0b18808aaa","title":"Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"},{"paperId":"3448e136046d22d4f90d7a4c875890f5fb64b811","title":"PuoBERTa: Training and evaluation of a curated language model for Setswana"},{"paperId":"997cebb936d88577da59ba460a9141bdd5dcb36f","title":"To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer"},{"paperId":"64384525318f934fa085de86badc24403487d38a","title":"Tokenizer Choice For LLM Training: Negligible or Crucial?"},{"paperId":"c30deb8f4b0e942a36e5ef5c87bed01f575c0c6e","title":"Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention"},{"paperId":"2e9f3a96d6f6011ff1b4eab541ef1df12cde042f","title":"Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation"},{"paperId":"b4eb0ca9f019254f7d1ec6432a095e526fa3abc2","title":"Large Synthetic Data from the arXiv for OCR Post Correction of Historic Scientific Articles"},{"paperId":"10af858834ad69f1c30721e6aa732d77fa369161","title":"Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models"},{"paperId":"376f0d8bc222a33fb382933552e796bb21e6191b","title":"Abbreviation Disambiguation in Polish Press News Using Encoder-Decoder Models"},{"paperId":"e892a225c417fbac7545c3e31b45d1c42dc9c933","title":"Chain-of-Thought Reasoning is a Policy Improvement Operator"},{"paperId":"ff50383c09346e3e7d16856af3519fa09a9f8580","title":"Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning"},{"paperId":"119639d87c7ce418c57070a41505dc7ac9eb7e1d","title":"Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap"},{"paperId":"3fa61652b1a4f314590e5e8bfe8f023bbc5c3568","title":"Ngambay-French Neural Machine Translation (sba-Fr)"},{"paperId":"dc0436318a08f4df8f9653f164a830f245caca8b","title":"GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering"},{"paperId":"064af570792c61f5cc46814c648fd3969f0999e7","title":"Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction"},{"paperId":"1943144f998c2620616b3e89d376ff801b6e04c6","title":"Lightweight Adaptation of Neural Language Models via Subspace Embedding"},{"paperId":"ce5ea17bffd6333e414422d680827fef2b0ce41a","title":"Correcting Wide-Range of Arabic Spelling Mistakes Using Machine Learning and Transformers"},{"paperId":"8ce219059d777c2333ee21cb2af2aad71275c98f","title":"LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"},{"paperId":"d57cbec10dfee4e244657d3cdf9ba4cf53086744","title":"An Empirical Analysis Towards Replacing Vocabulary-Rigid Embeddings by a Vocabulary-Free Mechanism"},{"paperId":"d5241be77ff0aec9a2ac1e1b14fb69df897eb8f3","title":"Bi-Phone: Modeling Inter Language Phonetic Influences in Text"},{"paperId":"d7890d1906d95c4ae4c430b350455156d6d8aed9","title":"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis"},{"paperId":"e83c3f46e036df811d50aca56e53199f79f1a415","title":"Estimating Post-OCR Denoising Complexity on Numerical Texts"},{"paperId":"714671a26060d289a5e12888a92e904a19034982","title":"MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems"},{"paperId":"2adc13eb55c92e026c4cefc89a47a0ee0ac95111","title":"SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding"},{"paperId":"d5d936b142eb81826002888230649c805532630e","title":"VisText: A Benchmark for Semantically Rich Chart Captioning"},{"paperId":"87875a07976c26f82705de1fc70041169e5d652b","title":"LeanDojo: Theorem Proving with Retrieval-Augmented Language Models"},{"paperId":"4e12e831bc1627545479c65d7b2296d6d2562c9a","title":"A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing"},{"paperId":"5cd4fe8174b582f27d79859e4b79e0eba2a1d5f0","title":"Is Anisotropy Inherent to Transformers?"},{"paperId":"4bd35d344c635b05f97f4d749741d196ff541bf3","title":"A Primer on Seq2Seq Models for Generative Chatbots"},{"paperId":"a903e1e0ffd04dd666f3537f6570d742d7be3486","title":"Grounded Text-to-Image Synthesis with Attention Refocusing"},{"paperId":"02dde8e9fefafae8e0b9ee2eef0ddc74c511f7cd","title":"Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models"},{"paperId":"29a54c072bcf85fa6934b6f701962a7c9aeb6489","title":"Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora"},{"paperId":"5fbe4c92791fbecb179c1ab79bba9a59b2e155ba","title":"GlyphControl: Glyph Conditional Control for Visual Text Generation"},{"paperId":"b4ee74a1f294cb5bb028a958ce43219cabb0f347","title":"TranSFormer: Slow-Fast Transformer for Machine Translation"},{"paperId":"ac36b65a6fa4a9e84de051f0d3e9d50348fa4160","title":"Lexinvariant Language Models"},{"paperId":"36dfcdc43664f03b15e5e03373a9d46728672e28","title":"mPLM-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models"},{"paperId":"b3cff6abe401a244c21d4706b0931e48acaeeb4e","title":"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models"},{"paperId":"d1e7b054578ae0606effe63ab8402861ec54a49e","title":"Sāmayik: A Benchmark and Dataset for English-Sanskrit Translation"},{"paperId":"2cfb1f44b34204213d789731871e599c756bdb83","title":"Exploring Large Language Models for Classical Philology"},{"paperId":"d7a379254ac2dee2e31dfdeedb440176f0285aa5","title":"From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding"},{"paperId":"c60736e61f8961ec535ecfdc6f0398925d34d0b8","title":"Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer"},{"paperId":"7c217cc7524251f42887438834912e06129c3299","title":"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis"},{"paperId":"39bca01efce8765f0a5d3a8981bc30d56f196b96","title":"XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages"},{"paperId":"879a7f5abdb7ab803d48172d4f0830965f989d46","title":"Language Model Tokenizers Introduce Unfairness Between Languages"},{"paperId":"2183c88e9056e931b07d48f1dc44360785952073","title":"SoundStorm: Efficient Parallel Audio Generation"},{"paperId":"ea8197cb357af6f89e8b6e5548897236a24719b1","title":"What is the best recipe for character-level encoder-only modelling?"},{"paperId":"b4790ca1a967b44c3028b73c0c00d501fcd81728","title":"Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages"},{"paperId":"af6c6941acecfb23d899cd5266efdf2e27463f12","title":"Causal Language Model Aided Sequential Decoding With Natural Redundancy"},{"paperId":"37d8bc1c268773dc69c92c6f550393c93e9a1f7a","title":"Automatic document classification via transformers for regulations compliance management in large utility companies"},{"paperId":"e0d09d91784a6d426ffcd2fd12448dd9c8ceefe9","title":"Does Manipulating Tokenization Aid Cross-Lingual Transfer? A Study on POS Tagging for Non-Standardized Languages"},{"paperId":"ba184d335a9a08c52c5d25eabd7f4a8ea987918b","title":"UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining"},{"paperId":"e0c2a2ba6054b18f570a169790b015d333928789","title":"Romanization-based Large-scale Adaptation of Multilingual Language Models"},{"paperId":"2b8d28149a43b9659a6da2c56014ec4206a912b4","title":"Ticket automation: An insight into current research with applications to multi-level classification scenarios"},{"paperId":"10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7","title":"GlyphDraw: Seamlessly Rendering Text with Intricate Spatial Structures in Text-to-Image Generation"},{"paperId":"3add55c068fe19bb2e5392cbe994602a91630ec1","title":"Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams"},{"paperId":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!"},{"paperId":"eef5b8f3c4e60d596a04101d8261c222ab739861","title":"Fine-Tashkeel: Fine-Tuning Byte-Level Models for Accurate Arabic Text Diacritization"},{"paperId":"0a438980ac42451d6d32dd2ad8ead7b55520408d","title":"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning"},{"paperId":"4e97303aeb299ee736b1b8c29cef046212690354","title":"Generation-based Code Review Automation: How Far Are Weƒ"},{"paperId":"0dfc7eecc5f12b8613152f8c62f9ccfadc4f92b7","title":"DTT: An Example-Driven Tabular Transformer for Joinability by Leveraging Large Language Models"},{"paperId":"2c5460afa19ad6fc2568b7e210115acacc14a40c","title":"An Overview on Language Models: Recent Developments and Outlook"},{"paperId":"f3d894cf6f7be14a545019f4621ccce41f45b088","title":"Learning the Legibility of Visual Text Perturbations"},{"paperId":"b169f2ce55e14584d2db6f64eeb5ad2702d39d40","title":"Artificial intelligence foundation and pre-trained models: Fundamentals, applications, opportunities, and social impacts"},{"paperId":"2c8935ec872eca14636a090e5f6b49bc1c90c30d","title":"Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation"},{"paperId":"96b005d1d92211cf053e4114a85a5e64e428d896","title":"Extending English IR methods to multi-lingual IR"},{"paperId":"67c28d697460b684a0ba97d989719b4ed3c9cffc","title":"Elementwise Language Representation"},{"paperId":"1e4b6567ff66de6cdcbe58c863538241e2f62b45","title":"Aligning Text-to-Image Models using Human Feedback"},{"paperId":"a794a92e1fa516250390514eeeb3c3b3140876a3","title":"RetVec: Resilient and Efficient Text Vectorizer"},{"paperId":"0704a96e1c57c12031f1c3ca492a91dbed1f61ce","title":"Distillation of encoder-decoder transformers for sequence labelling"},{"paperId":"62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c","title":"XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models"},{"paperId":"20c7d73609ba98fa27b1edc7b537ef59442e4ba2","title":"Truveta Mapper: A Zero-shot Ontology Alignment Framework"},{"paperId":"8969ea3d254e149aebcfd1ffc8f46910d7cb160e","title":"Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models"},{"paperId":"7cdebb73662387d9040da4f27a7dc04dbffa3c3e","title":"ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"},{"paperId":"f8ef90a8cac1a8edf9477688aac24864c547d6cd","title":"Character-Aware Models Improve Visual Text Rendering"},{"paperId":"205cc15fca6963b355e4c071071368e874ee103e","title":"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training"},{"paperId":"2f272ca3394656835bf32a30b80c6f8ba1c8f4c4","title":"Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks"},{"paperId":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only"},{"paperId":"4aa09cba27a489ca02471fad011ea4854fc63cc1","title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue"},{"paperId":"7c1c95f5fb7fce563171fcc0060c850390753b3c","title":"Advancing Multilingual Pre-training: TRIP Triangular Document-level Pre-training for Multilingual Language Models"},{"paperId":"bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"},{"paperId":"be2df0fafa89b6355d1ff1336c10e0d4c8d27276","title":"Massively Multilingual Natural Language Understanding 2022 (MMNLU-22) Workshop and Competition"},{"paperId":"3af872a4bd314db9e7964876fee811823628f83f","title":"Text Normalization on Code-Mixed Twitter Text using Language Detection"},{"paperId":"ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6","title":"Subword-Delimited Downsampling for Better Character-Level Translation"},{"paperId":"e541fb54b8b9f2b4d8253f678a28830cd4f52d86","title":"A Survey of Text Representation Methods and Their Genealogy"},{"paperId":"9f4351600c72d5dac0251cd49984f691dca2fcd1","title":"Word-Level Representation From Bytes For Language Modeling"},{"paperId":"5e52d654fd31f04c1bd884cd5480e6af8c95ad50","title":"Efficient Transformers with Dynamic Token Pooling"},{"paperId":"0783c214623c18f6a8ad96b8eaf4a67a382e68ee","title":"QAmeleon: Multilingual QA with Only 5 Examples"},{"paperId":"af277778904463965c60626e22783d3e1740058b","title":"Introducing Semantics into Speech Encoders"},{"paperId":"d50b9db750ded246bde13e2c263e341bcbd8a335","title":"A Benchmark and Dataset for Post-OCR text correction in Sanskrit"},{"paperId":"d7e0f0cec28c34710fa631df410b717186741db5","title":"A Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces"},{"paperId":"9a5b2dc77bda19759df8481aaf283da353ac7e77","title":"Local Structure Matters Most in Most Languages"},{"paperId":"92302ab168429c7c3a8f699b35ba8302916c6e7c","title":"Bridging Speech and Textual Pre-Trained Models With Unsupervised ASR"},{"paperId":"5e786d47e3dc5ffa65ba8548ff67139a2764acf8","title":"T5lephone: Bridging Speech and Text Self-Supervised Models for Spoken Language Understanding Via Phoneme Level T5"},{"paperId":"d2b1405ac8d17f8643ed1f8f7d801e9e406d8da6","title":"SLING: Sino Linguistic Evaluation of Large Language Models"},{"paperId":"6eaa0aa5cc9d3eaa9f578a07fcaa1878cfd21cbc","title":"Graphemic Normalization of the Perso-Arabic Script"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"12d2a697f1ee6a63859efe6b288ddb5eb6bd97c1","title":"HashFormers: Towards Vocabulary-independent Pre-trained Transformers"},{"paperId":"70dd68c07b322b68836eded1fb4f78c0efcad685","title":"A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models"},{"paperId":"a0568d0bf4cbfcb5a17a20c576dfc421c4ccfb7c","title":"Incorporating Context into Subword Vocabularies"},{"paperId":"3e08d84af266026e7d254729f3afc6dcd572d264","title":"One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks"},{"paperId":"539b6862f7e6ce9f98043f00a4ccdb4a9ff8de72","title":"Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic Reasoning"},{"paperId":"cd9fdf84f1cbb482e3952ca63de70d03f9030644","title":"Robustification of Multilingual Language Models to Real-world Noise in Crosslingual Zero-shot Settings with Robust Contrastive Pretraining"},{"paperId":"aa94724d1dbc9cad0ad3377903174e776175837a","title":"Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic N-Gram Rule Generation for Spelling Normalization in Filipino"},{"paperId":"2a8aac78df5e1e9658a02d381662ed26c6577713","title":"Disease diagnostic method based on cascade backbone network for apple leaf disease classification"},{"paperId":"bd6f44bab6dbb5c93854f67f95f82274adc7d2d0","title":"MonoByte: A Pool of Monolingual Byte-level Language Models"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"134e4d72e23bca51e290db171d063989883020f4","title":"Layer or Representation Space: What Makes BERT-based Evaluation Metrics Robust?"},{"paperId":"e4437293a703fcf282bf1b38bf7900ed8ca711bb","title":"Training a T5 Using Lab-sized Resources"},{"paperId":"a806041621acb5cf648fc780f1ff14939aa3a721","title":"How Much Does Lookahead Matter for Disambiguation? Partial Arabic Diacritization Case Study"},{"paperId":"062cfd5cbafa47950a5ebbd306cd0059dfd79f75","title":"CLOWER: A Pre-trained Language Model with Contrastive Learning over Word and Character Representations"},{"paperId":"54123c8de8ecacb30ae2e9fb8c4635f4030965b5","title":"MockingBERT: A Method for Retroactively Adding Resilience to NLP Models"},{"paperId":"00d9a73c54053a32e2aba92b53fc3e6bc71d3238","title":"Sequence-to-sequence pretraining for a less-resourced Slovenian language"},{"paperId":"23f4b6432b74e5db05da04e354341807f5044f7e","title":"Language Modelling with Pixels"},{"paperId":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","title":"Confident Adaptive Language Modeling"},{"paperId":"b308511ac52e1c2da915d1e55d5246dfdb4cbe28","title":"Romanian Question Answering Using Transformer Based Neural Networks"},{"paperId":"a766ef0678aade6a9798552618819cf4d0fac406","title":"TEVR: Improving Speech Recognition by Token Entropy Variance Reduction"},{"paperId":"bbeb9e5d4d51194d8e557ef7a6d927bdd62e9309","title":"Patching Leaks in the Charformer for Efficient Character-Level Generation"},{"paperId":"0232715f9089e3a2fc002cff6737bb9939805b8d","title":"Local Byte Fusion for Neural Machine Translation"},{"paperId":"32afdb07021fda775ceaedd231c58bfed0aa980a","title":"Automated Crossword Solving"},{"paperId":"c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6","title":"Lifting the Curse of Multilinguality by Pre-training Modular Transformers"},{"paperId":"b21670e8061a06ab97e7d6052c9345a326e84ff8","title":"UL2: Unifying Language Learning Paradigms"},{"paperId":"063d9fa4861356500219b7e81d5a654aa921da6f","title":"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation"},{"paperId":"7267812178393b8ae0b99648f02661ca1ff2b412","title":"Bilingual End-to-End ASR with Byte-Level Subwords"},{"paperId":"17e2977b907aad2532c45185947539e83ac639cd","title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?"},{"paperId":"0de580957d23dd65e31b6c95e6bc5d15bc15c57d","title":"Impact of Tokenization on Language Models: An Analysis for Turkish"},{"paperId":"cb16b85891172572cd856142880b503db0c2bc61","title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment"},{"paperId":"0b9e130c6305de7766697ba7655f56010aaffd61","title":"A Hierarchical N-Gram Framework for Zero-Shot Link Prediction"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"880c8973ea1376c6bff23226b3f64792657aa666","title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion"},{"paperId":"943487997ecd26e871a2ab16160bd5640020369d","title":"Toward Best Practices for Training Multilingual Dense Retrieval Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"1ed66e048bb025e75aa5ea660545285212e5341f","title":"Scaling Up Models and Data with t5x and seqio"},{"paperId":"b1dad820853464b30c93b52366b32690ba4b99a6","title":"A Brief Overview of Universal Sentence Representation Methods: A Linguistic View"},{"paperId":"a747e8f2659df479c0092301b9658fc582423df1","title":"One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia"},{"paperId":"8a4eec70e3d0c43abf26757252ad6210c9717f6c","title":"Towards Lithuanian grammatical error correction"},{"paperId":"19e2f3a1e0c3b8340c14cb83e9ca6878a2598852","title":"IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation"},{"paperId":"7016eb4f34611f97fe8c99176246e314678e03f4","title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"},{"paperId":"8f2bca9d684005675e294b33c26481e36f528cdb","title":"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"},{"paperId":"f357b35e068bbfbb8468b48198ab63241713629d","title":"Correcting diacritics and typos with ByT5 transformer model"},{"paperId":"7e3081b0d698f8abf16dee626d782f3339482fe7","title":"An Assessment of the Impact of OCR Noise on Language Models"},{"paperId":"e53f455b3300d95738dac117419c88fbde58ac4e","title":"Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics"},{"paperId":"d617f51833860dc50d202af7f80be71304b2e994","title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"},{"paperId":"0ae82f96cb8c1d2da5d8284765dd76a139ba6ff6","title":"PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers"},{"paperId":"9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e","title":"Large Dual Encoders Are Generalizable Retrievers"},{"paperId":"6e2682eb2fbec93b329028b23764c1164e232c41","title":"ÚFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5"},{"paperId":"f3bed81c50e293c03f6b650cdb9351d14e9be347","title":"Deciphering “the language of nature”: A transformer-based language model for deleterious mutations in proteins"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"66d735987a31d666a6459566ae026c40ab9a1c3a","title":"The Efficiency Misnomer"},{"paperId":"11ccb9b509d84e845901ee097e7d0a6419fdc182","title":"EncT5: A Framework for Fine-tuning T5 as Non-autoregressive Models"},{"paperId":"f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0","title":"Why don’t people use character-level machine translation?"},{"paperId":"e35357ac461a669fe7e4b877ee1fad0dfda26303","title":"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings"},{"paperId":"c107835a05ca6fda6e73b64e2ed9884de4fcec0f","title":"A proposed conceptual framework for a representational approach to information retrieval"},{"paperId":"a70fc86508bd0133d5d984a4e777abef1934d76c","title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese"},{"paperId":"6e0f8a47072de7e549182a5d0fc07c6d0a207325","title":"Predicting Ethnicity from Names with rethnicity: Methodology and Application"},{"paperId":"39262814fa3e47905a2e5facf13465a1f70706b9","title":"Single-Read Reconstruction for DNA Data Storage Using Transformers"},{"paperId":"d87647784c12517d31964cc508d5b8423cc24f50","title":"Integrating Approaches to Word Representation"},{"paperId":"972808b92159a782f5ca1c1ab3a8ed3867acfb2d","title":"mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset"},{"paperId":"89a4f4d1f8c93da85d829a1acfe8cafea2b50c00","title":"Transfer Learning for Multi-lingual Tasks - a Survey"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"c41819413725668fbf8e3ca1a0bcaf7fb691e582","title":"How Optimal is Greedy Decoding for Extractive Question Answering?"},{"paperId":"9c2e4e5ee224c20a45c37244924138b50f3fe603","title":"Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information"},{"paperId":"9933a5af7895354087baf6c96b64dc8a8973eaed","title":"Perceiver IO: A General Architecture for Structured Inputs & Outputs"},{"paperId":"91c3f9ed60a383f97a8c38f1e6a28a0e9be232fa","title":"Local Structure Matters Most: Perturbation Study in NLU"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"06431546c21d7c2528aaa170c2e1078e0a82d12e","title":"Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer"},{"paperId":"31852f9fc732c0868af12d631c72693702d80521","title":"Text Data Augmentation for Deep Learning"},{"paperId":"e79d1206292bc5e67ba19737d87d4b2ea4a37105","title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"},{"paperId":"4d6f9558ccfd712e2ef0921a7cb6625698c3cb0c","title":"Evaluating Various Tokenizers for Arabic Text Classification"},{"paperId":"5dee4ece8ec1725a7d071b74bf60f4f5fd2e78ad","title":"Sub-Character Tokenization for Chinese Pretrained Language Models"},{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"7072db6eddb85ecd2c117365d91bd694760f726e","title":"Position Information in Transformers: An Overview"},{"paperId":"0f16ba2afde6db0a71ea93f733ff393590eeb471","title":"MaintNorm: A corpus and benchmark model for lexical normalisation and masking of industrial maintenance short text"},{"paperId":"13be1e6782439e599ce3416c3bac29b13ec58ca4","title":"Post-OCR Correction of Digitized Swedish Newspapers with ByT5"},{"paperId":"15f0a97a0857ed8ac8df201bc9c1fe92d0bfd1e2","title":"Embible: Reconstruction of Ancient Hebrew and Aramaic Texts Using Transformers"},{"paperId":"0a152bd2b474e1ece1e2f0ea5fd27651cbcab77c","title":"Investigating the Potential of Task Arithmetic for Cross-Lingual Transfer"},{"paperId":"dbf84a9e3c75f6790f4ba8f25ac5c70282a6e427","title":"Where are we Still Split on Tokenization?"},{"paperId":"164575bf818b53d125c7eaf3b77f6ca62abb76e1","title":"Exploring Data Augmentation in Neural DRS-to-Text Generation"},{"paperId":"ed7b51e4a5c4835218f6697b280afb2849211939","title":"Are Character-level Translations Worth the Wait? An Extensive Comparison of Character- and Subword-level Models for Machine Translation"},{"paperId":"117a0c2c23a5a558cf3b39468d917a750bca720c","title":"Are Character-level Translations Worth the Wait? Comparing Character-and Subword-level Models for Machine Translation"},{"paperId":"8cc3c64e1aee320609b6b964a9a6f6f50177e20f","title":"Publish or Hold? Automatic Comment Moderation in Luxembourgish News Articles"},{"paperId":"2efefcb2d48eecf308f856cab48f23621e4a88f1","title":"Scaling Neural ITN for Numbers and Temporal Expressions in Tamil: Findings for an Agglutinative Low-resource Language"},{"paperId":"a92b837f2fd0ba023d5a927fde41d83b73191cf5","title":"Good Meta-tasks Make A Better Cross-lingual Meta-transfer Learning for Low-resource Languages"},{"paperId":"cfe7cb66390ea0b433383d498e6eff555a198c54","title":"Murreviikko - A Dialectologically Annotated and Normalized Dataset of Finnish Tweets"},{"paperId":"d53d4ff9f3bda51534184344845a78f8c02badd9","title":"DiatopIt: A Corpus of Social Media Posts for the Study of Diatopic Language Variation in Italy"},{"paperId":"5088cd22bbd7e1b798df39eb7f3c4ae305ab7625","title":"Findings from the Bambara - French Machine Translation Competition (BFMT 2023)"},{"paperId":"380605105531d27474190451183bf6ad6126cac8","title":"GPoeT: a Language Model Trained for Rhyme Generation on Synthetic Data"},{"paperId":"82e1313f28afde442930b94bc6ed582d17e8d4b3","title":"Generating Errors: OCR Post-Processing for Icelandic"},{"paperId":"13b8060acc3db1fc555f6e55368f6d02899a1698","title":"FairPrism: Evaluating Fairness-Related Harms in Text Generation"},{"paperId":"1d578d2bdb5c920b224cfca73868566731eaeebd","title":"Towards Analysis of Biblical Entities and Names using Deep Learning"},{"paperId":"035315281c72763a3e0956775732e64f5f193d82","title":"Natural Language Generation with Pixels"},{"paperId":"e5adb9bf3f5ed9c253f38949b22e86775dca443a","title":"Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning"},{"paperId":"c3904ef47bec4fc2bfd3d390370681c33542d62d","title":"Fast Whitespace Correction with Encoder-Only Transformers"},{"paperId":"ffa4ac4a51148208cadb4084dddab954e5f57400","title":"Resolving Elliptical Compounds in German Medical Text"},{"paperId":"88108f061379045c299d62f487694cb4e6d6d4ff","title":"Findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing"},{"paperId":"659be1ff350634f50cc066d258ee6a45e697e552","title":"SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing"},{"paperId":"4044c062683533d92f598715afe2508be29c739c","title":"The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations"},{"paperId":"8e410a836174847e3fc4e5993f625e89e34dd3d1","title":"RST Discourse Parsing as Text-to-Text Generation"},{"paperId":"17d2c990dd6d25f433b02ce611ce0a57db038dc5","title":"Jetsons at the FinNLP-2023: Using Synthetic Data and Transfer Learning for Multilingual ESG Issue Classification"},{"paperId":"f83740ef449e705187a0f7d6f76b819c99340bd1","title":"Exploring the Limits of Small Language Models"},{"paperId":"e4a1e9bb360f29aceb56079f52484c4a4de1298d","title":"Intégration du raisonnement numérique dans les modèles de langue : État de l’art et direction de recherche"},{"paperId":"29e593736c95fd043b7cbb0211d2f7b5cb70e5bf","title":"LLI-UAM Team at FinancES 2023: Noise, Data Augmentation and Hallucinations"},{"paperId":"2b4369d50ac7310b9908a2baef89a63c68cbd893","title":"Bridging the Gap between Subword and Character Segmentation in Pretrained Language Models"},{"paperId":"30869d285642a8b981702d2a54be0ac54f01aa01","title":"Composer's Assistant: Interactive Transformers for Multi-Track MIDI Infilling"},{"paperId":"ee26bb846f04240fe6e82170a4b15f3b8bd3a09e","title":"Designing Human-Centric Foundation Models"},{"paperId":"c6410d9791f390aa43ed26bf5567fdde6fc89f25","title":"Type Enhanced BERT for Correcting NER Errors"},{"paperId":"ce093816af5d8c1a7aab74808245048b7f3669a5","title":"The Helsinki-NLP Submissions at NADI 2023 Shared Task: Walking the Baseline"},{"paperId":"104974b36654acbc613226b9f70c4ad503655aa1","title":"The Linearity of the Effect of Surprisal on Reading Times across Languages"},{"paperId":"8a930572177545e7394ba5cd03e9342142da564e","title":"Better Quality Pre-training Data and T5 Models for African Languages"},{"paperId":"7ef91d8896c406865e91d90f60a8592a03ed6ded","title":"TRIP: Accelerating Document-level Multilingual Pre-training via Triangular Document-level Pre-training on Parallel Data Triplets"},{"paperId":"cbeb7f5050790d5abc8c617f6bfd70706c52ae0e","title":"Automatic Translation of Span-Prediction Datasets"},{"paperId":"64668e46d52302851d62afbc9779ce8c4d46c5eb","title":"Dialect-to-Standard Normalization: A Large-Scale Multilingual Evaluation"},{"paperId":"5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9","title":"On the Influence of Tokenizers in NLP Pipelines"},{"paperId":"9758782feed4b4b9bf0ec18b802462e8023a7f83","title":"AfriTeVA: Extending ?Small Data? Pretraining Approaches to Sequence-to-Sequence Models"},{"paperId":"b639124771f9c62cd656a24e8e685a456918e0ff","title":"Character-Level Encoding with S4 for QA"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"b54edea6cac055d8ff9e35c2781f5e000ebdff89","title":"Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data"},{"paperId":"32290d428b50f64d1cfee6ea658dc6ba977b26e9","title":"Sammaan@LT-EDI-ACL2022: Ensembled Transformers Against Homophobia and Transphobia"},{"paperId":"2f07f97563a73d9b691ec6144e4bba25a347ab87","title":"An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"},{"paperId":"e75037b638aca1dfc8a9b013bb7dcb8d19633986","title":"Examining Single Sentence Label Leakage in Natural Language Inference Datasets"},{"paperId":"b1786a81fe804da6f2aeab0f4a8a0f60a3c4ad83","title":"A Deep Transfer Learning Method for Cross-Lingual Natural Language Inference"},{"paperId":"2005311276a1a90dc17cf6e2ca7725fee2e76e1e","title":"BasqueGLUE: A Natural Language Understanding Benchmark for Basque"},{"paperId":"143659fcf93f33e9139f594cf8111c6bc85c04fa","title":"Overview of the EvaLatin 2022 Evaluation Campaign"},{"paperId":"6102fe88a512290b80e83ed2fe17606b166e505a","title":"Transformer-based Part-of-Speech Tagging and Lemmatization for Latin"},{"paperId":"7c496490abcd8d5c6e90aa3d3c82bde02680f5b0","title":"MutFormer: A context-dependent transformer-based model to predict deleterious missense mutations from protein sequences in the human genome"},{"paperId":"373588ff1fb9f7590db000a04de8d838b1516e5a","title":"On the Effectiveness of Quasi Character-Level Models for Machine Translation"},{"paperId":"4da5c8acd25a7dc7357d6070bb0a89bad187ebd8","title":"A Framework for Sexism Detection on Social Media via ByT5 and TabNet"},{"paperId":"038103632b24619818b159f5ca37b848744817db","title":"DENTRA: Denoising and Translation Pre-training for Multilingual Machine Translation"},{"paperId":"710ceab1ff91b96e8596b8400ebf2912ee6e2836","title":"Machine Translation for Multilingual Intent Detection and Slots Filling"},{"paperId":"3fa2e17332bb2888318f504cf37026001b932900","title":"Comparison of Token- and Character-Level Approaches to Restoration of Spaces, Punctuation, and Capitalization in Various Languages"},{"paperId":"c31da40e092e808f20f782eb1ee9d4dec6351708","title":"Overview of EXIST 2022: sEXism Identification in Social neTworks"},{"paperId":"2b6b38b66fcca218cb2f381e5d4711b5c99a784f","title":"Training Text-to-Text Transformers with Privacy Guarantees"},{"paperId":"5a6f6f44a2e05709d81245526786f8dc8f8ab263","title":"Relation Leakage in Elicited Natural Language Inference Datasets"},{"paperId":"de0e1f9980afa7949df64d53b8ae7a2f59c55579","title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages"},{"paperId":"7175caf7568d46c857380d0e5b64653819d5cc45","title":"MultiLexNorm: A Shared Task on Multilingual Lexical Normalization"},{"paperId":"921c1216edbf6b2931b15874f24847ff1007ad8c","title":"EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks"},{"paperId":"e5f6506f9332fcdb574f13a791e4f3c42b80ca90","title":"Demystifying Neural Language Models' Insensitivity to Word-Order"},{"paperId":"e873ddaf58ff92ae0492983cf57221fb25837f4e","title":"Strategies in subword tokenization: humans vs. algorithms"},{"paperId":"b8146800f8fd63c29e5f00b1e1b441da55f28c05","title":"Sentence-Level Discourse Parsing as Text-to-Text Generation"},{"paperId":"ae7fc32df9401a86353185515f4fd5e2020ac922","title":"MutFormer: A context-dependent transformer-based model to predict pathogenic missense mutations"},{"paperId":"8496c62bb8ecc67dfb0959ef9321f875d5ff067b","title":"Phone-ing it in: Towards Flexible, Multi-Modal Language Model Training using Phonetic Representations of Data"},{"paperId":"103dc8d8572eadafa984e4b4c15855f9cdc754fb","title":"Few-shot Controllable Style Transfer for Low-Resource Multilinugal Settings"},{"paperId":"4c70347a3b9b5a27f4964c8703ae22d015bb49c1","title":"Machine Reading Comprehension: Generative or Extractive Reader?"},{"paperId":"f31a11257aad37847893b5495024865ca5f41ef9","title":"End-to-end style-conditioned poetry generation: What does it take to learn from examples alone?"},{"paperId":"28f68458f8c73da7ba1493e1a2c50862580fe0b4","title":"EncT5: Fine-tuning T5 Encoder for Discriminative Tasks"},{"paperId":"3c55d5e88c301237ecfb362cae5adcd87a1bc4af","title":"VU Research Portal Analyzing Cognitive Plausibility of Subword Tokenization"},{"paperId":"afbcc8ba3959f55dd5cc3b4d34c4238cc1622382","title":"Diacritization for the World’s Scripts"},{"paperId":"f8665a1a5dcf4c771c146edc67c353f007355911","title":"Leveraging Pre-training Models for Speech Processing"}],"references":[{"paperId":"d13a0c8d49cb268d8d245925baee0316c1fe1875","title":"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"},{"paperId":"969287b8a96e242793b11f0dbb99ec341228106f","title":"Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"},{"paperId":"2b66a0d8a00b0ba7b585e11ab34879420ad351cb","title":"Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution"},{"paperId":"824cd8db8a68732db04f4d8b7139eb4475e59ff2","title":"The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"},{"paperId":"03a0781af10c80ecd93bc0b0e7a3b99375c729b9","title":"Training Multilingual Pre-trained Language Model with Byte-level Subwords"},{"paperId":"fdacf2a732f55befdc410ea927091cad3b791f13","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"},{"paperId":"09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc","title":"Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"473921de1b52f98f34f37afd507e57366ff7d1ca","title":"CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"},{"paperId":"387d59877a641815d79516a7264ff5d20384c596","title":"Question Directed Graph Attention Network for Numerical Reasoning over Text"},{"paperId":"7e5709d81558d3ef4265de29ea75931afeb1f2dd","title":"Efficient Transformers: A Survey"},{"paperId":"41efaa96494c0ae19db23700826e4b35d401c8d8","title":"Neural Machine Translation without Embeddings"},{"paperId":"e46c97b2d3bea4c21eb462d5771647a0de99b81c","title":"Ensemble Self-Training for Low-Resource Languages: Grapheme-to-Phoneme Conversion and Morphological Inflection"},{"paperId":"278f4f68309fc4ffb83bfc4ba86a0ea005106682","title":"The SIGMORPHON 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion"},{"paperId":"8c122ff82dedf72fa7ef5342622667bf5848916e","title":"One-Size-Fits-All Multilingual Models"},{"paperId":"b044395ed89de33b43d304c008d3c5a7727d423d","title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection"},{"paperId":"c1601b8b7a60fe4e8fa121a7cf2acd9ec4a8043c","title":"Processing South Asian Languages Written in the Latin Script: the Dakshina Dataset"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"efe638a32c6bd9ad24a233784008bfe5b33cfc83","title":"Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"9e9d919c1de684ca42c8b581ec62c7aa685f431e","title":"On the Cross-lingual Transferability of Monolingual Representations"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2e347a977f14eca7cc5bbbb4c71145b75637340c","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering"},{"paperId":"ae677b0441bfaea0e0c78acfa8758fff353ab715","title":"Neural Machine Translation with Byte-Level Subwords"},{"paperId":"20f1c639458dd11d38f228a6dbbcfeb4c1913116","title":"Bridging the Gap for Tokenizer-Free Language Models"},{"paperId":"04a7021fe6be6bddcfae476493fcc7571e7c613c","title":"PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"},{"paperId":"b00cda1bf34cc674676bcde38a46e8fe86d8b825","title":"TWEETQA: A Social Media Focused Question Answering Dataset"},{"paperId":"be564b861e5a9bf5d1dec531807e711000027380","title":"One Size Does Not Fit All: Comparing NMT Representations of Different Granularities"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"162515d87256f13888d9d7ba95275ac4b6c35396","title":"Combating Adversarial Misspellings with Robust Word Recognition"},{"paperId":"dda6fb309f62e2557a071522354d8c2c897a2805","title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"},{"paperId":"5e93a30656ef091f55ce42374d498f2b881e0c47","title":"Bytes Are All You Need: End-to-end Multilingual Speech Recognition and Synthesis with Bytes"},{"paperId":"1125d986433735ffc63dc0a8be466043e4f5c5b4","title":"Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models"},{"paperId":"1c3112ef8a346b9817382ed34a8c146c53d5bcf5","title":"XNLI: Evaluating Cross-lingual Sentence Representations"},{"paperId":"87639a90e0ab573236efeb79cf24efafc2463dcf","title":"Revisiting Character-Based Neural Machine Translation with Capacity and Compression"},{"paperId":"305b2cf37e5dece81e95c92883d5a6e28ac93b22","title":"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b9de9599d7241459db9213b5cdd7059696f5ef8d","title":"Character-Level Language Modeling with Deeper Self-Attention"},{"paperId":"421fc2556836a6b441de806d7b393a35b6eaea58","title":"Contextual String Embeddings for Sequence Labeling"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"89cb259f39eb8350dd337fc1d02e2f4128c8398c","title":"Byte-based Neural Machine Translation"},{"paperId":"2397ce306e5d7f3d0492276e357fb1833536b5d8","title":"On the State of the Art of Evaluation in Neural Language Models"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"98445f4172659ec5e891e031d8202c102135c644","title":"Neural Machine Translation in Linear Time"},{"paperId":"b8bc86a1bc281b15ce45e967cbdd045bcf23a952","title":"Fully Character-Level Neural Machine Translation without Explicit Segmentation"},{"paperId":"563783de03452683a9206e85fe6d661714436686","title":"HyperNetworks"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"65eee67dee969fdf8b44c87c560d66ad4d78e233","title":"Hierarchical Multiscale Recurrent Neural Networks"},{"paperId":"7dba53e72c182e25e98e8f73a99d75ff69dda0c2","title":"Recurrent Highway Networks"},{"paperId":"04cca8e341a5da42b29b0bc831cb25a0f784fa01","title":"Adaptive Computation Time for Recurrent Neural Networks"},{"paperId":"acec46ffd3f6046af97529127d98f1d623816ea4","title":"A Character-level Decoder without Explicit Segmentation for Neural Machine Translation"},{"paperId":"4d070993cb75407b285e14cb8aac0077624ef4d9","title":"Character-based Neural Machine Translation"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"4dabd6182ce2681c758f654561d351739e8df7bf","title":"Multilingual Language Processing From Bytes"},{"paperId":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","title":"Character-level Convolutional Networks for Text Classification"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"891ce1687e2befddd19f54e4eef1d3f39c8dbaf7","title":"Character-Aware Neural Language Models"},{"paperId":"6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"93c20e38c85b69fc2d2eb314b3c1217913f7db11","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"d266d00cc0348bdd0692687befef68fc56433630","title":"Fifth Conference of the European Chapter of the Association for Computational Linguistics"},{"paperId":"b9f25555d550219b979fe12d409070a4798a02f9","title":"Louisiana"},{"paperId":"616253f6b1e83ede361457de2f51b0bf70555b13","title":"Cross-lingual Name Tagging and Linking for 282 Languages"},{"paperId":null,"title":"Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu"},{"paperId":null,"title":"Technologies, Volume 1 (Long and Short Papers) pages"}],"id":"1006d191e9eb5b4dbc35fc0bb389328ddc75cba7","summary":"This paper shows that a standard Transformer architecture can be used with minimal modifications to process byte sequences, characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and shows that byte-level models are competitive with their token-level counterparts."}]}